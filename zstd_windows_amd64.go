// Code generated for windows/amd64 by 'ccgo --package-name libzstd -std=c17 -ignore-link-errors -D_IMMINTRIN_H_INCLUDED -D_FMA4INTRIN_H_INCLUDED -D_XOPMMINTRIN_H_INCLUDED /tmp/zstd-build-1536185704/zstd.c -o /home/bxd/GolandProjects/libzstd/zstd_windows_amd64.go', DO NOT EDIT.

//go:build windows && amd64

package libzstd

import (
	"reflect"
	"unsafe"

	"modernc.org/libc"
)

var _ reflect.Type
var _ unsafe.Pointer

const ABORTDOC = 2
const ABOVE_NORMAL_PRIORITY_CLASS = 0x8000
const ABSOLUTE = 1
const ACCESS_ALLOWED_ACE_TYPE = 0x0
const ACCESS_ALLOWED_CALLBACK_ACE_TYPE = 0x9
const ACCESS_ALLOWED_CALLBACK_OBJECT_ACE_TYPE = 0xB
const ACCESS_ALLOWED_COMPOUND_ACE_TYPE = 0x4
const ACCESS_ALLOWED_OBJECT_ACE_TYPE = 0x5
const ACCESS_DENIED_ACE_TYPE = 0x1
const ACCESS_DENIED_CALLBACK_ACE_TYPE = 0xA
const ACCESS_DENIED_CALLBACK_OBJECT_ACE_TYPE = 0xC
const ACCESS_DENIED_OBJECT_ACE_TYPE = 0x6
const ACCESS_DS_OBJECT_TYPE_NAME_A = "Directory Service Object"
const ACCESS_DS_OBJECT_TYPE_NAME_W = "Directory Service Object"
const ACCESS_DS_SOURCE_A = "DS"
const ACCESS_DS_SOURCE_W = "DS"
const ACCESS_FILTERKEYS = 0x0002
const ACCESS_MAX_LEVEL = 4
const ACCESS_MAX_MS_ACE_TYPE = 0x8
const ACCESS_MAX_MS_OBJECT_ACE_TYPE = 0x8
const ACCESS_MAX_MS_V2_ACE_TYPE = 0x3
const ACCESS_MAX_MS_V3_ACE_TYPE = 0x4
const ACCESS_MAX_MS_V4_ACE_TYPE = 0x8
const ACCESS_MAX_MS_V5_ACE_TYPE = 0x15
const ACCESS_MIN_MS_ACE_TYPE = 0x0
const ACCESS_MIN_MS_OBJECT_ACE_TYPE = 0x5
const ACCESS_MOUSEKEYS = 0x0003
const ACCESS_OBJECT_GUID = 0
const ACCESS_PROPERTY_GUID = 2
const ACCESS_PROPERTY_SET_GUID = 1
const ACCESS_REASON_DATA_MASK = 0x0000ffff
const ACCESS_REASON_EXDATA_MASK = 0x7f000000
const ACCESS_REASON_STAGING_MASK = 0x80000000
const ACCESS_REASON_TYPE_MASK = 0x00ff0000
const ACCESS_STICKYKEYS = 0x0001
const ACE_INHERITED_OBJECT_TYPE_PRESENT = 0x2
const ACE_OBJECT_TYPE_PRESENT = 0x1
const ACL_REVISION = 2
const ACL_REVISION1 = 1
const ACL_REVISION2 = 2
const ACL_REVISION3 = 3
const ACL_REVISION4 = 4
const ACL_REVISION_DS = 4
const ACPI_PPM_HARDWARE_ALL = 0xfe
const ACPI_PPM_SOFTWARE_ALL = 0xfc
const ACPI_PPM_SOFTWARE_ANY = 0xfd
const ACTCTX_FLAG_APPLICATION_NAME_VALID = 0x00000020
const ACTCTX_FLAG_ASSEMBLY_DIRECTORY_VALID = 0x00000004
const ACTCTX_FLAG_HMODULE_VALID = 0x00000080
const ACTCTX_FLAG_LANGID_VALID = 0x00000002
const ACTCTX_FLAG_PROCESSOR_ARCHITECTURE_VALID = 0x00000001
const ACTCTX_FLAG_RESOURCE_NAME_VALID = 0x00000008
const ACTCTX_FLAG_SET_PROCESS_DEFAULT = 0x00000010
const ACTCTX_FLAG_SOURCE_IS_ASSEMBLYREF = 0x00000040
const ACTIVATIONCONTEXTINFOCLASS = "ACTIVATION_CONTEXT_INFO_CLASS"
const ACTIVATION_CONTEXT_BASIC_INFORMATION_DEFINED = 1
const ACTIVATION_CONTEXT_PATH_TYPE_ASSEMBLYREF = 4
const ACTIVATION_CONTEXT_PATH_TYPE_NONE = 1
const ACTIVATION_CONTEXT_PATH_TYPE_URL = 3
const ACTIVATION_CONTEXT_PATH_TYPE_WIN32_FILE = 2
const ACTIVATION_CONTEXT_SECTION_APPLICATION_SETTINGS = 10
const ACTIVATION_CONTEXT_SECTION_ASSEMBLY_INFORMATION = 1
const ACTIVATION_CONTEXT_SECTION_CLR_SURROGATES = 9
const ACTIVATION_CONTEXT_SECTION_COMPATIBILITY_INFO = 11
const ACTIVATION_CONTEXT_SECTION_COM_INTERFACE_REDIRECTION = 5
const ACTIVATION_CONTEXT_SECTION_COM_PROGID_REDIRECTION = 7
const ACTIVATION_CONTEXT_SECTION_COM_SERVER_REDIRECTION = 4
const ACTIVATION_CONTEXT_SECTION_COM_TYPE_LIBRARY_REDIRECTION = 6
const ACTIVATION_CONTEXT_SECTION_DLL_REDIRECTION = 2
const ACTIVATION_CONTEXT_SECTION_GLOBAL_OBJECT_RENAME_TABLE = 8
const ACTIVATION_CONTEXT_SECTION_WINDOW_CLASS_REDIRECTION = 3
const ACTIVATION_CONTEXT_SECTION_WINRT_ACTIVATABLE_CLASSES = 12
const AC_LINE_BACKUP_POWER = 0x02
const AC_LINE_OFFLINE = 0x00
const AC_LINE_ONLINE = 0x01
const AC_LINE_UNKNOWN = 0xff
const AC_SRC_ALPHA = 0x01
const AC_SRC_OVER = 0x00
const ADDRESS_TAG_BIT = "0x40000000000U"
const ADVANCED_SEQS = "STORED_SEQS"
const AD_CLOCKWISE = 2
const AD_COUNTERCLOCKWISE = 1
const ALERT_SYSTEM_CRITICAL = 5
const ALERT_SYSTEM_ERROR = 3
const ALERT_SYSTEM_INFORMATIONAL = 1
const ALERT_SYSTEM_QUERY = 4
const ALERT_SYSTEM_WARNING = 2
const ALPHABET_SIZE = 256
const ALTERNATE = 1
const ALTNUMPAD_BIT = 0x04000000
const ANSI_CHARSET = 0
const ANSI_FIXED_FONT = 11
const ANSI_VAR_FONT = 12
const ANTIALIASED_QUALITY = 4
const ANYSIZE_ARRAY = 1
const APIENTRY = "WINAPI"
const APIPRIVATE = "__stdcall"
const API_SET_EXTENSION_NAME_A = "EXT-"
const API_SET_EXTENSION_NAME_U = "EXT-"
const API_SET_HELPER_NAME = "ApiSetHelp"
const API_SET_LOAD_SCHEMA_ORDINAL = 1
const API_SET_LOOKUP_ORDINAL = 2
const API_SET_PREFIX_NAME_A = "API-"
const API_SET_PREFIX_NAME_U = "API-"
const API_SET_RELEASE_SCHEMA_ORDINAL = 3
const API_SET_SCHEMA_NAME = "ApiSetSchema"
const API_SET_SCHEMA_SUFFIX = ".sys"
const API_SET_SECTION_NAME = ".apiset"
const APPCOMMAND_BASS_BOOST = 20
const APPCOMMAND_BASS_DOWN = 19
const APPCOMMAND_BASS_UP = 21
const APPCOMMAND_BROWSER_BACKWARD = 1
const APPCOMMAND_BROWSER_FAVORITES = 6
const APPCOMMAND_BROWSER_FORWARD = 2
const APPCOMMAND_BROWSER_HOME = 7
const APPCOMMAND_BROWSER_REFRESH = 3
const APPCOMMAND_BROWSER_SEARCH = 5
const APPCOMMAND_BROWSER_STOP = 4
const APPCOMMAND_CLOSE = 31
const APPCOMMAND_COPY = 36
const APPCOMMAND_CORRECTION_LIST = 45
const APPCOMMAND_CUT = 37
const APPCOMMAND_DELETE = 53
const APPCOMMAND_DICTATE_OR_COMMAND_CONTROL_TOGGLE = 43
const APPCOMMAND_DWM_FLIP3D = 54
const APPCOMMAND_FIND = 28
const APPCOMMAND_FORWARD_MAIL = 40
const APPCOMMAND_HELP = 27
const APPCOMMAND_LAUNCH_APP1 = 17
const APPCOMMAND_LAUNCH_APP2 = 18
const APPCOMMAND_LAUNCH_MAIL = 15
const APPCOMMAND_LAUNCH_MEDIA_SELECT = 16
const APPCOMMAND_MEDIA_CHANNEL_DOWN = 52
const APPCOMMAND_MEDIA_CHANNEL_UP = 51
const APPCOMMAND_MEDIA_FAST_FORWARD = 49
const APPCOMMAND_MEDIA_NEXTTRACK = 11
const APPCOMMAND_MEDIA_PAUSE = 47
const APPCOMMAND_MEDIA_PLAY = 46
const APPCOMMAND_MEDIA_PLAY_PAUSE = 14
const APPCOMMAND_MEDIA_PREVIOUSTRACK = 12
const APPCOMMAND_MEDIA_RECORD = 48
const APPCOMMAND_MEDIA_REWIND = 50
const APPCOMMAND_MEDIA_STOP = 13
const APPCOMMAND_MICROPHONE_VOLUME_DOWN = 25
const APPCOMMAND_MICROPHONE_VOLUME_MUTE = 24
const APPCOMMAND_MICROPHONE_VOLUME_UP = 26
const APPCOMMAND_MIC_ON_OFF_TOGGLE = 44
const APPCOMMAND_NEW = 29
const APPCOMMAND_OPEN = 30
const APPCOMMAND_PASTE = 38
const APPCOMMAND_PRINT = 33
const APPCOMMAND_REDO = 35
const APPCOMMAND_REPLY_TO_MAIL = 39
const APPCOMMAND_SAVE = 32
const APPCOMMAND_SEND_MAIL = 41
const APPCOMMAND_SPELL_CHECK = 42
const APPCOMMAND_TREBLE_DOWN = 22
const APPCOMMAND_TREBLE_UP = 23
const APPCOMMAND_UNDO = 34
const APPCOMMAND_VOLUME_DOWN = 9
const APPCOMMAND_VOLUME_MUTE = 8
const APPCOMMAND_VOLUME_UP = 10
const APPLICATION_ERROR_MASK = 0x20000000
const APPLICATION_VERIFIER_ACCESS_VIOLATION = 0x0002
const APPLICATION_VERIFIER_BAD_HEAP_HANDLE = 0x0005
const APPLICATION_VERIFIER_COM_API_IN_DLLMAIN = 0x0401
const APPLICATION_VERIFIER_COM_CF_SUCCESS_WITH_NULL = 0x040A
const APPLICATION_VERIFIER_COM_ERROR = 0x0400
const APPLICATION_VERIFIER_COM_GCO_SUCCESS_WITH_NULL = 0x040B
const APPLICATION_VERIFIER_COM_HOLDING_LOCKS_ON_CALL = 0x0410
const APPLICATION_VERIFIER_COM_NULL_DACL = 0x0406
const APPLICATION_VERIFIER_COM_OBJECT_IN_FREED_MEMORY = 0x040C
const APPLICATION_VERIFIER_COM_OBJECT_IN_UNLOADED_DLL = 0x040D
const APPLICATION_VERIFIER_COM_SMUGGLED_PROXY = 0x0409
const APPLICATION_VERIFIER_COM_SMUGGLED_WRAPPER = 0x0408
const APPLICATION_VERIFIER_COM_UNBALANCED_COINIT = 0x0403
const APPLICATION_VERIFIER_COM_UNBALANCED_OLEINIT = 0x0404
const APPLICATION_VERIFIER_COM_UNBALANCED_SWC = 0x0405
const APPLICATION_VERIFIER_COM_UNHANDLED_EXCEPTION = 0x0402
const APPLICATION_VERIFIER_COM_UNSAFE_IMPERSONATION = 0x0407
const APPLICATION_VERIFIER_COM_VTBL_IN_FREED_MEMORY = 0x040E
const APPLICATION_VERIFIER_COM_VTBL_IN_UNLOADED_DLL = 0x040F
const APPLICATION_VERIFIER_CONTINUABLE_BREAK = 0x10000000
const APPLICATION_VERIFIER_CORRUPTED_FREED_HEAP_BLOCK = 0x000E
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK = 0x0008
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK_END_STAMP = 0x0011
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK_EXCEPTION_RAISED_FOR_HEADER = 0x000B
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK_EXCEPTION_RAISED_FOR_PROBING = 0x000C
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK_HEADER = 0x000D
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK_PREFIX = 0x0012
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK_START_STAMP = 0x0010
const APPLICATION_VERIFIER_CORRUPTED_HEAP_BLOCK_SUFFIX = 0x000F
const APPLICATION_VERIFIER_CORRUPTED_HEAP_LIST = 0x0014
const APPLICATION_VERIFIER_DESTROY_PROCESS_HEAP = 0x0009
const APPLICATION_VERIFIER_DOUBLE_FREE = 0x0007
const APPLICATION_VERIFIER_EXIT_THREAD_OWNS_LOCK = 0x0200
const APPLICATION_VERIFIER_EXTREME_SIZE_REQUEST = 0x0004
const APPLICATION_VERIFIER_FIRST_CHANCE_ACCESS_VIOLATION = 0x0013
const APPLICATION_VERIFIER_INCORRECT_WAIT_CALL = 0x0302
const APPLICATION_VERIFIER_INTERNAL_ERROR = 0x80000000
const APPLICATION_VERIFIER_INTERNAL_WARNING = 0x40000000
const APPLICATION_VERIFIER_INVALID_ALLOCMEM = 0x0601
const APPLICATION_VERIFIER_INVALID_EXIT_PROCESS_CALL = 0x0102
const APPLICATION_VERIFIER_INVALID_FREEMEM = 0x0600
const APPLICATION_VERIFIER_INVALID_HANDLE = 0x0300
const APPLICATION_VERIFIER_INVALID_MAPVIEW = 0x0602
const APPLICATION_VERIFIER_INVALID_TLS_VALUE = 0x0301
const APPLICATION_VERIFIER_LOCK_ALREADY_INITIALIZED = 0x0211
const APPLICATION_VERIFIER_LOCK_CORRUPTED = 0x0205
const APPLICATION_VERIFIER_LOCK_DOUBLE_INITIALIZE = 0x0203
const APPLICATION_VERIFIER_LOCK_INVALID_LOCK_COUNT = 0x0208
const APPLICATION_VERIFIER_LOCK_INVALID_OWNER = 0x0206
const APPLICATION_VERIFIER_LOCK_INVALID_RECURSION_COUNT = 0x0207
const APPLICATION_VERIFIER_LOCK_IN_FREED_HEAP = 0x0202
const APPLICATION_VERIFIER_LOCK_IN_FREED_MEMORY = 0x0204
const APPLICATION_VERIFIER_LOCK_IN_FREED_VMEM = 0x0212
const APPLICATION_VERIFIER_LOCK_IN_UNLOADED_DLL = 0x0201
const APPLICATION_VERIFIER_LOCK_IN_UNMAPPED_MEM = 0x0213
const APPLICATION_VERIFIER_LOCK_NOT_INITIALIZED = 0x0210
const APPLICATION_VERIFIER_LOCK_OVER_RELEASED = 0x0209
const APPLICATION_VERIFIER_NO_BREAK = 0x20000000
const APPLICATION_VERIFIER_NULL_HANDLE = 0x0303
const APPLICATION_VERIFIER_PROBE_FREE_MEM = 0x0604
const APPLICATION_VERIFIER_PROBE_GUARD_PAGE = 0x0605
const APPLICATION_VERIFIER_PROBE_INVALID_ADDRESS = 0x0603
const APPLICATION_VERIFIER_PROBE_INVALID_START_OR_SIZE = 0x0607
const APPLICATION_VERIFIER_PROBE_NULL = 0x0606
const APPLICATION_VERIFIER_RPC_ERROR = 0x0500
const APPLICATION_VERIFIER_SIZE_HEAP_UNEXPECTED_EXCEPTION = 0x0618
const APPLICATION_VERIFIER_STACK_OVERFLOW = 0x0101
const APPLICATION_VERIFIER_SWITCHED_HEAP_HANDLE = 0x0006
const APPLICATION_VERIFIER_TERMINATE_THREAD_CALL = 0x0100
const APPLICATION_VERIFIER_THREAD_NOT_LOCK_OWNER = 0x0214
const APPLICATION_VERIFIER_UNEXPECTED_EXCEPTION = 0x000A
const APPLICATION_VERIFIER_UNKNOWN_ERROR = 0x0001
const APPLICATION_VERIFIER_UNSYNCHRONIZED_ACCESS = 0x0003
const APPLICATION_VERIFIER_WAIT_IN_DLLMAIN = 0x0304
const APP_LOCAL_DEVICE_ID_SIZE = 32
const ARABIC_CHARSET = 178
const ARM64_MAX_BREAKPOINTS = 8
const ARM64_MAX_WATCHPOINTS = 2
const ARM_CACHE_ALIGNMENT_SIZE = 128
const ASPECTX = 40
const ASPECTXY = 44
const ASPECTY = 42
const ASPECT_FILTERING = 0x0001
const ASSEMBLY_DLL_REDIRECTION_DETAILED_INFORMATION = "ASSEMBLY_FILE_DETAILED_INFORMATION"
const ATF_ONOFFFEEDBACK = 0x00000002
const ATF_TIMEOUTON = 0x00000001
const ATOM_FLAG_GLOBAL = 0x2
const ATTR_CONVERTED = 0x02
const ATTR_FIXEDCONVERTED = 0x05
const ATTR_INPUT = 0x00
const ATTR_INPUT_ERROR = 0x04
const ATTR_TARGET_CONVERTED = 0x01
const ATTR_TARGET_NOTCONVERTED = 0x03
const AUDIT_ALLOW_NO_PRIVILEGE = 0x1
const AW_ACTIVATE = 0x00020000
const AW_BLEND = 0x00080000
const AW_CENTER = 0x00000010
const AW_HIDE = 0x00010000
const AW_HOR_NEGATIVE = 0x00000002
const AW_HOR_POSITIVE = 0x00000001
const AW_SLIDE = 0x00040000
const AW_VER_NEGATIVE = 0x00000008
const AW_VER_POSITIVE = 0x00000004
const AbnormalTermination = "_abnormal_termination"
const AccessCheckAndAuditAlarm = "AccessCheckAndAuditAlarmA"
const AccessCheckByTypeAndAuditAlarm = "AccessCheckByTypeAndAuditAlarmA"
const AccessCheckByTypeResultListAndAuditAlarm = "AccessCheckByTypeResultListAndAuditAlarmA"
const AccessCheckByTypeResultListAndAuditAlarmByHandle = "AccessCheckByTypeResultListAndAuditAlarmByHandleA"
const AnsiLower = "CharLowerA"
const AnsiLowerBuff = "CharLowerBuffA"
const AnsiNext = "CharNextA"
const AnsiPrev = "CharPrevA"
const AnsiToOem = "CharToOemA"
const AnsiToOemBuff = "CharToOemBuffA"
const AnsiUpper = "CharUpperA"
const AnsiUpperBuff = "CharUpperBuffA"
const BACKGROUND_BLUE = 0x0010
const BACKGROUND_GREEN = 0x0020
const BACKGROUND_INTENSITY = 0x0080
const BACKGROUND_RED = 0x0040
const BACKUP_ALTERNATE_DATA = 0x00000004
const BACKUP_DATA = 0x00000001
const BACKUP_EA_DATA = 0x00000002
const BACKUP_GHOSTED_FILE_EXTENTS = 0x0000000b
const BACKUP_INVALID = 0x00000000
const BACKUP_LINK = 0x00000005
const BACKUP_OBJECT_ID = 0x00000007
const BACKUP_PROPERTY_DATA = 0x00000006
const BACKUP_REPARSE_DATA = 0x00000008
const BACKUP_SECURITY_DATA = 0x00000003
const BACKUP_SPARSE_BLOCK = 0x00000009
const BACKUP_TXFS_DATA = 0x0000000a
const BALTIC_CHARSET = 186
const BANDINFO = 24
const BASE_SEARCH_PATH_DISABLE_SAFE_SEARCHMODE = 0x10000
const BASE_SEARCH_PATH_ENABLE_SAFE_SEARCHMODE = 0x1
const BASE_SEARCH_PATH_PERMANENT = 0x8000
const BATTERY_DISCHARGE_FLAGS_ENABLE = 0x80000000
const BATTERY_DISCHARGE_FLAGS_EVENTCODE_MASK = 0x00000007
const BATTERY_FLAG_CHARGING = 0x08
const BATTERY_FLAG_CRITICAL = 0x04
const BATTERY_FLAG_HIGH = 0x01
const BATTERY_FLAG_LOW = 0x02
const BATTERY_FLAG_NO_BATTERY = 0x80
const BATTERY_FLAG_UNKNOWN = 0xff
const BATTERY_LIFE_UNKNOWN = 0xffffffff
const BATTERY_PERCENTAGE_UNKNOWN = 0xff
const BDR_RAISEDINNER = 0x0004
const BDR_RAISEDOUTER = 0x0001
const BDR_SUNKENINNER = 0x0008
const BDR_SUNKENOUTER = 0x0002
const BEGIN_PATH = 4096
const BELOW_NORMAL_PRIORITY_CLASS = 0x4000
const BF_ADJUST = 0x2000
const BF_BOTTOM = 0x0008
const BF_DIAGONAL = 0x0010
const BF_FLAT = 0x4000
const BF_LEFT = 0x0001
const BF_MIDDLE = 0x0800
const BF_MONO = 0x8000
const BF_RIGHT = 0x0004
const BF_SOFT = 0x1000
const BF_TOP = 0x0002
const BIT0 = 1
const BIT1 = 2
const BIT4 = 16
const BIT5 = 32
const BIT6 = 64
const BIT7 = 128
const BITCOST_ACCURACY = 8
const BITSPIXEL = 12
const BKMODE_LAST = 2
const BLACKONWHITE = 1
const BLACK_BRUSH = 4
const BLACK_PEN = 7
const BLOCKSIZE_MIN = 3500
const BLTALIGNMENT = 119
const BM_CLICK = 0x00F5
const BM_GETCHECK = 0x00F0
const BM_GETIMAGE = 0x00F6
const BM_GETSTATE = 0x00F2
const BM_SETCHECK = 0x00F1
const BM_SETDONTCLICK = 0x00f8
const BM_SETIMAGE = 0x00F7
const BM_SETSTATE = 0x00F3
const BM_SETSTYLE = 0x00F4
const BN_CLICKED = 0
const BN_DBLCLK = "BN_DOUBLECLICKED"
const BN_DISABLE = 4
const BN_DOUBLECLICKED = 5
const BN_HILITE = 2
const BN_KILLFOCUS = 7
const BN_PAINT = 1
const BN_PUSHED = "BN_HILITE"
const BN_SETFOCUS = 6
const BN_UNHILITE = 3
const BN_UNPUSHED = "BN_UNHILITE"
const BROADCAST_QUERY_DENY = 0x424D5144
const BSF_ALLOWSFW = 0x00000080
const BSF_FLUSHDISK = 0x00000004
const BSF_FORCEIFHUNG = 0x00000020
const BSF_IGNORECURRENTTASK = 0x00000002
const BSF_LUID = 0x00000400
const BSF_NOHANG = 0x00000008
const BSF_NOTIMEOUTIFNOTHUNG = 0x00000040
const BSF_POSTMESSAGE = 0x00000010
const BSF_QUERY = 0x00000001
const BSF_RETURNHDESK = 0x00000200
const BSF_SENDNOTIFYMESSAGE = 0x00000100
const BSM_ALLCOMPONENTS = 0x00000000
const BSM_ALLDESKTOPS = 0x00000010
const BSM_APPLICATIONS = 0x00000008
const BSM_INSTALLABLEDRIVERS = 0x00000004
const BSM_NETDRIVER = 0x00000002
const BSM_VXDS = 0x00000001
const BST_CHECKED = 0x0001
const BST_FOCUS = 0x0008
const BST_INDETERMINATE = 0x0002
const BST_PUSHED = 0x0004
const BST_UNCHECKED = 0x0000
const BS_DIBPATTERN = 5
const BS_DIBPATTERN8X8 = 8
const BS_DIBPATTERNPT = 6
const BS_HATCHED = 2
const BS_HOLLOW = "BS_NULL"
const BS_INDEXED = 4
const BS_MONOPATTERN = 9
const BS_NULL = 1
const BS_PATTERN = 3
const BS_PATTERN8X8 = 7
const BS_RIGHTBUTTON = "BS_LEFTTEXT"
const BS_SOLID = 0
const BUCKET_A_SIZE = "ALPHABET_SIZE"
const BUFSIZ = 512
const BYTESCALE = 256
const BitScanForward = "_BitScanForward"
const BitScanForward64 = "_BitScanForward64"
const BitScanReverse = "_BitScanReverse"
const BitScanReverse64 = "_BitScanReverse64"
const BitTest = "_bittest"
const BitTest64 = "_bittest64"
const BitTestAndComplement = "_bittestandcomplement"
const BitTestAndComplement64 = "_bittestandcomplement64"
const BitTestAndReset = "_bittestandreset"
const BitTestAndReset64 = "_bittestandreset64"
const BitTestAndSet = "_bittestandset"
const BitTestAndSet64 = "_bittestandset64"
const C1_ALPHA = 0x0100
const C1_BLANK = 0x0040
const C1_CNTRL = 0x0020
const C1_DEFINED = 0x0200
const C1_DIGIT = 0x0004
const C1_LOWER = 0x0002
const C1_PUNCT = 0x0010
const C1_SPACE = 0x0008
const C1_UPPER = 0x0001
const C1_XDIGIT = 0x0080
const C2_ARABICNUMBER = 0x0006
const C2_BLOCKSEPARATOR = 0x0008
const C2_COMMONSEPARATOR = 0x0007
const C2_EUROPENUMBER = 0x0003
const C2_EUROPESEPARATOR = 0x0004
const C2_EUROPETERMINATOR = 0x0005
const C2_LEFTTORIGHT = 0x0001
const C2_NOTAPPLICABLE = 0x0000
const C2_OTHERNEUTRAL = 0x000b
const C2_RIGHTTOLEFT = 0x0002
const C2_SEGMENTSEPARATOR = 0x0009
const C2_WHITESPACE = 0x000a
const C3_ALPHA = 0x8000
const C3_DIACRITIC = 0x0002
const C3_FULLWIDTH = 0x0080
const C3_HALFWIDTH = 0x0040
const C3_HIGHSURROGATE = 0x0800
const C3_HIRAGANA = 0x0020
const C3_IDEOGRAPH = 0x0100
const C3_KASHIDA = 0x0200
const C3_KATAKANA = 0x0010
const C3_LEXICAL = 0x0400
const C3_LOWSURROGATE = 0x1000
const C3_NONSPACING = 0x0001
const C3_NOTAPPLICABLE = 0x0000
const C3_SYMBOL = 0x0008
const C3_VOWELMARK = 0x0004
const CACHELINE_SIZE = 64
const CACHE_FULLY_ASSOCIATIVE = 0xFF
const CALERT_SYSTEM = 6
const CALINFO_ENUMPROC = "CALINFO_ENUMPROCA"
const CALINFO_ENUMPROCEX = "CALINFO_ENUMPROCEXA"
const CALLBACK = "__stdcall"
const CALLBACK_CHUNK_FINISHED = 0x0
const CALLBACK_STREAM_SWITCH = 0x1
const CAL_GREGORIAN = 1
const CAL_GREGORIAN_ARABIC = 10
const CAL_GREGORIAN_ME_FRENCH = 9
const CAL_GREGORIAN_US = 2
const CAL_GREGORIAN_XLIT_ENGLISH = 11
const CAL_GREGORIAN_XLIT_FRENCH = 12
const CAL_HEBREW = 8
const CAL_HIJRI = 6
const CAL_ICALINTVALUE = 0x00000001
const CAL_ITWODIGITYEARMAX = 0x00000030
const CAL_IYEAROFFSETRANGE = 0x00000003
const CAL_JAPAN = 3
const CAL_KOREA = 5
const CAL_NOUSEROVERRIDE = "LOCALE_NOUSEROVERRIDE"
const CAL_RETURN_NUMBER = "LOCALE_RETURN_NUMBER"
const CAL_SABBREVDAYNAME1 = 0x0000000e
const CAL_SABBREVDAYNAME2 = 0x0000000f
const CAL_SABBREVDAYNAME3 = 0x00000010
const CAL_SABBREVDAYNAME4 = 0x00000011
const CAL_SABBREVDAYNAME5 = 0x00000012
const CAL_SABBREVDAYNAME6 = 0x00000013
const CAL_SABBREVDAYNAME7 = 0x00000014
const CAL_SABBREVMONTHNAME1 = 0x00000022
const CAL_SABBREVMONTHNAME10 = 0x0000002b
const CAL_SABBREVMONTHNAME11 = 0x0000002c
const CAL_SABBREVMONTHNAME12 = 0x0000002d
const CAL_SABBREVMONTHNAME13 = 0x0000002e
const CAL_SABBREVMONTHNAME2 = 0x00000023
const CAL_SABBREVMONTHNAME3 = 0x00000024
const CAL_SABBREVMONTHNAME4 = 0x00000025
const CAL_SABBREVMONTHNAME5 = 0x00000026
const CAL_SABBREVMONTHNAME6 = 0x00000027
const CAL_SABBREVMONTHNAME7 = 0x00000028
const CAL_SABBREVMONTHNAME8 = 0x00000029
const CAL_SABBREVMONTHNAME9 = 0x0000002a
const CAL_SCALNAME = 0x00000002
const CAL_SDAYNAME1 = 0x00000007
const CAL_SDAYNAME2 = 0x00000008
const CAL_SDAYNAME3 = 0x00000009
const CAL_SDAYNAME4 = 0x0000000a
const CAL_SDAYNAME5 = 0x0000000b
const CAL_SDAYNAME6 = 0x0000000c
const CAL_SDAYNAME7 = 0x0000000d
const CAL_SERASTRING = 0x00000004
const CAL_SJAPANESEERAFIRSTYEAR = 0x0000003d
const CAL_SLONGDATE = 0x00000006
const CAL_SMONTHNAME1 = 0x00000015
const CAL_SMONTHNAME10 = 0x0000001e
const CAL_SMONTHNAME11 = 0x0000001f
const CAL_SMONTHNAME12 = 0x00000020
const CAL_SMONTHNAME13 = 0x00000021
const CAL_SMONTHNAME2 = 0x00000016
const CAL_SMONTHNAME3 = 0x00000017
const CAL_SMONTHNAME4 = 0x00000018
const CAL_SMONTHNAME5 = 0x00000019
const CAL_SMONTHNAME6 = 0x0000001a
const CAL_SMONTHNAME7 = 0x0000001b
const CAL_SMONTHNAME8 = 0x0000001c
const CAL_SMONTHNAME9 = 0x0000001d
const CAL_SSHORTDATE = 0x00000005
const CAL_SSHORTESTDAYNAME1 = 0x00000031
const CAL_SSHORTESTDAYNAME2 = 0x00000032
const CAL_SSHORTESTDAYNAME3 = 0x00000033
const CAL_SSHORTESTDAYNAME4 = 0x00000034
const CAL_SSHORTESTDAYNAME5 = 0x00000035
const CAL_SSHORTESTDAYNAME6 = 0x00000036
const CAL_SSHORTESTDAYNAME7 = 0x00000037
const CAL_SYEARMONTH = 0x0000002f
const CAL_TAIWAN = 4
const CAL_THAI = 7
const CAL_UMALQURA = 23
const CAL_USE_CP_ACP = "LOCALE_USE_CP_ACP"
const CAPSLOCK_ON = 0x0080
const CA_LOG_FILTER = 0x0002
const CA_NEGATIVE = 0x0001
const CBN_CLOSEUP = 8
const CBN_DBLCLK = 2
const CBN_DROPDOWN = 7
const CBN_EDITCHANGE = 5
const CBN_EDITUPDATE = 6
const CBN_KILLFOCUS = 4
const CBN_SELCHANGE = 1
const CBN_SELENDCANCEL = 10
const CBN_SELENDOK = 9
const CBN_SETFOCUS = 3
const CBR_110 = 110
const CBR_115200 = 115200
const CBR_1200 = 1200
const CBR_128000 = 128000
const CBR_14400 = 14400
const CBR_19200 = 19200
const CBR_2400 = 2400
const CBR_256000 = 256000
const CBR_300 = 300
const CBR_38400 = 38400
const CBR_4800 = 4800
const CBR_56000 = 56000
const CBR_57600 = 57600
const CBR_600 = 600
const CBR_9600 = 9600
const CB_ADDSTRING = 0x0143
const CB_DELETESTRING = 0x0144
const CB_DIR = 0x0145
const CB_FINDSTRING = 0x014C
const CB_FINDSTRINGEXACT = 0x0158
const CB_GETCOMBOBOXINFO = 0x0164
const CB_GETCOUNT = 0x0146
const CB_GETCURSEL = 0x0147
const CB_GETDROPPEDCONTROLRECT = 0x0152
const CB_GETDROPPEDSTATE = 0x0157
const CB_GETDROPPEDWIDTH = 0x015f
const CB_GETEDITSEL = 0x0140
const CB_GETEXTENDEDUI = 0x0156
const CB_GETHORIZONTALEXTENT = 0x015d
const CB_GETITEMDATA = 0x0150
const CB_GETITEMHEIGHT = 0x0154
const CB_GETLBTEXT = 0x0148
const CB_GETLBTEXTLEN = 0x0149
const CB_GETLOCALE = 0x015A
const CB_GETTOPINDEX = 0x015b
const CB_INITSTORAGE = 0x0161
const CB_INSERTSTRING = 0x014A
const CB_LIMITTEXT = 0x0141
const CB_MSGMAX = 0x0165
const CB_OKAY = 0
const CB_RESETCONTENT = 0x014B
const CB_SELECTSTRING = 0x014D
const CB_SETCURSEL = 0x014E
const CB_SETDROPPEDWIDTH = 0x0160
const CB_SETEDITSEL = 0x0142
const CB_SETEXTENDEDUI = 0x0155
const CB_SETHORIZONTALEXTENT = 0x015e
const CB_SETITEMDATA = 0x0151
const CB_SETITEMHEIGHT = 0x0153
const CB_SETLOCALE = 0x0159
const CB_SETTOPINDEX = 0x015c
const CB_SHOWDROPDOWN = 0x014F
const CCHDEVICENAME = 32
const CCHFORMNAME = 32
const CCHILDREN_SCROLLBAR = 5
const CCHILDREN_TITLEBAR = 5
const CC_CHORD = 4
const CC_CIRCLES = 1
const CC_ELLIPSES = 8
const CC_INTERIORS = 128
const CC_NONE = 0
const CC_PIE = 2
const CC_ROUNDRECT = 256
const CC_STYLED = 32
const CC_WIDE = 16
const CC_WIDESTYLED = 64
const CDS_DISABLE_UNSAFE_MODES = 0x00000200
const CDS_ENABLE_UNSAFE_MODES = 0x00000100
const CDS_FULLSCREEN = 0x00000004
const CDS_GLOBAL = 0x00000008
const CDS_NORESET = 0x10000000
const CDS_RESET = 0x40000000
const CDS_RESET_EX = 0x20000000
const CDS_SET_PRIMARY = 0x00000010
const CDS_TEST = 0x00000002
const CDS_UPDATEREGISTRY = 0x00000001
const CDS_VIDEOPARAMETERS = 0x00000020
const CE_BREAK = 0x10
const CE_DNS = 0x800
const CE_FRAME = 0x8
const CE_IOE = 0x400
const CE_MODE = 0x8000
const CE_OOP = 0x1000
const CE_OVERRUN = 0x2
const CE_PTO = 0x200
const CE_RXOVER = 0x1
const CE_RXPARITY = 0x4
const CE_TXFULL = 0x100
const CFG_CALL_TARGET_CONVERT_EXPORT_SUPPRESSED_TO_VALID = 0x04
const CFG_CALL_TARGET_CONVERT_XFG_TO_CFG = 0x10
const CFG_CALL_TARGET_PROCESSED = 0x02
const CFG_CALL_TARGET_VALID = 0x01
const CFG_CALL_TARGET_VALID_XFG = 0x08
const CFS_CANDIDATEPOS = 0x0040
const CFS_DEFAULT = 0x0000
const CFS_EXCLUDE = 0x0080
const CFS_FORCE_POSITION = 0x0020
const CFS_POINT = 0x0002
const CFS_RECT = 0x0001
const CF_BITMAP = 2
const CF_DIB = 8
const CF_DIBV5 = 17
const CF_DIF = 5
const CF_DSPBITMAP = 0x0082
const CF_DSPENHMETAFILE = 0x008E
const CF_DSPMETAFILEPICT = 0x0083
const CF_DSPTEXT = 0x0081
const CF_ENHMETAFILE = 14
const CF_GDIOBJFIRST = 0x0300
const CF_GDIOBJLAST = 0x03FF
const CF_HDROP = 15
const CF_LOCALE = 16
const CF_MAX = 18
const CF_METAFILEPICT = 3
const CF_OEMTEXT = 7
const CF_OWNERDISPLAY = 0x0080
const CF_PALETTE = 9
const CF_PENDATA = 10
const CF_PRIVATEFIRST = 0x0200
const CF_PRIVATELAST = 0x02FF
const CF_RIFF = 11
const CF_SYLK = 4
const CF_TEXT = 1
const CF_TIFF = 6
const CF_UNICODETEXT = 13
const CF_WAVE = 12
const CHECKJPEGFORMAT = 4119
const CHECKPNGFORMAT = 4120
const CHILDID_SELF = 0
const CHINESEBIG5_CHARSET = 136
const CLAIM_SECURITY_ATTRIBUTES_INFORMATION_VERSION = "CLAIM_SECURITY_ATTRIBUTES_INFORMATION_VERSION_V1"
const CLAIM_SECURITY_ATTRIBUTES_INFORMATION_VERSION_V1 = 1
const CLAIM_SECURITY_ATTRIBUTE_CUSTOM_FLAGS = 0xffff0000
const CLAIM_SECURITY_ATTRIBUTE_DISABLED = 0x0010
const CLAIM_SECURITY_ATTRIBUTE_DISABLED_BY_DEFAULT = 0x0008
const CLAIM_SECURITY_ATTRIBUTE_MANDATORY = 0x0020
const CLAIM_SECURITY_ATTRIBUTE_NON_INHERITABLE = 0x0001
const CLAIM_SECURITY_ATTRIBUTE_TYPE_BOOLEAN = 0x06
const CLAIM_SECURITY_ATTRIBUTE_TYPE_FQBN = 0x04
const CLAIM_SECURITY_ATTRIBUTE_TYPE_INT64 = 0x01
const CLAIM_SECURITY_ATTRIBUTE_TYPE_INVALID = 0x00
const CLAIM_SECURITY_ATTRIBUTE_TYPE_OCTET_STRING = 0x10
const CLAIM_SECURITY_ATTRIBUTE_TYPE_SID = 0x05
const CLAIM_SECURITY_ATTRIBUTE_TYPE_STRING = 0x03
const CLAIM_SECURITY_ATTRIBUTE_TYPE_UINT64 = 0x02
const CLAIM_SECURITY_ATTRIBUTE_USE_FOR_DENY_ONLY = 0x0004
const CLAIM_SECURITY_ATTRIBUTE_VALUE_CASE_SENSITIVE = 0x0002
const CLEARTYPE_NATURAL_QUALITY = 6
const CLEARTYPE_QUALITY = 5
const CLIPCAPS = 36
const CLIP_CHARACTER_PRECIS = 1
const CLIP_DEFAULT_PRECIS = 0
const CLIP_MASK = 0xf
const CLIP_STROKE_PRECIS = 2
const CLIP_TO_PATH = 4097
const CLK_TCK = "CLOCKS_PER_SEC"
const CLOCKS_PER_SEC = 1000
const CLOCK_MONOTONIC = 1
const CLOCK_PROCESS_CPUTIME_ID = 2
const CLOCK_REALTIME = 0
const CLOCK_REALTIME_COARSE = 4
const CLOCK_THREAD_CPUTIME_ID = 3
const CLOSECHANNEL = 4112
const CLRBREAK = 9
const CLRDTR = 6
const CLRRTS = 4
const CLR_INVALID = 0xFFFFFFFF
const CLSID_NULL = "GUID_NULL"
const CMAPI = "DECLSPEC_IMPORT"
const CM_CMYK_COLOR = 0x00000004
const CM_DEVICE_ICM = 0x00000001
const CM_GAMMA_RAMP = 0x00000002
const CM_IN_GAMUT = 0
const CM_NONE = 0x00000000
const CM_OUT_OF_GAMUT = 255
const CM_SERVICE_MEASURED_BOOT_LOAD = 0x00000020
const CM_SERVICE_NETWORK_BOOT_LOAD = 0x00000001
const CM_SERVICE_SD_DISK_BOOT_LOAD = 0x00000008
const CM_SERVICE_USB3_DISK_BOOT_LOAD = 0x00000010
const CM_SERVICE_USB_DISK_BOOT_LOAD = 0x00000004
const CM_SERVICE_VERIFIER_BOOT_LOAD = 0x00000040
const CM_SERVICE_VIRTUAL_DISK_BOOT_LOAD = 0x00000002
const CM_SERVICE_WINPE_BOOT_LOAD = 0x00000080
const CODEPAGE_ENUMPROC = "CODEPAGE_ENUMPROCA"
const COLORMATCHTOTARGET_EMBEDED = 0x00000001
const COLORMGMTCAPS = 121
const COLORONCOLOR = 3
const COLORRES = 108
const COLOR_3DDKSHADOW = 21
const COLOR_3DFACE = "COLOR_BTNFACE"
const COLOR_3DHIGHLIGHT = "COLOR_BTNHIGHLIGHT"
const COLOR_3DHILIGHT = "COLOR_BTNHIGHLIGHT"
const COLOR_3DLIGHT = 22
const COLOR_3DSHADOW = "COLOR_BTNSHADOW"
const COLOR_ACTIVEBORDER = 10
const COLOR_ACTIVECAPTION = 2
const COLOR_APPWORKSPACE = 12
const COLOR_BACKGROUND = 1
const COLOR_BTNFACE = 15
const COLOR_BTNHIGHLIGHT = 20
const COLOR_BTNHILIGHT = "COLOR_BTNHIGHLIGHT"
const COLOR_BTNSHADOW = 16
const COLOR_BTNTEXT = 18
const COLOR_CAPTIONTEXT = 9
const COLOR_DESKTOP = "COLOR_BACKGROUND"
const COLOR_GRADIENTACTIVECAPTION = 27
const COLOR_GRADIENTINACTIVECAPTION = 28
const COLOR_GRAYTEXT = 17
const COLOR_HIGHLIGHT = 13
const COLOR_HIGHLIGHTTEXT = 14
const COLOR_HOTLIGHT = 26
const COLOR_INACTIVEBORDER = 11
const COLOR_INACTIVECAPTION = 3
const COLOR_INACTIVECAPTIONTEXT = 19
const COLOR_INFOBK = 24
const COLOR_INFOTEXT = 23
const COLOR_MENU = 4
const COLOR_MENUBAR = 30
const COLOR_MENUHILIGHT = 29
const COLOR_MENUTEXT = 7
const COLOR_SCROLLBAR = 0
const COLOR_WINDOW = 5
const COLOR_WINDOWFRAME = 6
const COLOR_WINDOWTEXT = 8
const COMMON_LVB_GRID_HORIZONTAL = 0x0400
const COMMON_LVB_GRID_LVERTICAL = 0x0800
const COMMON_LVB_GRID_RVERTICAL = 0x1000
const COMMON_LVB_LEADING_BYTE = 0x0100
const COMMON_LVB_REVERSE_VIDEO = 0x4000
const COMMON_LVB_SBCSDBCS = 0x0300
const COMMON_LVB_TRAILING_BYTE = 0x0200
const COMMON_LVB_UNDERSCORE = 0x8000
const COMPLEXREGION = 3
const COMPONENT_KTM = 0x01
const COMPONENT_VALID_FLAGS = "COMPONENT_KTM"
const COMPRESSION_ENGINE_HIBER = 0x0200
const COMPRESSION_ENGINE_MAXIMUM = 0x0100
const COMPRESSION_ENGINE_STANDARD = 0x0000
const COMPRESSION_FORMAT_DEFAULT = 0x0001
const COMPRESSION_FORMAT_LZNT1 = 0x0002
const COMPRESSION_FORMAT_NONE = 0x0000
const COMPRESSION_FORMAT_XPRESS = 0x0003
const COMPRESSION_FORMAT_XPRESS_HUFF = 0x0004
const COMPRESS_LITERALS_SIZE_MIN = 63
const CONDITION_VARIABLE_INIT = "RTL_CONDITION_VARIABLE_INIT"
const CONDITION_VARIABLE_LOCKMODE_SHARED = "RTL_CONDITION_VARIABLE_LOCKMODE_SHARED"
const CONNDLG_CONN_POINT = 0x00000002
const CONNDLG_HIDE_BOX = 0x00000008
const CONNDLG_NOT_PERSIST = 0x00000020
const CONNDLG_PERSIST = 0x00000010
const CONNDLG_RO_PATH = 0x00000001
const CONNDLG_USE_MRU = 0x00000004
const CONNECT_CMD_SAVECRED = 0x00001000
const CONNECT_COMMANDLINE = 0x00000800
const CONNECT_CRED_RESET = 0x00002000
const CONNECT_CURRENT_MEDIA = 0x00000200
const CONNECT_DEFERRED = 0x00000400
const CONNECT_INTERACTIVE = 0x00000008
const CONNECT_LOCALDRIVE = 0x00000100
const CONNECT_NEED_DRIVE = 0x00000020
const CONNECT_PROMPT = 0x00000010
const CONNECT_REDIRECT = 0x00000080
const CONNECT_REFCOUNT = 0x00000040
const CONNECT_RESERVED = 0xFF000000
const CONNECT_TEMPORARY = 0x00000004
const CONNECT_UPDATE_PROFILE = 0x00000001
const CONNECT_UPDATE_RECENT = 0x00000002
const CONSOLE_APPLICATION_16BIT = 0x0000
const CONSOLE_CARET_SELECTION = 0x0001
const CONSOLE_CARET_VISIBLE = 0x0002
const CONSOLE_FULLSCREEN = 1
const CONSOLE_FULLSCREEN_HARDWARE = 2
const CONSOLE_FULLSCREEN_MODE = 1
const CONSOLE_MOUSE_DOWN = 0x0008
const CONSOLE_MOUSE_SELECTION = 0x0004
const CONSOLE_NO_SELECTION = 0x0000
const CONSOLE_SELECTION_IN_PROGRESS = 0x0001
const CONSOLE_SELECTION_NOT_EMPTY = 0x0002
const CONSOLE_TEXTMODE_BUFFER = 1
const CONSOLE_WINDOWED_MODE = 2
const CONST = "const"
const CONTAINER_INHERIT_ACE = 0x2
const CONTEXT_AMD64 = 0x100000
const CONTEXT_ARM64 = 0x400000
const CONTEXT_ARM64_UNWOUND_TO_CALL = 0x20000000
const CONTEXT_EXCEPTION_ACTIVE = 0x8000000
const CONTEXT_EXCEPTION_REPORTING = 0x80000000
const CONTEXT_EXCEPTION_REQUEST = 0x40000000
const CONTEXT_SERVICE_ACTIVE = 0x10000000
const CONTEXT_UNWOUND_TO_CALL = 0x20000000
const CONTROL_C_EXIT = "STATUS_CONTROL_C_EXIT"
const COPY_FILE_ALLOW_DECRYPTED_DESTINATION = 0x8
const COPY_FILE_COPY_SYMLINK = 0x800
const COPY_FILE_FAIL_IF_EXISTS = 0x1
const COPY_FILE_NO_BUFFERING = 0x1000
const COPY_FILE_OPEN_SOURCE_FOR_WRITE = 0x4
const COPY_FILE_RESTARTABLE = 0x2
const CORE_PARKING_POLICY_CHANGE_IDEAL = 0
const CORE_PARKING_POLICY_CHANGE_MAX = "CORE_PARKING_POLICY_CHANGE_MULTISTEP"
const CORE_PARKING_POLICY_CHANGE_MULTISTEP = 3
const CORE_PARKING_POLICY_CHANGE_ROCKET = 2
const CORE_PARKING_POLICY_CHANGE_SINGLE = 1
const COVER_DEFAULT_SPLITPOINT = 1
const CPS_CANCEL = 0x0004
const CPS_COMPLETE = 0x0001
const CPS_CONVERT = 0x0002
const CPS_REVERT = 0x0003
const CP_ACP = 0
const CP_INSTALLED = 0x00000001
const CP_MACCP = 2
const CP_NONE = 0
const CP_OEMCP = 1
const CP_RECTANGLE = 1
const CP_REGION = 2
const CP_SUPPORTED = 0x00000002
const CP_SYMBOL = 42
const CP_THREAD_ACP = 3
const CP_UTF7 = 65000
const CP_UTF8 = 65001
const CREATECOLORSPACE_EMBEDED = 0x00000001
const CREATE_ALWAYS = 2
const CREATE_BOUNDARY_DESCRIPTOR_ADD_APPCONTAINER_SID = 0x1
const CREATE_BREAKAWAY_FROM_JOB = 0x1000000
const CREATE_DEFAULT_ERROR_MODE = 0x4000000
const CREATE_EVENT_INITIAL_SET = 0x2
const CREATE_EVENT_MANUAL_RESET = 0x1
const CREATE_FORCEDOS = 0x2000
const CREATE_FOR_DIR = 2
const CREATE_FOR_IMPORT = 1
const CREATE_IGNORE_SYSTEM_DEFAULT = 0x80000000
const CREATE_MUTEX_INITIAL_OWNER = 0x1
const CREATE_NEW = 1
const CREATE_NEW_CONSOLE = 0x10
const CREATE_NEW_PROCESS_GROUP = 0x200
const CREATE_NO_WINDOW = 0x8000000
const CREATE_PRESERVE_CODE_AUTHZ_LEVEL = 0x2000000
const CREATE_PROCESS_DEBUG_EVENT = 3
const CREATE_PROTECTED_PROCESS = 0x40000
const CREATE_SECURE_PROCESS = 0x400000
const CREATE_SEPARATE_WOW_VDM = 0x800
const CREATE_SHARED_WOW_VDM = 0x1000
const CREATE_SUSPENDED = 0x4
const CREATE_THREAD_DEBUG_EVENT = 2
const CREATE_UNICODE_ENVIRONMENT = 0x400
const CREATE_WAITABLE_TIMER_HIGH_RESOLUTION = 0x2
const CREATE_WAITABLE_TIMER_MANUAL_RESET = 0x1
const CREDUIAPI = "DECLSPEC_IMPORT"
const CRITICAL_ACE_FLAG = 0x20
const CRITICAL_SECTION_NO_DEBUG_INFO = "RTL_CRITICAL_SECTION_FLAG_NO_DEBUG_INFO"
const CRM_PROTOCOL_DYNAMIC_MARSHAL_INFO = 0x00000002
const CRM_PROTOCOL_EXPLICIT_MARSHAL_ONLY = 0x00000001
const CRM_PROTOCOL_MAXIMUM_OPTION = 0x00000003
const CSOUND_SYSTEM = 16
const CSTR_EQUAL = 2
const CSTR_GREATER_THAN = 3
const CSTR_LESS_THAN = 1
const CS_BYTEALIGNCLIENT = 0x1000
const CS_BYTEALIGNWINDOW = 0x2000
const CS_CLASSDC = 0x0040
const CS_DBLCLKS = 0x0008
const CS_DROPSHADOW = 0x00020000
const CS_GLOBALCLASS = 0x4000
const CS_HREDRAW = 0x0002
const CS_IME = 0x00010000
const CS_INSERTCHAR = 0x2000
const CS_NOCLOSE = 0x0200
const CS_NOMOVECARET = 0x4000
const CS_OWNDC = 0x0020
const CS_PARENTDC = 0x0080
const CS_SAVEBITS = 0x0800
const CS_VREDRAW = 0x0001
const CTLCOLOR_BTN = 3
const CTLCOLOR_DLG = 4
const CTLCOLOR_EDIT = 1
const CTLCOLOR_LISTBOX = 2
const CTLCOLOR_MAX = 7
const CTLCOLOR_MSGBOX = 0
const CTLCOLOR_SCROLLBAR = 5
const CTLCOLOR_STATIC = 6
const CTRL_BREAK_EVENT = 1
const CTRL_CLOSE_EVENT = 2
const CTRL_C_EVENT = 0
const CTRL_LOGOFF_EVENT = 5
const CTRL_SHUTDOWN_EVENT = 6
const CTRY_ALBANIA = 355
const CTRY_ALGERIA = 213
const CTRY_ARGENTINA = 54
const CTRY_ARMENIA = 374
const CTRY_AUSTRALIA = 61
const CTRY_AUSTRIA = 43
const CTRY_AZERBAIJAN = 994
const CTRY_BAHRAIN = 973
const CTRY_BELARUS = 375
const CTRY_BELGIUM = 32
const CTRY_BELIZE = 501
const CTRY_BOLIVIA = 591
const CTRY_BRAZIL = 55
const CTRY_BRUNEI_DARUSSALAM = 673
const CTRY_BULGARIA = 359
const CTRY_CANADA = 2
const CTRY_CARIBBEAN = 1
const CTRY_CHILE = 56
const CTRY_COLOMBIA = 57
const CTRY_COSTA_RICA = 506
const CTRY_CROATIA = 385
const CTRY_CZECH = 420
const CTRY_DEFAULT = 0
const CTRY_DENMARK = 45
const CTRY_DOMINICAN_REPUBLIC = 1
const CTRY_ECUADOR = 593
const CTRY_EGYPT = 20
const CTRY_EL_SALVADOR = 503
const CTRY_ESTONIA = 372
const CTRY_FAEROE_ISLANDS = 298
const CTRY_FINLAND = 358
const CTRY_FRANCE = 33
const CTRY_GEORGIA = 995
const CTRY_GERMANY = 49
const CTRY_GREECE = 30
const CTRY_GUATEMALA = 502
const CTRY_HONDURAS = 504
const CTRY_HONG_KONG = 852
const CTRY_HUNGARY = 36
const CTRY_ICELAND = 354
const CTRY_INDIA = 91
const CTRY_INDONESIA = 62
const CTRY_IRAN = 981
const CTRY_IRAQ = 964
const CTRY_IRELAND = 353
const CTRY_ISRAEL = 972
const CTRY_ITALY = 39
const CTRY_JAMAICA = 1
const CTRY_JAPAN = 81
const CTRY_JORDAN = 962
const CTRY_KAZAKSTAN = 7
const CTRY_KENYA = 254
const CTRY_KUWAIT = 965
const CTRY_KYRGYZSTAN = 996
const CTRY_LATVIA = 371
const CTRY_LEBANON = 961
const CTRY_LIBYA = 218
const CTRY_LIECHTENSTEIN = 41
const CTRY_LITHUANIA = 370
const CTRY_LUXEMBOURG = 352
const CTRY_MACAU = 853
const CTRY_MACEDONIA = 389
const CTRY_MALAYSIA = 60
const CTRY_MALDIVES = 960
const CTRY_MEXICO = 52
const CTRY_MONACO = 33
const CTRY_MONGOLIA = 976
const CTRY_MOROCCO = 212
const CTRY_NETHERLANDS = 31
const CTRY_NEW_ZEALAND = 64
const CTRY_NICARAGUA = 505
const CTRY_NORWAY = 47
const CTRY_OMAN = 968
const CTRY_PAKISTAN = 92
const CTRY_PANAMA = 507
const CTRY_PARAGUAY = 595
const CTRY_PERU = 51
const CTRY_PHILIPPINES = 63
const CTRY_POLAND = 48
const CTRY_PORTUGAL = 351
const CTRY_PRCHINA = 86
const CTRY_PUERTO_RICO = 1
const CTRY_QATAR = 974
const CTRY_ROMANIA = 40
const CTRY_RUSSIA = 7
const CTRY_SAUDI_ARABIA = 966
const CTRY_SERBIA = 381
const CTRY_SINGAPORE = 65
const CTRY_SLOVAK = 421
const CTRY_SLOVENIA = 386
const CTRY_SOUTH_AFRICA = 27
const CTRY_SOUTH_KOREA = 82
const CTRY_SPAIN = 34
const CTRY_SWEDEN = 46
const CTRY_SWITZERLAND = 41
const CTRY_SYRIA = 963
const CTRY_TAIWAN = 886
const CTRY_TATARSTAN = 7
const CTRY_THAILAND = 66
const CTRY_TRINIDAD_Y_TOBAGO = 1
const CTRY_TUNISIA = 216
const CTRY_TURKEY = 90
const CTRY_UAE = 971
const CTRY_UKRAINE = 380
const CTRY_UNITED_KINGDOM = 44
const CTRY_UNITED_STATES = 1
const CTRY_URUGUAY = 598
const CTRY_UZBEKISTAN = 7
const CTRY_VENEZUELA = 58
const CTRY_VIET_NAM = 84
const CTRY_YEMEN = 967
const CTRY_ZIMBABWE = 263
const CT_CTYPE1 = 0x00000001
const CT_CTYPE2 = 0x00000002
const CT_CTYPE3 = 0x00000004
const CURRENT_IMPORT_REDIRECTION_VERSION = 1
const CURSOR_SHOWING = 0x00000001
const CURVECAPS = 28
const CWF_CREATE_ONLY = 0x00000001
const CWP_ALL = 0x0000
const CWP_SKIPDISABLED = 0x0002
const CWP_SKIPINVISIBLE = 0x0001
const CWP_SKIPTRANSPARENT = 0x0004
const CaptureStackBackTrace = "RtlCaptureStackBackTrace"
const CompareString = "CompareStringA"
const CopyMemory = "RtlCopyMemory"
const CreateFileMapping = "CreateFileMappingA"
const CreateFileMappingNuma = "CreateFileMappingNumaA"
const CreateNamedPipe = "CreateNamedPipeA"
const CreateProcessAsUser = "CreateProcessAsUserA"
const CreateSemaphoreEx = "CreateSemaphoreExA"
const CreateWaitableTimerEx = "CreateWaitableTimerExA"
const DATEFMT_ENUMPROC = "DATEFMT_ENUMPROCA"
const DATEFMT_ENUMPROCEX = "DATEFMT_ENUMPROCEXA"
const DATE_LONGDATE = 0x00000002
const DATE_LTRREADING = 0x00000010
const DATE_RTLREADING = 0x00000020
const DATE_SHORTDATE = 0x00000001
const DATE_USE_ALT_CALENDAR = 0x00000004
const DATE_YEARMONTH = 0x00000008
const DCBA_FACEDOWNCENTER = 0x0101
const DCBA_FACEDOWNLEFT = 0x0102
const DCBA_FACEDOWNNONE = 0x0100
const DCBA_FACEDOWNRIGHT = 0x0103
const DCBA_FACEUPCENTER = 0x0001
const DCBA_FACEUPLEFT = 0x0002
const DCBA_FACEUPNONE = 0x0000
const DCBA_FACEUPRIGHT = 0x0003
const DCB_ACCUMULATE = 0x0002
const DCB_DIRTY = "DCB_ACCUMULATE"
const DCB_DISABLE = 0x0008
const DCB_ENABLE = 0x0004
const DCB_RESET = 0x0001
const DC_ACTIVE = 0x0001
const DC_BINADJUST = 19
const DC_BINNAMES = 12
const DC_BINS = 6
const DC_BRUSH = 18
const DC_BUTTONS = 0x1000
const DC_COLLATE = 22
const DC_COLORDEVICE = 32
const DC_COPIES = 18
const DC_DATATYPE_PRODUCED = 21
const DC_DRIVER = 11
const DC_DUPLEX = 7
const DC_EMF_COMPLIANT = 20
const DC_ENUMRESOLUTIONS = 13
const DC_EXTRA = 9
const DC_FIELDS = 1
const DC_FILEDEPENDENCIES = 14
const DC_GRADIENT = 0x0020
const DC_HASDEFID = 0x534B
const DC_ICON = 0x0004
const DC_INBUTTON = 0x0010
const DC_MANUFACTURER = 23
const DC_MAXEXTENT = 5
const DC_MEDIAREADY = 29
const DC_MEDIATYPENAMES = 34
const DC_MEDIATYPES = 35
const DC_MINEXTENT = 4
const DC_MODEL = 24
const DC_NUP = 33
const DC_ORIENTATION = 17
const DC_PAPERNAMES = 16
const DC_PAPERS = 2
const DC_PAPERSIZE = 3
const DC_PEN = 19
const DC_PERSONALITY = 25
const DC_PRINTERMEM = 28
const DC_PRINTRATE = 26
const DC_PRINTRATEPPM = 31
const DC_PRINTRATEUNIT = 27
const DC_SIZE = 8
const DC_SMALLCAP = 0x0002
const DC_STAPLE = 30
const DC_TEXT = 0x0008
const DC_TRUETYPE = 15
const DC_VERSION = 10
const DDD_EXACT_MATCH_ON_REMOVE = 0x00000004
const DDD_LUID_BROADCAST_DRIVE = 0x00000010
const DDD_NO_BROADCAST_SYSTEM = 0x00000008
const DDD_RAW_TARGET_PATH = 0x00000001
const DDD_REMOVE_DEFINITION = 0x00000002
const DDICT_HASHSET_MAX_LOAD_FACTOR_COUNT_MULT = 4
const DDICT_HASHSET_MAX_LOAD_FACTOR_SIZE_MULT = 3
const DDICT_HASHSET_RESIZE_FACTOR = 2
const DDICT_HASHSET_TABLE_BASE_SIZE = 64
const DDL_ARCHIVE = 0x0020
const DDL_DIRECTORY = 0x0010
const DDL_DRIVES = 0x4000
const DDL_EXCLUSIVE = 0x8000
const DDL_HIDDEN = 0x0002
const DDL_POSTMSGS = 0x2000
const DDL_READONLY = 0x0001
const DDL_READWRITE = 0x0000
const DDL_SYSTEM = 0x0004
const DEACTIVATE_ACTCTX_FLAG_FORCE_EARLY_DEACTIVATION = 0x00000001
const DEBUGLEVEL = 0
const DEBUG_ONLY_THIS_PROCESS = 0x2
const DEBUG_PROCESS = 0x1
const DEDICATED_MEMORY_CACHE_ELIGIBLE = 0x1
const DEFAULT_ACCEL = 1
const DEFAULT_CHARSET = 1
const DEFAULT_F = 20
const DEFAULT_GUI_FONT = 17
const DEFAULT_IMPERSONATION_LEVEL = "SecurityImpersonation"
const DEFAULT_PALETTE = 15
const DEFAULT_PITCH = 0
const DEFAULT_QUALITY = 0
const DESKTOPHORZRES = 118
const DESKTOPVERTRES = 117
const DETACHED_PROCESS = 0x8
const DEVICEDATA = 19
const DEVICE_DEFAULT_FONT = 14
const DEVICE_FONTTYPE = 0x002
const DEVICE_NOTIFY_ALL_INTERFACE_CLASSES = 0x00000004
const DEVICE_NOTIFY_SERVICE_HANDLE = 0x00000001
const DEVICE_NOTIFY_WINDOW_HANDLE = 0x00000000
const DFCS_ADJUSTRECT = 0x2000
const DFCS_BUTTON3STATE = 0x0008
const DFCS_BUTTONCHECK = 0x0000
const DFCS_BUTTONPUSH = 0x0010
const DFCS_BUTTONRADIO = 0x0004
const DFCS_BUTTONRADIOIMAGE = 0x0001
const DFCS_BUTTONRADIOMASK = 0x0002
const DFCS_CAPTIONCLOSE = 0x0000
const DFCS_CAPTIONHELP = 0x0004
const DFCS_CAPTIONMAX = 0x0002
const DFCS_CAPTIONMIN = 0x0001
const DFCS_CAPTIONRESTORE = 0x0003
const DFCS_CHECKED = 0x0400
const DFCS_FLAT = 0x4000
const DFCS_HOT = 0x1000
const DFCS_INACTIVE = 0x0100
const DFCS_MENUARROW = 0x0000
const DFCS_MENUARROWRIGHT = 0x0004
const DFCS_MENUBULLET = 0x0002
const DFCS_MENUCHECK = 0x0001
const DFCS_MONO = 0x8000
const DFCS_PUSHED = 0x0200
const DFCS_SCROLLCOMBOBOX = 0x0005
const DFCS_SCROLLDOWN = 0x0001
const DFCS_SCROLLLEFT = 0x0002
const DFCS_SCROLLRIGHT = 0x0003
const DFCS_SCROLLSIZEGRIP = 0x0008
const DFCS_SCROLLSIZEGRIPRIGHT = 0x0010
const DFCS_SCROLLUP = 0x0000
const DFCS_TRANSPARENT = 0x0800
const DFC_BUTTON = 4
const DFC_CAPTION = 1
const DFC_MENU = 2
const DFC_POPUPMENU = 5
const DFC_SCROLL = 3
const DIAGNOSTIC_REASON_DETAILED_STRING = 0x00000002
const DIAGNOSTIC_REASON_NOT_SPECIFIED = 0x80000000
const DIAGNOSTIC_REASON_SIMPLE_STRING = 0x00000001
const DIAGNOSTIC_REASON_VERSION = 0
const DIALOPTION_BILLING = 0x00000040
const DIALOPTION_DIALTONE = 0x00000100
const DIALOPTION_QUIET = 0x00000080
const DIB_PAL_COLORS = 1
const DIB_RGB_COLORS = 0
const DICTLISTSIZE_DEFAULT = 10000
const DIFFERENCE = 11
const DISABLE_MAX_PRIVILEGE = 0x1
const DISABLE_NEWLINE_AUTO_RETURN = 0x0008
const DISCHARGE_POLICY_CRITICAL = 0
const DISCHARGE_POLICY_LOW = 1
const DISC_NO_FORCE = 0x00000040
const DISC_UPDATE_PROFILE = 0x00000001
const DISPLAY_DEVICE_ACTIVE = 0x00000001
const DISPLAY_DEVICE_ATTACHED = 0x00000002
const DISPLAY_DEVICE_ATTACHED_TO_DESKTOP = 0x00000001
const DISPLAY_DEVICE_DISCONNECT = 0x02000000
const DISPLAY_DEVICE_MIRRORING_DRIVER = 0x00000008
const DISPLAY_DEVICE_MODESPRUNED = 0x08000000
const DISPLAY_DEVICE_MULTI_DRIVER = 0x00000002
const DISPLAY_DEVICE_PRIMARY_DEVICE = 0x00000004
const DISPLAY_DEVICE_RDPUDD = 0x01000000
const DISPLAY_DEVICE_REMOTE = 0x04000000
const DISPLAY_DEVICE_REMOVABLE = 0x00000020
const DISPLAY_DEVICE_TS_COMPATIBLE = 0x00200000
const DISPLAY_DEVICE_UNSAFE_MODES_ON = 0x00080000
const DISPLAY_DEVICE_VGA_COMPATIBLE = 0x00000010
const DISP_CHANGE_RESTART = 1
const DISP_CHANGE_SUCCESSFUL = 0
const DI_APPBANDING = 0x00000001
const DI_COMPAT = 0x0004
const DI_DEFAULTSIZE = 0x0008
const DI_IMAGE = 0x0002
const DI_MASK = 0x0001
const DI_NOMIRROR = 0x0010
const DI_NORMAL = 0x0003
const DI_ROPS_READ_DESTINATION = 0x00000002
const DKGRAY_BRUSH = 3
const DLGC_BUTTON = 0x2000
const DLGC_DEFPUSHBUTTON = 0x0010
const DLGC_HASSETSEL = 0x0008
const DLGC_RADIOBUTTON = 0x0040
const DLGC_STATIC = 0x0100
const DLGC_UNDEFPUSHBUTTON = 0x0020
const DLGC_WANTALLKEYS = 0x0004
const DLGC_WANTARROWS = 0x0001
const DLGC_WANTCHARS = 0x0080
const DLGC_WANTMESSAGE = 0x0004
const DLGC_WANTTAB = 0x0002
const DLGWINDOWEXTRA = 30
const DLL_PROCESS_ATTACH = 1
const DLL_PROCESS_DETACH = 0
const DLL_PROCESS_VERIFIER = 4
const DLL_THREAD_ATTACH = 2
const DLL_THREAD_DETACH = 3
const DMBIN_AUTO = 7
const DMBIN_CASSETTE = 14
const DMBIN_ENVELOPE = 5
const DMBIN_ENVMANUAL = 6
const DMBIN_FIRST = "DMBIN_UPPER"
const DMBIN_FORMSOURCE = 15
const DMBIN_LARGECAPACITY = 11
const DMBIN_LARGEFMT = 10
const DMBIN_LAST = "DMBIN_FORMSOURCE"
const DMBIN_LOWER = 2
const DMBIN_MANUAL = 4
const DMBIN_MIDDLE = 3
const DMBIN_ONLYONE = 1
const DMBIN_SMALLFMT = 9
const DMBIN_TRACTOR = 8
const DMBIN_UPPER = 1
const DMBIN_USER = 256
const DMCOLLATE_FALSE = 0
const DMCOLLATE_TRUE = 1
const DMCOLOR_COLOR = 2
const DMCOLOR_MONOCHROME = 1
const DMDFO_CENTER = 2
const DMDFO_DEFAULT = 0
const DMDFO_STRETCH = 1
const DMDISPLAYFLAGS_TEXTMODE = 0x00000004
const DMDITHER_COARSE = 2
const DMDITHER_ERRORDIFFUSION = 5
const DMDITHER_FINE = 3
const DMDITHER_GRAYSCALE = 10
const DMDITHER_LINEART = 4
const DMDITHER_NONE = 1
const DMDITHER_RESERVED6 = 6
const DMDITHER_RESERVED7 = 7
const DMDITHER_RESERVED8 = 8
const DMDITHER_RESERVED9 = 9
const DMDITHER_USER = 256
const DMDO_180 = 2
const DMDO_270 = 3
const DMDO_90 = 1
const DMDO_DEFAULT = 0
const DMDUP_HORIZONTAL = 3
const DMDUP_SIMPLEX = 1
const DMDUP_VERTICAL = 2
const DMICMMETHOD_DEVICE = 4
const DMICMMETHOD_DRIVER = 3
const DMICMMETHOD_NONE = 1
const DMICMMETHOD_SYSTEM = 2
const DMICMMETHOD_USER = 256
const DMICM_ABS_COLORIMETRIC = 4
const DMICM_COLORIMETRIC = 3
const DMICM_CONTRAST = 2
const DMICM_SATURATE = 1
const DMICM_USER = 256
const DMMEDIA_GLOSSY = 3
const DMMEDIA_STANDARD = 1
const DMMEDIA_TRANSPARENCY = 2
const DMMEDIA_USER = 256
const DMNUP_ONEUP = 2
const DMNUP_SYSTEM = 1
const DMORIENT_LANDSCAPE = 2
const DMORIENT_PORTRAIT = 1
const DMPAPER_10X11 = 45
const DMPAPER_10X14 = 16
const DMPAPER_11X17 = 17
const DMPAPER_12X11 = 90
const DMPAPER_15X11 = 46
const DMPAPER_9X11 = 44
const DMPAPER_A2 = 66
const DMPAPER_A3 = 8
const DMPAPER_A3_EXTRA = 63
const DMPAPER_A3_EXTRA_TRANSVERSE = 68
const DMPAPER_A3_ROTATED = 76
const DMPAPER_A3_TRANSVERSE = 67
const DMPAPER_A4 = 9
const DMPAPER_A4SMALL = 10
const DMPAPER_A4_EXTRA = 53
const DMPAPER_A4_PLUS = 60
const DMPAPER_A4_ROTATED = 77
const DMPAPER_A4_TRANSVERSE = 55
const DMPAPER_A5 = 11
const DMPAPER_A5_EXTRA = 64
const DMPAPER_A5_ROTATED = 78
const DMPAPER_A5_TRANSVERSE = 61
const DMPAPER_A6 = 70
const DMPAPER_A6_ROTATED = 83
const DMPAPER_A_PLUS = 57
const DMPAPER_B4 = 12
const DMPAPER_B4_JIS_ROTATED = 79
const DMPAPER_B5 = 13
const DMPAPER_B5_EXTRA = 65
const DMPAPER_B5_JIS_ROTATED = 80
const DMPAPER_B5_TRANSVERSE = 62
const DMPAPER_B6_JIS = 88
const DMPAPER_B6_JIS_ROTATED = 89
const DMPAPER_B_PLUS = 58
const DMPAPER_CSHEET = 24
const DMPAPER_DBL_JAPANESE_POSTCARD = 69
const DMPAPER_DBL_JAPANESE_POSTCARD_ROTATED = 82
const DMPAPER_DSHEET = 25
const DMPAPER_ENV_10 = 20
const DMPAPER_ENV_11 = 21
const DMPAPER_ENV_12 = 22
const DMPAPER_ENV_14 = 23
const DMPAPER_ENV_9 = 19
const DMPAPER_ENV_B4 = 33
const DMPAPER_ENV_B5 = 34
const DMPAPER_ENV_B6 = 35
const DMPAPER_ENV_C3 = 29
const DMPAPER_ENV_C4 = 30
const DMPAPER_ENV_C5 = 28
const DMPAPER_ENV_C6 = 31
const DMPAPER_ENV_C65 = 32
const DMPAPER_ENV_DL = 27
const DMPAPER_ENV_INVITE = 47
const DMPAPER_ENV_ITALY = 36
const DMPAPER_ENV_MONARCH = 37
const DMPAPER_ENV_PERSONAL = 38
const DMPAPER_ESHEET = 26
const DMPAPER_EXECUTIVE = 7
const DMPAPER_FANFOLD_LGL_GERMAN = 41
const DMPAPER_FANFOLD_STD_GERMAN = 40
const DMPAPER_FANFOLD_US = 39
const DMPAPER_FIRST = "DMPAPER_LETTER"
const DMPAPER_FOLIO = 14
const DMPAPER_ISO_B4 = 42
const DMPAPER_JAPANESE_POSTCARD = 43
const DMPAPER_JAPANESE_POSTCARD_ROTATED = 81
const DMPAPER_JENV_CHOU3 = 73
const DMPAPER_JENV_CHOU3_ROTATED = 86
const DMPAPER_JENV_CHOU4 = 74
const DMPAPER_JENV_CHOU4_ROTATED = 87
const DMPAPER_JENV_KAKU2 = 71
const DMPAPER_JENV_KAKU2_ROTATED = 84
const DMPAPER_JENV_KAKU3 = 72
const DMPAPER_JENV_KAKU3_ROTATED = 85
const DMPAPER_JENV_YOU4 = 91
const DMPAPER_JENV_YOU4_ROTATED = 92
const DMPAPER_LAST = "DMPAPER_PENV_10_ROTATED"
const DMPAPER_LEDGER = 4
const DMPAPER_LEGAL = 5
const DMPAPER_LEGAL_EXTRA = 51
const DMPAPER_LETTER = 1
const DMPAPER_LETTERSMALL = 2
const DMPAPER_LETTER_EXTRA = 50
const DMPAPER_LETTER_EXTRA_TRANSVERSE = 56
const DMPAPER_LETTER_PLUS = 59
const DMPAPER_LETTER_ROTATED = 75
const DMPAPER_LETTER_TRANSVERSE = 54
const DMPAPER_NOTE = 18
const DMPAPER_P16K = 93
const DMPAPER_P16K_ROTATED = 106
const DMPAPER_P32K = 94
const DMPAPER_P32KBIG = 95
const DMPAPER_P32KBIG_ROTATED = 108
const DMPAPER_P32K_ROTATED = 107
const DMPAPER_PENV_1 = 96
const DMPAPER_PENV_10 = 105
const DMPAPER_PENV_10_ROTATED = 118
const DMPAPER_PENV_1_ROTATED = 109
const DMPAPER_PENV_2 = 97
const DMPAPER_PENV_2_ROTATED = 110
const DMPAPER_PENV_3 = 98
const DMPAPER_PENV_3_ROTATED = 111
const DMPAPER_PENV_4 = 99
const DMPAPER_PENV_4_ROTATED = 112
const DMPAPER_PENV_5 = 100
const DMPAPER_PENV_5_ROTATED = 113
const DMPAPER_PENV_6 = 101
const DMPAPER_PENV_6_ROTATED = 114
const DMPAPER_PENV_7 = 102
const DMPAPER_PENV_7_ROTATED = 115
const DMPAPER_PENV_8 = 103
const DMPAPER_PENV_8_ROTATED = 116
const DMPAPER_PENV_9 = 104
const DMPAPER_PENV_9_ROTATED = 117
const DMPAPER_QUARTO = 15
const DMPAPER_RESERVED_48 = 48
const DMPAPER_RESERVED_49 = 49
const DMPAPER_STATEMENT = 6
const DMPAPER_TABLOID = 3
const DMPAPER_TABLOID_EXTRA = 52
const DMPAPER_USER = 256
const DMTT_BITMAP = 1
const DMTT_DOWNLOAD = 2
const DMTT_DOWNLOAD_OUTLINE = 4
const DMTT_SUBDEV = 3
const DM_COPY = 2
const DM_INTERLACED = 0x00000002
const DM_IN_BUFFER = "DM_MODIFY"
const DM_IN_PROMPT = "DM_PROMPT"
const DM_MODIFY = 8
const DM_OUT_BUFFER = "DM_COPY"
const DM_OUT_DEFAULT = "DM_UPDATE"
const DM_PROMPT = 4
const DM_SPECVERSION = 0x0401
const DM_UPDATE = 1
const DNS_ERROR_DATABASE_BASE = 9700
const DNS_ERROR_DATAFILE_BASE = 9650
const DNS_ERROR_DNSSEC_BASE = 9100
const DNS_ERROR_DP_BASE = 9900
const DNS_ERROR_GENERAL_API_BASE = 9550
const DNS_ERROR_INVALID_DATA = "ERROR_INVALID_DATA"
const DNS_ERROR_INVALID_NAME = "ERROR_INVALID_NAME"
const DNS_ERROR_MASK = 0x00002328
const DNS_ERROR_NO_MEMORY = "ERROR_OUTOFMEMORY"
const DNS_ERROR_OPERATION_BASE = 9750
const DNS_ERROR_PACKET_FMT_BASE = 9500
const DNS_ERROR_RCODE_LAST = "DNS_ERROR_RCODE_BADTIME"
const DNS_ERROR_RCODE_NO_ERROR = "NO_ERROR"
const DNS_ERROR_RESPONSE_CODES_BASE = 9000
const DNS_ERROR_SECURE_BASE = 9800
const DNS_ERROR_SETUP_BASE = 9850
const DNS_ERROR_ZONE_BASE = 9600
const DNS_STATUS_PACKET_UNSECURE = "DNS_ERROR_UNSECURE_PACKET"
const DOCKINFO_DOCKED = 0x2
const DOCKINFO_UNDOCKED = 0x1
const DOCKINFO_USER_SUPPLIED = 0x4
const DOF_DIRECTORY = 0x8003
const DOF_DOCUMENT = 0x8002
const DOF_EXECUTABLE = 0x8001
const DOF_MULTIPLE = 0x8004
const DOF_PROGMAN = 0x0001
const DOF_SHELLDATA = 0x0002
const DONT_RESOLVE_DLL_REFERENCES = 0x1
const DOUBLE_CLICK = 0x0002
const DOWNLOADFACE = 514
const DOWNLOADHEADER = 4111
const DRAFTMODE = 7
const DRAFT_QUALITY = 1
const DRAWPATTERNRECT = 25
const DRIVERVERSION = 0
const DRIVE_CDROM = 5
const DRIVE_FIXED = 3
const DRIVE_NO_ROOT_DIR = 1
const DRIVE_RAMDISK = 6
const DRIVE_REMOTE = 4
const DRIVE_REMOVABLE = 2
const DRIVE_UNKNOWN = 0
const DSS_DISABLED = 0x0020
const DSS_HIDEPREFIX = 0x0200
const DSS_MONO = 0x0080
const DSS_NORMAL = 0x0000
const DSS_PREFIXONLY = 0x0400
const DSS_RIGHT = 0x8000
const DSS_UNION = 0x0010
const DST_BITMAP = 0x0004
const DST_COMPLEX = 0x0000
const DST_ICON = 0x0003
const DST_PREFIXTEXT = 0x0002
const DST_TEXT = 0x0001
const DS_S_SUCCESS = "NO_ERROR"
const DTR_CONTROL_DISABLE = 0x0
const DTR_CONTROL_ENABLE = 0x1
const DTR_CONTROL_HANDSHAKE = 0x2
const DT_BOTTOM = 0x00000008
const DT_CALCRECT = 0x00000400
const DT_CENTER = 0x00000001
const DT_CHARSTREAM = 4
const DT_DISPFILE = 6
const DT_EDITCONTROL = 0x00002000
const DT_END_ELLIPSIS = 0x00008000
const DT_EXPANDTABS = 0x00000040
const DT_EXTERNALLEADING = 0x00000200
const DT_HIDEPREFIX = 0x00100000
const DT_INTERNAL = 0x00001000
const DT_LEFT = 0x00000000
const DT_METAFILE = 5
const DT_MODIFYSTRING = 0x00010000
const DT_NOCLIP = 0x00000100
const DT_NOFULLWIDTHCHARBREAK = 0x00080000
const DT_NOPREFIX = 0x00000800
const DT_PATH_ELLIPSIS = 0x00004000
const DT_PLOTTER = 0
const DT_PREFIXONLY = 0x00200000
const DT_RASCAMERA = 3
const DT_RASDISPLAY = 1
const DT_RASPRINTER = 2
const DT_RIGHT = 0x00000002
const DT_RTLREADING = 0x00020000
const DT_SINGLELINE = 0x00000020
const DT_TABSTOP = 0x00000080
const DT_TOP = 0x00000000
const DT_VCENTER = 0x00000004
const DT_WORDBREAK = 0x00000010
const DT_WORD_ELLIPSIS = 0x00040000
const DUPLICATE_CLOSE_SOURCE = 0x00000001
const DUPLICATE_SAME_ACCESS = 0x00000002
const DWLP_MSGRESULT = 0
const DYNAMIC_BMI2 = 1
const DYNAMIC_EH_CONTINUATION_TARGET_ADD = 0x00000001
const DYNAMIC_EH_CONTINUATION_TARGET_PROCESSED = 0x00000002
const DYNAMIC_ENFORCED_ADDRESS_RANGE_ADD = 0x00000001
const DYNAMIC_ENFORCED_ADDRESS_RANGE_PROCESSED = 0x00000002
const DbgRaiseAssertionFailure = "__int2c"
const DefaultMaxOff = 28
const DefineDosDevice = "DefineDosDeviceA"
const DeleteVolumeMountPoint = "DeleteVolumeMountPointA"
const E2BIG = 7
const EACCES = 13
const EADDRINUSE = 100
const EADDRNOTAVAIL = 101
const EAFNOSUPPORT = 102
const EAGAIN = 11
const EALREADY = 103
const EASTEUROPE_CHARSET = 238
const EBADF = 9
const EBADMSG = 104
const EBUSY = 16
const ECANCELED = 105
const ECHILD = 10
const ECONNABORTED = 106
const ECONNREFUSED = 107
const ECONNRESET = 108
const EC_LEFTMARGIN = 0x0001
const EC_RIGHTMARGIN = 0x0002
const EC_USEFONTINFO = 0xffff
const EDD_GET_DEVICE_INTERFACE_NAME = 0x00000001
const EDEADLK = 36
const EDEADLOCK = "EDEADLK"
const EDESTADDRREQ = 109
const EDOM = 33
const EDS_RAWMODE = 0x00000002
const EDS_ROTATEDMODE = 0x00000004
const EEXIST = 17
const EFAULT = 14
const EFBIG = 27
const EFSRPC_SECURE_ONLY = 8
const EFS_DROP_ALTERNATE_STREAMS = 0x10
const EFS_USE_RECOVERY_KEYS = 0x1
const EHOSTUNREACH = 110
const EIDRM = 111
const EILSEQ = 42
const EIMES_CANCELCOMPSTRINFOCUS = 0x0002
const EIMES_COMPLETECOMPSTRKILLFOCUS = 0x0004
const EIMES_GETCOMPSTRATONCE = 0x0001
const EINPROGRESS = 112
const EINTR = 4
const EINVAL = 22
const EIO = 5
const EISCONN = 113
const EISDIR = 21
const ELF_CULTURE_LATIN = 0
const ELF_VENDOR_SIZE = 4
const ELF_VERSION = 0
const ELOOP = 114
const EMARCH_ENC_I17_IC_INST_WORD_POS_X = 12
const EMARCH_ENC_I17_IC_INST_WORD_X = 3
const EMARCH_ENC_I17_IC_SIZE_X = 1
const EMARCH_ENC_I17_IC_VAL_POS_X = 21
const EMARCH_ENC_I17_IMM41a_INST_WORD_POS_X = 14
const EMARCH_ENC_I17_IMM41a_INST_WORD_X = 1
const EMARCH_ENC_I17_IMM41a_SIZE_X = 10
const EMARCH_ENC_I17_IMM41a_VAL_POS_X = 22
const EMARCH_ENC_I17_IMM41b_INST_WORD_POS_X = 24
const EMARCH_ENC_I17_IMM41b_INST_WORD_X = 1
const EMARCH_ENC_I17_IMM41b_SIZE_X = 8
const EMARCH_ENC_I17_IMM41b_VAL_POS_X = 32
const EMARCH_ENC_I17_IMM41c_INST_WORD_POS_X = 0
const EMARCH_ENC_I17_IMM41c_INST_WORD_X = 2
const EMARCH_ENC_I17_IMM41c_SIZE_X = 23
const EMARCH_ENC_I17_IMM41c_VAL_POS_X = 40
const EMARCH_ENC_I17_IMM5C_INST_WORD_POS_X = 13
const EMARCH_ENC_I17_IMM5C_INST_WORD_X = 3
const EMARCH_ENC_I17_IMM5C_SIZE_X = 5
const EMARCH_ENC_I17_IMM5C_VAL_POS_X = 16
const EMARCH_ENC_I17_IMM7B_INST_WORD_POS_X = 4
const EMARCH_ENC_I17_IMM7B_INST_WORD_X = 3
const EMARCH_ENC_I17_IMM7B_SIZE_X = 7
const EMARCH_ENC_I17_IMM7B_VAL_POS_X = 0
const EMARCH_ENC_I17_IMM9D_INST_WORD_POS_X = 18
const EMARCH_ENC_I17_IMM9D_INST_WORD_X = 3
const EMARCH_ENC_I17_IMM9D_SIZE_X = 9
const EMARCH_ENC_I17_IMM9D_VAL_POS_X = 7
const EMARCH_ENC_I17_SIGN_INST_WORD_POS_X = 27
const EMARCH_ENC_I17_SIGN_INST_WORD_X = 3
const EMARCH_ENC_I17_SIGN_SIZE_X = 1
const EMARCH_ENC_I17_SIGN_VAL_POS_X = 63
const EMFILE = 24
const EMLINK = 31
const EMR_ABORTPATH = 68
const EMR_ALPHABLEND = 114
const EMR_ANGLEARC = 41
const EMR_ARC = 45
const EMR_ARCTO = 55
const EMR_BEGINPATH = 59
const EMR_BITBLT = 76
const EMR_CHORD = 46
const EMR_CLOSEFIGURE = 61
const EMR_COLORCORRECTPALETTE = 111
const EMR_COLORMATCHTOTARGETW = 121
const EMR_CREATEBRUSHINDIRECT = 39
const EMR_CREATECOLORSPACE = 99
const EMR_CREATECOLORSPACEW = 122
const EMR_CREATEDIBPATTERNBRUSHPT = 94
const EMR_CREATEMONOBRUSH = 93
const EMR_CREATEPALETTE = 49
const EMR_CREATEPEN = 38
const EMR_DELETECOLORSPACE = 101
const EMR_DELETEOBJECT = 40
const EMR_ELLIPSE = 42
const EMR_ENDPATH = 60
const EMR_EOF = 14
const EMR_EXCLUDECLIPRECT = 29
const EMR_EXTCREATEFONTINDIRECTW = 82
const EMR_EXTCREATEPEN = 95
const EMR_EXTFLOODFILL = 53
const EMR_EXTSELECTCLIPRGN = 75
const EMR_EXTTEXTOUTA = 83
const EMR_EXTTEXTOUTW = 84
const EMR_FILLPATH = 62
const EMR_FILLRGN = 71
const EMR_FLATTENPATH = 65
const EMR_FRAMERGN = 72
const EMR_GDICOMMENT = 70
const EMR_GLSBOUNDEDRECORD = 103
const EMR_GLSRECORD = 102
const EMR_GRADIENTFILL = 118
const EMR_HEADER = 1
const EMR_INTERSECTCLIPRECT = 30
const EMR_INVERTRGN = 73
const EMR_LINETO = 54
const EMR_MASKBLT = 78
const EMR_MAX = 122
const EMR_MIN = 1
const EMR_MODIFYWORLDTRANSFORM = 36
const EMR_MOVETOEX = 27
const EMR_OFFSETCLIPRGN = 26
const EMR_PAINTRGN = 74
const EMR_PIE = 47
const EMR_PIXELFORMAT = 104
const EMR_PLGBLT = 79
const EMR_POLYBEZIER = 2
const EMR_POLYBEZIER16 = 85
const EMR_POLYBEZIERTO = 5
const EMR_POLYBEZIERTO16 = 88
const EMR_POLYDRAW = 56
const EMR_POLYDRAW16 = 92
const EMR_POLYGON = 3
const EMR_POLYGON16 = 86
const EMR_POLYLINE = 4
const EMR_POLYLINE16 = 87
const EMR_POLYLINETO = 6
const EMR_POLYLINETO16 = 89
const EMR_POLYPOLYGON = 8
const EMR_POLYPOLYGON16 = 91
const EMR_POLYPOLYLINE = 7
const EMR_POLYPOLYLINE16 = 90
const EMR_POLYTEXTOUTA = 96
const EMR_POLYTEXTOUTW = 97
const EMR_REALIZEPALETTE = 52
const EMR_RECTANGLE = 43
const EMR_RESERVED_105 = 105
const EMR_RESERVED_106 = 106
const EMR_RESERVED_107 = 107
const EMR_RESERVED_108 = 108
const EMR_RESERVED_109 = 109
const EMR_RESERVED_110 = 110
const EMR_RESERVED_117 = 117
const EMR_RESERVED_119 = 119
const EMR_RESERVED_120 = 120
const EMR_RESIZEPALETTE = 51
const EMR_RESTOREDC = 34
const EMR_ROUNDRECT = 44
const EMR_SAVEDC = 33
const EMR_SCALEVIEWPORTEXTEX = 31
const EMR_SCALEWINDOWEXTEX = 32
const EMR_SELECTCLIPPATH = 67
const EMR_SELECTOBJECT = 37
const EMR_SELECTPALETTE = 48
const EMR_SETARCDIRECTION = 57
const EMR_SETBKCOLOR = 25
const EMR_SETBKMODE = 18
const EMR_SETBRUSHORGEX = 13
const EMR_SETCOLORADJUSTMENT = 23
const EMR_SETCOLORSPACE = 100
const EMR_SETDIBITSTODEVICE = 80
const EMR_SETICMMODE = 98
const EMR_SETICMPROFILEA = 112
const EMR_SETICMPROFILEW = 113
const EMR_SETLAYOUT = 115
const EMR_SETMAPMODE = 17
const EMR_SETMAPPERFLAGS = 16
const EMR_SETMETARGN = 28
const EMR_SETMITERLIMIT = 58
const EMR_SETPALETTEENTRIES = 50
const EMR_SETPIXELV = 15
const EMR_SETPOLYFILLMODE = 19
const EMR_SETROP2 = 20
const EMR_SETSTRETCHBLTMODE = 21
const EMR_SETTEXTALIGN = 22
const EMR_SETTEXTCOLOR = 24
const EMR_SETVIEWPORTEXTEX = 11
const EMR_SETVIEWPORTORGEX = 12
const EMR_SETWINDOWEXTEX = 9
const EMR_SETWINDOWORGEX = 10
const EMR_SETWORLDTRANSFORM = 35
const EMR_STRETCHBLT = 77
const EMR_STRETCHDIBITS = 81
const EMR_STROKEANDFILLPATH = 63
const EMR_STROKEPATH = 64
const EMR_TRANSPARENTBLT = 116
const EMR_WIDENPATH = 66
const EMSGSIZE = 115
const EMSIS_COMPOSITIONSTRING = 0x0001
const EM_CANUNDO = 0x00C6
const EM_CHARFROMPOS = 0x00D7
const EM_EMPTYUNDOBUFFER = 0x00CD
const EM_FMTLINES = 0x00C8
const EM_GETFIRSTVISIBLELINE = 0x00CE
const EM_GETHANDLE = 0x00BD
const EM_GETIMESTATUS = 0x00D9
const EM_GETLIMITTEXT = 0x00D5
const EM_GETLINE = 0x00C4
const EM_GETLINECOUNT = 0x00BA
const EM_GETMARGINS = 0x00D4
const EM_GETMODIFY = 0x00B8
const EM_GETPASSWORDCHAR = 0x00D2
const EM_GETRECT = 0x00B2
const EM_GETSEL = 0x00B0
const EM_GETTHUMB = 0x00BE
const EM_GETWORDBREAKPROC = 0x00D1
const EM_LIMITTEXT = 0x00C5
const EM_LINEFROMCHAR = 0x00C9
const EM_LINEINDEX = 0x00BB
const EM_LINELENGTH = 0x00C1
const EM_LINESCROLL = 0x00B6
const EM_POSFROMCHAR = 0x00D6
const EM_REPLACESEL = 0x00C2
const EM_SCROLL = 0x00B5
const EM_SCROLLCARET = 0x00B7
const EM_SETHANDLE = 0x00BC
const EM_SETIMESTATUS = 0x00D8
const EM_SETLIMITTEXT = "EM_LIMITTEXT"
const EM_SETMARGINS = 0x00D3
const EM_SETMODIFY = 0x00B9
const EM_SETPASSWORDCHAR = 0x00CC
const EM_SETREADONLY = 0x00CF
const EM_SETRECT = 0x00B3
const EM_SETRECTNP = 0x00B4
const EM_SETSEL = 0x00B1
const EM_SETTABSTOPS = 0x00CB
const EM_SETWORDBREAKPROC = 0x00D0
const EM_UNDO = 0x00C7
const ENABLEDUPLEX = 28
const ENABLEPAIRKERNING = 769
const ENABLERELATIVEWIDTHS = 768
const ENABLE_AUTO_POSITION = 0x0100
const ENABLE_ECHO_INPUT = 0x0004
const ENABLE_EXTENDED_FLAGS = 0x0080
const ENABLE_INSERT_MODE = 0x0020
const ENABLE_LINE_INPUT = 0x0002
const ENABLE_LVB_GRID_WORLDWIDE = 0x0010
const ENABLE_MOUSE_INPUT = 0x0010
const ENABLE_PROCESSED_INPUT = 0x0001
const ENABLE_PROCESSED_OUTPUT = 0x0001
const ENABLE_QUICK_EDIT_MODE = 0x0040
const ENABLE_VIRTUAL_TERMINAL_INPUT = 0x0200
const ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004
const ENABLE_WINDOW_INPUT = 0x0008
const ENABLE_WRAP_AT_EOL_OUTPUT = 0x0002
const ENAMETOOLONG = 38
const ENCAPSULATED_POSTSCRIPT = 4116
const ENCLAVE_LONG_ID_LENGTH = 32
const ENCLAVE_SHORT_ID_LENGTH = 16
const ENCLAVE_TYPE_SGX = 0x00000001
const ENCLAVE_TYPE_SGX2 = 0x00000002
const ENCLAVE_TYPE_VBS = 0x00000010
const ENCLAVE_TYPE_VBS_BASIC = 0x00000011
const ENCLAVE_VBS_FLAG_DEBUG = 0x00000001
const ENDDOC = 11
const ENDSESSION_CLOSEAPP = 0x00000001
const ENDSESSION_CRITICAL = 0x40000000
const ENDSESSION_LOGOFF = 0x80000000
const END_PATH = 4098
const ENETDOWN = 116
const ENETRESET = 117
const ENETUNREACH = 118
const ENFILE = 23
const ENHANCED_KEY = 0x0100
const ENHMETA_SIGNATURE = 0x464D4520
const ENHMETA_STOCK_OBJECT = 0x80000000
const ENLISTMENT_MAXIMUM_OPTION = 0x00000001
const ENLISTMENT_OBJECT_PATH = "\\\\Enlistment\\\\"
const ENLISTMENT_QUERY_INFORMATION = 1
const ENLISTMENT_RECOVER = 4
const ENLISTMENT_SET_INFORMATION = 2
const ENLISTMENT_SUBORDINATE_RIGHTS = 8
const ENLISTMENT_SUPERIOR = 0x00000001
const ENLISTMENT_SUPERIOR_RIGHTS = 0x10
const ENOBUFS = 119
const ENODATA = 120
const ENODEV = 19
const ENOENT = 2
const ENOEXEC = 8
const ENOFILE = "ENOENT"
const ENOLCK = 39
const ENOLINK = 121
const ENOMEM = 12
const ENOMSG = 122
const ENOPROTOOPT = 123
const ENOSPC = 28
const ENOSR = 124
const ENOSTR = 125
const ENOSYS = 40
const ENOTCONN = 126
const ENOTDIR = 20
const ENOTEMPTY = 41
const ENOTRECOVERABLE = 127
const ENOTSOCK = 128
const ENOTSUP = 129
const ENOTTY = 25
const ENUMPAPERBINS = 31
const ENUMPAPERMETRICS = 34
const ENUM_ALL_CALENDARS = 0xffffffff
const ENXIO = 6
const EN_ALIGN_LTR_EC = 0x0700
const EN_ALIGN_RTL_EC = 0x0701
const EN_CHANGE = 0x0300
const EN_ERRSPACE = 0x0500
const EN_HSCROLL = 0x0601
const EN_KILLFOCUS = 0x0200
const EN_MAXTEXT = 0x0501
const EN_SETFOCUS = 0x0100
const EN_UPDATE = 0x0400
const EN_VSCROLL = 0x0602
const EOPNOTSUPP = 130
const EOVERFLOW = 132
const EOWNERDEAD = 133
const EPERM = 1
const EPIPE = 32
const EPROTO = 134
const EPROTONOSUPPORT = 135
const EPROTOTYPE = 136
const EPSPRINTING = 33
const EPS_SIGNATURE = 0x46535045
const ERANGE = 34
const EROFS = 30
const ERROR_SEVERITY_ERROR = 0xC0000000
const ERROR_SEVERITY_INFORMATIONAL = 0x40000000
const ERROR_SEVERITY_SUCCESS = 0x00000000
const ERROR_SEVERITY_WARNING = 0x80000000
const ESB_DISABLE_BOTH = 0x0003
const ESB_DISABLE_DOWN = 0x0002
const ESB_DISABLE_LEFT = 0x0001
const ESB_DISABLE_LTUP = "ESB_DISABLE_LEFT"
const ESB_DISABLE_RIGHT = 0x0002
const ESB_DISABLE_RTDN = "ESB_DISABLE_RIGHT"
const ESB_DISABLE_UP = 0x0001
const ESB_ENABLE_BOTH = 0x0000
const ESPIPE = 29
const ESRCH = 3
const ETIME = 137
const ETIMEDOUT = 138
const ETO_CLIPPED = 0x0004
const ETO_GLYPH_INDEX = 0x0010
const ETO_IGNORELANGUAGE = 0x1000
const ETO_NUMERICSLATIN = 0x0800
const ETO_NUMERICSLOCAL = 0x0400
const ETO_OPAQUE = 0x0002
const ETO_PDY = 0x2000
const ETO_REVERSE_INDEX_MAP = 0x10000
const ETO_RTLREADING = 0x0080
const ETXTBSY = 139
const EVENPARITY = 2
const EVENTLOG_AUDIT_FAILURE = 0x0010
const EVENTLOG_AUDIT_SUCCESS = 0x0008
const EVENTLOG_BACKWARDS_READ = 0x0008
const EVENTLOG_END_ALL_PAIRED_EVENTS = 0x0004
const EVENTLOG_END_PAIRED_EVENT = 0x0002
const EVENTLOG_ERROR_TYPE = 0x0001
const EVENTLOG_FORWARDS_READ = 0x0004
const EVENTLOG_FULL_INFO = 0
const EVENTLOG_INFORMATION_TYPE = 0x0004
const EVENTLOG_PAIRED_EVENT_ACTIVE = 0x0008
const EVENTLOG_PAIRED_EVENT_INACTIVE = 0x0010
const EVENTLOG_SEEK_READ = 0x0002
const EVENTLOG_SEQUENTIAL_READ = 0x0001
const EVENTLOG_START_PAIRED_EVENT = 0x0001
const EVENTLOG_SUCCESS = 0x0000
const EVENTLOG_WARNING_TYPE = 0x0002
const EVENT_CONSOLE_CARET = 0x4001
const EVENT_CONSOLE_END_APPLICATION = 0x4007
const EVENT_CONSOLE_LAYOUT = 0x4005
const EVENT_CONSOLE_START_APPLICATION = 0x4006
const EVENT_CONSOLE_UPDATE_REGION = 0x4002
const EVENT_CONSOLE_UPDATE_SCROLL = 0x4004
const EVENT_CONSOLE_UPDATE_SIMPLE = 0x4003
const EVENT_MAX = 0x7FFFFFFF
const EVENT_MIN = 0x00000001
const EVENT_MODIFY_STATE = 0x0002
const EVENT_OBJECT_ACCELERATORCHANGE = 0x8012
const EVENT_OBJECT_CONTENTSCROLLED = 0x8015
const EVENT_OBJECT_CREATE = 0x8000
const EVENT_OBJECT_DEFACTIONCHANGE = 0x8011
const EVENT_OBJECT_DESCRIPTIONCHANGE = 0x800D
const EVENT_OBJECT_DESTROY = 0x8001
const EVENT_OBJECT_FOCUS = 0x8005
const EVENT_OBJECT_HELPCHANGE = 0x8010
const EVENT_OBJECT_HIDE = 0x8003
const EVENT_OBJECT_INVOKED = 0x8013
const EVENT_OBJECT_LOCATIONCHANGE = 0x800B
const EVENT_OBJECT_NAMECHANGE = 0x800C
const EVENT_OBJECT_PARENTCHANGE = 0x800F
const EVENT_OBJECT_REORDER = 0x8004
const EVENT_OBJECT_SELECTION = 0x8006
const EVENT_OBJECT_SELECTIONADD = 0x8007
const EVENT_OBJECT_SELECTIONREMOVE = 0x8008
const EVENT_OBJECT_SELECTIONWITHIN = 0x8009
const EVENT_OBJECT_SHOW = 0x8002
const EVENT_OBJECT_STATECHANGE = 0x800A
const EVENT_OBJECT_TEXTSELECTIONCHANGED = 0x8014
const EVENT_OBJECT_VALUECHANGE = 0x800E
const EVENT_SYSTEM_ALERT = 0x0002
const EVENT_SYSTEM_CAPTUREEND = 0x0009
const EVENT_SYSTEM_CAPTURESTART = 0x0008
const EVENT_SYSTEM_CONTEXTHELPEND = 0x000D
const EVENT_SYSTEM_CONTEXTHELPSTART = 0x000C
const EVENT_SYSTEM_DESKTOPSWITCH = 0x0020
const EVENT_SYSTEM_DIALOGEND = 0x0011
const EVENT_SYSTEM_DIALOGSTART = 0x0010
const EVENT_SYSTEM_DRAGDROPEND = 0x000F
const EVENT_SYSTEM_DRAGDROPSTART = 0x000E
const EVENT_SYSTEM_FOREGROUND = 0x0003
const EVENT_SYSTEM_MENUEND = 0x0005
const EVENT_SYSTEM_MENUPOPUPEND = 0x0007
const EVENT_SYSTEM_MENUPOPUPSTART = 0x0006
const EVENT_SYSTEM_MENUSTART = 0x0004
const EVENT_SYSTEM_MINIMIZEEND = 0x0017
const EVENT_SYSTEM_MINIMIZESTART = 0x0016
const EVENT_SYSTEM_MOVESIZEEND = 0x000B
const EVENT_SYSTEM_MOVESIZESTART = 0x000A
const EVENT_SYSTEM_SCROLLINGEND = 0x0013
const EVENT_SYSTEM_SCROLLINGSTART = 0x0012
const EVENT_SYSTEM_SOUND = 0x0001
const EVENT_SYSTEM_SWITCHEND = 0x0015
const EVENT_SYSTEM_SWITCHSTART = 0x0014
const EV_BREAK = 0x40
const EV_CTS = 0x8
const EV_DSR = 0x10
const EV_ERR = 0x80
const EV_EVENT1 = 0x800
const EV_EVENT2 = 0x1000
const EV_PERR = 0x200
const EV_RING = 0x100
const EV_RLSD = 0x20
const EV_RX80FULL = 0x400
const EV_RXCHAR = 0x1
const EV_RXFLAG = 0x2
const EV_TXEMPTY = 0x4
const EWOULDBLOCK = 140
const EWX_ARSO = 0x04000000
const EWX_BOOTOPTIONS = 0x01000000
const EWX_CHECK_SAFE_FOR_SERVER = 0x08000000
const EWX_FORCE = 0x00000004
const EWX_FORCEIFHUNG = 0x00000010
const EWX_HYBRID_SHUTDOWN = 0x00400000
const EWX_LOGOFF = 0x00000000
const EWX_POWEROFF = 0x00000008
const EWX_QUICKRESOLVE = 0x00000020
const EWX_REBOOT = 0x00000002
const EWX_RESTARTAPPS = 0x00000040
const EWX_SHUTDOWN = 0x00000001
const EXCEPTION_ACCESS_VIOLATION = "STATUS_ACCESS_VIOLATION"
const EXCEPTION_ARRAY_BOUNDS_EXCEEDED = "STATUS_ARRAY_BOUNDS_EXCEEDED"
const EXCEPTION_BREAKPOINT = "STATUS_BREAKPOINT"
const EXCEPTION_COLLIDED_UNWIND = 0x40
const EXCEPTION_CONTINUE_SEARCH = 0
const EXCEPTION_DATATYPE_MISALIGNMENT = "STATUS_DATATYPE_MISALIGNMENT"
const EXCEPTION_DEBUG_EVENT = 1
const EXCEPTION_DISPOSITION = "int"
const EXCEPTION_EXECUTE_FAULT = 8
const EXCEPTION_EXECUTE_HANDLER = 1
const EXCEPTION_EXIT_UNWIND = 0x4
const EXCEPTION_FLT_DENORMAL_OPERAND = "STATUS_FLOAT_DENORMAL_OPERAND"
const EXCEPTION_FLT_DIVIDE_BY_ZERO = "STATUS_FLOAT_DIVIDE_BY_ZERO"
const EXCEPTION_FLT_INEXACT_RESULT = "STATUS_FLOAT_INEXACT_RESULT"
const EXCEPTION_FLT_INVALID_OPERATION = "STATUS_FLOAT_INVALID_OPERATION"
const EXCEPTION_FLT_OVERFLOW = "STATUS_FLOAT_OVERFLOW"
const EXCEPTION_FLT_STACK_CHECK = "STATUS_FLOAT_STACK_CHECK"
const EXCEPTION_FLT_UNDERFLOW = "STATUS_FLOAT_UNDERFLOW"
const EXCEPTION_GUARD_PAGE = "STATUS_GUARD_PAGE_VIOLATION"
const EXCEPTION_ILLEGAL_INSTRUCTION = "STATUS_ILLEGAL_INSTRUCTION"
const EXCEPTION_INT_DIVIDE_BY_ZERO = "STATUS_INTEGER_DIVIDE_BY_ZERO"
const EXCEPTION_INT_OVERFLOW = "STATUS_INTEGER_OVERFLOW"
const EXCEPTION_INVALID_DISPOSITION = "STATUS_INVALID_DISPOSITION"
const EXCEPTION_INVALID_HANDLE = "STATUS_INVALID_HANDLE"
const EXCEPTION_IN_PAGE_ERROR = "STATUS_IN_PAGE_ERROR"
const EXCEPTION_MAXIMUM_PARAMETERS = 15
const EXCEPTION_NESTED_CALL = 0x10
const EXCEPTION_NONCONTINUABLE = 0x1
const EXCEPTION_NONCONTINUABLE_EXCEPTION = "STATUS_NONCONTINUABLE_EXCEPTION"
const EXCEPTION_POSSIBLE_DEADLOCK = "STATUS_POSSIBLE_DEADLOCK"
const EXCEPTION_PRIV_INSTRUCTION = "STATUS_PRIVILEGED_INSTRUCTION"
const EXCEPTION_READ_FAULT = 0
const EXCEPTION_SINGLE_STEP = "STATUS_SINGLE_STEP"
const EXCEPTION_STACK_INVALID = 0x8
const EXCEPTION_STACK_OVERFLOW = "STATUS_STACK_OVERFLOW"
const EXCEPTION_TARGET_UNWIND = 0x20
const EXCEPTION_UNWIND = 0x66
const EXCEPTION_UNWINDING = 0x2
const EXCEPTION_WRITE_FAULT = 1
const EXDEV = 18
const EXIT_FAILURE = 1
const EXIT_PROCESS_DEBUG_EVENT = 5
const EXIT_SUCCESS = 0
const EXIT_THREAD_DEBUG_EVENT = 4
const EXTENDED_STARTUPINFO_PRESENT = 0x80000
const EXTERN_C = "extern"
const EXTTEXTOUT = 512
const EXT_DEVICE_CAPS = 4099
const EnumResourceNames = "EnumResourceNamesA"
const ExceptionCollidedUnwind = 3
const ExceptionContinueExecution = 0
const ExceptionContinueSearch = 1
const ExceptionExecuteHandler = 4
const ExceptionNestedException = 2
const FACILITY_AAF = 18
const FACILITY_ACCELERATOR = 1536
const FACILITY_ACS = 20
const FACILITY_ACTION_QUEUE = 44
const FACILITY_AUDCLNT = 2185
const FACILITY_AUDIO = 102
const FACILITY_AUDIOSTREAMING = 1094
const FACILITY_BACKGROUNDCOPY = 32
const FACILITY_BCD = 57
const FACILITY_BLB = 120
const FACILITY_BLBUI = 128
const FACILITY_BLB_CLI = 121
const FACILITY_BLUETOOTH_ATT = 101
const FACILITY_CERT = 11
const FACILITY_CMI = 54
const FACILITY_COMPLUS = 17
const FACILITY_CONFIGURATION = 33
const FACILITY_CONTROL = 10
const FACILITY_DAF = 100
const FACILITY_DEBUGGERS = 176
const FACILITY_DEFRAG = 2304
const FACILITY_DELIVERY_OPTIMIZATION = 208
const FACILITY_DEPLOYMENT_SERVICES_BINLSVC = 261
const FACILITY_DEPLOYMENT_SERVICES_CONTENT_PROVIDER = 293
const FACILITY_DEPLOYMENT_SERVICES_DRIVER_PROVISIONING = 278
const FACILITY_DEPLOYMENT_SERVICES_IMAGING = 258
const FACILITY_DEPLOYMENT_SERVICES_MANAGEMENT = 259
const FACILITY_DEPLOYMENT_SERVICES_MULTICAST_CLIENT = 290
const FACILITY_DEPLOYMENT_SERVICES_MULTICAST_SERVER = 289
const FACILITY_DEPLOYMENT_SERVICES_PXE = 263
const FACILITY_DEPLOYMENT_SERVICES_SERVER = 257
const FACILITY_DEPLOYMENT_SERVICES_TFTP = 264
const FACILITY_DEPLOYMENT_SERVICES_TRANSPORT_MANAGEMENT = 272
const FACILITY_DEPLOYMENT_SERVICES_UTIL = 260
const FACILITY_DEVICE_UPDATE_AGENT = 135
const FACILITY_DIRECT2D = 2201
const FACILITY_DIRECT3D10 = 2169
const FACILITY_DIRECT3D11 = 2172
const FACILITY_DIRECT3D11_DEBUG = 2173
const FACILITY_DIRECT3D12 = 2174
const FACILITY_DIRECT3D12_DEBUG = 2175
const FACILITY_DIRECTMUSIC = 2168
const FACILITY_DIRECTORYSERVICE = 37
const FACILITY_DISPATCH = 2
const FACILITY_DLS = 153
const FACILITY_DMSERVER = 256
const FACILITY_DPLAY = 21
const FACILITY_DRVSERVICING = 136
const FACILITY_DXCORE = 2176
const FACILITY_DXGI = 2170
const FACILITY_DXGI_DDI = 2171
const FACILITY_EAP = 66
const FACILITY_EAS = 85
const FACILITY_FVE = 49
const FACILITY_FWP = 50
const FACILITY_GAME = 2340
const FACILITY_GRAPHICS = 38
const FACILITY_HSP_SERVICES = 296
const FACILITY_HSP_SOFTWARE = 297
const FACILITY_HTTP = 25
const FACILITY_INPUT = 64
const FACILITY_INTERNET = 12
const FACILITY_IORING = 70
const FACILITY_ITF = 4
const FACILITY_JSCRIPT = 2306
const FACILITY_LEAP = 2184
const FACILITY_LINGUISTIC_SERVICES = 305
const FACILITY_MBN = 84
const FACILITY_MEDIASERVER = 13
const FACILITY_METADIRECTORY = 35
const FACILITY_MOBILE = 1793
const FACILITY_MSMQ = 14
const FACILITY_NAP = 39
const FACILITY_NDIS = 52
const FACILITY_NT_BIT = 0x10000000
const FACILITY_NULL = 0
const FACILITY_OCP_UPDATE_AGENT = 173
const FACILITY_ONLINE_ID = 134
const FACILITY_OPC = 81
const FACILITY_P2P = 99
const FACILITY_P2P_INT = 98
const FACILITY_PARSE = 113
const FACILITY_PIDGENX = 2561
const FACILITY_PIX = 2748
const FACILITY_PLA = 48
const FACILITY_POWERSHELL = 84
const FACILITY_PRESENTATION = 2177
const FACILITY_QUIC = 65
const FACILITY_RAS = 83
const FACILITY_RESTORE = 256
const FACILITY_RPC = 1
const FACILITY_SCARD = 16
const FACILITY_SCRIPT = 112
const FACILITY_SDIAG = 60
const FACILITY_SECURITY = 9
const FACILITY_SERVICE_FABRIC = 1968
const FACILITY_SETUPAPI = 15
const FACILITY_SHELL = 39
const FACILITY_SOS = 160
const FACILITY_SPP = 256
const FACILITY_SQLITE = 1967
const FACILITY_SSPI = 9
const FACILITY_STATEREPOSITORY = 103
const FACILITY_STATE_MANAGEMENT = 34
const FACILITY_STORAGE = 3
const FACILITY_SXS = 23
const FACILITY_SYNCENGINE = 2050
const FACILITY_TIERING = 131
const FACILITY_TPM_SERVICES = 40
const FACILITY_TPM_SOFTWARE = 41
const FACILITY_TTD = 1490
const FACILITY_UI = 42
const FACILITY_UMI = 22
const FACILITY_URT = 19
const FACILITY_USERMODE_COMMONLOG = 26
const FACILITY_USERMODE_FILTER_MANAGER = 31
const FACILITY_USERMODE_HNS = 59
const FACILITY_USERMODE_HYPERVISOR = 53
const FACILITY_USERMODE_LICENSING = 234
const FACILITY_USERMODE_PRM = 2342
const FACILITY_USERMODE_SDBUS = 2305
const FACILITY_USERMODE_SPACES = 231
const FACILITY_USERMODE_UNIONFS = 2341
const FACILITY_USERMODE_VHD = 58
const FACILITY_USERMODE_VIRTUALIZATION = 55
const FACILITY_USERMODE_VOLMGR = 56
const FACILITY_USERMODE_VOLSNAP = 130
const FACILITY_USERMODE_WIN_ACCEL = 2343
const FACILITY_USER_MODE_SECURITY_CORE = 232
const FACILITY_USN = 129
const FACILITY_UTC = 1989
const FACILITY_VISUALCPP = 109
const FACILITY_WEB = 885
const FACILITY_WEBSERVICES = 61
const FACILITY_WEB_SOCKET = 886
const FACILITY_WEP = 2049
const FACILITY_WER = 27
const FACILITY_WIA = 33
const FACILITY_WIN32 = 7
const FACILITY_WINCODEC_DWRITE_DWM = 2200
const FACILITY_WINDOWS = 8
const FACILITY_WINDOWSUPDATE = 36
const FACILITY_WINDOWS_CE = 24
const FACILITY_WINDOWS_DEFENDER = 80
const FACILITY_WINDOWS_SETUP = 48
const FACILITY_WINDOWS_STORE = 63
const FACILITY_WINML = 2192
const FACILITY_WINPE = 61
const FACILITY_WINRM = 51
const FACILITY_WMAAECMA = 1996
const FACILITY_WPN = 62
const FACILITY_WSBAPP = 122
const FACILITY_WSB_ONLINE = 133
const FACILITY_XAML = 43
const FACILITY_XBOX = 2339
const FACILITY_XPS = 82
const FAILED_ACCESS_ACE_FLAG = 0x80
const FAIL_FAST_GENERATE_EXCEPTION_ADDRESS = 0x1
const FAIL_FAST_NO_HARD_ERROR_DLG = 0x2
const FALSE = 0
const FALT = 0x10
const FAPPCOMMAND_KEY = 0
const FAPPCOMMAND_MASK = 0xF000
const FAPPCOMMAND_MOUSE = 0x8000
const FAPPCOMMAND_OEM = 0x1000
const FASTCOVER_DEFAULT_SPLITPOINT = 0.75
const FASTCOVER_MAX_ACCEL = 10
const FASTCOVER_MAX_F = 31
const FAST_FAIL_CORRUPT_LIST_ENTRY = 3
const FAST_FAIL_FATAL_APP_EXIT = 7
const FAST_FAIL_GS_COOKIE_INIT = 6
const FAST_FAIL_INCORRECT_STACK = 4
const FAST_FAIL_INVALID_ARG = 5
const FAST_FAIL_INVALID_FAST_FAIL_CODE = 0xffffffff
const FAST_FAIL_LEGACY_GS_VIOLATION = 0
const FAST_FAIL_RANGE_CHECK_FAILURE = 8
const FAST_FAIL_STACK_COOKIE_CHECK_FAILURE = 2
const FAST_FAIL_UNSAFE_REGISTRY_ACCESS = 9
const FAST_FAIL_VTGUARD_CHECK_FAILURE = 1
const FCONTROL = 0x08
const FEATURESETTING_CUSTPAPER = 3
const FEATURESETTING_MIRROR = 4
const FEATURESETTING_NEGATIVE = 5
const FEATURESETTING_NUP = 0
const FEATURESETTING_OUTPUT = 1
const FEATURESETTING_PRIVATE_BEGIN = 0x1000
const FEATURESETTING_PRIVATE_END = 0x1FFF
const FEATURESETTING_PROTOCOL = 6
const FEATURESETTING_PSLEVEL = 2
const FE_FONTSMOOTHINGCLEARTYPE = 0x0002
const FE_FONTSMOOTHINGDOCKING = 0x8000
const FE_FONTSMOOTHINGORIENTATIONBGR = 0x0000
const FE_FONTSMOOTHINGORIENTATIONRGB = 0x0001
const FE_FONTSMOOTHINGSTANDARD = 0x0001
const FIBER_FLAG_FLOAT_SWITCH = 0x1
const FILENAME_MAX = 260
const FILE_ACTION_ADDED = 0x00000001
const FILE_ACTION_MODIFIED = 0x00000003
const FILE_ACTION_REMOVED = 0x00000002
const FILE_ACTION_RENAMED_NEW_NAME = 0x00000005
const FILE_ACTION_RENAMED_OLD_NAME = 0x00000004
const FILE_ADD_FILE = 0x0002
const FILE_ADD_SUBDIRECTORY = 0x0004
const FILE_APPEND_DATA = 0x0004
const FILE_ATTRIBUTE_ARCHIVE = 0x00000020
const FILE_ATTRIBUTE_COMPRESSED = 0x00000800
const FILE_ATTRIBUTE_DEVICE = 0x00000040
const FILE_ATTRIBUTE_DIRECTORY = 0x00000010
const FILE_ATTRIBUTE_EA = 0x00040000
const FILE_ATTRIBUTE_ENCRYPTED = 0x00004000
const FILE_ATTRIBUTE_HIDDEN = 0x00000002
const FILE_ATTRIBUTE_INTEGRITY_STREAM = 0x00008000
const FILE_ATTRIBUTE_NORMAL = 0x00000080
const FILE_ATTRIBUTE_NOT_CONTENT_INDEXED = 0x00002000
const FILE_ATTRIBUTE_NO_SCRUB_DATA = 0x00020000
const FILE_ATTRIBUTE_OFFLINE = 0x00001000
const FILE_ATTRIBUTE_PINNED = 0x00080000
const FILE_ATTRIBUTE_READONLY = 0x00000001
const FILE_ATTRIBUTE_RECALL_ON_DATA_ACCESS = 0x00400000
const FILE_ATTRIBUTE_RECALL_ON_OPEN = 0x00040000
const FILE_ATTRIBUTE_REPARSE_POINT = 0x00000400
const FILE_ATTRIBUTE_SPARSE_FILE = 0x00000200
const FILE_ATTRIBUTE_STRICTLY_SEQUENTIAL = 0x20000000
const FILE_ATTRIBUTE_SYSTEM = 0x00000004
const FILE_ATTRIBUTE_TEMPORARY = 0x00000100
const FILE_ATTRIBUTE_UNPINNED = 0x00100000
const FILE_ATTRIBUTE_VIRTUAL = 0x00010000
const FILE_BEGIN = 0
const FILE_CACHE_MAX_HARD_DISABLE = 0x00000002
const FILE_CACHE_MAX_HARD_ENABLE = 0x00000001
const FILE_CACHE_MIN_HARD_DISABLE = 0x00000008
const FILE_CACHE_MIN_HARD_ENABLE = 0x00000004
const FILE_CASE_PRESERVED_NAMES = 0x00000002
const FILE_CASE_SENSITIVE_SEARCH = 0x00000001
const FILE_COMPLETE_IF_OPLOCKED = 0x00000100
const FILE_CREATE = 0x00000002
const FILE_CREATE_PIPE_INSTANCE = 0x0004
const FILE_CREATE_TREE_CONNECTION = 0x00000080
const FILE_CS_FLAG_CASE_SENSITIVE_DIR = 0x00000001
const FILE_CURRENT = 1
const FILE_DAX_VOLUME = 0x20000000
const FILE_DELETE_CHILD = 0x0040
const FILE_DELETE_ON_CLOSE = 0x00001000
const FILE_DIRECTORY_FILE = 0x00000001
const FILE_DIR_DISALLOWED = 9
const FILE_DISPOSITION_FLAG_DELETE = 0x00000001
const FILE_DISPOSITION_FLAG_DO_NOT_DELETE = 0x00000000
const FILE_DISPOSITION_FLAG_FORCE_IMAGE_SECTION_CHECK = 0x00000004
const FILE_DISPOSITION_FLAG_IGNORE_READONLY_ATTRIBUTE = 0x00000010
const FILE_DISPOSITION_FLAG_ON_CLOSE = 0x00000008
const FILE_DISPOSITION_FLAG_POSIX_SEMANTICS = 0x00000002
const FILE_ENCRYPTABLE = 0
const FILE_END = 2
const FILE_EXECUTE = 0x0020
const FILE_FILE_COMPRESSION = 0x00000010
const FILE_FLAG_BACKUP_SEMANTICS = 0x2000000
const FILE_FLAG_DELETE_ON_CLOSE = 0x4000000
const FILE_FLAG_FIRST_PIPE_INSTANCE = 0x80000
const FILE_FLAG_NO_BUFFERING = 0x20000000
const FILE_FLAG_OPEN_NO_RECALL = 0x100000
const FILE_FLAG_OPEN_REPARSE_POINT = 0x200000
const FILE_FLAG_OVERLAPPED = 0x40000000
const FILE_FLAG_POSIX_SEMANTICS = 0x1000000
const FILE_FLAG_RANDOM_ACCESS = 0x10000000
const FILE_FLAG_SEQUENTIAL_SCAN = 0x8000000
const FILE_FLAG_SESSION_AWARE = 0x800000
const FILE_FLAG_WRITE_THROUGH = 0x80000000
const FILE_IS_ENCRYPTED = 1
const FILE_LIST_DIRECTORY = 0x0001
const FILE_MAP_ALL_ACCESS = "SECTION_ALL_ACCESS"
const FILE_MAP_COPY = 0x1
const FILE_MAP_EXECUTE = "SECTION_MAP_EXECUTE_EXPLICIT"
const FILE_MAP_LARGE_PAGES = 0x20000000
const FILE_MAP_READ = "SECTION_MAP_READ"
const FILE_MAP_RESERVE = 0x80000000
const FILE_MAP_TARGETS_INVALID = 0x40000000
const FILE_MAP_WRITE = "SECTION_MAP_WRITE"
const FILE_MAXIMUM_DISPOSITION = 0x00000005
const FILE_NAMED_STREAMS = 0x00040000
const FILE_NAME_FLAGS_UNSPECIFIED = 0x80
const FILE_NAME_FLAG_BOTH = 0x03
const FILE_NAME_FLAG_DOS = 0x02
const FILE_NAME_FLAG_HARDLINK = 0
const FILE_NAME_FLAG_NTFS = 0x01
const FILE_NAME_NORMALIZED = 0x0
const FILE_NAME_OPENED = 0x8
const FILE_NON_DIRECTORY_FILE = 0x00000040
const FILE_NOTIFY_CHANGE_ATTRIBUTES = 0x00000004
const FILE_NOTIFY_CHANGE_CREATION = 0x00000040
const FILE_NOTIFY_CHANGE_DIR_NAME = 0x00000002
const FILE_NOTIFY_CHANGE_FILE_NAME = 0x00000001
const FILE_NOTIFY_CHANGE_LAST_ACCESS = 0x00000020
const FILE_NOTIFY_CHANGE_LAST_WRITE = 0x00000010
const FILE_NOTIFY_CHANGE_SECURITY = 0x00000100
const FILE_NOTIFY_CHANGE_SIZE = 0x00000008
const FILE_NO_COMPRESSION = 0x00008000
const FILE_NO_EA_KNOWLEDGE = 0x00000200
const FILE_NO_INTERMEDIATE_BUFFERING = 0x00000008
const FILE_OPEN = 0x00000001
const FILE_OPEN_BY_FILE_ID = 0x00002000
const FILE_OPEN_FOR_BACKUP_INTENT = 0x00004000
const FILE_OPEN_FOR_FREE_SPACE_QUERY = 0x00800000
const FILE_OPEN_IF = 0x00000003
const FILE_OPEN_NO_RECALL = 0x00400000
const FILE_OPEN_REMOTE_INSTANCE = 0x00000400
const FILE_OPEN_REPARSE_POINT = 0x00200000
const FILE_OVERWRITE = 0x00000004
const FILE_OVERWRITE_IF = 0x00000005
const FILE_PERSISTENT_ACLS = 0x00000008
const FILE_RANDOM_ACCESS = 0x00000800
const FILE_READ_ATTRIBUTES = 0x0080
const FILE_READ_DATA = 0x0001
const FILE_READ_EA = 0x0008
const FILE_READ_ONLY = 8
const FILE_READ_ONLY_VOLUME = 0x00080000
const FILE_RENAME_FLAG_POSIX_SEMANTICS = 0x00000002
const FILE_RENAME_FLAG_REPLACE_IF_EXISTS = 0x00000001
const FILE_RENAME_FLAG_SUPPRESS_PIN_STATE_INHERITANCE = 0x00000004
const FILE_RESERVE_OPFILTER = 0x00100000
const FILE_RETURNS_CLEANUP_RESULT_INFO = 0x00000200
const FILE_ROOT_DIR = 3
const FILE_SEQUENTIAL_ONLY = 0x00000004
const FILE_SEQUENTIAL_WRITE_ONCE = 0x00100000
const FILE_SHARE_DELETE = 0x00000004
const FILE_SHARE_READ = 0x00000001
const FILE_SHARE_VALID_FLAGS = 0x00000007
const FILE_SHARE_WRITE = 0x00000002
const FILE_SKIP_COMPLETION_PORT_ON_SUCCESS = 0x1
const FILE_SKIP_SET_EVENT_ON_HANDLE = 0x2
const FILE_SUPERSEDE = 0x00000000
const FILE_SUPPORTS_BLOCK_REFCOUNTING = 0x08000000
const FILE_SUPPORTS_BYPASS_IO = 0x00000800
const FILE_SUPPORTS_CASE_SENSITIVE_DIRS = 0x00002000
const FILE_SUPPORTS_ENCRYPTION = 0x00020000
const FILE_SUPPORTS_EXTENDED_ATTRIBUTES = 0x00800000
const FILE_SUPPORTS_GHOSTING = 0x40000000
const FILE_SUPPORTS_HARD_LINKS = 0x00400000
const FILE_SUPPORTS_INTEGRITY_STREAMS = 0x04000000
const FILE_SUPPORTS_OBJECT_IDS = 0x00010000
const FILE_SUPPORTS_OPEN_BY_FILE_ID = 0x01000000
const FILE_SUPPORTS_POSIX_UNLINK_RENAME = 0x00000400
const FILE_SUPPORTS_REMOTE_STORAGE = 0x00000100
const FILE_SUPPORTS_REPARSE_POINTS = 0x00000080
const FILE_SUPPORTS_SPARSE_FILES = 0x00000040
const FILE_SUPPORTS_SPARSE_VDL = 0x10000000
const FILE_SUPPORTS_STREAM_SNAPSHOTS = 0x00001000
const FILE_SUPPORTS_TRANSACTIONS = 0x00200000
const FILE_SUPPORTS_USN_JOURNAL = 0x02000000
const FILE_SYNCHRONOUS_IO_ALERT = 0x00000010
const FILE_SYNCHRONOUS_IO_NONALERT = 0x00000020
const FILE_SYSTEM_ATTR = 2
const FILE_SYSTEM_DIR = 4
const FILE_SYSTEM_NOT_SUPPORT = 6
const FILE_TRAVERSE = 0x0020
const FILE_TYPE_CHAR = 0x2
const FILE_TYPE_DISK = 0x1
const FILE_TYPE_PIPE = 0x3
const FILE_TYPE_REMOTE = 0x8000
const FILE_TYPE_UNKNOWN = 0x0
const FILE_UNICODE_ON_DISK = 0x00000004
const FILE_UNKNOWN = 5
const FILE_USER_DISALLOWED = 7
const FILE_VER_GET_LOCALISED = 0x01
const FILE_VER_GET_NEUTRAL = 0x02
const FILE_VER_GET_PREFETCHED = 0x04
const FILE_VOLUME_IS_COMPRESSED = 0x00008000
const FILE_VOLUME_QUOTAS = 0x00000020
const FILE_WRITE_ATTRIBUTES = 0x0100
const FILE_WRITE_DATA = 0x0002
const FILE_WRITE_EA = 0x0010
const FILE_WRITE_THROUGH = 0x00000002
const FIND_ACTCTX_SECTION_KEY_RETURN_ASSEMBLY_METADATA = 0x00000004
const FIND_ACTCTX_SECTION_KEY_RETURN_FLAGS = 0x00000002
const FIND_ACTCTX_SECTION_KEY_RETURN_HACTCTX = 0x00000001
const FIND_ENDSWITH = 0x00200000
const FIND_FIRST_EX_CASE_SENSITIVE = 0x00000001
const FIND_FIRST_EX_LARGE_FETCH = 0x00000002
const FIND_FROMEND = 0x00800000
const FIND_FROMSTART = 0x00400000
const FIND_RESOURCE_DIRECTORY_LANGUAGES = 0x0400
const FIND_RESOURCE_DIRECTORY_NAMES = 0x0200
const FIND_RESOURCE_DIRECTORY_TYPES = 0x0100
const FIND_STARTSWITH = 0x00100000
const FIXED_PITCH = 1
const FKF_AVAILABLE = 0x00000002
const FKF_CLICKON = 0x00000040
const FKF_CONFIRMHOTKEY = 0x00000008
const FKF_FILTERKEYSON = 0x00000001
const FKF_HOTKEYACTIVE = 0x00000004
const FKF_HOTKEYSOUND = 0x00000010
const FKF_INDICATOR = 0x00000020
const FLASHW_CAPTION = 0x00000001
const FLASHW_STOP = 0
const FLASHW_TIMER = 0x00000004
const FLASHW_TIMERNOFG = 0x0000000c
const FLASHW_TRAY = 0x00000002
const FLI_MASK = 0x103B
const FLOODFILLBORDER = 0
const FLOODFILLSURFACE = 1
const FLS_MAXIMUM_AVAILABLE = 128
const FLUSHOUTPUT = 6
const FMTID_NULL = "GUID_NULL"
const FNOINVERT = 0x02
const FOCUS_EVENT = 0x0010
const FONTMAPPER_MAX = 10
const FOPEN_MAX = 20
const FORCEINLINE = "__forceinline"
const FOREGROUND_BLUE = 0x0001
const FOREGROUND_GREEN = 0x0002
const FOREGROUND_INTENSITY = 0x0008
const FOREGROUND_RED = 0x0004
const FORMAT_MESSAGE_ALLOCATE_BUFFER = 0x00000100
const FORMAT_MESSAGE_ARGUMENT_ARRAY = 0x00002000
const FORMAT_MESSAGE_FROM_HMODULE = 0x00000800
const FORMAT_MESSAGE_FROM_STRING = 0x00000400
const FORMAT_MESSAGE_FROM_SYSTEM = 0x00001000
const FORMAT_MESSAGE_IGNORE_INSERTS = 0x00000200
const FORMAT_MESSAGE_MAX_WIDTH_MASK = 0x000000ff
const FRAME_FPO = 0
const FRAME_NONFPO = 3
const FRAME_TRAP = 1
const FRAME_TSS = 2
const FROM_LEFT_1ST_BUTTON_PRESSED = 0x0001
const FROM_LEFT_2ND_BUTTON_PRESSED = 0x0004
const FROM_LEFT_3RD_BUTTON_PRESSED = 0x0008
const FROM_LEFT_4TH_BUTTON_PRESSED = 0x0010
const FR_NOT_ENUM = 0x20
const FR_PRIVATE = 0x10
const FSE_DECODE_TYPE = "FSE_decode_t"
const FSE_DEFAULT_MEMORY_USAGE = 13
const FSE_FUNCTION_TYPE = "BYTE"
const FSE_MAX_MEMORY_USAGE = 14
const FSE_MAX_SYMBOL_VALUE = 255
const FSE_MIN_TABLELOG = 5
const FSE_NCOUNTBOUND = 512
const FSE_TABLELOG_ABSOLUTE_MAX = 15
const FSE_VERSION_MAJOR = 0
const FSE_VERSION_MINOR = 9
const FSE_VERSION_RELEASE = 0
const FSE_isError1 = "ERR_isError"
const FSHIFT = 0x04
const FS_CASE_IS_PRESERVED = "FILE_CASE_PRESERVED_NAMES"
const FS_CASE_SENSITIVE = "FILE_CASE_SENSITIVE_SEARCH"
const FS_FILE_COMPRESSION = "FILE_FILE_COMPRESSION"
const FS_FILE_ENCRYPTION = "FILE_SUPPORTS_ENCRYPTION"
const FS_PERSISTENT_ACLS = "FILE_PERSISTENT_ACLS"
const FS_UNICODE_STORED_ON_DISK = "FILE_UNICODE_ON_DISK"
const FS_VOL_IS_COMPRESSED = "FILE_VOLUME_IS_COMPRESSED"
const FVIRTKEY = "TRUE"
const FW_BLACK = "FW_HEAVY"
const FW_BOLD = 700
const FW_DEMIBOLD = "FW_SEMIBOLD"
const FW_DONTCARE = 0
const FW_EXTRABOLD = 800
const FW_EXTRALIGHT = 200
const FW_HEAVY = 900
const FW_LIGHT = 300
const FW_MEDIUM = 500
const FW_NORMAL = 400
const FW_REGULAR = "FW_NORMAL"
const FW_SEMIBOLD = 600
const FW_THIN = 100
const FW_ULTRABOLD = "FW_EXTRABOLD"
const FW_ULTRALIGHT = "FW_EXTRALIGHT"
const FastFence = "__faststorefence"
const FillMemory = "RtlFillMemory"
const FindFirstVolume = "FindFirstVolumeA"
const FindNextVolume = "FindNextVolumeA"
const FindResource = "FindResourceA"
const FindResourceEx = "FindResourceExA"
const FoldString = "FoldStringA"
const GA_PARENT = 1
const GA_ROOT = 2
const GA_ROOTOWNER = 3
const GB2312_CHARSET = 134
const GCL_CONVERSION = 0x0001
const GCL_REVERSECONVERSION = 0x0002
const GCL_REVERSE_LENGTH = 0x0003
const GCPCLASS_ARABIC = 2
const GCPCLASS_HEBREW = 2
const GCPCLASS_LATIN = 1
const GCPCLASS_LATINNUMBER = 5
const GCPCLASS_LATINNUMERICSEPARATOR = 7
const GCPCLASS_LATINNUMERICTERMINATOR = 6
const GCPCLASS_LOCALNUMBER = 4
const GCPCLASS_NEUTRAL = 3
const GCPCLASS_NUMERICSEPARATOR = 8
const GCPCLASS_POSTBOUNDLTR = 0x20
const GCPCLASS_POSTBOUNDRTL = 0x10
const GCPCLASS_PREBOUNDLTR = 0x80
const GCPCLASS_PREBOUNDRTL = 0x40
const GCPGLYPH_LINKAFTER = 0x4000
const GCPGLYPH_LINKBEFORE = 0x8000
const GCP_DBCS = 0x0001
const GCP_DIACRITIC = 0x0100
const GCP_ERROR = 0x8000
const GCP_GLYPHSHAPE = 0x0010
const GCP_KASHIDA = 0x0400
const GCP_LIGATE = 0x0020
const GCP_REORDER = 0x0002
const GCP_USEKERNING = 0x0008
const GCS_COMPATTR = 0x0010
const GCS_COMPCLAUSE = 0x0020
const GCS_COMPREADATTR = 0x0002
const GCS_COMPREADCLAUSE = 0x0004
const GCS_COMPREADSTR = 0x0001
const GCS_COMPSTR = 0x0008
const GCS_CURSORPOS = 0x0080
const GCS_DELTASTART = 0x0100
const GCS_RESULTCLAUSE = 0x1000
const GCS_RESULTREADCLAUSE = 0x0400
const GCS_RESULTREADSTR = 0x0200
const GCS_RESULTSTR = 0x0800
const GDICOMMENT_BEGINGROUP = 0x00000002
const GDICOMMENT_ENDGROUP = 0x00000003
const GDICOMMENT_IDENTIFIER = 0x43494447
const GDICOMMENT_MULTIFORMATS = 0x40000004
const GDICOMMENT_UNICODE_END = 0x00000080
const GDICOMMENT_UNICODE_STRING = 0x00000040
const GDICOMMENT_WINDOWS_METAFILE = 0x80000001
const GDIPLUS_TS_QUERYVER = 4122
const GDIPLUS_TS_RECORD = 4123
const GDI_MAX_OBJ_TYPE = "GDI_OBJ_LAST"
const GDI_MIN_OBJ_TYPE = "OBJ_PEN"
const GDI_OBJ_LAST = "OBJ_COLORSPACE"
const GETCOLORTABLE = 5
const GETDEVICEUNITS = 42
const GETEXTENDEDTEXTMETRICS = 256
const GETEXTENTTABLE = 257
const GETFACENAME = 513
const GETPAIRKERNTABLE = 258
const GETPENWIDTH = 16
const GETPHYSPAGESIZE = 12
const GETPRINTINGOFFSET = 13
const GETSCALINGFACTOR = 14
const GETSETPAPERBINS = 29
const GETSETPAPERMETRICS = 35
const GETSETPRINTORIENT = 30
const GETSETSCREENPARAMS = 3072
const GETTECHNOLGY = 20
const GETTECHNOLOGY = 20
const GETTRACKKERNTABLE = 259
const GETVECTORBRUSHSIZE = 27
const GETVECTORPENSIZE = 26
const GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS = 0x4
const GET_MODULE_HANDLE_EX_FLAG_PIN = 0x1
const GET_MODULE_HANDLE_EX_FLAG_UNCHANGED_REFCOUNT = 0x2
const GET_MOUSEORKEY_LPARAM = "GET_DEVICE_LPARAM"
const GET_PS_FEATURESETTING = 4121
const GET_SYSTEM_WOW64_DIRECTORY_NAME_A_A = "GetSystemWow64DirectoryA"
const GET_SYSTEM_WOW64_DIRECTORY_NAME_A_W = "GetSystemWow64DirectoryA"
const GET_SYSTEM_WOW64_DIRECTORY_NAME_W_A = "GetSystemWow64DirectoryW"
const GET_SYSTEM_WOW64_DIRECTORY_NAME_W_W = "GetSystemWow64DirectoryW"
const GET_TAPE_DRIVE_INFORMATION = 1
const GET_TAPE_MEDIA_INFORMATION = 0
const GGI_MARK_NONEXISTING_GLYPHS = 0x0001
const GGL_INDEX = 0x00000002
const GGL_LEVEL = 0x00000001
const GGL_PRIVATE = 0x00000004
const GGL_STRING = 0x00000003
const GGO_BEZIER = 3
const GGO_BITMAP = 1
const GGO_GLYPH_INDEX = 0x0080
const GGO_GRAY2_BITMAP = 4
const GGO_GRAY4_BITMAP = 5
const GGO_GRAY8_BITMAP = 6
const GGO_METRICS = 0
const GGO_NATIVE = 2
const GGO_UNHINTED = 0x0100
const GIDC_ARRIVAL = 1
const GIDC_REMOVAL = 2
const GL_ID_CANNOTSAVE = 0x00000011
const GL_ID_CHOOSECANDIDATE = 0x00000028
const GL_ID_INPUTCODE = 0x00000026
const GL_ID_INPUTRADICAL = 0x00000025
const GL_ID_INPUTREADING = 0x00000024
const GL_ID_INPUTSYMBOL = 0x00000027
const GL_ID_NOCONVERT = 0x00000020
const GL_ID_NODICTIONARY = 0x00000010
const GL_ID_NOMODULE = 0x00000001
const GL_ID_PRIVATE_FIRST = 0x00008000
const GL_ID_PRIVATE_LAST = 0x0000FFFF
const GL_ID_READINGCONFLICT = 0x00000023
const GL_ID_REVERSECONVERSION = 0x00000029
const GL_ID_TOOMANYSTROKE = 0x00000022
const GL_ID_TYPINGERROR = 0x00000021
const GL_ID_UNKNOWN = 0x00000000
const GL_LEVEL_ERROR = 0x00000002
const GL_LEVEL_FATAL = 0x00000001
const GL_LEVEL_INFORMATION = 0x00000004
const GL_LEVEL_NOGUIDELINE = 0x00000000
const GL_LEVEL_WARNING = 0x00000003
const GMEM_DDESHARE = 0x2000
const GMEM_DISCARDABLE = 0x100
const GMEM_DISCARDED = 0x4000
const GMEM_FIXED = 0x0
const GMEM_INVALID_HANDLE = 0x8000
const GMEM_LOCKCOUNT = 0x00ff
const GMEM_LOWER = "GMEM_NOT_BANKED"
const GMEM_MODIFY = 0x80
const GMEM_MOVEABLE = 0x2
const GMEM_NOCOMPACT = 0x10
const GMEM_NODISCARD = 0x20
const GMEM_NOTIFY = 0x4000
const GMEM_NOT_BANKED = 0x1000
const GMEM_SHARE = 0x2000
const GMEM_VALID_FLAGS = 0x7f72
const GMEM_ZEROINIT = 0x40
const GMMP_USE_DISPLAY_POINTS = 1
const GMMP_USE_HIGH_RESOLUTION_POINTS = 2
const GM_ADVANCED = 2
const GM_COMPATIBLE = 1
const GM_LAST = 2
const GRADIENT_FILL_OP_FLAG = 0x000000ff
const GRADIENT_FILL_RECT_H = 0x00000000
const GRADIENT_FILL_RECT_V = 0x00000001
const GRADIENT_FILL_TRIANGLE = 0x00000002
const GRAY_BRUSH = 2
const GREEK_CHARSET = 161
const GR_GDIOBJECTS = 0
const GR_USEROBJECTS = 1
const GSS_ALLOW_INHERITED_COMMON = 0x0001
const GS_8BIT_INDICES = 0x00000001
const GUI_16BITTASK = 0x00000000
const GUI_CARETBLINKING = 0x00000001
const GUI_INMENUMODE = 0x00000004
const GUI_INMOVESIZE = 0x00000002
const GUI_POPUPMENUMODE = 0x00000010
const GUI_SYSTEMMENUMODE = 0x00000008
const GW_CHILD = 5
const GW_ENABLEDPOPUP = 6
const GW_HWNDFIRST = 0
const GW_HWNDLAST = 1
const GW_HWNDNEXT = 2
const GW_HWNDPREV = 3
const GW_MAX = 6
const GW_OWNER = 4
const GetEnvironmentStringsA = "GetEnvironmentStrings"
const GetExceptionCode = "_exception_code"
const GetFileSecurity = "GetFileSecurityA"
const GetLocaleInfo = "GetLocaleInfoA"
const GetLogicalDriveStrings = "GetLogicalDriveStringsA"
const GetNamedPipeClientComputerName = "GetNamedPipeClientComputerNameA"
const GetSegmentLimit = "__segmentlimit"
const GetShortPathName = "GetShortPathNameA"
const GetStartupInfo = "GetStartupInfoA"
const GetStringTypeEx = "GetStringTypeExA"
const GetVolumeInformation = "GetVolumeInformationA"
const GetVolumeNameForVolumeMountPoint = "GetVolumeNameForVolumeMountPointA"
const GetVolumePathName = "GetVolumePathNameA"
const GetVolumePathNamesForVolumeName = "GetVolumePathNamesForVolumeNameA"
const HALFTONE = 4
const HANDLE_FLAG_INHERIT = 0x1
const HANDLE_FLAG_PROTECT_FROM_CLOSE = 0x2
const HANGEUL_CHARSET = 129
const HANGUL_CHARSET = 129
const HASHLENGTH = 2
const HASHLOG_MAX = 10
const HASH_READ_SIZE = 8
const HBUFFSIZE = 256
const HCBT_ACTIVATE = 5
const HCBT_CLICKSKIPPED = 6
const HCBT_CREATEWND = 3
const HCBT_DESTROYWND = 4
const HCBT_KEYSKIPPED = 7
const HCBT_MINMAX = 1
const HCBT_MOVESIZE = 0
const HCBT_QS = 2
const HCBT_SETFOCUS = 9
const HCBT_SYSCOMMAND = 8
const HCF_AVAILABLE = 0x00000002
const HCF_CONFIRMHOTKEY = 0x00000008
const HCF_DEFAULTDESKTOP = 0x00000200
const HCF_HIGHCONTRASTON = 0x00000001
const HCF_HOTKEYACTIVE = 0x00000004
const HCF_HOTKEYAVAILABLE = 0x00000040
const HCF_HOTKEYSOUND = 0x00000010
const HCF_INDICATOR = 0x00000020
const HCF_LOGONDESKTOP = 0x00000100
const HCF_OPTION_NOTHEMECHANGE = 0x00001000
const HC_ACTION = 0
const HC_GETNEXT = 1
const HC_NOREM = "HC_NOREMOVE"
const HC_NOREMOVE = 3
const HC_SKIP = 2
const HC_SYSMODALOFF = 5
const HC_SYSMODALON = 4
const HEAP_CREATE_ALIGN_16 = 0x00010000
const HEAP_CREATE_ENABLE_EXECUTE = 0x00040000
const HEAP_CREATE_ENABLE_TRACING = 0x00020000
const HEAP_DISABLE_COALESCE_ON_FREE = 0x00000080
const HEAP_FREE_CHECKING_ENABLED = 0x00000040
const HEAP_GENERATE_EXCEPTIONS = 0x00000004
const HEAP_GROWABLE = 0x00000002
const HEAP_MAXIMUM_TAG = 0x0FFF
const HEAP_NO_SERIALIZE = 0x00000001
const HEAP_PSEUDO_TAG_FLAG = 0x8000
const HEAP_REALLOC_IN_PLACE_ONLY = 0x00000010
const HEAP_TAG_SHIFT = 18
const HEAP_TAIL_CHECKING_ENABLED = 0x00000020
const HEAP_ZERO_MEMORY = 0x00000008
const HEBREW_CHARSET = 177
const HELPINFO_MENUITEM = 0x0002
const HELPINFO_WINDOW = 0x0001
const HELP_COMMAND = 0x0102
const HELP_CONTENTS = 0x0003
const HELP_CONTEXT = 0x0001
const HELP_CONTEXTMENU = 0x000a
const HELP_CONTEXTPOPUP = 0x0008
const HELP_FINDER = 0x000b
const HELP_FORCEFILE = 0x0009
const HELP_HELPONHELP = 0x0004
const HELP_INDEX = 0x0003
const HELP_KEY = 0x0101
const HELP_MULTIKEY = 0x0201
const HELP_PARTIALKEY = 0x0105
const HELP_QUIT = 0x0002
const HELP_SETCONTENTS = 0x0005
const HELP_SETINDEX = 0x0005
const HELP_SETPOPUP_POS = 0x000d
const HELP_SETWINPOS = 0x0203
const HELP_TCARD = 0x8000
const HELP_TCARD_DATA = 0x0010
const HELP_TCARD_OTHER_CALLER = 0x0011
const HELP_WM_HELP = 0x000c
const HIDE_WINDOW = 0
const HIGH_PRIORITY_CLASS = 0x80
const HIGH_SURROGATE_END = 0xdbff
const HIGH_SURROGATE_START = 0xd800
const HINSTANCE_ERROR = 32
const HINT_INLINE = "FORCE_INLINE_TEMPLATE"
const HISTORY_NO_DUP_FLAG = 0x1
const HIST_WKSP_SIZE_U32 = 1024
const HKL_NEXT = 1
const HKL_PREV = 0
const HMONITOR_DECLARED = 1
const HOLLOW_BRUSH = "NULL_BRUSH"
const HORZRES = 8
const HORZSIZE = 4
const HOVER_DEFAULT = 0xFFFFFFFF
const HSHELL_ACCESSIBILITYSTATE = 11
const HSHELL_ACTIVATESHELLWINDOW = 3
const HSHELL_APPCOMMAND = 12
const HSHELL_ENDTASK = 10
const HSHELL_GETMINRECT = 5
const HSHELL_HIGHBIT = 0x8000
const HSHELL_LANGUAGE = 8
const HSHELL_REDRAW = 6
const HSHELL_SYSMENU = 9
const HSHELL_TASKMAN = 7
const HSHELL_WINDOWACTIVATED = 4
const HSHELL_WINDOWCREATED = 1
const HSHELL_WINDOWDESTROYED = 2
const HSHELL_WINDOWREPLACED = 13
const HSHELL_WINDOWREPLACING = 14
const HS_API_MAX = 12
const HS_BDIAGONAL = 3
const HS_CROSS = 4
const HS_DIAGCROSS = 5
const HS_FDIAGONAL = 2
const HS_HORIZONTAL = 0
const HS_VERTICAL = 1
const HTBORDER = 18
const HTBOTTOM = 15
const HTBOTTOMLEFT = 16
const HTBOTTOMRIGHT = 17
const HTCAPTION = 2
const HTCLIENT = 1
const HTCLOSE = 20
const HTGROWBOX = 4
const HTHELP = 21
const HTHSCROLL = 6
const HTLEFT = 10
const HTMAXBUTTON = 9
const HTMENU = 5
const HTMINBUTTON = 8
const HTNOWHERE = 0
const HTOBJECT = 19
const HTREDUCE = "HTMINBUTTON"
const HTRIGHT = 11
const HTSIZE = "HTGROWBOX"
const HTSIZEFIRST = "HTLEFT"
const HTSIZELAST = "HTBOTTOMRIGHT"
const HTSYSMENU = 3
const HTTOP = 12
const HTTOPLEFT = 13
const HTTOPRIGHT = 14
const HTVSCROLL = 7
const HTZOOM = "HTMAXBUTTON"
const HUF_ASM_DECL = "HUF_EXTERN_C"
const HUF_CTABLEBOUND = 129
const HUF_DECODER_FAST_TABLELOG = 11
const HUF_ENABLE_FAST_DECODE = 1
const HUF_FAST_BMI2_ATTRS = "BMI2_TARGET_ATTRIBUTE"
const HUF_NEED_BMI2_FUNCTION = 1
const HUF_OPTIMAL_DEPTH_THRESHOLD = 8
const HUF_SYMBOLVALUE_MAX = 255
const HUF_TABLELOG_ABSOLUTEMAX = 12
const HUF_TABLELOG_DEFAULT = 11
const HUF_TABLELOG_MAX = 12
const HUF_WORKSPACE_MAX_ALIGNMENT = 8
const HUF_isError1 = "ERR_isError"
const HW_PROFILE_GUIDLEN = 39
const IACE_CHILDREN = 0x0001
const IACE_DEFAULT = 0x0010
const IACE_IGNORENOCONTEXT = 0x0020
const ICM_ADDPROFILE = 1
const ICM_DELETEPROFILE = 2
const ICM_DONE_OUTSIDEDC = 4
const ICM_OFF = 1
const ICM_ON = 2
const ICM_QUERY = 3
const ICM_QUERYMATCH = 7
const ICM_QUERYPROFILE = 3
const ICM_REGISTERICMATCHER = 5
const ICM_SETDEFAULTPROFILE = 4
const ICM_UNREGISTERICMATCHER = 6
const ICON_BIG = 1
const ICON_SMALL = 0
const ICON_SMALL2 = 2
const IDABORT = 3
const IDANI_CAPTION = 3
const IDANI_OPEN = 1
const IDCANCEL = 2
const IDCLOSE = 8
const IDCONTINUE = 11
const IDHELP = 9
const IDH_CANCEL = 28444
const IDH_GENERIC_HELP_BUTTON = 28442
const IDH_HELP = 28445
const IDH_MISSING_CONTEXT = 28441
const IDH_NO_HELP = 28440
const IDH_OK = 28443
const IDIGNORE = 5
const IDI_ERROR = "IDI_HAND"
const IDI_INFORMATION = "IDI_ASTERISK"
const IDI_WARNING = "IDI_EXCLAMATION"
const IDLE_PRIORITY_CLASS = 0x40
const IDNO = 7
const IDN_ALLOW_UNASSIGNED = 0x01
const IDN_EMAIL_ADDRESS = 0x04
const IDN_RAW_PUNYCODE = 0x08
const IDN_USE_STD3_ASCII_RULES = 0x02
const IDOK = 1
const IDRETRY = 4
const IDTIMEOUT = 32000
const IDTRYAGAIN = 10
const IDYES = 6
const IFACEMETHODIMP = "STDMETHODIMP"
const IFACEMETHODIMPV = "STDMETHODIMPV"
const IGIMIF_RIGHTMENU = 0x0001
const IGIMII_CMODE = 0x0001
const IGIMII_CONFIGURE = 0x0004
const IGIMII_HELP = 0x0010
const IGIMII_INPUTTOOLS = 0x0040
const IGIMII_OTHER = 0x0020
const IGIMII_SMODE = 0x0002
const IGIMII_TOOLS = 0x0008
const IGNORE = 0
const IGP_CONVERSION = 0x00000008
const IGP_PROPERTY = 0x00000004
const IGP_SELECT = 0x00000018
const IGP_SENTENCE = 0x0000000c
const IGP_SETCOMPSTR = 0x00000014
const IGP_UI = 0x00000010
const IID_NULL = "GUID_NULL"
const ILLUMINANT_A = 1
const ILLUMINANT_B = 2
const ILLUMINANT_C = 3
const ILLUMINANT_D50 = 4
const ILLUMINANT_D55 = 5
const ILLUMINANT_D65 = 6
const ILLUMINANT_D75 = 7
const ILLUMINANT_DAYLIGHT = "ILLUMINANT_C"
const ILLUMINANT_DEVICE_DEFAULT = 0
const ILLUMINANT_F2 = 8
const ILLUMINANT_FLUORESCENT = "ILLUMINANT_F2"
const ILLUMINANT_MAX_INDEX = "ILLUMINANT_F2"
const ILLUMINANT_NTSC = "ILLUMINANT_C"
const ILLUMINANT_TUNGSTEN = "ILLUMINANT_A"
const IMAGE_ARCHIVE_END = "`\\n"
const IMAGE_ARCHIVE_LINKER_MEMBER = "/               "
const IMAGE_ARCHIVE_LONGNAMES_MEMBER = "//              "
const IMAGE_ARCHIVE_PAD = "\\n"
const IMAGE_ARCHIVE_START = "!<arch>\\n"
const IMAGE_ARCHIVE_START_SIZE = 8
const IMAGE_BITMAP = 0
const IMAGE_COMDAT_SELECT_ANY = 2
const IMAGE_COMDAT_SELECT_ASSOCIATIVE = 5
const IMAGE_COMDAT_SELECT_EXACT_MATCH = 4
const IMAGE_COMDAT_SELECT_LARGEST = 6
const IMAGE_COMDAT_SELECT_NEWEST = 7
const IMAGE_COMDAT_SELECT_NODUPLICATES = 1
const IMAGE_COMDAT_SELECT_SAME_SIZE = 3
const IMAGE_CURSOR = 2
const IMAGE_DEBUG_MISC_EXENAME = 1
const IMAGE_DEBUG_TYPE_BORLAND = 9
const IMAGE_DEBUG_TYPE_CLSID = 11
const IMAGE_DEBUG_TYPE_CODEVIEW = 2
const IMAGE_DEBUG_TYPE_COFF = 1
const IMAGE_DEBUG_TYPE_EXCEPTION = 5
const IMAGE_DEBUG_TYPE_FIXUP = 6
const IMAGE_DEBUG_TYPE_FPO = 3
const IMAGE_DEBUG_TYPE_MISC = 4
const IMAGE_DEBUG_TYPE_OMAP_FROM_SRC = 8
const IMAGE_DEBUG_TYPE_OMAP_TO_SRC = 7
const IMAGE_DEBUG_TYPE_RESERVED10 = 10
const IMAGE_DEBUG_TYPE_UNKNOWN = 0
const IMAGE_DIRECTORY_ENTRY_ARCHITECTURE = 7
const IMAGE_DIRECTORY_ENTRY_BASERELOC = 5
const IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT = 11
const IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR = 14
const IMAGE_DIRECTORY_ENTRY_DEBUG = 6
const IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT = 13
const IMAGE_DIRECTORY_ENTRY_EXCEPTION = 3
const IMAGE_DIRECTORY_ENTRY_EXPORT = 0
const IMAGE_DIRECTORY_ENTRY_GLOBALPTR = 8
const IMAGE_DIRECTORY_ENTRY_IAT = 12
const IMAGE_DIRECTORY_ENTRY_IMPORT = 1
const IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG = 10
const IMAGE_DIRECTORY_ENTRY_RESOURCE = 2
const IMAGE_DIRECTORY_ENTRY_SECURITY = 4
const IMAGE_DIRECTORY_ENTRY_TLS = 9
const IMAGE_DLLCHARACTERISTICS_APPCONTAINER = 0x1000
const IMAGE_DLLCHARACTERISTICS_DYNAMIC_BASE = 0x0040
const IMAGE_DLLCHARACTERISTICS_FORCE_INTEGRITY = 0x0080
const IMAGE_DLLCHARACTERISTICS_GUARD_CF = 0x4000
const IMAGE_DLLCHARACTERISTICS_HIGH_ENTROPY_VA = 0x0020
const IMAGE_DLLCHARACTERISTICS_NO_BIND = 0x0800
const IMAGE_DLLCHARACTERISTICS_NO_ISOLATION = 0x0200
const IMAGE_DLLCHARACTERISTICS_NO_SEH = 0x0400
const IMAGE_DLLCHARACTERISTICS_NX_COMPAT = 0x0100
const IMAGE_DLLCHARACTERISTICS_TERMINAL_SERVER_AWARE = 0x8000
const IMAGE_DLLCHARACTERISTICS_WDM_DRIVER = 0x2000
const IMAGE_DOS_SIGNATURE = 0x5A4D
const IMAGE_ENHMETAFILE = 3
const IMAGE_FILE_32BIT_MACHINE = 0x0100
const IMAGE_FILE_AGGRESIVE_WS_TRIM = 0x0010
const IMAGE_FILE_BYTES_REVERSED_HI = 0x8000
const IMAGE_FILE_BYTES_REVERSED_LO = 0x0080
const IMAGE_FILE_DEBUG_STRIPPED = 0x0200
const IMAGE_FILE_DLL = 0x2000
const IMAGE_FILE_EXECUTABLE_IMAGE = 0x0002
const IMAGE_FILE_LARGE_ADDRESS_AWARE = 0x0020
const IMAGE_FILE_LINE_NUMS_STRIPPED = 0x0004
const IMAGE_FILE_LOCAL_SYMS_STRIPPED = 0x0008
const IMAGE_FILE_MACHINE_ALPHA = 0x0184
const IMAGE_FILE_MACHINE_ALPHA64 = 0x0284
const IMAGE_FILE_MACHINE_AM33 = 0x01d3
const IMAGE_FILE_MACHINE_AMD64 = 0x8664
const IMAGE_FILE_MACHINE_ARM = 0x01c0
const IMAGE_FILE_MACHINE_ARM64 = 0xaa64
const IMAGE_FILE_MACHINE_ARMNT = 0x01c4
const IMAGE_FILE_MACHINE_ARMV7 = 0x01c4
const IMAGE_FILE_MACHINE_AXP64 = "IMAGE_FILE_MACHINE_ALPHA64"
const IMAGE_FILE_MACHINE_CEE = 0xc0ee
const IMAGE_FILE_MACHINE_CEF = 0x0CEF
const IMAGE_FILE_MACHINE_EBC = 0x0EBC
const IMAGE_FILE_MACHINE_I386 = 0x014c
const IMAGE_FILE_MACHINE_IA64 = 0x0200
const IMAGE_FILE_MACHINE_M32R = 0x9041
const IMAGE_FILE_MACHINE_MIPS16 = 0x0266
const IMAGE_FILE_MACHINE_MIPSFPU = 0x0366
const IMAGE_FILE_MACHINE_MIPSFPU16 = 0x0466
const IMAGE_FILE_MACHINE_POWERPC = 0x01F0
const IMAGE_FILE_MACHINE_POWERPCFP = 0x01f1
const IMAGE_FILE_MACHINE_R10000 = 0x0168
const IMAGE_FILE_MACHINE_R3000 = 0x0162
const IMAGE_FILE_MACHINE_R4000 = 0x0166
const IMAGE_FILE_MACHINE_SH3 = 0x01a2
const IMAGE_FILE_MACHINE_SH3DSP = 0x01a3
const IMAGE_FILE_MACHINE_SH3E = 0x01a4
const IMAGE_FILE_MACHINE_SH4 = 0x01a6
const IMAGE_FILE_MACHINE_SH5 = 0x01a8
const IMAGE_FILE_MACHINE_THUMB = 0x01c2
const IMAGE_FILE_MACHINE_TRICORE = 0x0520
const IMAGE_FILE_MACHINE_UNKNOWN = 0
const IMAGE_FILE_MACHINE_WCEMIPSV2 = 0x0169
const IMAGE_FILE_NET_RUN_FROM_SWAP = 0x0800
const IMAGE_FILE_RELOCS_STRIPPED = 0x0001
const IMAGE_FILE_REMOVABLE_RUN_FROM_SWAP = 0x0400
const IMAGE_FILE_SYSTEM = 0x1000
const IMAGE_FILE_UP_SYSTEM_ONLY = 0x4000
const IMAGE_ICON = 1
const IMAGE_NT_OPTIONAL_HDR32_MAGIC = 0x10b
const IMAGE_NT_OPTIONAL_HDR64_MAGIC = 0x20b
const IMAGE_NT_OPTIONAL_HDR_MAGIC = "IMAGE_NT_OPTIONAL_HDR64_MAGIC"
const IMAGE_NT_SIGNATURE = 0x00004550
const IMAGE_NUMBEROF_DIRECTORY_ENTRIES = 16
const IMAGE_ORDINAL_FLAG = "IMAGE_ORDINAL_FLAG64"
const IMAGE_ORDINAL_FLAG32 = 0x80000000
const IMAGE_ORDINAL_FLAG64 = "0x8000000000000000ull"
const IMAGE_OS2_SIGNATURE = 0x454E
const IMAGE_OS2_SIGNATURE_LE = 0x454C
const IMAGE_REL_ALPHA_ABSOLUTE = 0x0000
const IMAGE_REL_ALPHA_BRADDR = 0x0007
const IMAGE_REL_ALPHA_GPDISP = 0x0006
const IMAGE_REL_ALPHA_GPREL32 = 0x0003
const IMAGE_REL_ALPHA_GPRELHI = 0x0017
const IMAGE_REL_ALPHA_GPRELLO = 0x0016
const IMAGE_REL_ALPHA_HINT = 0x0008
const IMAGE_REL_ALPHA_INLINE_REFLONG = 0x0009
const IMAGE_REL_ALPHA_LITERAL = 0x0004
const IMAGE_REL_ALPHA_LITUSE = 0x0005
const IMAGE_REL_ALPHA_MATCH = 0x000D
const IMAGE_REL_ALPHA_PAIR = 0x000C
const IMAGE_REL_ALPHA_REFHI = 0x000A
const IMAGE_REL_ALPHA_REFLO = 0x000B
const IMAGE_REL_ALPHA_REFLONG = 0x0001
const IMAGE_REL_ALPHA_REFLONGNB = 0x0010
const IMAGE_REL_ALPHA_REFQ1 = 0x0015
const IMAGE_REL_ALPHA_REFQ2 = 0x0014
const IMAGE_REL_ALPHA_REFQ3 = 0x0013
const IMAGE_REL_ALPHA_REFQUAD = 0x0002
const IMAGE_REL_ALPHA_SECREL = 0x000F
const IMAGE_REL_ALPHA_SECRELHI = 0x0012
const IMAGE_REL_ALPHA_SECRELLO = 0x0011
const IMAGE_REL_ALPHA_SECTION = 0x000E
const IMAGE_REL_AMD64_ABSOLUTE = 0x0000
const IMAGE_REL_AMD64_ADDR32 = 0x0002
const IMAGE_REL_AMD64_ADDR32NB = 0x0003
const IMAGE_REL_AMD64_ADDR64 = 0x0001
const IMAGE_REL_AMD64_PAIR = 0x000F
const IMAGE_REL_AMD64_REL32 = 0x0004
const IMAGE_REL_AMD64_REL32_1 = 0x0005
const IMAGE_REL_AMD64_REL32_2 = 0x0006
const IMAGE_REL_AMD64_REL32_3 = 0x0007
const IMAGE_REL_AMD64_REL32_4 = 0x0008
const IMAGE_REL_AMD64_REL32_5 = 0x0009
const IMAGE_REL_AMD64_SECREL = 0x000B
const IMAGE_REL_AMD64_SECREL7 = 0x000C
const IMAGE_REL_AMD64_SECTION = 0x000A
const IMAGE_REL_AMD64_SREL32 = 0x000E
const IMAGE_REL_AMD64_SSPAN32 = 0x0010
const IMAGE_REL_AMD64_TOKEN = 0x000D
const IMAGE_REL_AM_ABSOLUTE = 0x0000
const IMAGE_REL_AM_ADDR32 = 0x0001
const IMAGE_REL_AM_ADDR32NB = 0x0002
const IMAGE_REL_AM_CALL32 = 0x0003
const IMAGE_REL_AM_FUNCINFO = 0x0004
const IMAGE_REL_AM_REL32_1 = 0x0005
const IMAGE_REL_AM_REL32_2 = 0x0006
const IMAGE_REL_AM_SECREL = 0x0007
const IMAGE_REL_AM_SECTION = 0x0008
const IMAGE_REL_AM_TOKEN = 0x0009
const IMAGE_REL_ARM_ABSOLUTE = 0x0000
const IMAGE_REL_ARM_ADDR32 = 0x0001
const IMAGE_REL_ARM_ADDR32NB = 0x0002
const IMAGE_REL_ARM_BLX11 = 0x0009
const IMAGE_REL_ARM_BLX23T = 0x0015
const IMAGE_REL_ARM_BLX24 = 0x0008
const IMAGE_REL_ARM_BRANCH11 = 0x0004
const IMAGE_REL_ARM_BRANCH20T = 0x0012
const IMAGE_REL_ARM_BRANCH24 = 0x0003
const IMAGE_REL_ARM_BRANCH24T = 0x0014
const IMAGE_REL_ARM_GPREL12 = 0x0006
const IMAGE_REL_ARM_GPREL7 = 0x0007
const IMAGE_REL_ARM_MOV32 = 0x0010
const IMAGE_REL_ARM_MOV32A = 0x0010
const IMAGE_REL_ARM_MOV32T = 0x0011
const IMAGE_REL_ARM_SECREL = 0x000F
const IMAGE_REL_ARM_SECTION = 0x000E
const IMAGE_REL_ARM_TOKEN = 0x0005
const IMAGE_REL_BASED_ABSOLUTE = 0
const IMAGE_REL_BASED_ARM_MOV32 = 5
const IMAGE_REL_BASED_DIR64 = 10
const IMAGE_REL_BASED_HIGH = 1
const IMAGE_REL_BASED_HIGHADJ = 4
const IMAGE_REL_BASED_HIGHLOW = 3
const IMAGE_REL_BASED_IA64_IMM64 = 9
const IMAGE_REL_BASED_LOW = 2
const IMAGE_REL_BASED_MIPS_JMPADDR = 5
const IMAGE_REL_BASED_MIPS_JMPADDR16 = 9
const IMAGE_REL_BASED_THUMB_MOV32 = 7
const IMAGE_REL_CEE_ABSOLUTE = 0x0000
const IMAGE_REL_CEE_ADDR32 = 0x0001
const IMAGE_REL_CEE_ADDR32NB = 0x0003
const IMAGE_REL_CEE_ADDR64 = 0x0002
const IMAGE_REL_CEE_SECREL = 0x0005
const IMAGE_REL_CEE_SECTION = 0x0004
const IMAGE_REL_CEE_TOKEN = 0x0006
const IMAGE_REL_CEF_ABSOLUTE = 0x0000
const IMAGE_REL_CEF_ADDR32 = 0x0001
const IMAGE_REL_CEF_ADDR32NB = 0x0003
const IMAGE_REL_CEF_ADDR64 = 0x0002
const IMAGE_REL_CEF_SECREL = 0x0005
const IMAGE_REL_CEF_SECTION = 0x0004
const IMAGE_REL_CEF_TOKEN = 0x0006
const IMAGE_REL_EBC_ABSOLUTE = 0x0000
const IMAGE_REL_EBC_ADDR32NB = 0x0001
const IMAGE_REL_EBC_REL32 = 0x0002
const IMAGE_REL_EBC_SECREL = 0x0004
const IMAGE_REL_EBC_SECTION = 0x0003
const IMAGE_REL_I386_ABSOLUTE = 0x0000
const IMAGE_REL_I386_DIR16 = 0x0001
const IMAGE_REL_I386_DIR32 = 0x0006
const IMAGE_REL_I386_DIR32NB = 0x0007
const IMAGE_REL_I386_REL16 = 0x0002
const IMAGE_REL_I386_REL32 = 0x0014
const IMAGE_REL_I386_SECREL = 0x000B
const IMAGE_REL_I386_SECREL7 = 0x000D
const IMAGE_REL_I386_SECTION = 0x000A
const IMAGE_REL_I386_SEG12 = 0x0009
const IMAGE_REL_I386_TOKEN = 0x000C
const IMAGE_REL_IA64_ABSOLUTE = 0x0000
const IMAGE_REL_IA64_ADDEND = 0x001F
const IMAGE_REL_IA64_DIR32 = 0x0004
const IMAGE_REL_IA64_DIR32NB = 0x0010
const IMAGE_REL_IA64_DIR64 = 0x0005
const IMAGE_REL_IA64_GPREL22 = 0x0009
const IMAGE_REL_IA64_GPREL32 = 0x001C
const IMAGE_REL_IA64_IMM14 = 0x0001
const IMAGE_REL_IA64_IMM22 = 0x0002
const IMAGE_REL_IA64_IMM64 = 0x0003
const IMAGE_REL_IA64_IMMGPREL64 = 0x001A
const IMAGE_REL_IA64_LTOFF22 = 0x000A
const IMAGE_REL_IA64_PCREL21B = 0x0006
const IMAGE_REL_IA64_PCREL21F = 0x0008
const IMAGE_REL_IA64_PCREL21M = 0x0007
const IMAGE_REL_IA64_PCREL60B = 0x0016
const IMAGE_REL_IA64_PCREL60F = 0x0017
const IMAGE_REL_IA64_PCREL60I = 0x0018
const IMAGE_REL_IA64_PCREL60M = 0x0019
const IMAGE_REL_IA64_PCREL60X = 0x0015
const IMAGE_REL_IA64_SECREL22 = 0x000C
const IMAGE_REL_IA64_SECREL32 = 0x000E
const IMAGE_REL_IA64_SECREL64I = 0x000D
const IMAGE_REL_IA64_SECTION = 0x000B
const IMAGE_REL_IA64_SREL14 = 0x0011
const IMAGE_REL_IA64_SREL22 = 0x0012
const IMAGE_REL_IA64_SREL32 = 0x0013
const IMAGE_REL_IA64_TOKEN = 0x001B
const IMAGE_REL_IA64_UREL32 = 0x0014
const IMAGE_REL_M32R_ABSOLUTE = 0x0000
const IMAGE_REL_M32R_ADDR24 = 0x0003
const IMAGE_REL_M32R_ADDR32 = 0x0001
const IMAGE_REL_M32R_ADDR32NB = 0x0002
const IMAGE_REL_M32R_GPREL16 = 0x0004
const IMAGE_REL_M32R_PAIR = 0x000B
const IMAGE_REL_M32R_PCREL16 = 0x0006
const IMAGE_REL_M32R_PCREL24 = 0x0005
const IMAGE_REL_M32R_PCREL8 = 0x0007
const IMAGE_REL_M32R_REFHALF = 0x0008
const IMAGE_REL_M32R_REFHI = 0x0009
const IMAGE_REL_M32R_REFLO = 0x000A
const IMAGE_REL_M32R_SECREL32 = 0x000D
const IMAGE_REL_M32R_SECTION = 0x000C
const IMAGE_REL_M32R_TOKEN = 0x000E
const IMAGE_REL_MIPS_ABSOLUTE = 0x0000
const IMAGE_REL_MIPS_GPREL = 0x0006
const IMAGE_REL_MIPS_JMPADDR = 0x0003
const IMAGE_REL_MIPS_JMPADDR16 = 0x0010
const IMAGE_REL_MIPS_LITERAL = 0x0007
const IMAGE_REL_MIPS_PAIR = 0x0025
const IMAGE_REL_MIPS_REFHALF = 0x0001
const IMAGE_REL_MIPS_REFHI = 0x0004
const IMAGE_REL_MIPS_REFLO = 0x0005
const IMAGE_REL_MIPS_REFWORD = 0x0002
const IMAGE_REL_MIPS_REFWORDNB = 0x0022
const IMAGE_REL_MIPS_SECREL = 0x000B
const IMAGE_REL_MIPS_SECRELHI = 0x000D
const IMAGE_REL_MIPS_SECRELLO = 0x000C
const IMAGE_REL_MIPS_SECTION = 0x000A
const IMAGE_REL_MIPS_TOKEN = 0x000E
const IMAGE_REL_PPC_ABSOLUTE = 0x0000
const IMAGE_REL_PPC_ADDR14 = 0x0005
const IMAGE_REL_PPC_ADDR16 = 0x0004
const IMAGE_REL_PPC_ADDR24 = 0x0003
const IMAGE_REL_PPC_ADDR32 = 0x0002
const IMAGE_REL_PPC_ADDR32NB = 0x000A
const IMAGE_REL_PPC_ADDR64 = 0x0001
const IMAGE_REL_PPC_BRNTAKEN = 0x0400
const IMAGE_REL_PPC_BRTAKEN = 0x0200
const IMAGE_REL_PPC_GPREL = 0x0015
const IMAGE_REL_PPC_IFGLUE = 0x000D
const IMAGE_REL_PPC_IMGLUE = 0x000E
const IMAGE_REL_PPC_NEG = 0x0100
const IMAGE_REL_PPC_PAIR = 0x0012
const IMAGE_REL_PPC_REFHI = 0x0010
const IMAGE_REL_PPC_REFLO = 0x0011
const IMAGE_REL_PPC_REL14 = 0x0007
const IMAGE_REL_PPC_REL24 = 0x0006
const IMAGE_REL_PPC_SECREL = 0x000B
const IMAGE_REL_PPC_SECREL16 = 0x000F
const IMAGE_REL_PPC_SECRELHI = 0x0014
const IMAGE_REL_PPC_SECRELLO = 0x0013
const IMAGE_REL_PPC_SECTION = 0x000C
const IMAGE_REL_PPC_TOCDEFN = 0x0800
const IMAGE_REL_PPC_TOCREL14 = 0x0009
const IMAGE_REL_PPC_TOCREL16 = 0x0008
const IMAGE_REL_PPC_TOKEN = 0x0016
const IMAGE_REL_PPC_TYPEMASK = 0x00FF
const IMAGE_REL_SH3_ABSOLUTE = 0x0000
const IMAGE_REL_SH3_DIRECT16 = 0x0001
const IMAGE_REL_SH3_DIRECT32 = 0x0002
const IMAGE_REL_SH3_DIRECT32_NB = 0x0010
const IMAGE_REL_SH3_DIRECT4 = 0x0006
const IMAGE_REL_SH3_DIRECT4_LONG = 0x0008
const IMAGE_REL_SH3_DIRECT4_WORD = 0x0007
const IMAGE_REL_SH3_DIRECT8 = 0x0003
const IMAGE_REL_SH3_DIRECT8_LONG = 0x0005
const IMAGE_REL_SH3_DIRECT8_WORD = 0x0004
const IMAGE_REL_SH3_GPREL4_LONG = 0x0011
const IMAGE_REL_SH3_PCREL12_WORD = 0x000B
const IMAGE_REL_SH3_PCREL8_LONG = 0x000A
const IMAGE_REL_SH3_PCREL8_WORD = 0x0009
const IMAGE_REL_SH3_SECREL = 0x000F
const IMAGE_REL_SH3_SECTION = 0x000E
const IMAGE_REL_SH3_SIZEOF_SECTION = 0x000D
const IMAGE_REL_SH3_STARTOF_SECTION = 0x000C
const IMAGE_REL_SH3_TOKEN = 0x0012
const IMAGE_REL_SHM_PAIR = 0x0018
const IMAGE_REL_SHM_PCRELPT = 0x0013
const IMAGE_REL_SHM_REFHALF = 0x0015
const IMAGE_REL_SHM_REFLO = 0x0014
const IMAGE_REL_SHM_RELHALF = 0x0017
const IMAGE_REL_SHM_RELLO = 0x0016
const IMAGE_REL_SH_NOMODE = 0x8000
const IMAGE_REL_THUMB_BLX23 = 0x0015
const IMAGE_REL_THUMB_BRANCH20 = 0x0012
const IMAGE_REL_THUMB_BRANCH24 = 0x0014
const IMAGE_REL_THUMB_MOV32 = 0x0011
const IMAGE_RESOURCE_DATA_IS_DIRECTORY = 0x80000000
const IMAGE_RESOURCE_NAME_IS_STRING = 0x80000000
const IMAGE_ROM_OPTIONAL_HDR_MAGIC = 0x107
const IMAGE_SCN_ALIGN_1024BYTES = 0x00B00000
const IMAGE_SCN_ALIGN_128BYTES = 0x00800000
const IMAGE_SCN_ALIGN_16BYTES = 0x00500000
const IMAGE_SCN_ALIGN_1BYTES = 0x00100000
const IMAGE_SCN_ALIGN_2048BYTES = 0x00C00000
const IMAGE_SCN_ALIGN_256BYTES = 0x00900000
const IMAGE_SCN_ALIGN_2BYTES = 0x00200000
const IMAGE_SCN_ALIGN_32BYTES = 0x00600000
const IMAGE_SCN_ALIGN_4096BYTES = 0x00D00000
const IMAGE_SCN_ALIGN_4BYTES = 0x00300000
const IMAGE_SCN_ALIGN_512BYTES = 0x00A00000
const IMAGE_SCN_ALIGN_64BYTES = 0x00700000
const IMAGE_SCN_ALIGN_8192BYTES = 0x00E00000
const IMAGE_SCN_ALIGN_8BYTES = 0x00400000
const IMAGE_SCN_ALIGN_MASK = 0x00F00000
const IMAGE_SCN_CNT_CODE = 0x00000020
const IMAGE_SCN_CNT_INITIALIZED_DATA = 0x00000040
const IMAGE_SCN_CNT_UNINITIALIZED_DATA = 0x00000080
const IMAGE_SCN_GPREL = 0x00008000
const IMAGE_SCN_LNK_COMDAT = 0x00001000
const IMAGE_SCN_LNK_INFO = 0x00000200
const IMAGE_SCN_LNK_NRELOC_OVFL = 0x01000000
const IMAGE_SCN_LNK_OTHER = 0x00000100
const IMAGE_SCN_LNK_REMOVE = 0x00000800
const IMAGE_SCN_MEM_16BIT = 0x00020000
const IMAGE_SCN_MEM_DISCARDABLE = 0x02000000
const IMAGE_SCN_MEM_EXECUTE = 0x20000000
const IMAGE_SCN_MEM_FARDATA = 0x00008000
const IMAGE_SCN_MEM_LOCKED = 0x00040000
const IMAGE_SCN_MEM_NOT_CACHED = 0x04000000
const IMAGE_SCN_MEM_NOT_PAGED = 0x08000000
const IMAGE_SCN_MEM_PRELOAD = 0x00080000
const IMAGE_SCN_MEM_PURGEABLE = 0x00020000
const IMAGE_SCN_MEM_READ = 0x40000000
const IMAGE_SCN_MEM_SHARED = 0x10000000
const IMAGE_SCN_MEM_WRITE = 0x80000000
const IMAGE_SCN_NO_DEFER_SPEC_EXC = 0x00004000
const IMAGE_SCN_SCALE_INDEX = 0x00000001
const IMAGE_SCN_TYPE_NO_PAD = 0x00000008
const IMAGE_SEPARATE_DEBUG_FLAGS_MASK = 0x8000
const IMAGE_SEPARATE_DEBUG_MISMATCH = 0x8000
const IMAGE_SEPARATE_DEBUG_SIGNATURE = 0x4944
const IMAGE_SIZEOF_ARCHIVE_MEMBER_HDR = 60
const IMAGE_SIZEOF_AUX_SYMBOL = 18
const IMAGE_SIZEOF_BASE_RELOCATION = 8
const IMAGE_SIZEOF_FILE_HEADER = 20
const IMAGE_SIZEOF_LINENUMBER = 6
const IMAGE_SIZEOF_NT_OPTIONAL32_HEADER = 224
const IMAGE_SIZEOF_NT_OPTIONAL64_HEADER = 240
const IMAGE_SIZEOF_NT_OPTIONAL_HEADER = "IMAGE_SIZEOF_NT_OPTIONAL64_HEADER"
const IMAGE_SIZEOF_RELOCATION = 10
const IMAGE_SIZEOF_ROM_OPTIONAL_HEADER = 56
const IMAGE_SIZEOF_SECTION_HEADER = 40
const IMAGE_SIZEOF_SHORT_NAME = 8
const IMAGE_SIZEOF_STD_OPTIONAL_HEADER = 28
const IMAGE_SIZEOF_SYMBOL = 18
const IMAGE_SUBSYSTEM_EFI_APPLICATION = 10
const IMAGE_SUBSYSTEM_EFI_BOOT_SERVICE_DRIVER = 11
const IMAGE_SUBSYSTEM_EFI_ROM = 13
const IMAGE_SUBSYSTEM_EFI_RUNTIME_DRIVER = 12
const IMAGE_SUBSYSTEM_NATIVE = 1
const IMAGE_SUBSYSTEM_NATIVE_WINDOWS = 8
const IMAGE_SUBSYSTEM_OS2_CUI = 5
const IMAGE_SUBSYSTEM_POSIX_CUI = 7
const IMAGE_SUBSYSTEM_UNKNOWN = 0
const IMAGE_SUBSYSTEM_WINDOWS_BOOT_APPLICATION = 16
const IMAGE_SUBSYSTEM_WINDOWS_CE_GUI = 9
const IMAGE_SUBSYSTEM_WINDOWS_CUI = 3
const IMAGE_SUBSYSTEM_WINDOWS_GUI = 2
const IMAGE_SUBSYSTEM_XBOX = 14
const IMAGE_SYM_CLASS_ARGUMENT = 0x0009
const IMAGE_SYM_CLASS_AUTOMATIC = 0x0001
const IMAGE_SYM_CLASS_BIT_FIELD = 0x0012
const IMAGE_SYM_CLASS_BLOCK = 0x0064
const IMAGE_SYM_CLASS_CLR_TOKEN = 0x006B
const IMAGE_SYM_CLASS_END_OF_STRUCT = 0x0066
const IMAGE_SYM_CLASS_ENUM_TAG = 0x000F
const IMAGE_SYM_CLASS_EXTERNAL = 0x0002
const IMAGE_SYM_CLASS_EXTERNAL_DEF = 0x0005
const IMAGE_SYM_CLASS_FAR_EXTERNAL = 0x0044
const IMAGE_SYM_CLASS_FILE = 0x0067
const IMAGE_SYM_CLASS_FUNCTION = 0x0065
const IMAGE_SYM_CLASS_LABEL = 0x0006
const IMAGE_SYM_CLASS_MEMBER_OF_ENUM = 0x0010
const IMAGE_SYM_CLASS_MEMBER_OF_STRUCT = 0x0008
const IMAGE_SYM_CLASS_MEMBER_OF_UNION = 0x000B
const IMAGE_SYM_CLASS_NULL = 0x0000
const IMAGE_SYM_CLASS_REGISTER = 0x0004
const IMAGE_SYM_CLASS_REGISTER_PARAM = 0x0011
const IMAGE_SYM_CLASS_SECTION = 0x0068
const IMAGE_SYM_CLASS_STATIC = 0x0003
const IMAGE_SYM_CLASS_STRUCT_TAG = 0x000A
const IMAGE_SYM_CLASS_TYPE_DEFINITION = 0x000D
const IMAGE_SYM_CLASS_UNDEFINED_LABEL = 0x0007
const IMAGE_SYM_CLASS_UNDEFINED_STATIC = 0x000E
const IMAGE_SYM_CLASS_UNION_TAG = 0x000C
const IMAGE_SYM_CLASS_WEAK_EXTERNAL = 0x0069
const IMAGE_SYM_DTYPE_ARRAY = 3
const IMAGE_SYM_DTYPE_FUNCTION = 2
const IMAGE_SYM_DTYPE_NULL = 0
const IMAGE_SYM_DTYPE_POINTER = 1
const IMAGE_SYM_SECTION_MAX = 0xFEFF
const IMAGE_SYM_SECTION_MAX_EX = "MAXLONG"
const IMAGE_SYM_TYPE_BYTE = 0x000C
const IMAGE_SYM_TYPE_CHAR = 0x0002
const IMAGE_SYM_TYPE_DOUBLE = 0x0007
const IMAGE_SYM_TYPE_DWORD = 0x000F
const IMAGE_SYM_TYPE_ENUM = 0x000A
const IMAGE_SYM_TYPE_FLOAT = 0x0006
const IMAGE_SYM_TYPE_INT = 0x0004
const IMAGE_SYM_TYPE_LONG = 0x0005
const IMAGE_SYM_TYPE_MOE = 0x000B
const IMAGE_SYM_TYPE_NULL = 0x0000
const IMAGE_SYM_TYPE_PCODE = 0x8000
const IMAGE_SYM_TYPE_SHORT = 0x0003
const IMAGE_SYM_TYPE_STRUCT = 0x0008
const IMAGE_SYM_TYPE_UINT = 0x000E
const IMAGE_SYM_TYPE_UNION = 0x0009
const IMAGE_SYM_TYPE_VOID = 0x0001
const IMAGE_SYM_TYPE_WORD = 0x000D
const IMAGE_VXD_SIGNATURE = 0x454C
const IMAGE_WEAK_EXTERN_SEARCH_ALIAS = 3
const IMAGE_WEAK_EXTERN_SEARCH_LIBRARY = 2
const IMAGE_WEAK_EXTERN_SEARCH_NOLIBRARY = 1
const IMC_CLOSESTATUSWINDOW = 0x0021
const IMC_GETCANDIDATEPOS = 0x0007
const IMC_GETCOMPOSITIONFONT = 0x0009
const IMC_GETCOMPOSITIONWINDOW = 0x000B
const IMC_GETSTATUSWINDOWPOS = 0x000F
const IMC_OPENSTATUSWINDOW = 0x0022
const IMC_SETCANDIDATEPOS = 0x0008
const IMC_SETCOMPOSITIONFONT = 0x000A
const IMC_SETCOMPOSITIONWINDOW = 0x000C
const IMC_SETSTATUSWINDOWPOS = 0x0010
const IMEMENUITEM_STRING_SIZE = 80
const IMEVER_0310 = 0x0003000A
const IMEVER_0400 = 0x00040000
const IME_CAND_CODE = 0x0002
const IME_CAND_MEANING = 0x0003
const IME_CAND_RADICAL = 0x0004
const IME_CAND_READ = 0x0001
const IME_CAND_STROKE = 0x0005
const IME_CAND_UNKNOWN = 0x0000
const IME_CHOTKEY_IME_NONIME_TOGGLE = 0x10
const IME_CHOTKEY_SHAPE_TOGGLE = 0x11
const IME_CHOTKEY_SYMBOL_TOGGLE = 0x12
const IME_CMODE_ALPHANUMERIC = 0x0000
const IME_CMODE_CHARCODE = 0x0020
const IME_CMODE_CHINESE = "IME_CMODE_NATIVE"
const IME_CMODE_EUDC = 0x0200
const IME_CMODE_FIXED = 0x0800
const IME_CMODE_FULLSHAPE = 0x0008
const IME_CMODE_HANGEUL = "IME_CMODE_NATIVE"
const IME_CMODE_HANGUL = "IME_CMODE_NATIVE"
const IME_CMODE_HANJACONVERT = 0x0040
const IME_CMODE_JAPANESE = "IME_CMODE_NATIVE"
const IME_CMODE_KATAKANA = 0x0002
const IME_CMODE_LANGUAGE = 0x0003
const IME_CMODE_NATIVE = 0x0001
const IME_CMODE_NOCONVERSION = 0x0100
const IME_CMODE_RESERVED = 0xF0000000
const IME_CMODE_ROMAN = 0x0010
const IME_CMODE_SOFTKBD = 0x0080
const IME_CMODE_SYMBOL = 0x0400
const IME_CONFIG_GENERAL = 1
const IME_CONFIG_REGISTERWORD = 2
const IME_CONFIG_SELECTDICTIONARY = 3
const IME_ESC_AUTOMATA = 0x1009
const IME_ESC_GETHELPFILENAME = 0x100b
const IME_ESC_GET_EUDC_DICTIONARY = 0x1003
const IME_ESC_HANJA_MODE = 0x1008
const IME_ESC_IME_NAME = 0x1006
const IME_ESC_MAX_KEY = 0x1005
const IME_ESC_PRIVATE_FIRST = 0x0800
const IME_ESC_PRIVATE_HOTKEY = 0x100a
const IME_ESC_PRIVATE_LAST = 0x0FFF
const IME_ESC_QUERY_SUPPORT = 0x0003
const IME_ESC_RESERVED_FIRST = 0x0004
const IME_ESC_RESERVED_LAST = 0x07FF
const IME_ESC_SEQUENCE_TO_INTERNAL = 0x1001
const IME_ESC_SET_EUDC_DICTIONARY = 0x1004
const IME_ESC_SYNC_HOTKEY = 0x1007
const IME_HOTKEY_DSWITCH_FIRST = 0x100
const IME_HOTKEY_DSWITCH_LAST = 0x11F
const IME_HOTKEY_PRIVATE_FIRST = 0x200
const IME_HOTKEY_PRIVATE_LAST = 0x21F
const IME_ITHOTKEY_PREVIOUS_COMPOSITION = 0x201
const IME_ITHOTKEY_RECONVERTSTRING = 0x203
const IME_ITHOTKEY_RESEND_RESULTSTR = 0x200
const IME_ITHOTKEY_UISTYLE_TOGGLE = 0x202
const IME_JHOTKEY_CLOSE_OPEN = 0x30
const IME_KHOTKEY_ENGLISH = 0x52
const IME_KHOTKEY_HANJACONVERT = 0x51
const IME_KHOTKEY_SHAPE_TOGGLE = 0x50
const IME_PROP_AT_CARET = 0x00010000
const IME_PROP_CANDLIST_START_FROM_1 = 0x00040000
const IME_PROP_COMPLETE_ON_UNSELECT = 0x00100000
const IME_PROP_SPECIAL_UI = 0x00020000
const IME_PROP_UNICODE = 0x00080000
const IME_REGWORD_STYLE_EUDC = 0x00000001
const IME_REGWORD_STYLE_USER_FIRST = 0x80000000
const IME_REGWORD_STYLE_USER_LAST = 0xFFFFFFFF
const IME_SMODE_AUTOMATIC = 0x0004
const IME_SMODE_CONVERSATION = 0x0010
const IME_SMODE_NONE = 0x0000
const IME_SMODE_PHRASEPREDICT = 0x0008
const IME_SMODE_PLAURALCLAUSE = 0x0001
const IME_SMODE_RESERVED = 0x0000F000
const IME_SMODE_SINGLECONVERT = 0x0002
const IME_THOTKEY_IME_NONIME_TOGGLE = 0x70
const IME_THOTKEY_SHAPE_TOGGLE = 0x71
const IME_THOTKEY_SYMBOL_TOGGLE = 0x72
const IMFS_CHECKED = "MFS_CHECKED"
const IMFS_DEFAULT = "MFS_DEFAULT"
const IMFS_DISABLED = "MFS_DISABLED"
const IMFS_ENABLED = "MFS_ENABLED"
const IMFS_GRAYED = "MFS_GRAYED"
const IMFS_HILITE = "MFS_HILITE"
const IMFS_UNCHECKED = "MFS_UNCHECKED"
const IMFS_UNHILITE = "MFS_UNHILITE"
const IMFT_RADIOCHECK = 0x00001
const IMFT_SEPARATOR = 0x00002
const IMFT_SUBMENU = 0x00004
const IMN_CHANGECANDIDATE = 0x0003
const IMN_CLOSECANDIDATE = 0x0004
const IMN_CLOSESTATUSWINDOW = 0x0001
const IMN_GUIDELINE = 0x000D
const IMN_OPENCANDIDATE = 0x0005
const IMN_OPENSTATUSWINDOW = 0x0002
const IMN_PRIVATE = 0x000E
const IMN_SETCANDIDATEPOS = 0x0009
const IMN_SETCOMPOSITIONFONT = 0x000A
const IMN_SETCOMPOSITIONWINDOW = 0x000B
const IMN_SETCONVERSIONMODE = 0x0006
const IMN_SETOPENSTATUS = 0x0008
const IMN_SETSENTENCEMODE = 0x0007
const IMN_SETSTATUSWINDOWPOS = 0x000C
const IMPORT_OBJECT_HDR_SIG2 = 0xffff
const IMR_CANDIDATEWINDOW = 0x0002
const IMR_COMPOSITIONFONT = 0x0003
const IMR_COMPOSITIONWINDOW = 0x0001
const IMR_CONFIRMRECONVERTSTRING = 0x0005
const IMR_DOCUMENTFEED = 0x0007
const IMR_QUERYCHARPOSITION = 0x0006
const IMR_RECONVERTSTRING = 0x0004
const INDEXID_CONTAINER = 0
const INDEXID_OBJECT = 0
const INFINITE = 4294967295
const INHERITED_ACE = 0x10
const INHERIT_CALLER_PRIORITY = 0x20000
const INHERIT_ONLY_ACE = 0x8
const INHERIT_PARENT_AFFINITY = 0x10000
const INITIAL_FPCSR = 0x027f
const INITIAL_MXCSR = 0x1f80
const INIT_ONCE_ASYNC = "RTL_RUN_ONCE_ASYNC"
const INIT_ONCE_CHECK_ONLY = "RTL_RUN_ONCE_CHECK_ONLY"
const INIT_ONCE_CTX_RESERVED_BITS = "RTL_RUN_ONCE_CTX_RESERVED_BITS"
const INIT_ONCE_INIT_FAILED = "RTL_RUN_ONCE_INIT_FAILED"
const INIT_ONCE_STATIC_INIT = "RTL_RUN_ONCE_INIT"
const INLINE = "__inline"
const INLINE_KEYWORD = "inline"
const INPUTLANGCHANGE_BACKWARD = 0x0004
const INPUTLANGCHANGE_FORWARD = 0x0002
const INPUTLANGCHANGE_SYSCHARSET = 0x0001
const INPUT_HARDWARE = 2
const INPUT_KEYBOARD = 1
const INPUT_MOUSE = 0
const INT16_MAX = 32767
const INT32_MAX = 2147483647
const INT64_MAX = 9223372036854775807
const INT8_MAX = 127
const INTMAX_MAX = "INT64_MAX"
const INTMAX_MIN = "INT64_MIN"
const INTPTR_MAX = "INT64_MAX"
const INTPTR_MIN = "INT64_MIN"
const INT_FAST16_MAX = "INT16_MAX"
const INT_FAST16_MIN = "INT16_MIN"
const INT_FAST32_MAX = "INT32_MAX"
const INT_FAST32_MIN = "INT32_MIN"
const INT_FAST64_MAX = "INT64_MAX"
const INT_FAST64_MIN = "INT64_MIN"
const INT_FAST8_MAX = "INT8_MAX"
const INT_FAST8_MIN = "INT8_MIN"
const INT_LEAST16_MAX = "INT16_MAX"
const INT_LEAST16_MIN = "INT16_MIN"
const INT_LEAST32_MAX = "INT32_MAX"
const INT_LEAST32_MIN = "INT32_MIN"
const INT_LEAST64_MAX = "INT64_MAX"
const INT_LEAST64_MIN = "INT64_MIN"
const INT_LEAST8_MAX = "INT8_MAX"
const INT_LEAST8_MIN = "INT8_MIN"
const INVALID_OS_COUNT = 0xffff
const IO_COMPLETION_MODIFY_STATE = 0x0002
const IO_REPARSE_TAG_RESERVED_ONE = 1
const IO_REPARSE_TAG_RESERVED_RANGE = "IO_REPARSE_TAG_RESERVED_TWO"
const IO_REPARSE_TAG_RESERVED_TWO = 2
const IO_REPARSE_TAG_RESERVED_ZERO = 0
const ISC_SHOWUIALL = 0xC000000F
const ISC_SHOWUIALLCANDIDATEWINDOW = 0x0000000F
const ISC_SHOWUICANDIDATEWINDOW = 0x00000001
const ISC_SHOWUICOMPOSITIONWINDOW = 0x80000000
const ISC_SHOWUIGUIDELINE = 0x40000000
const ISMEX_CALLBACK = 0x00000004
const ISMEX_NOSEND = 0x00000000
const ISMEX_NOTIFY = 0x00000002
const ISMEX_REPLIED = 0x00000008
const ISMEX_SEND = 0x00000001
const IS_TEXT_UNICODE_ASCII16 = 0x0001
const IS_TEXT_UNICODE_CONTROLS = 0x0004
const IS_TEXT_UNICODE_DBCS_LEADBYTE = 0x0400
const IS_TEXT_UNICODE_ILLEGAL_CHARS = 0x0100
const IS_TEXT_UNICODE_NOT_ASCII_MASK = 0xF000
const IS_TEXT_UNICODE_NOT_UNICODE_MASK = 0x0F00
const IS_TEXT_UNICODE_NULL_BYTES = 0x1000
const IS_TEXT_UNICODE_ODD_LENGTH = 0x0200
const IS_TEXT_UNICODE_REVERSE_ASCII16 = 0x0010
const IS_TEXT_UNICODE_REVERSE_CONTROLS = 0x0040
const IS_TEXT_UNICODE_REVERSE_MASK = 0x00F0
const IS_TEXT_UNICODE_REVERSE_SIGNATURE = 0x0080
const IS_TEXT_UNICODE_REVERSE_STATISTICS = 0x0020
const IS_TEXT_UNICODE_SIGNATURE = 0x0008
const IS_TEXT_UNICODE_STATISTICS = 0x0002
const IS_TEXT_UNICODE_UNICODE_MASK = 0x000F
const InterlockedAdd = "_InterlockedAdd"
const InterlockedAdd64 = "_InterlockedAdd64"
const InterlockedAnd = "_InterlockedAnd"
const InterlockedAnd16 = "_InterlockedAnd16"
const InterlockedAnd64 = "_InterlockedAnd64"
const InterlockedAnd8 = "_InterlockedAnd8"
const InterlockedAndAffinity = "InterlockedAnd64"
const InterlockedCompareExchange = "_InterlockedCompareExchange"
const InterlockedCompareExchange16 = "_InterlockedCompareExchange16"
const InterlockedCompareExchange64 = "_InterlockedCompareExchange64"
const InterlockedCompareExchangeAcquire = "InterlockedCompareExchange"
const InterlockedCompareExchangeAcquire64 = "InterlockedCompareExchange64"
const InterlockedCompareExchangePointer = "_InterlockedCompareExchangePointer"
const InterlockedCompareExchangePointerAcquire = "_InterlockedCompareExchangePointer"
const InterlockedCompareExchangePointerRelease = "_InterlockedCompareExchangePointer"
const InterlockedCompareExchangeRelease = "InterlockedCompareExchange"
const InterlockedCompareExchangeRelease64 = "InterlockedCompareExchange64"
const InterlockedDecrement = "_InterlockedDecrement"
const InterlockedDecrement16 = "_InterlockedDecrement16"
const InterlockedDecrement64 = "_InterlockedDecrement64"
const InterlockedDecrementAcquire = "InterlockedDecrement"
const InterlockedDecrementRelease = "InterlockedDecrement"
const InterlockedExchange = "_InterlockedExchange"
const InterlockedExchange64 = "_InterlockedExchange64"
const InterlockedExchangeAcquire64 = "InterlockedExchange64"
const InterlockedExchangeAdd = "_InterlockedExchangeAdd"
const InterlockedExchangeAdd64 = "_InterlockedExchangeAdd64"
const InterlockedExchangePointer = "_InterlockedExchangePointer"
const InterlockedIncrement = "_InterlockedIncrement"
const InterlockedIncrement16 = "_InterlockedIncrement16"
const InterlockedIncrement64 = "_InterlockedIncrement64"
const InterlockedIncrementAcquire = "InterlockedIncrement"
const InterlockedIncrementRelease = "InterlockedIncrement"
const InterlockedOr = "_InterlockedOr"
const InterlockedOr16 = "_InterlockedOr16"
const InterlockedOr64 = "_InterlockedOr64"
const InterlockedOr8 = "_InterlockedOr8"
const InterlockedOrAffinity = "InterlockedOr64"
const InterlockedXor = "_InterlockedXor"
const InterlockedXor16 = "_InterlockedXor16"
const InterlockedXor64 = "_InterlockedXor64"
const InterlockedXor8 = "_InterlockedXor8"
const JOB_OBJECT_ASSIGN_PROCESS = 0x0001
const JOB_OBJECT_BASIC_LIMIT_VALID_FLAGS = 0x000000ff
const JOB_OBJECT_CPU_RATE_CONTROL_ENABLE = 0x1
const JOB_OBJECT_CPU_RATE_CONTROL_HARD_CAP = 0x4
const JOB_OBJECT_CPU_RATE_CONTROL_MIN_MAX_RATE = 0x10
const JOB_OBJECT_CPU_RATE_CONTROL_NOTIFY = 0x8
const JOB_OBJECT_CPU_RATE_CONTROL_VALID_FLAGS = 0x1f
const JOB_OBJECT_CPU_RATE_CONTROL_WEIGHT_BASED = 0x2
const JOB_OBJECT_EXTENDED_LIMIT_VALID_FLAGS = 0x00007fff
const JOB_OBJECT_IMPERSONATE = 0x0020
const JOB_OBJECT_LIMIT_ACTIVE_PROCESS = 0x00000008
const JOB_OBJECT_LIMIT_AFFINITY = 0x00000010
const JOB_OBJECT_LIMIT_BREAKAWAY_OK = 0x00000800
const JOB_OBJECT_LIMIT_CPU_RATE_CONTROL = "JOB_OBJECT_LIMIT_RATE_CONTROL"
const JOB_OBJECT_LIMIT_DIE_ON_UNHANDLED_EXCEPTION = 0x00000400
const JOB_OBJECT_LIMIT_IO_RATE_CONTROL = 0x00080000
const JOB_OBJECT_LIMIT_JOB_MEMORY = 0x00000200
const JOB_OBJECT_LIMIT_JOB_MEMORY_HIGH = "JOB_OBJECT_LIMIT_JOB_MEMORY"
const JOB_OBJECT_LIMIT_JOB_MEMORY_LOW = 0x00008000
const JOB_OBJECT_LIMIT_JOB_READ_BYTES = 0x00010000
const JOB_OBJECT_LIMIT_JOB_TIME = 0x00000004
const JOB_OBJECT_LIMIT_JOB_WRITE_BYTES = 0x00020000
const JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE = 0x00002000
const JOB_OBJECT_LIMIT_NET_RATE_CONTROL = 0x00100000
const JOB_OBJECT_LIMIT_PRESERVE_JOB_TIME = 0x00000040
const JOB_OBJECT_LIMIT_PRIORITY_CLASS = 0x00000020
const JOB_OBJECT_LIMIT_PROCESS_MEMORY = 0x00000100
const JOB_OBJECT_LIMIT_PROCESS_TIME = 0x00000002
const JOB_OBJECT_LIMIT_RATE_CONTROL = 0x00040000
const JOB_OBJECT_LIMIT_RESERVED3 = 0x00008000
const JOB_OBJECT_LIMIT_RESERVED4 = 0x00010000
const JOB_OBJECT_LIMIT_RESERVED5 = 0x00020000
const JOB_OBJECT_LIMIT_RESERVED6 = 0x00040000
const JOB_OBJECT_LIMIT_SCHEDULING_CLASS = 0x00000080
const JOB_OBJECT_LIMIT_SILENT_BREAKAWAY_OK = 0x00001000
const JOB_OBJECT_LIMIT_SUBSET_AFFINITY = 0x00004000
const JOB_OBJECT_LIMIT_VALID_FLAGS = 0x0007ffff
const JOB_OBJECT_LIMIT_WORKINGSET = 0x00000001
const JOB_OBJECT_MSG_ABNORMAL_EXIT_PROCESS = 8
const JOB_OBJECT_MSG_ACTIVE_PROCESS_LIMIT = 3
const JOB_OBJECT_MSG_ACTIVE_PROCESS_ZERO = 4
const JOB_OBJECT_MSG_END_OF_JOB_TIME = 1
const JOB_OBJECT_MSG_END_OF_PROCESS_TIME = 2
const JOB_OBJECT_MSG_EXIT_PROCESS = 7
const JOB_OBJECT_MSG_JOB_CYCLE_TIME_LIMIT = 12
const JOB_OBJECT_MSG_JOB_MEMORY_LIMIT = 10
const JOB_OBJECT_MSG_MAXIMUM = 12
const JOB_OBJECT_MSG_MINIMUM = 1
const JOB_OBJECT_MSG_NEW_PROCESS = 6
const JOB_OBJECT_MSG_NOTIFICATION_LIMIT = 11
const JOB_OBJECT_MSG_PROCESS_MEMORY_LIMIT = 9
const JOB_OBJECT_MSG_SILO_TERMINATED = 13
const JOB_OBJECT_NET_RATE_CONTROL_MAX_DSCP_TAG = 64
const JOB_OBJECT_NOTIFICATION_LIMIT_VALID_FLAGS = 0x00070204
const JOB_OBJECT_POST_AT_END_OF_JOB = 1
const JOB_OBJECT_QUERY = 0x0004
const JOB_OBJECT_RESERVED_LIMIT_VALID_FLAGS = 0x0007ffff
const JOB_OBJECT_SECURITY_FILTER_TOKENS = 0x00000008
const JOB_OBJECT_SECURITY_NO_ADMIN = 0x00000001
const JOB_OBJECT_SECURITY_ONLY_TOKEN = 0x00000004
const JOB_OBJECT_SECURITY_RESTRICTED_TOKEN = 0x00000002
const JOB_OBJECT_SECURITY_VALID_FLAGS = 0x0000000f
const JOB_OBJECT_SET_ATTRIBUTES = 0x0002
const JOB_OBJECT_SET_SECURITY_ATTRIBUTES = 0x0010
const JOB_OBJECT_TERMINATE = 0x0008
const JOB_OBJECT_TERMINATE_AT_END_OF_JOB = 0
const JOB_OBJECT_UILIMIT_ALL = 0x000000FF
const JOB_OBJECT_UILIMIT_DESKTOP = 0x00000040
const JOB_OBJECT_UILIMIT_DISPLAYSETTINGS = 0x00000010
const JOB_OBJECT_UILIMIT_EXITWINDOWS = 0x00000080
const JOB_OBJECT_UILIMIT_GLOBALATOMS = 0x00000020
const JOB_OBJECT_UILIMIT_HANDLES = 0x00000001
const JOB_OBJECT_UILIMIT_NONE = 0x00000000
const JOB_OBJECT_UILIMIT_READCLIPBOARD = 0x00000002
const JOB_OBJECT_UILIMIT_SYSTEMPARAMETERS = 0x00000008
const JOB_OBJECT_UILIMIT_WRITECLIPBOARD = 0x00000004
const JOB_OBJECT_UI_VALID_FLAGS = 0x000000FF
const JOHAB_CHARSET = 130
const KEYBOARD_OVERRUN_MAKE_CODE = 0xFF
const KEYEVENTF_EXTENDEDKEY = 0x0001
const KEYEVENTF_KEYUP = 0x0002
const KEYEVENTF_SCANCODE = 0x0008
const KEYEVENTF_UNICODE = 0x0004
const KEY_CREATE_LINK = 0x0020
const KEY_CREATE_SUB_KEY = 0x0004
const KEY_ENUMERATE_SUB_KEYS = 0x0008
const KEY_EVENT = 0x0001
const KEY_NOTIFY = 0x0010
const KEY_QUERY_VALUE = 0x0001
const KEY_SET_VALUE = 0x0002
const KEY_WOW64_32KEY = 0x0200
const KEY_WOW64_64KEY = 0x0100
const KEY_WOW64_RES = 0x0300
const KF_ALTDOWN = 0x2000
const KF_DLGMODE = 0x0800
const KF_EXTENDED = 0x0100
const KF_MENUMODE = 0x1000
const KF_REPEAT = 0x4000
const KF_UP = 0x8000
const KLF_ACTIVATE = 0x00000001
const KLF_NOTELLSHELL = 0x00000080
const KLF_REORDER = 0x00000008
const KLF_REPLACELANG = 0x00000010
const KLF_RESET = 0x40000000
const KLF_SETFORPROCESS = 0x00000100
const KLF_SHIFTLOCK = 0x00010000
const KLF_SUBSTITUTE_OK = 0x00000002
const KL_NAMELENGTH = 9
const KNUTH = 2654435769
const KTM_MARSHAL_BLOB_VERSION_MAJOR = 1
const KTM_MARSHAL_BLOB_VERSION_MINOR = 1
const LANGGROUPLOCALE_ENUMPROC = "LANGGROUPLOCALE_ENUMPROCA"
const LANGUAGEGROUP_ENUMPROC = "LANGUAGEGROUP_ENUMPROCA"
const LANG_AFRIKAANS = 0x36
const LANG_ALBANIAN = 0x1c
const LANG_ALSATIAN = 0x84
const LANG_AMHARIC = 0x5e
const LANG_ARABIC = 0x01
const LANG_ARMENIAN = 0x2b
const LANG_ASSAMESE = 0x4d
const LANG_AZERBAIJANI = 0x2c
const LANG_AZERI = 0x2c
const LANG_BANGLA = 0x45
const LANG_BASHKIR = 0x6d
const LANG_BASQUE = 0x2d
const LANG_BELARUSIAN = 0x23
const LANG_BENGALI = 0x45
const LANG_BOSNIAN = 0x1a
const LANG_BOSNIAN_NEUTRAL = 0x781a
const LANG_BRETON = 0x7e
const LANG_BULGARIAN = 0x02
const LANG_CATALAN = 0x03
const LANG_CENTRAL_KURDISH = 0x92
const LANG_CHEROKEE = 0x5c
const LANG_CHINESE = 0x04
const LANG_CHINESE_SIMPLIFIED = 0x04
const LANG_CHINESE_TRADITIONAL = 0x7c04
const LANG_CORSICAN = 0x83
const LANG_CROATIAN = 0x1a
const LANG_CZECH = 0x05
const LANG_DANISH = 0x06
const LANG_DARI = 0x8c
const LANG_DIVEHI = 0x65
const LANG_DUTCH = 0x13
const LANG_ENGLISH = 0x09
const LANG_ESTONIAN = 0x25
const LANG_FAEROESE = 0x38
const LANG_FARSI = 0x29
const LANG_FILIPINO = 0x64
const LANG_FINNISH = 0x0b
const LANG_FRENCH = 0x0c
const LANG_FRISIAN = 0x62
const LANG_FULAH = 0x67
const LANG_GALICIAN = 0x56
const LANG_GEORGIAN = 0x37
const LANG_GERMAN = 0x07
const LANG_GREEK = 0x08
const LANG_GREENLANDIC = 0x6f
const LANG_GUJARATI = 0x47
const LANG_HAUSA = 0x68
const LANG_HAWAIIAN = 0x75
const LANG_HEBREW = 0x0d
const LANG_HINDI = 0x39
const LANG_HUNGARIAN = 0x0e
const LANG_ICELANDIC = 0x0f
const LANG_IGBO = 0x70
const LANG_INDONESIAN = 0x21
const LANG_INUKTITUT = 0x5d
const LANG_INVARIANT = 0x7f
const LANG_IRISH = 0x3c
const LANG_ITALIAN = 0x10
const LANG_JAPANESE = 0x11
const LANG_KANNADA = 0x4b
const LANG_KASHMIRI = 0x60
const LANG_KAZAK = 0x3f
const LANG_KHMER = 0x53
const LANG_KICHE = 0x86
const LANG_KINYARWANDA = 0x87
const LANG_KONKANI = 0x57
const LANG_KOREAN = 0x12
const LANG_KYRGYZ = 0x40
const LANG_LAO = 0x54
const LANG_LATVIAN = 0x26
const LANG_LITHUANIAN = 0x27
const LANG_LOWER_SORBIAN = 0x2e
const LANG_LUXEMBOURGISH = 0x6e
const LANG_MACEDONIAN = 0x2f
const LANG_MALAY = 0x3e
const LANG_MALAYALAM = 0x4c
const LANG_MALTESE = 0x3a
const LANG_MANIPURI = 0x58
const LANG_MAORI = 0x81
const LANG_MAPUDUNGUN = 0x7a
const LANG_MARATHI = 0x4e
const LANG_MOHAWK = 0x7c
const LANG_MONGOLIAN = 0x50
const LANG_NEPALI = 0x61
const LANG_NEUTRAL = 0x00
const LANG_NORWEGIAN = 0x14
const LANG_OCCITAN = 0x82
const LANG_ODIA = 0x48
const LANG_ORIYA = 0x48
const LANG_PASHTO = 0x63
const LANG_PERSIAN = 0x29
const LANG_POLISH = 0x15
const LANG_PORTUGUESE = 0x16
const LANG_PULAR = 0x67
const LANG_PUNJABI = 0x46
const LANG_QUECHUA = 0x6b
const LANG_ROMANIAN = 0x18
const LANG_ROMANSH = 0x17
const LANG_RUSSIAN = 0x19
const LANG_SAKHA = 0x85
const LANG_SAMI = 0x3b
const LANG_SANSKRIT = 0x4f
const LANG_SCOTTISH_GAELIC = 0x91
const LANG_SERBIAN = 0x1a
const LANG_SERBIAN_NEUTRAL = 0x7c1a
const LANG_SINDHI = 0x59
const LANG_SINHALESE = 0x5b
const LANG_SLOVAK = 0x1b
const LANG_SLOVENIAN = 0x24
const LANG_SOTHO = 0x6c
const LANG_SPANISH = 0x0a
const LANG_SWAHILI = 0x41
const LANG_SWEDISH = 0x1d
const LANG_SYRIAC = 0x5a
const LANG_TAJIK = 0x28
const LANG_TAMAZIGHT = 0x5f
const LANG_TAMIL = 0x49
const LANG_TATAR = 0x44
const LANG_TELUGU = 0x4a
const LANG_THAI = 0x1e
const LANG_TIBETAN = 0x51
const LANG_TIGRIGNA = 0x73
const LANG_TIGRINYA = 0x73
const LANG_TSWANA = 0x32
const LANG_TURKISH = 0x1f
const LANG_TURKMEN = 0x42
const LANG_UIGHUR = 0x80
const LANG_UKRAINIAN = 0x22
const LANG_UPPER_SORBIAN = 0x2e
const LANG_URDU = 0x20
const LANG_UZBEK = 0x43
const LANG_VALENCIAN = 0x03
const LANG_VIETNAMESE = 0x2a
const LANG_WELSH = 0x52
const LANG_WOLOF = 0x88
const LANG_XHOSA = 0x34
const LANG_YAKUT = 0x85
const LANG_YI = 0x78
const LANG_YORUBA = 0x6a
const LANG_ZULU = 0x35
const LAYOUT_BITMAPORIENTATIONPRESERVED = 0x00000008
const LAYOUT_BTT = 0x00000002
const LAYOUT_RTL = 0x00000001
const LAYOUT_VBH = 0x00000004
const LBN_DBLCLK = 2
const LBN_KILLFOCUS = 5
const LBN_SELCANCEL = 3
const LBN_SELCHANGE = 1
const LBN_SETFOCUS = 4
const LB_ADDFILE = 0x0196
const LB_ADDSTRING = 0x0180
const LB_DELETESTRING = 0x0182
const LB_DIR = 0x018D
const LB_FINDSTRING = 0x018F
const LB_FINDSTRINGEXACT = 0x01A2
const LB_GETANCHORINDEX = 0x019D
const LB_GETCARETINDEX = 0x019F
const LB_GETCOUNT = 0x018B
const LB_GETCURSEL = 0x0188
const LB_GETHORIZONTALEXTENT = 0x0193
const LB_GETITEMDATA = 0x0199
const LB_GETITEMHEIGHT = 0x01A1
const LB_GETITEMRECT = 0x0198
const LB_GETLISTBOXINFO = 0x01B2
const LB_GETLOCALE = 0x01A6
const LB_GETSEL = 0x0187
const LB_GETSELCOUNT = 0x0190
const LB_GETSELITEMS = 0x0191
const LB_GETTEXT = 0x0189
const LB_GETTEXTLEN = 0x018A
const LB_GETTOPINDEX = 0x018E
const LB_INITSTORAGE = 0x01A8
const LB_INSERTSTRING = 0x0181
const LB_ITEMFROMPOINT = 0x01A9
const LB_MSGMAX = 0x01B3
const LB_OKAY = 0
const LB_RESETCONTENT = 0x0184
const LB_SELECTSTRING = 0x018C
const LB_SELITEMRANGE = 0x019B
const LB_SELITEMRANGEEX = 0x0183
const LB_SETANCHORINDEX = 0x019C
const LB_SETCARETINDEX = 0x019E
const LB_SETCOLUMNWIDTH = 0x0195
const LB_SETCOUNT = 0x01A7
const LB_SETCURSEL = 0x0186
const LB_SETHORIZONTALEXTENT = 0x0194
const LB_SETITEMDATA = 0x019A
const LB_SETITEMHEIGHT = 0x01A0
const LB_SETLOCALE = 0x01A5
const LB_SETSEL = 0x0185
const LB_SETTABSTOPS = 0x0192
const LB_SETTOPINDEX = 0x0197
const LCID_ALTERNATE_SORTS = 0x00000004
const LCID_INSTALLED = 0x00000001
const LCID_SUPPORTED = 0x00000002
const LCMAP_BYTEREV = 0x00000800
const LCMAP_FULLWIDTH = 0x00800000
const LCMAP_HALFWIDTH = 0x00400000
const LCMAP_HIRAGANA = 0x00100000
const LCMAP_KATAKANA = 0x00200000
const LCMAP_LINGUISTIC_CASING = 0x01000000
const LCMAP_LOWERCASE = 0x00000100
const LCMAP_SIMPLIFIED_CHINESE = 0x02000000
const LCMAP_SORTKEY = 0x00000400
const LCMAP_TRADITIONAL_CHINESE = 0x04000000
const LCMAP_UPPERCASE = 0x00000200
const LCMapString = "LCMapStringA"
const LC_INTERIORS = 128
const LC_MARKER = 4
const LC_NONE = 0
const LC_POLYLINE = 2
const LC_POLYMARKER = 8
const LC_STYLED = 32
const LC_WIDE = 16
const LC_WIDESTYLED = 64
const LDM_BATCH_SIZE = 64
const LDM_BUCKET_SIZE_LOG = 4
const LDM_HASH_RLOG = 7
const LDM_MIN_MATCH_LENGTH = 64
const LEFT_ALT_PRESSED = 0x0002
const LEFT_CTRL_PRESSED = 0x0008
const LF_FACESIZE = 32
const LF_FULLFACESIZE = 64
const LGRPID_ARABIC = 0x000d
const LGRPID_ARMENIAN = 0x0011
const LGRPID_BALTIC = 0x0003
const LGRPID_CENTRAL_EUROPE = 0x0002
const LGRPID_CYRILLIC = 0x0005
const LGRPID_GEORGIAN = 0x0010
const LGRPID_GREEK = 0x0004
const LGRPID_HEBREW = 0x000c
const LGRPID_INDIC = 0x000f
const LGRPID_INSTALLED = 0x00000001
const LGRPID_JAPANESE = 0x0007
const LGRPID_KOREAN = 0x0008
const LGRPID_SIMPLIFIED_CHINESE = 0x000a
const LGRPID_SUPPORTED = 0x00000002
const LGRPID_THAI = 0x000b
const LGRPID_TRADITIONAL_CHINESE = 0x0009
const LGRPID_TURKIC = 0x0006
const LGRPID_TURKISH = 0x0006
const LGRPID_VIETNAMESE = 0x000e
const LGRPID_WESTERN_EUROPE = 0x0001
const LINECAPS = 30
const LINGUISTIC_IGNORECASE = 0x00000010
const LINGUISTIC_IGNOREDIACRITIC = 0x00000020
const LLFSELog = 9
const LLIMIT = 64
const LLKHF_INJECTED = 0x00000010
const LLKHF_LOWER_IL_INJECTED = 0x00000002
const LLMHF_INJECTED = 0x00000001
const LLMHF_LOWER_IL_INJECTED = 0x00000002
const LL_DEFAULTNORMLOG = 6
const LMEM_DISCARDABLE = 0xf00
const LMEM_DISCARDED = 0x4000
const LMEM_FIXED = 0x0
const LMEM_INVALID_HANDLE = 0x8000
const LMEM_LOCKCOUNT = 0xff
const LMEM_MODIFY = 0x80
const LMEM_MOVEABLE = 0x2
const LMEM_NOCOMPACT = 0x10
const LMEM_NODISCARD = 0x20
const LMEM_VALID_FLAGS = 0xf72
const LMEM_ZEROINIT = 0x40
const LOAD_DLL_DEBUG_EVENT = 6
const LOAD_IGNORE_CODE_AUTHZ_LEVEL = 0x10
const LOAD_LIBRARY_AS_DATAFILE = 0x2
const LOAD_LIBRARY_AS_DATAFILE_EXCLUSIVE = 0x40
const LOAD_LIBRARY_AS_IMAGE_RESOURCE = 0x20
const LOAD_LIBRARY_REQUIRE_SIGNED_TARGET = 0x80
const LOAD_LIBRARY_SEARCH_APPLICATION_DIR = 0x200
const LOAD_LIBRARY_SEARCH_DEFAULT_DIRS = 0x1000
const LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR = 0x100
const LOAD_LIBRARY_SEARCH_SYSTEM32 = 0x800
const LOAD_LIBRARY_SEARCH_SYSTEM32_NO_FORWARDER = "LOAD_LIBRARY_SEARCH_SYSTEM32"
const LOAD_LIBRARY_SEARCH_USER_DIRS = 0x400
const LOAD_WITH_ALTERED_SEARCH_PATH = 0x8
const LOCALE_ALL = 0
const LOCALE_ALTERNATE_SORTS = 0x00000004
const LOCALE_ENUMPROC = "LOCALE_ENUMPROCA"
const LOCALE_FONTSIGNATURE = 0x00000058
const LOCALE_ICALENDARTYPE = 0x00001009
const LOCALE_ICENTURY = 0x00000024
const LOCALE_ICOUNTRY = 0x00000005
const LOCALE_ICURRDIGITS = 0x00000019
const LOCALE_ICURRENCY = 0x0000001b
const LOCALE_IDATE = 0x00000021
const LOCALE_IDAYLZERO = 0x00000026
const LOCALE_IDEFAULTANSICODEPAGE = 0x00001004
const LOCALE_IDEFAULTCODEPAGE = 0x0000000b
const LOCALE_IDEFAULTCOUNTRY = 0x0000000a
const LOCALE_IDEFAULTEBCDICCODEPAGE = 0x00001012
const LOCALE_IDEFAULTLANGUAGE = 0x00000009
const LOCALE_IDEFAULTMACCODEPAGE = 0x00001011
const LOCALE_IDIALINGCODE = 0x00000005
const LOCALE_IDIGITS = 0x00000011
const LOCALE_IDIGITSUBSTITUTION = 0x00001014
const LOCALE_IFIRSTDAYOFWEEK = 0x0000100c
const LOCALE_IFIRSTWEEKOFYEAR = 0x0000100d
const LOCALE_IGEOID = 0x0000005b
const LOCALE_IINTLCURRDIGITS = 0x0000001a
const LOCALE_ILANGUAGE = 0x00000001
const LOCALE_ILDATE = 0x00000022
const LOCALE_ILZERO = 0x00000012
const LOCALE_IMEASURE = 0x0000000d
const LOCALE_IMONLZERO = 0x00000027
const LOCALE_INEGCURR = 0x0000001c
const LOCALE_INEGNUMBER = 0x00001010
const LOCALE_INEGSEPBYSPACE = 0x00000057
const LOCALE_INEGSIGNPOSN = 0x00000053
const LOCALE_INEGSYMPRECEDES = 0x00000056
const LOCALE_IOPTIONALCALENDAR = 0x0000100b
const LOCALE_IPAPERSIZE = 0x0000100a
const LOCALE_IPOSSEPBYSPACE = 0x00000055
const LOCALE_IPOSSIGNPOSN = 0x00000052
const LOCALE_IPOSSYMPRECEDES = 0x00000054
const LOCALE_ITIME = 0x00000023
const LOCALE_ITIMEMARKPOSN = 0x00001005
const LOCALE_ITLZERO = 0x00000025
const LOCALE_NAME_INVARIANT = ""
const LOCALE_NAME_MAX_LENGTH = 85
const LOCALE_NAME_SYSTEM_DEFAULT = "!x-sys-default-locale"
const LOCALE_NAME_USER_DEFAULT = "NULL"
const LOCALE_NOUSEROVERRIDE = 0x80000000
const LOCALE_REPLACEMENT = 0x00000008
const LOCALE_RETURN_NUMBER = 0x20000000
const LOCALE_S1159 = 0x00000028
const LOCALE_S2359 = 0x00000029
const LOCALE_SABBREVCTRYNAME = 0x00000007
const LOCALE_SABBREVDAYNAME1 = 0x00000031
const LOCALE_SABBREVDAYNAME2 = 0x00000032
const LOCALE_SABBREVDAYNAME3 = 0x00000033
const LOCALE_SABBREVDAYNAME4 = 0x00000034
const LOCALE_SABBREVDAYNAME5 = 0x00000035
const LOCALE_SABBREVDAYNAME6 = 0x00000036
const LOCALE_SABBREVDAYNAME7 = 0x00000037
const LOCALE_SABBREVLANGNAME = 0x00000003
const LOCALE_SABBREVMONTHNAME1 = 0x00000044
const LOCALE_SABBREVMONTHNAME10 = 0x0000004d
const LOCALE_SABBREVMONTHNAME11 = 0x0000004e
const LOCALE_SABBREVMONTHNAME12 = 0x0000004f
const LOCALE_SABBREVMONTHNAME13 = 0x0000100f
const LOCALE_SABBREVMONTHNAME2 = 0x00000045
const LOCALE_SABBREVMONTHNAME3 = 0x00000046
const LOCALE_SABBREVMONTHNAME4 = 0x00000047
const LOCALE_SABBREVMONTHNAME5 = 0x00000048
const LOCALE_SABBREVMONTHNAME6 = 0x00000049
const LOCALE_SABBREVMONTHNAME7 = 0x0000004a
const LOCALE_SABBREVMONTHNAME8 = 0x0000004b
const LOCALE_SABBREVMONTHNAME9 = 0x0000004c
const LOCALE_SAM = 0x00000028
const LOCALE_SCONSOLEFALLBACKNAME = 0x0000006e
const LOCALE_SCOUNTRY = 0x00000006
const LOCALE_SCURRENCY = 0x00000014
const LOCALE_SDATE = 0x0000001d
const LOCALE_SDAYNAME1 = 0x0000002a
const LOCALE_SDAYNAME2 = 0x0000002b
const LOCALE_SDAYNAME3 = 0x0000002c
const LOCALE_SDAYNAME4 = 0x0000002d
const LOCALE_SDAYNAME5 = 0x0000002e
const LOCALE_SDAYNAME6 = 0x0000002f
const LOCALE_SDAYNAME7 = 0x00000030
const LOCALE_SDECIMAL = 0x0000000e
const LOCALE_SDURATION = 0x0000005d
const LOCALE_SENGCOUNTRY = 0x00001002
const LOCALE_SENGCURRNAME = 0x00001007
const LOCALE_SENGLANGUAGE = 0x00001001
const LOCALE_SENGLISHCOUNTRYNAME = 0x00001002
const LOCALE_SENGLISHLANGUAGENAME = 0x00001001
const LOCALE_SGROUPING = 0x00000010
const LOCALE_SINTLSYMBOL = 0x00000015
const LOCALE_SISO3166CTRYNAME = 0x0000005a
const LOCALE_SISO3166CTRYNAME2 = 0x00000068
const LOCALE_SISO639LANGNAME = 0x00000059
const LOCALE_SISO639LANGNAME2 = 0x00000067
const LOCALE_SKEYBOARDSTOINSTALL = 0x0000005e
const LOCALE_SLANGDISPLAYNAME = 0x0000006f
const LOCALE_SLANGUAGE = 0x00000002
const LOCALE_SLIST = 0x0000000c
const LOCALE_SLOCALIZEDCOUNTRYNAME = 0x00000006
const LOCALE_SLOCALIZEDDISPLAYNAME = 0x00000002
const LOCALE_SLOCALIZEDLANGUAGENAME = 0x0000006f
const LOCALE_SLONGDATE = 0x00000020
const LOCALE_SMONDECIMALSEP = 0x00000016
const LOCALE_SMONGROUPING = 0x00000018
const LOCALE_SMONTHNAME1 = 0x00000038
const LOCALE_SMONTHNAME10 = 0x00000041
const LOCALE_SMONTHNAME11 = 0x00000042
const LOCALE_SMONTHNAME12 = 0x00000043
const LOCALE_SMONTHNAME13 = 0x0000100e
const LOCALE_SMONTHNAME2 = 0x00000039
const LOCALE_SMONTHNAME3 = 0x0000003a
const LOCALE_SMONTHNAME4 = 0x0000003b
const LOCALE_SMONTHNAME5 = 0x0000003c
const LOCALE_SMONTHNAME6 = 0x0000003d
const LOCALE_SMONTHNAME7 = 0x0000003e
const LOCALE_SMONTHNAME8 = 0x0000003f
const LOCALE_SMONTHNAME9 = 0x00000040
const LOCALE_SMONTHOUSANDSEP = 0x00000017
const LOCALE_SNAME = 0x0000005c
const LOCALE_SNAN = 0x00000069
const LOCALE_SNATIVECOUNTRYNAME = 0x00000008
const LOCALE_SNATIVECTRYNAME = 0x00000008
const LOCALE_SNATIVECURRNAME = 0x00001008
const LOCALE_SNATIVEDIGITS = 0x00000013
const LOCALE_SNATIVELANGNAME = 0x00000004
const LOCALE_SNATIVELANGUAGENAME = 0x00000004
const LOCALE_SNEGATIVESIGN = 0x00000051
const LOCALE_SNEGINFINITY = 0x0000006b
const LOCALE_SPARENT = 0x0000006d
const LOCALE_SPM = 0x00000029
const LOCALE_SPOSINFINITY = 0x0000006a
const LOCALE_SPOSITIVESIGN = 0x00000050
const LOCALE_SSCRIPTS = 0x0000006c
const LOCALE_SSHORTDATE = 0x0000001f
const LOCALE_SSHORTESTDAYNAME1 = 0x00000060
const LOCALE_SSHORTESTDAYNAME2 = 0x00000061
const LOCALE_SSHORTESTDAYNAME3 = 0x00000062
const LOCALE_SSHORTESTDAYNAME4 = 0x00000063
const LOCALE_SSHORTESTDAYNAME5 = 0x00000064
const LOCALE_SSHORTESTDAYNAME6 = 0x00000065
const LOCALE_SSHORTESTDAYNAME7 = 0x00000066
const LOCALE_SSORTNAME = 0x00001013
const LOCALE_STHOUSAND = 0x0000000f
const LOCALE_STIME = 0x0000001e
const LOCALE_STIMEFORMAT = 0x00001003
const LOCALE_SUPPLEMENTAL = 0x00000002
const LOCALE_SYEARMONTH = 0x00001006
const LOCALE_TRANSIENT_KEYBOARD1 = 0x2000
const LOCALE_TRANSIENT_KEYBOARD2 = 0x2400
const LOCALE_TRANSIENT_KEYBOARD3 = 0x2800
const LOCALE_TRANSIENT_KEYBOARD4 = 0x2c00
const LOCALE_UNASSIGNED_LCID = "LOCALE_CUSTOM_UNSPECIFIED"
const LOCALE_USE_CP_ACP = 0x40000000
const LOCALE_WINDOWS = 0x00000001
const LOCKFILE_EXCLUSIVE_LOCK = 0x2
const LOCKFILE_FAIL_IMMEDIATELY = 0x1
const LOGON32_LOGON_BATCH = 4
const LOGON32_LOGON_INTERACTIVE = 2
const LOGON32_LOGON_NETWORK = 3
const LOGON32_LOGON_NETWORK_CLEARTEXT = 8
const LOGON32_LOGON_NEW_CREDENTIALS = 9
const LOGON32_LOGON_SERVICE = 5
const LOGON32_LOGON_UNLOCK = 7
const LOGON32_PROVIDER_DEFAULT = 0
const LOGON32_PROVIDER_VIRTUAL = 4
const LOGON32_PROVIDER_WINNT35 = 1
const LOGON32_PROVIDER_WINNT40 = 2
const LOGON32_PROVIDER_WINNT50 = 3
const LOGON_NETCREDENTIALS_ONLY = 0x00000002
const LOGON_WITH_PROFILE = 0x00000001
const LOGON_ZERO_PASSWORD_BUFFER = 0x80000000
const LOGPIXELSX = 88
const LOGPIXELSY = 90
const LONGNBSEQ = 32512
const LOW_SURROGATE_END = 0xdfff
const LOW_SURROGATE_START = 0xdc00
const LPD_DOUBLEBUFFER = 0x00000001
const LPD_SHARE_ACCUM = 0x00000100
const LPD_SHARE_DEPTH = 0x00000040
const LPD_SHARE_STENCIL = 0x00000080
const LPD_STEREO = 0x00000002
const LPD_SUPPORT_GDI = 0x00000010
const LPD_SUPPORT_OPENGL = 0x00000020
const LPD_SWAP_COPY = 0x00000400
const LPD_SWAP_EXCHANGE = 0x00000200
const LPD_TRANSPARENT = 0x00001000
const LPD_TYPE_COLORINDEX = 1
const LPD_TYPE_RGBA = 0
const LPTx = 0x80
const LR_COLOR = 0x0002
const LR_COPYDELETEORG = 0x0008
const LR_COPYFROMRESOURCE = 0x4000
const LR_COPYRETURNORG = 0x0004
const LR_CREATEDIBSECTION = 0x2000
const LR_DEFAULTCOLOR = 0x0000
const LR_DEFAULTSIZE = 0x0040
const LR_LOADFROMFILE = 0x0010
const LR_LOADMAP3DCOLORS = 0x1000
const LR_LOADTRANSPARENT = 0x0020
const LR_MONOCHROME = 0x0001
const LR_SHARED = 0x8000
const LR_VGACOLOR = 0x0080
const LSFW_LOCK = 1
const LSFW_UNLOCK = 2
const LTGRAY_BRUSH = 1
const LTP_PC_SMT = 0x1
const LUA_TOKEN = 0x4
const LWA_ALPHA = 0x00000002
const LWA_COLORKEY = 0x00000001
const LX_FILE_CASE_SENSITIVE_DIR = 0x10
const LX_FILE_METADATA_HAS_DEVICE_ID = 0x8
const LX_FILE_METADATA_HAS_GID = 0x2
const LX_FILE_METADATA_HAS_MODE = 0x4
const LX_FILE_METADATA_HAS_UID = 0x1
const L_tmpnam_s = "L_tmpnam"
const LitHufLog = 11
const Litbits = 8
const LoadFence = "_mm_lfence"
const MAC_CHARSET = 77
const MAPVK_VK_TO_CHAR = 2
const MAPVK_VK_TO_VSC = 0
const MAPVK_VK_TO_VSC_EX = 4
const MAPVK_VSC_TO_VK = 1
const MAPVK_VSC_TO_VK_EX = 3
const MAP_COMPOSITE = 0x00000040
const MAP_EXPAND_LIGATURES = 0x00002000
const MAP_FOLDCZONE = 0x00000010
const MAP_FOLDDIGITS = 0x00000080
const MAP_PRECOMPOSED = 0x00000020
const MARKPARITY = 3
const MAXBYTE = 0xff
const MAXCHAR = 0x7f
const MAXDWORD = 0xffffffff
const MAXIMUM_PROCESSORS = "MAXIMUM_PROC_PER_GROUP"
const MAXIMUM_PROC_PER_GROUP = 64
const MAXIMUM_SUSPEND_COUNT = "MAXCHAR"
const MAXIMUM_WAIT_OBJECTS = 64
const MAXIMUM_XSTATE_FEATURES = 64
const MAXINTATOM = 0xc000
const MAXLOGICALLOGNAMESIZE = 256
const MAXLONG = 0x7fffffff
const MAXLONGLONG = "0x7fffffffffffffffll"
const MAXREPOFFSET = 1024
const MAXSHORT = 0x7fff
const MAXSTRETCHBLTMODE = 4
const MAXWORD = 0xffff
const MAX_ACL_REVISION = "ACL_REVISION4"
const MAX_COMPUTERNAME_LENGTH = 15
const MAX_DEFAULTCHAR = 2
const MAX_FSE_TABLELOG_FOR_HUFF_HEADER = 6
const MAX_HW_COUNTERS = 16
const MAX_LEADBYTES = 12
const MAX_NUM_REASONS = 256
const MAX_PATH = 260
const MAX_PROFILE_LEN = 80
const MAX_REASON_BUGID_LEN = 32
const MAX_REASON_COMMENT_LEN = 512
const MAX_REASON_DESC_LEN = 256
const MAX_REASON_NAME_LEN = 64
const MAX_RESOURCEMANAGER_DESCRIPTION_LENGTH = 64
const MAX_STR_BLOCKREASON = 256
const MAX_SUPPORTED_OS_NUM = 4
const MAX_TRANSACTION_DESCRIPTION_LENGTH = 64
const MAX_UCSCHAR = 0x0010ffff
const MA_ACTIVATE = 1
const MA_ACTIVATEANDEAT = 2
const MA_NOACTIVATE = 3
const MA_NOACTIVATEANDEAT = 4
const MB_COMPOSITE = 0x00000002
const MB_ERR_INVALID_CHARS = 0x00000008
const MB_ICONERROR = "MB_ICONHAND"
const MB_ICONINFORMATION = "MB_ICONASTERISK"
const MB_ICONSTOP = "MB_ICONHAND"
const MB_ICONWARNING = "MB_ICONEXCLAMATION"
const MB_LEN_MAX = 5
const MB_PRECOMPOSED = 0x00000001
const MB_USEGLYPHCHARS = 0x00000004
const MDIS_ALLCHILDSTYLES = 0x0001
const MDITILE_HORIZONTAL = 0x0001
const MDITILE_SKIPDISABLED = 0x0002
const MDITILE_VERTICAL = 0x0000
const MDITILE_ZORDER = 0x0004
const MDMSPKRFLAG_CALLSETUP = 0x00000008
const MDMSPKRFLAG_DIAL = 0x00000002
const MDMSPKRFLAG_OFF = 0x00000001
const MDMSPKRFLAG_ON = 0x00000004
const MDMSPKR_CALLSETUP = 0x00000003
const MDMSPKR_DIAL = 0x00000001
const MDMSPKR_OFF = 0x00000000
const MDMSPKR_ON = 0x00000002
const MDMVOLFLAG_HIGH = 0x00000004
const MDMVOLFLAG_LOW = 0x00000001
const MDMVOLFLAG_MEDIUM = 0x00000002
const MDMVOL_HIGH = 0x00000002
const MDMVOL_LOW = 0x00000000
const MDMVOL_MEDIUM = 0x00000001
const MDM_ANALOG_RLP_OFF = 0x1
const MDM_ANALOG_RLP_ON = 0x0
const MDM_ANALOG_V34 = 0x2
const MDM_AUTO_ML_2 = 0x2
const MDM_AUTO_ML_DEFAULT = 0x0
const MDM_AUTO_ML_NONE = 0x1
const MDM_AUTO_SPEED_DEFAULT = 0x0
const MDM_BEARERMODE_ANALOG = 0x0
const MDM_BEARERMODE_GSM = 0x2
const MDM_BEARERMODE_ISDN = 0x1
const MDM_BLIND_DIAL = 0x00000200
const MDM_CCITT_OVERRIDE = 0x00000040
const MDM_CELLULAR = 0x00000008
const MDM_COMPRESSION = 0x00000001
const MDM_DIAGNOSTICS = 0x00000800
const MDM_ERROR_CONTROL = 0x00000002
const MDM_FLOWCONTROL_HARD = 0x00000010
const MDM_FLOWCONTROL_SOFT = 0x00000020
const MDM_FORCED_EC = 0x00000004
const MDM_HDLCPPP_AUTH_CHAP = 0x3
const MDM_HDLCPPP_AUTH_DEFAULT = 0x0
const MDM_HDLCPPP_AUTH_MSCHAP = 0x4
const MDM_HDLCPPP_AUTH_NONE = 0x1
const MDM_HDLCPPP_AUTH_PAP = 0x2
const MDM_HDLCPPP_ML_2 = 0x2
const MDM_HDLCPPP_ML_DEFAULT = 0x0
const MDM_HDLCPPP_ML_NONE = 0x1
const MDM_HDLCPPP_SPEED_56K = 0x2
const MDM_HDLCPPP_SPEED_64K = 0x1
const MDM_HDLCPPP_SPEED_DEFAULT = 0x0
const MDM_MASK_AUTO_SPEED = 0x7
const MDM_MASK_BEARERMODE = 0x0000f000
const MDM_MASK_HDLCPPP_SPEED = 0x7
const MDM_MASK_PROTOCOLDATA = 0x0ff00000
const MDM_MASK_PROTOCOLID = 0x000f0000
const MDM_MASK_V110_SPEED = 0xf
const MDM_MASK_V120_SPEED = 0x7
const MDM_MASK_X75_DATA = 0x7
const MDM_PIAFS_INCOMING = 0
const MDM_PIAFS_OUTGOING = 1
const MDM_PROTOCOLID_ANALOG = 0x7
const MDM_PROTOCOLID_AUTO = 0x6
const MDM_PROTOCOLID_DEFAULT = 0x0
const MDM_PROTOCOLID_GPRS = 0x8
const MDM_PROTOCOLID_HDLCPPP = 0x1
const MDM_PROTOCOLID_PIAFS = 0x9
const MDM_PROTOCOLID_V110 = 0x4
const MDM_PROTOCOLID_V120 = 0x5
const MDM_PROTOCOLID_V128 = 0x2
const MDM_PROTOCOLID_X75 = 0x3
const MDM_SHIFT_AUTO_ML = 0x6
const MDM_SHIFT_AUTO_SPEED = 0x0
const MDM_SHIFT_BEARERMODE = 12
const MDM_SHIFT_EXTENDEDINFO = "MDM_SHIFT_BEARERMODE"
const MDM_SHIFT_HDLCPPP_AUTH = 0x3
const MDM_SHIFT_HDLCPPP_ML = 0x6
const MDM_SHIFT_HDLCPPP_SPEED = 0x0
const MDM_SHIFT_PROTOCOLDATA = 20
const MDM_SHIFT_PROTOCOLID = 16
const MDM_SHIFT_PROTOCOLINFO = "MDM_SHIFT_PROTOCOLID"
const MDM_SHIFT_V110_SPEED = 0x0
const MDM_SHIFT_V120_ML = 0x6
const MDM_SHIFT_V120_SPEED = 0x0
const MDM_SHIFT_X75_DATA = 0x0
const MDM_SPEED_ADJUST = 0x00000080
const MDM_TONE_DIAL = 0x00000100
const MDM_V110_SPEED_12DOT0K = 0x5
const MDM_V110_SPEED_14DOT4K = 0x6
const MDM_V110_SPEED_19DOT2K = 0x7
const MDM_V110_SPEED_1DOT2K = 0x1
const MDM_V110_SPEED_28DOT8K = 0x8
const MDM_V110_SPEED_2DOT4K = 0x2
const MDM_V110_SPEED_38DOT4K = 0x9
const MDM_V110_SPEED_4DOT8K = 0x3
const MDM_V110_SPEED_57DOT6K = 0xA
const MDM_V110_SPEED_9DOT6K = 0x4
const MDM_V110_SPEED_DEFAULT = 0x0
const MDM_V120_ML_2 = 0x2
const MDM_V120_ML_DEFAULT = 0x0
const MDM_V120_ML_NONE = 0x1
const MDM_V120_SPEED_56K = 0x2
const MDM_V120_SPEED_64K = 0x1
const MDM_V120_SPEED_DEFAULT = 0x0
const MDM_V23_OVERRIDE = 0x00000400
const MDM_X75_DATA_128K = 0x2
const MDM_X75_DATA_64K = 0x1
const MDM_X75_DATA_BTX = 0x4
const MDM_X75_DATA_DEFAULT = 0x0
const MDM_X75_DATA_T_70 = 0x3
const MEMORY_ALLOCATION_ALIGNMENT = 16
const MEMORY_PARTITION_MODIFY_ACCESS = 0x0002
const MEMORY_PARTITION_QUERY_ACCESS = 0x0001
const MEMORY_PRIORITY_BELOW_NORMAL = 4
const MEMORY_PRIORITY_LOW = 2
const MEMORY_PRIORITY_MEDIUM = 3
const MEMORY_PRIORITY_NORMAL = 5
const MEMORY_PRIORITY_VERY_LOW = 1
const MEM_4MB_PAGES = 0x80000000
const MEM_COALESCE_PLACEHOLDERS = 0x00000001
const MEM_COMMIT = 0x1000
const MEM_DECOMMIT = 0x4000
const MEM_DIFFERENT_IMAGE_BASE_OK = 0x800000
const MEM_EXTENDED_PARAMETER_EC_CODE = 0x40
const MEM_EXTENDED_PARAMETER_GRAPHICS = 0x01
const MEM_EXTENDED_PARAMETER_IMAGE_NO_HPAT = 0x80
const MEM_EXTENDED_PARAMETER_NONPAGED = 0x02
const MEM_EXTENDED_PARAMETER_NONPAGED_HUGE = 0x10
const MEM_EXTENDED_PARAMETER_NONPAGED_LARGE = 0x08
const MEM_EXTENDED_PARAMETER_NUMA_NODE_MANDATORY = "MINLONG64"
const MEM_EXTENDED_PARAMETER_SOFT_FAULT_PAGES = 0x20
const MEM_EXTENDED_PARAMETER_TYPE_BITS = 8
const MEM_EXTENDED_PARAMETER_ZERO_PAGES_OPTIONAL = 0x04
const MEM_FORCE_MEMORY_ACCESS = 1
const MEM_FREE = 0x10000
const MEM_IMAGE = "SEC_IMAGE"
const MEM_LARGE_PAGES = 0x20000000
const MEM_MAPPED = 0x40000
const MEM_PHYSICAL = 0x400000
const MEM_PRESERVE_PLACEHOLDER = 0x00000002
const MEM_PRIVATE = 0x20000
const MEM_RELEASE = 0x8000
const MEM_REPLACE_PLACEHOLDER = 0x4000
const MEM_RESERVE = 0x2000
const MEM_RESERVE_PLACEHOLDER = 0x40000
const MEM_RESET = 0x80000
const MEM_RESET_UNDO = 0x1000000
const MEM_ROTATE = 0x800000
const MEM_TOP_DOWN = 0x100000
const MEM_UNMAP_WITH_TRANSIENT_BOOST = 0x00000001
const MEM_WRITE_WATCH = 0x200000
const MENU_EVENT = 0x0008
const MESSAGE_RESOURCE_UNICODE = 0x0001
const METAFILE_DRIVER = 2049
const META_ANIMATEPALETTE = 0x0436
const META_ARC = 0x0817
const META_BITBLT = 0x0922
const META_CHORD = 0x0830
const META_CREATEBRUSHINDIRECT = 0x02FC
const META_CREATEFONTINDIRECT = 0x02FB
const META_CREATEPALETTE = 0x00f7
const META_CREATEPATTERNBRUSH = 0x01F9
const META_CREATEPENINDIRECT = 0x02FA
const META_CREATEREGION = 0x06FF
const META_DELETEOBJECT = 0x01f0
const META_DIBBITBLT = 0x0940
const META_DIBCREATEPATTERNBRUSH = 0x0142
const META_DIBSTRETCHBLT = 0x0b41
const META_ELLIPSE = 0x0418
const META_ESCAPE = 0x0626
const META_EXCLUDECLIPRECT = 0x0415
const META_EXTFLOODFILL = 0x0548
const META_EXTTEXTOUT = 0x0a32
const META_FILLREGION = 0x0228
const META_FLOODFILL = 0x0419
const META_FRAMEREGION = 0x0429
const META_INTERSECTCLIPRECT = 0x0416
const META_INVERTREGION = 0x012A
const META_LINETO = 0x0213
const META_MOVETO = 0x0214
const META_OFFSETCLIPRGN = 0x0220
const META_OFFSETVIEWPORTORG = 0x0211
const META_OFFSETWINDOWORG = 0x020F
const META_PAINTREGION = 0x012B
const META_PATBLT = 0x061D
const META_PIE = 0x081A
const META_POLYGON = 0x0324
const META_POLYLINE = 0x0325
const META_POLYPOLYGON = 0x0538
const META_REALIZEPALETTE = 0x0035
const META_RECTANGLE = 0x041B
const META_RESIZEPALETTE = 0x0139
const META_RESTOREDC = 0x0127
const META_ROUNDRECT = 0x061C
const META_SAVEDC = 0x001E
const META_SCALEVIEWPORTEXT = 0x0412
const META_SCALEWINDOWEXT = 0x0410
const META_SELECTCLIPREGION = 0x012C
const META_SELECTOBJECT = 0x012D
const META_SELECTPALETTE = 0x0234
const META_SETBKCOLOR = 0x0201
const META_SETBKMODE = 0x0102
const META_SETDIBTODEV = 0x0d33
const META_SETLAYOUT = 0x0149
const META_SETMAPMODE = 0x0103
const META_SETMAPPERFLAGS = 0x0231
const META_SETPALENTRIES = 0x0037
const META_SETPIXEL = 0x041F
const META_SETPOLYFILLMODE = 0x0106
const META_SETRELABS = 0x0105
const META_SETROP2 = 0x0104
const META_SETSTRETCHBLTMODE = 0x0107
const META_SETTEXTALIGN = 0x012E
const META_SETTEXTCHAREXTRA = 0x0108
const META_SETTEXTCOLOR = 0x0209
const META_SETTEXTJUSTIFICATION = 0x020A
const META_SETVIEWPORTEXT = 0x020E
const META_SETVIEWPORTORG = 0x020D
const META_SETWINDOWEXT = 0x020C
const META_SETWINDOWORG = 0x020B
const META_STRETCHBLT = 0x0B23
const META_STRETCHDIB = 0x0f43
const META_TEXTOUT = 0x0521
const MFCOMMENT = 15
const MFS_CHECKED = "MF_CHECKED"
const MFS_DEFAULT = "MF_DEFAULT"
const MFS_DISABLED = "MFS_GRAYED"
const MFS_ENABLED = "MF_ENABLED"
const MFS_HILITE = "MF_HILITE"
const MFS_UNCHECKED = "MF_UNCHECKED"
const MFS_UNHILITE = "MF_UNHILITE"
const MFT_BITMAP = "MF_BITMAP"
const MFT_MENUBARBREAK = "MF_MENUBARBREAK"
const MFT_MENUBREAK = "MF_MENUBREAK"
const MFT_OWNERDRAW = "MF_OWNERDRAW"
const MFT_RIGHTJUSTIFY = "MF_RIGHTJUSTIFY"
const MFT_SEPARATOR = "MF_SEPARATOR"
const MFT_STRING = "MF_STRING"
const MICROSOFT_WINDOWS_WINBASE_H_DEFINE_INTERLOCKED_CPLUSPLUS_OVERLOADS = 1
const MIIM_BITMAP = 0x00000080
const MIIM_CHECKMARKS = 0x00000008
const MIIM_DATA = 0x00000020
const MIIM_FTYPE = 0x00000100
const MIIM_ID = 0x00000002
const MIIM_STATE = 0x00000001
const MIIM_STRING = 0x00000040
const MIIM_SUBMENU = 0x00000004
const MIIM_TYPE = 0x00000010
const MILCORE_TS_QUERYVER_RESULT_FALSE = 0x0
const MILCORE_TS_QUERYVER_RESULT_TRUE = 0x7FFFFFFF
const MIM_APPLYTOSUBMENUS = 0x80000000
const MIM_BACKGROUND = 0x00000002
const MIM_HELPID = 0x00000004
const MIM_MAXHEIGHT = 0x00000001
const MIM_MENUDATA = 0x00000008
const MIM_STYLE = 0x00000010
const MINCHAR = 0x80
const MINGW_HAS_DDK_H = 1
const MINGW_HAS_SECURE_API = 1
const MINLONG = 0x80000000
const MINMATCH = 3
const MINMATCHLENGTH = 7
const MINRATIO = 4
const MINSHORT = 0x8000
const MIN_ACL_REVISION = "ACL_REVISION2"
const MIN_LITERALS_FOR_4_STREAMS = 6
const MIN_SEQUENCES_BLOCK_SPLITTING = 300
const MIN_SEQUENCES_SIZE = 1
const MIN_UCSCHAR = 0
const MKF_AVAILABLE = 0x00000002
const MKF_CONFIRMHOTKEY = 0x00000008
const MKF_HOTKEYACTIVE = 0x00000004
const MKF_HOTKEYSOUND = 0x00000010
const MKF_INDICATOR = 0x00000020
const MKF_LEFTBUTTONDOWN = 0x01000000
const MKF_LEFTBUTTONSEL = 0x10000000
const MKF_MODIFIERS = 0x00000040
const MKF_MOUSEKEYSON = 0x00000001
const MKF_MOUSEMODE = 0x80000000
const MKF_REPLACENUMBERS = 0x00000080
const MKF_RIGHTBUTTONDOWN = 0x02000000
const MKF_RIGHTBUTTONSEL = 0x20000000
const MK_CONTROL = 0x0008
const MK_LBUTTON = 0x0001
const MK_MBUTTON = 0x0010
const MK_RBUTTON = 0x0002
const MK_SHIFT = 0x0004
const MK_XBUTTON1 = 0x0020
const MK_XBUTTON2 = 0x0040
const MLFSELog = 9
const ML_DEFAULTNORMLOG = 6
const MM_ANISOTROPIC = 8
const MM_HIENGLISH = 5
const MM_HIMETRIC = 3
const MM_ISOTROPIC = 7
const MM_LOENGLISH = 4
const MM_LOMETRIC = 2
const MM_MAX = "MM_ANISOTROPIC"
const MM_MAX_AXES_NAMELEN = 16
const MM_MAX_FIXEDSCALE = "MM_TWIPS"
const MM_MAX_NUMAXES = 16
const MM_MIN = "MM_TEXT"
const MM_TEXT = 1
const MM_TWIPS = 6
const MNC_CLOSE = 1
const MNC_EXECUTE = 2
const MNC_IGNORE = 0
const MNC_SELECT = 3
const MND_CONTINUE = 0
const MND_ENDMENU = 1
const MNGOF_BOTTOMGAP = 0x00000002
const MNGOF_TOPGAP = 0x00000001
const MNGO_NOERROR = 0x00000001
const MNGO_NOINTERFACE = 0x00000000
const MNS_AUTODISMISS = 0x10000000
const MNS_CHECKORBMP = 0x04000000
const MNS_DRAGDROP = 0x20000000
const MNS_MODELESS = 0x40000000
const MNS_NOCHECK = 0x80000000
const MNS_NOTIFYBYPOS = 0x08000000
const MN_GETHMENU = 0x01E1
const MOD_ALT = 0x0001
const MOD_CONTROL = 0x0002
const MOD_IGNORE_ALL_MODIFIER = 0x0400
const MOD_LEFT = 0x8000
const MOD_ON_KEYUP = 0x0800
const MOD_RIGHT = 0x4000
const MOD_SHIFT = 0x0004
const MOD_WIN = 0x0008
const MONITORINFOF_PRIMARY = 0x00000001
const MONITOR_DEFAULTTONEAREST = 0x00000002
const MONITOR_DEFAULTTONULL = 0x00000000
const MONITOR_DEFAULTTOPRIMARY = 0x00000001
const MONO_FONT = 8
const MOUSEEVENTF_ABSOLUTE = 0x8000
const MOUSEEVENTF_HWHEEL = 0x01000
const MOUSEEVENTF_LEFTDOWN = 0x0002
const MOUSEEVENTF_LEFTUP = 0x0004
const MOUSEEVENTF_MIDDLEDOWN = 0x0020
const MOUSEEVENTF_MIDDLEUP = 0x0040
const MOUSEEVENTF_MOVE = 0x0001
const MOUSEEVENTF_MOVE_NOCOALESCE = 0x2000
const MOUSEEVENTF_RIGHTDOWN = 0x0008
const MOUSEEVENTF_RIGHTUP = 0x0010
const MOUSEEVENTF_VIRTUALDESK = 0x4000
const MOUSEEVENTF_WHEEL = 0x0800
const MOUSEEVENTF_XDOWN = 0x0080
const MOUSEEVENTF_XUP = 0x0100
const MOUSETRAILS = 39
const MOUSE_ATTRIBUTES_CHANGED = 0x04
const MOUSE_EVENT = 0x0002
const MOUSE_HWHEELED = 0x0008
const MOUSE_MOVED = 0x0001
const MOUSE_MOVE_ABSOLUTE = 1
const MOUSE_MOVE_NOCOALESCE = 0x08
const MOUSE_MOVE_RELATIVE = 0
const MOUSE_VIRTUAL_DESKTOP = 0x02
const MOUSE_WHEELED = 0x0004
const MOVEFILE_COPY_ALLOWED = 0x00000002
const MOVEFILE_CREATE_HARDLINK = 0x00000010
const MOVEFILE_DELAY_UNTIL_REBOOT = 0x00000004
const MOVEFILE_FAIL_IF_NOT_TRACKABLE = 0x00000020
const MOVEFILE_REPLACE_EXISTING = 0x00000001
const MOVEFILE_WRITE_THROUGH = 0x00000008
const MSGFLT_ADD = 1
const MSGFLT_REMOVE = 2
const MSGF_DIALOGBOX = 0
const MSGF_MAX = 8
const MSGF_MENU = 2
const MSGF_MESSAGEBOX = 1
const MSGF_NEXTWINDOW = 6
const MSGF_SCROLLBAR = 5
const MSGF_USER = 4096
const MS_PPM_SOFTWARE_ALL = 0x1
const MUI_CALLBACK_ALL_FLAGS = "MUI_CALLBACK_FLAG_UPGRADED_INSTALLATION"
const MUI_COMPLEX_SCRIPT_FILTER = 0x200
const MUI_CONSOLE_FILTER = 0x100
const MUI_FILEINFO_VERSION = 0x001
const MUI_FILETYPE_LANGUAGE_NEUTRAL_MAIN = 0x002
const MUI_FILETYPE_LANGUAGE_NEUTRAL_MUI = 0x004
const MUI_FILETYPE_NOT_LANGUAGE_NEUTRAL = 0x001
const MUI_FULL_LANGUAGE = 0x01
const MUI_LANGUAGE_ID = 0x4
const MUI_LANGUAGE_INSTALLED = 0x20
const MUI_LANGUAGE_LICENSED = 0x40
const MUI_LANGUAGE_NAME = 0x8
const MUI_LANG_NEUTRAL_PE_FILE = 0x100
const MUI_LIP_LANGUAGE = 0x04
const MUI_MACHINE_LANGUAGE_SETTINGS = 0x400
const MUI_MERGE_SYSTEM_FALLBACK = 0x10
const MUI_MERGE_USER_FALLBACK = 0x20
const MUI_NON_LANG_NEUTRAL_FILE = 0x200
const MUI_PARTIAL_LANGUAGE = 0x02
const MUI_QUERY_CHECKSUM = 0x002
const MUI_QUERY_LANGUAGE_NAME = 0x004
const MUI_QUERY_RESOURCE_TYPES = 0x008
const MUI_QUERY_TYPE = 0x001
const MUI_RESET_FILTERS = 0x001
const MUI_THREAD_LANGUAGES = 0x40
const MUI_USER_PREFERRED_UI_LANGUAGES = 0x10
const MUI_USE_INSTALLED_LANGUAGES = 0x20
const MUI_USE_SEARCH_ALL_LANGUAGES = 0x40
const MUTANT_QUERY_STATE = 0x0001
const MUTEX_ALL_ACCESS = "MUTANT_ALL_ACCESS"
const MUTEX_MODIFY_STATE = "MUTANT_QUERY_STATE"
const MWMO_ALERTABLE = 0x0002
const MWMO_INPUTAVAILABLE = 0x0004
const MWMO_WAITALL = 0x0001
const MWT_IDENTITY = 1
const MWT_LEFTMULTIPLY = 2
const MWT_MAX = "MWT_RIGHTMULTIPLY"
const MWT_MIN = "MWT_IDENTITY"
const MWT_RIGHTMULTIPLY = 3
const MaxLL = 35
const MaxLLBits = 16
const MaxML = 52
const MaxMLBits = 16
const MaxOff = 31
const MemoryBarrier = "_mm_mfence"
const MemoryFence = "_mm_mfence"
const MoveMemory = "RtlMoveMemory"
const Multiply128 = "_mul128"
const MultiplyHigh = "__mulh"
const NETINFO_DISKRED = 0x00000004
const NETINFO_DLL16 = 0x00000001
const NETINFO_PRINTERRED = 0x00000008
const NETPROPERTY_PERSISTENT = 1
const NEWFRAME = 1
const NEXTBAND = 3
const NFR_ANSI = 1
const NFR_UNICODE = 2
const NF_QUERY = 3
const NF_REQUERY = 4
const NI_CHANGECANDIDATELIST = 0x0013
const NI_CLOSECANDIDATE = 0x0011
const NI_COMPOSITIONSTR = 0x0015
const NI_FINALIZECONVERSIONRESULT = 0x0014
const NI_IMEMENUSELECTED = 0x0018
const NI_OPENCANDIDATE = 0x0010
const NI_SELECTCANDIDATESTR = 0x0012
const NI_SETCANDIDATE_PAGESIZE = 0x0017
const NI_SETCANDIDATE_PAGESTART = 0x0016
const NLS_ALPHANUMERIC = 0x00000000
const NLS_DBCSCHAR = 0x00010000
const NLS_HIRAGANA = 0x00040000
const NLS_IME_CONVERSION = 0x00800000
const NLS_IME_DISABLE = 0x20000000
const NLS_KATAKANA = 0x00020000
const NLS_ROMAN = 0x00400000
const NLS_VALID_LOCALE_MASK = 0x000fffff
const NMPWAIT_NOWAIT = 0x1
const NMPWAIT_USE_DEFAULT_WAIT = 0x0
const NMPWAIT_WAIT_FOREVER = 0xffffffff
const NOERROR = 0
const NOISELENGTH = 32
const NONANTIALIASED_QUALITY = 3
const NONVOL_FP_NUMREG_ARM64 = 8
const NONVOL_INT_NUMREG_ARM64 = 11
const NONZEROLHND = "LMEM_MOVEABLE"
const NONZEROLPTR = "LMEM_FIXED"
const NON_PAGED_DEBUG_SIGNATURE = 0x494E
const NOPARITY = 0
const NORMAL_PRIORITY_CLASS = 0x20
const NORM_IGNORECASE = 0x00000001
const NORM_IGNOREKANATYPE = 0x00010000
const NORM_IGNORENONSPACE = 0x00000002
const NORM_IGNORESYMBOLS = 0x00000004
const NORM_IGNOREWIDTH = 0x00020000
const NORM_LINGUISTIC_CASING = 0x08000000
const NO_PROPAGATE_INHERIT_ACE = 0x4
const NTAPI = "__stdcall"
const NTAPI_INLINE = "NTAPI"
const NTDDI_LONGHORN = "NTDDI_VISTA"
const NTDDI_VISTA = "NTDDI_WIN6"
const NTDDI_VISTASP1 = "NTDDI_WIN6SP1"
const NTDDI_VISTASP2 = "NTDDI_WIN6SP2"
const NTDDI_VISTASP3 = "NTDDI_WIN6SP3"
const NTDDI_VISTASP4 = "NTDDI_WIN6SP4"
const NTDDI_WIN10 = 0x0A000000
const NTDDI_WIN10_19H1 = 0x0A000007
const NTDDI_WIN10_CO = 0x0A00000B
const NTDDI_WIN10_CU = 0x0A00000D
const NTDDI_WIN10_FE = 0x0A00000A
const NTDDI_WIN10_MN = 0x0A000009
const NTDDI_WIN10_NI = 0x0A00000C
const NTDDI_WIN10_RS1 = 0x0A000002
const NTDDI_WIN10_RS2 = 0x0A000003
const NTDDI_WIN10_RS3 = 0x0A000004
const NTDDI_WIN10_RS4 = 0x0A000005
const NTDDI_WIN10_RS5 = 0x0A000006
const NTDDI_WIN10_TH2 = 0x0A000001
const NTDDI_WIN10_VB = 0x0A000008
const NTDDI_WIN11_GA = 0x0A00000F
const NTDDI_WIN11_GE = 0x0A000010
const NTDDI_WIN11_ZN = 0x0A00000E
const NTDDI_WIN2K = 0x05000000
const NTDDI_WIN2KSP1 = 0x05000100
const NTDDI_WIN2KSP2 = 0x05000200
const NTDDI_WIN2KSP3 = 0x05000300
const NTDDI_WIN2KSP4 = 0x05000400
const NTDDI_WIN6 = 0x06000000
const NTDDI_WIN6SP1 = 0x06000100
const NTDDI_WIN6SP2 = 0x06000200
const NTDDI_WIN6SP3 = 0x06000300
const NTDDI_WIN6SP4 = 0x06000400
const NTDDI_WIN7 = 0x06010000
const NTDDI_WIN8 = 0x06020000
const NTDDI_WINBLUE = 0x06030000
const NTDDI_WINTHRESHOLD = 0x0A000000
const NTDDI_WINXP = 0x05010000
const NTDDI_WINXPSP1 = 0x05010100
const NTDDI_WINXPSP2 = 0x05010200
const NTDDI_WINXPSP3 = 0x05010300
const NTDDI_WINXPSP4 = 0x05010400
const NTDDI_WS03 = 0x05020000
const NTDDI_WS03SP1 = 0x05020100
const NTDDI_WS03SP2 = 0x05020200
const NTDDI_WS03SP3 = 0x05020300
const NTDDI_WS03SP4 = 0x05020400
const NTDDI_WS08 = "NTDDI_WIN6SP1"
const NTDDI_WS08SP2 = "NTDDI_WIN6SP2"
const NTDDI_WS08SP3 = "NTDDI_WIN6SP3"
const NTDDI_WS08SP4 = "NTDDI_WIN6SP4"
const NTE_OP_OK = 0
const NTM_DSIG = 0x00200000
const NTM_MULTIPLEMASTER = 0x00080000
const NTM_NONNEGATIVE_AC = 0x00010000
const NTM_PS_OPENTYPE = 0x00020000
const NTM_TT_OPENTYPE = 0x00040000
const NTM_TYPE1 = 0x00100000
const NTSYSAPI = "DECLSPEC_IMPORT"
const NTSYSCALLAPI = "DECLSPEC_IMPORT"
const NULLREGION = 1
const NULL_BRUSH = 5
const NULL_PEN = 8
const NUMBRUSHES = 16
const NUMCOLORS = 24
const NUMFONTS = 22
const NUMLOCK_ON = 0x0020
const NUMMARKERS = 20
const NUMPENS = 18
const NUMRESERVED = 106
const NUM_DISCHARGE_POLICIES = 4
const N_BTMASK = 0x000F
const N_BTSHFT = 4
const N_TMASK = 0x0030
const N_TMASK1 = 0x00C0
const N_TMASK2 = 0x00F0
const N_TSHIFT = 2
const OBJECT_INHERIT_ACE = 0x1
const OBJ_BITMAP = 7
const OBJ_BRUSH = 2
const OBJ_COLORSPACE = 14
const OBJ_DC = 3
const OBJ_ENHMETADC = 12
const OBJ_ENHMETAFILE = 13
const OBJ_EXTPEN = 11
const OBJ_FONT = 6
const OBJ_MEMDC = 10
const OBJ_METADC = 4
const OBJ_METAFILE = 9
const OBJ_PAL = 5
const OBJ_PEN = 1
const OBJ_REGION = 8
const ODA_DRAWENTIRE = 0x0001
const ODA_FOCUS = 0x0004
const ODA_SELECT = 0x0002
const ODDPARITY = 1
const ODS_CHECKED = 0x0008
const ODS_COMBOBOXEDIT = 0x1000
const ODS_DEFAULT = 0x0020
const ODS_DISABLED = 0x0004
const ODS_FOCUS = 0x0010
const ODS_GRAYED = 0x0002
const ODS_HOTLIGHT = 0x0040
const ODS_INACTIVE = 0x0080
const ODS_NOACCEL = 0x0100
const ODS_NOFOCUSRECT = 0x0200
const ODS_SELECTED = 0x0001
const ODT_BUTTON = 4
const ODT_COMBOBOX = 3
const ODT_LISTBOX = 2
const ODT_MENU = 1
const ODT_STATIC = 5
const OEM_CHARSET = 255
const OEM_FIXED_FONT = 10
const OFFCODE_MAX = 30
const OFS_MAXPATHNAME = 128
const OF_CANCEL = 0x800
const OF_CREATE = 0x1000
const OF_DEFAULTNORMLOG = 5
const OF_DELETE = 0x200
const OF_EXIST = 0x4000
const OF_PARSE = 0x100
const OF_PROMPT = 0x2000
const OF_READ = 0x0
const OF_READWRITE = 0x2
const OF_REOPEN = 0x8000
const OF_SHARE_COMPAT = 0x0
const OF_SHARE_DENY_NONE = 0x40
const OF_SHARE_DENY_READ = 0x30
const OF_SHARE_DENY_WRITE = 0x20
const OF_SHARE_EXCLUSIVE = 0x10
const OF_VERIFY = 0x400
const OF_WRITE = 0x1
const OLD_P_OVERLAY = "_OLD_P_OVERLAY"
const ONE5STOPBITS = 1
const ONESTOPBIT = 0
const OPAQUE = 2
const OPENCHANNEL = 4110
const OPEN_ALWAYS = 4
const OPEN_EXISTING = 3
const ORD_LANGDRIVER = 1
const OSVERSION_MASK = 0xFFFF0000
const OUTPUT_DEBUG_STRING_EVENT = 8
const OUT_CHARACTER_PRECIS = 2
const OUT_DEFAULT_PRECIS = 0
const OUT_DEVICE_PRECIS = 5
const OUT_OF_PROCESS_FUNCTION_TABLE_CALLBACK_EXPORT_NAME = "OutOfProcessFunctionTableCallback"
const OUT_OUTLINE_PRECIS = 8
const OUT_PS_ONLY_PRECIS = 10
const OUT_RASTER_PRECIS = 6
const OUT_SCREEN_OUTLINE_PRECIS = 9
const OUT_STRING_PRECIS = 1
const OUT_STROKE_PRECIS = 3
const OUT_TT_ONLY_PRECIS = 7
const OUT_TT_PRECIS = 4
const OVERWRITE_HIDDEN = 4
const ObjectCloseAuditAlarm = "ObjectCloseAuditAlarmA"
const ObjectDeleteAuditAlarm = "ObjectDeleteAuditAlarmA"
const ObjectOpenAuditAlarm = "ObjectOpenAuditAlarmA"
const ObjectPrivilegeAuditAlarm = "ObjectPrivilegeAuditAlarmA"
const OemToAnsi = "OemToCharA"
const OemToAnsiBuff = "OemToCharBuffA"
const OffFSELog = 8
const OpenFileMapping = "OpenFileMappingA"
const OpenMutex = "OpenMutexA"
const OpenSemaphore = "OpenSemaphoreA"
const OpenWaitableTimer = "OpenWaitableTimerA"
const PAGE_ENCLAVE_MASK = 0x10000000
const PAGE_ENCLAVE_THREAD_CONTROL = 0x80000000
const PAGE_ENCLAVE_UNVALIDATED = 0x20000000
const PAGE_EXECUTE = 0x10
const PAGE_EXECUTE_READ = 0x20
const PAGE_EXECUTE_READWRITE = 0x40
const PAGE_EXECUTE_WRITECOPY = 0x80
const PAGE_GRAPHICS_COHERENT = 0x20000
const PAGE_GRAPHICS_EXECUTE = 0x4000
const PAGE_GRAPHICS_EXECUTE_READ = 0x8000
const PAGE_GRAPHICS_EXECUTE_READWRITE = 0x10000
const PAGE_GRAPHICS_NOACCESS = 0x0800
const PAGE_GRAPHICS_NOCACHE = 0x40000
const PAGE_GRAPHICS_READONLY = 0x1000
const PAGE_GRAPHICS_READWRITE = 0x2000
const PAGE_GUARD = 0x100
const PAGE_NOACCESS = 0x01
const PAGE_NOCACHE = 0x200
const PAGE_READONLY = 0x02
const PAGE_READWRITE = 0x04
const PAGE_REVERT_TO_FILE_MAP = 0x80000000
const PAGE_TARGETS_INVALID = 0x40000000
const PAGE_TARGETS_NO_UPDATE = 0x40000000
const PAGE_WRITECOMBINE = 0x400
const PAGE_WRITECOPY = 0x08
const PANOSE_COUNT = 10
const PAN_ANY = 0
const PAN_ARMSTYLE_INDEX = 6
const PAN_BENT_ARMS_DOUBLE_SERIF = 11
const PAN_BENT_ARMS_HORZ = 7
const PAN_BENT_ARMS_SINGLE_SERIF = 10
const PAN_BENT_ARMS_VERT = 9
const PAN_BENT_ARMS_WEDGE = 8
const PAN_CONTRAST_HIGH = 8
const PAN_CONTRAST_INDEX = 4
const PAN_CONTRAST_LOW = 4
const PAN_CONTRAST_MEDIUM = 6
const PAN_CONTRAST_MEDIUM_HIGH = 7
const PAN_CONTRAST_MEDIUM_LOW = 5
const PAN_CONTRAST_NONE = 2
const PAN_CONTRAST_VERY_HIGH = 9
const PAN_CONTRAST_VERY_LOW = 3
const PAN_CULTURE_LATIN = 0
const PAN_FAMILYTYPE_INDEX = 0
const PAN_FAMILY_DECORATIVE = 4
const PAN_FAMILY_PICTORIAL = 5
const PAN_FAMILY_SCRIPT = 3
const PAN_FAMILY_TEXT_DISPLAY = 2
const PAN_LETTERFORM_INDEX = 7
const PAN_LETT_NORMAL_BOXED = 4
const PAN_LETT_NORMAL_CONTACT = 2
const PAN_LETT_NORMAL_FLATTENED = 5
const PAN_LETT_NORMAL_OFF_CENTER = 7
const PAN_LETT_NORMAL_ROUNDED = 6
const PAN_LETT_NORMAL_SQUARE = 8
const PAN_LETT_NORMAL_WEIGHTED = 3
const PAN_LETT_OBLIQUE_BOXED = 11
const PAN_LETT_OBLIQUE_CONTACT = 9
const PAN_LETT_OBLIQUE_FLATTENED = 12
const PAN_LETT_OBLIQUE_OFF_CENTER = 14
const PAN_LETT_OBLIQUE_ROUNDED = 13
const PAN_LETT_OBLIQUE_SQUARE = 15
const PAN_LETT_OBLIQUE_WEIGHTED = 10
const PAN_MIDLINE_CONSTANT_POINTED = 9
const PAN_MIDLINE_CONSTANT_SERIFED = 10
const PAN_MIDLINE_CONSTANT_TRIMMED = 8
const PAN_MIDLINE_HIGH_POINTED = 6
const PAN_MIDLINE_HIGH_SERIFED = 7
const PAN_MIDLINE_HIGH_TRIMMED = 5
const PAN_MIDLINE_INDEX = 8
const PAN_MIDLINE_LOW_POINTED = 12
const PAN_MIDLINE_LOW_SERIFED = 13
const PAN_MIDLINE_LOW_TRIMMED = 11
const PAN_MIDLINE_STANDARD_POINTED = 3
const PAN_MIDLINE_STANDARD_SERIFED = 4
const PAN_MIDLINE_STANDARD_TRIMMED = 2
const PAN_NO_FIT = 1
const PAN_PROPORTION_INDEX = 3
const PAN_PROP_CONDENSED = 6
const PAN_PROP_EVEN_WIDTH = 4
const PAN_PROP_EXPANDED = 5
const PAN_PROP_MODERN = 3
const PAN_PROP_MONOSPACED = 9
const PAN_PROP_OLD_STYLE = 2
const PAN_PROP_VERY_CONDENSED = 8
const PAN_PROP_VERY_EXPANDED = 7
const PAN_SERIFSTYLE_INDEX = 1
const PAN_SERIF_BONE = 8
const PAN_SERIF_COVE = 2
const PAN_SERIF_EXAGGERATED = 9
const PAN_SERIF_FLARED = 14
const PAN_SERIF_NORMAL_SANS = 11
const PAN_SERIF_OBTUSE_COVE = 3
const PAN_SERIF_OBTUSE_SANS = 12
const PAN_SERIF_OBTUSE_SQUARE_COVE = 5
const PAN_SERIF_PERP_SANS = 13
const PAN_SERIF_ROUNDED = 15
const PAN_SERIF_SQUARE = 6
const PAN_SERIF_SQUARE_COVE = 4
const PAN_SERIF_THIN = 7
const PAN_SERIF_TRIANGLE = 10
const PAN_STRAIGHT_ARMS_DOUBLE_SERIF = 6
const PAN_STRAIGHT_ARMS_HORZ = 2
const PAN_STRAIGHT_ARMS_SINGLE_SERIF = 5
const PAN_STRAIGHT_ARMS_VERT = 4
const PAN_STRAIGHT_ARMS_WEDGE = 3
const PAN_STROKEVARIATION_INDEX = 5
const PAN_STROKE_GRADUAL_DIAG = 2
const PAN_STROKE_GRADUAL_HORZ = 5
const PAN_STROKE_GRADUAL_TRAN = 3
const PAN_STROKE_GRADUAL_VERT = 4
const PAN_STROKE_INSTANT_VERT = 8
const PAN_STROKE_RAPID_HORZ = 7
const PAN_STROKE_RAPID_VERT = 6
const PAN_WEIGHT_BLACK = 10
const PAN_WEIGHT_BOLD = 8
const PAN_WEIGHT_BOOK = 5
const PAN_WEIGHT_DEMI = 7
const PAN_WEIGHT_HEAVY = 9
const PAN_WEIGHT_INDEX = 2
const PAN_WEIGHT_LIGHT = 3
const PAN_WEIGHT_MEDIUM = 6
const PAN_WEIGHT_NORD = 11
const PAN_WEIGHT_THIN = 4
const PAN_WEIGHT_VERY_LIGHT = 2
const PAN_XHEIGHT_CONSTANT_LARGE = 4
const PAN_XHEIGHT_CONSTANT_SMALL = 2
const PAN_XHEIGHT_CONSTANT_STD = 3
const PAN_XHEIGHT_DUCKING_LARGE = 7
const PAN_XHEIGHT_DUCKING_SMALL = 5
const PAN_XHEIGHT_DUCKING_STD = 6
const PAN_XHEIGHT_INDEX = 9
const PASCAL = "__stdcall"
const PASSEMBLY_DLL_REDIRECTION_DETAILED_INFORMATION = "PASSEMBLY_FILE_DETAILED_INFORMATION"
const PASSTHROUGH = 19
const PATH_MAX = 260
const PBTF_APMRESUMEFROMFAILURE = 0x00000001
const PBT_APMBATTERYLOW = 0x0009
const PBT_APMOEMEVENT = 0x000B
const PBT_APMPOWERSTATUSCHANGE = 0x000A
const PBT_APMQUERYSTANDBY = 0x0001
const PBT_APMQUERYSTANDBYFAILED = 0x0003
const PBT_APMQUERYSUSPEND = 0x0000
const PBT_APMQUERYSUSPENDFAILED = 0x0002
const PBT_APMRESUMEAUTOMATIC = 0x0012
const PBT_APMRESUMECRITICAL = 0x0006
const PBT_APMRESUMESTANDBY = 0x0008
const PBT_APMRESUMESUSPEND = 0x0007
const PBT_APMSTANDBY = 0x0005
const PBT_APMSUSPEND = 0x0004
const PBT_POWERSETTINGCHANGE = 32787
const PCASSEMBLY_DLL_REDIRECTION_DETAILED_INFORMATION = "PCASSEMBLY_FILE_DETAILED_INFORMATION"
const PC_EXPLICIT = 0x02
const PC_INTERIORS = 128
const PC_NOCOLLAPSE = 0x04
const PC_NONE = 0
const PC_PATHS = 512
const PC_POLYGON = 1
const PC_POLYPOLYGON = 256
const PC_RECTANGLE = 2
const PC_RESERVED = 0x01
const PC_SCANLINE = 8
const PC_STYLED = 32
const PC_TRAPEZOID = 4
const PC_WIDE = 16
const PC_WIDESTYLED = 64
const PC_WINDPOLYGON = 4
const PDCAP_D0_SUPPORTED = 0x00000001
const PDCAP_D1_SUPPORTED = 0x00000002
const PDCAP_D2_SUPPORTED = 0x00000004
const PDCAP_D3_SUPPORTED = 0x00000008
const PDCAP_WAKE_FROM_D0_SUPPORTED = 0x00000010
const PDCAP_WAKE_FROM_D1_SUPPORTED = 0x00000020
const PDCAP_WAKE_FROM_D2_SUPPORTED = 0x00000040
const PDCAP_WAKE_FROM_D3_SUPPORTED = 0x00000080
const PDCAP_WARM_EJECT_SUPPORTED = 0x00000100
const PDEVICESIZE = 26
const PERFORMANCE_DATA_VERSION = 1
const PERFSTATE_POLICY_CHANGE_IDEAL = 0
const PERFSTATE_POLICY_CHANGE_MAX = "PERFSTATE_POLICY_CHANGE_ROCKET"
const PERFSTATE_POLICY_CHANGE_ROCKET = 2
const PERFSTATE_POLICY_CHANGE_SINGLE = 1
const PFD_DEPTH_DONTCARE = 0x20000000
const PFD_DIRECT3D_ACCELERATED = 0x00004000
const PFD_DOUBLEBUFFER = 0x00000001
const PFD_DOUBLEBUFFER_DONTCARE = 0x40000000
const PFD_DRAW_TO_BITMAP = 0x00000008
const PFD_DRAW_TO_WINDOW = 0x00000004
const PFD_GENERIC_ACCELERATED = 0x00001000
const PFD_GENERIC_FORMAT = 0x00000040
const PFD_MAIN_PLANE = 0
const PFD_NEED_PALETTE = 0x00000080
const PFD_NEED_SYSTEM_PALETTE = 0x00000100
const PFD_OVERLAY_PLANE = 1
const PFD_STEREO = 0x00000002
const PFD_STEREO_DONTCARE = 0x80000000
const PFD_SUPPORT_COMPOSITION = 0x00008000
const PFD_SUPPORT_DIRECTDRAW = 0x00002000
const PFD_SUPPORT_GDI = 0x00000010
const PFD_SUPPORT_OPENGL = 0x00000020
const PFD_SWAP_COPY = 0x00000400
const PFD_SWAP_EXCHANGE = 0x00000200
const PFD_SWAP_LAYER_BUFFERS = 0x00000800
const PFD_TYPE_COLORINDEX = 1
const PFD_TYPE_RGBA = 0
const PF_3DNOW_INSTRUCTIONS_AVAILABLE = 7
const PF_ALPHA_BYTE_INSTRUCTIONS = 5
const PF_ARM_64BIT_LOADSTORE_ATOMIC = 25
const PF_ARM_DIVIDE_INSTRUCTION_AVAILABLE = 24
const PF_ARM_EXTERNAL_CACHE_AVAILABLE = 26
const PF_ARM_FMAC_INSTRUCTIONS_AVAILABLE = 27
const PF_ARM_NEON_INSTRUCTIONS_AVAILABLE = 19
const PF_ARM_SVE2_1_INSTRUCTIONS_AVAILABLE = 48
const PF_ARM_SVE2_INSTRUCTIONS_AVAILABLE = 47
const PF_ARM_SVE_AES_INSTRUCTIONS_AVAILABLE = 49
const PF_ARM_SVE_B16B16_INSTRUCTIONS_AVAILABLE = 54
const PF_ARM_SVE_BF16_INSTRUCTIONS_AVAILABLE = 52
const PF_ARM_SVE_BITPERM_INSTRUCTIONS_AVAILABLE = 51
const PF_ARM_SVE_EBF16_INSTRUCTIONS_AVAILABLE = 53
const PF_ARM_SVE_F32MM_INSTRUCTIONS_AVAILABLE = 58
const PF_ARM_SVE_F64MM_INSTRUCTIONS_AVAILABLE = 59
const PF_ARM_SVE_I8MM_INSTRUCTIONS_AVAILABLE = 57
const PF_ARM_SVE_INSTRUCTIONS_AVAILABLE = 46
const PF_ARM_SVE_PMULL128_INSTRUCTIONS_AVAILABLE = 50
const PF_ARM_SVE_SHA3_INSTRUCTIONS_AVAILABLE = 55
const PF_ARM_SVE_SM4_INSTRUCTIONS_AVAILABLE = 56
const PF_ARM_V81_ATOMIC_INSTRUCTIONS_AVAILABLE = 34
const PF_ARM_V82_DP_INSTRUCTIONS_AVAILABLE = 43
const PF_ARM_V83_JSCVT_INSTRUCTIONS_AVAILABLE = 44
const PF_ARM_V83_LRCPC_INSTRUCTIONS_AVAILABLE = 45
const PF_ARM_V8_CRC32_INSTRUCTIONS_AVAILABLE = 31
const PF_ARM_V8_CRYPTO_INSTRUCTIONS_AVAILABLE = 30
const PF_ARM_V8_INSTRUCTIONS_AVAILABLE = 29
const PF_ARM_VFP_32_REGISTERS_AVAILABLE = 18
const PF_AVX2_INSTRUCTIONS_AVAILABLE = 40
const PF_AVX512F_INSTRUCTIONS_AVAILABLE = 41
const PF_AVX_INSTRUCTIONS_AVAILABLE = 39
const PF_BMI2_INSTRUCTIONS_AVAILABLE = 60
const PF_CHANNELS_ENABLED = 16
const PF_COMPARE64_EXCHANGE128 = 15
const PF_COMPARE_EXCHANGE128 = 14
const PF_COMPARE_EXCHANGE_DOUBLE = 2
const PF_ERMS_AVAILABLE = 42
const PF_FASTFAIL_AVAILABLE = 23
const PF_FLOATING_POINT_EMULATED = 1
const PF_FLOATING_POINT_PRECISION_ERRATA = 0
const PF_MMX_INSTRUCTIONS_AVAILABLE = 3
const PF_MONITORX_INSTRUCTION_AVAILABLE = 35
const PF_NON_TEMPORAL_LEVEL_ALL = "_MM_HINT_NTA"
const PF_NX_ENABLED = 12
const PF_PAE_ENABLED = 9
const PF_PPC_MOVEMEM_64BIT_OK = 4
const PF_RDPID_INSTRUCTION_AVAILABLE = 33
const PF_RDRAND_INSTRUCTION_AVAILABLE = 28
const PF_RDTSCP_INSTRUCTION_AVAILABLE = 32
const PF_RDTSC_INSTRUCTION_AVAILABLE = 8
const PF_RDWRFSGSBASE_AVAILABLE = 22
const PF_SECOND_LEVEL_ADDRESS_TRANSLATION = 20
const PF_SSE3_INSTRUCTIONS_AVAILABLE = 13
const PF_SSE4_1_INSTRUCTIONS_AVAILABLE = 37
const PF_SSE4_2_INSTRUCTIONS_AVAILABLE = 38
const PF_SSE_DAZ_MODE_AVAILABLE = 11
const PF_SSSE3_INSTRUCTIONS_AVAILABLE = 36
const PF_TEMPORAL_LEVEL_1 = "_MM_HINT_T0"
const PF_TEMPORAL_LEVEL_2 = "_MM_HINT_T1"
const PF_TEMPORAL_LEVEL_3 = "_MM_HINT_T2"
const PF_VIRT_FIRMWARE_ENABLED = 21
const PF_XMMI64_INSTRUCTIONS_AVAILABLE = 10
const PF_XMMI_INSTRUCTIONS_AVAILABLE = 6
const PF_XSAVE_ENABLED = 17
const PHYSICALHEIGHT = 111
const PHYSICALOFFSETX = 112
const PHYSICALOFFSETY = 113
const PHYSICALWIDTH = 110
const PIPE_ACCEPT_REMOTE_CLIENTS = 0x0
const PIPE_ACCESS_DUPLEX = 0x3
const PIPE_ACCESS_INBOUND = 0x1
const PIPE_ACCESS_OUTBOUND = 0x2
const PIPE_CLIENT_END = 0x0
const PIPE_NOWAIT = 0x1
const PIPE_READMODE_BYTE = 0x0
const PIPE_READMODE_MESSAGE = 0x2
const PIPE_REJECT_REMOTE_CLIENTS = 0x8
const PIPE_SERVER_END = 0x1
const PIPE_TYPE_BYTE = 0x0
const PIPE_TYPE_MESSAGE = 0x4
const PIPE_UNLIMITED_INSTANCES = 255
const PIPE_WAIT = 0x0
const PLANES = 14
const PMB_ACTIVE = 0x00000001
const PME_CURRENT_VERSION = 1
const PME_FAILFAST_ON_COMMIT_FAIL_DISABLE = 0x0
const PME_FAILFAST_ON_COMMIT_FAIL_ENABLE = 0x1
const PM_NOREMOVE = 0x0000
const PM_NOYIELD = 0x0002
const PM_REMOVE = 0x0001
const POLICY_AUDIT_SUBCATEGORY_COUNT = 56
const POLICY_SHOWREASONUI_ALWAYS = 1
const POLICY_SHOWREASONUI_NEVER = 0
const POLICY_SHOWREASONUI_SERVERONLY = 3
const POLICY_SHOWREASONUI_WORKSTATIONONLY = 2
const POLYFILL_LAST = 2
const POLYGONALCAPS = 32
const POSTSCRIPT_DATA = 37
const POSTSCRIPT_IDENTIFY = 4117
const POSTSCRIPT_IGNORE = 38
const POSTSCRIPT_INJECTION = 4118
const POSTSCRIPT_PASSTHROUGH = 4115
const POWERBUTTON_ACTION_INDEX_HIBERNATE = 2
const POWERBUTTON_ACTION_INDEX_NOTHING = 0
const POWERBUTTON_ACTION_INDEX_SHUTDOWN = 3
const POWERBUTTON_ACTION_INDEX_SLEEP = 1
const POWERBUTTON_ACTION_VALUE_HIBERNATE = 3
const POWERBUTTON_ACTION_VALUE_NOTHING = 0
const POWERBUTTON_ACTION_VALUE_SHUTDOWN = 6
const POWERBUTTON_ACTION_VALUE_SLEEP = 2
const POWER_ACTION_CRITICAL = 0x80000000
const POWER_ACTION_DISABLE_WAKES = 0x40000000
const POWER_ACTION_HIBERBOOT = 0x00000008
const POWER_ACTION_LIGHTEST_FIRST = 0x10000000
const POWER_ACTION_LOCK_CONSOLE = 0x20000000
const POWER_ACTION_OVERRIDE_APPS = 0x00000004
const POWER_ACTION_PSEUDO_TRANSITION = 0x08000000
const POWER_ACTION_QUERY_ALLOWED = 0x00000001
const POWER_ACTION_UI_ALLOWED = 0x00000002
const POWER_DEVICE_IDLE_POLICY_CONSERVATIVE = 1
const POWER_DEVICE_IDLE_POLICY_PERFORMANCE = 0
const POWER_FORCE_TRIGGER_RESET = 0x80000000
const POWER_LEVEL_USER_NOTIFY_EXEC = 0x00000004
const POWER_LEVEL_USER_NOTIFY_SOUND = 0x00000002
const POWER_LEVEL_USER_NOTIFY_TEXT = 0x00000001
const POWER_PLATFORM_ROLE_V1 = 0x00000001
const POWER_PLATFORM_ROLE_V2 = 0x00000002
const POWER_PLATFORM_ROLE_VERSION = "POWER_PLATFORM_ROLE_V1"
const POWER_PLATFORM_ROLE_VERSION_MAX = "POWER_PLATFORM_ROLE_V1_MAX"
const POWER_REQUEST_CONTEXT_DETAILED_STRING = 0x00000002
const POWER_REQUEST_CONTEXT_SIMPLE_STRING = 0x00000001
const POWER_REQUEST_CONTEXT_VERSION = 0
const POWER_SETTING_VALUE_VERSION = 0x1
const POWER_SYSTEM_MAXIMUM = 7
const POWER_USER_NOTIFY_BUTTON = 0x00000008
const POWER_USER_NOTIFY_FORCED_SHUTDOWN = 0x00000020
const POWER_USER_NOTIFY_SHUTDOWN = 0x00000010
const PO_THROTTLE_ADAPTIVE = 3
const PO_THROTTLE_CONSTANT = 1
const PO_THROTTLE_DEGRADE = 2
const PO_THROTTLE_MAXIMUM = 4
const PO_THROTTLE_NONE = 0
const PPM_FIRMWARE_ACPI1C2 = 0x1
const PPM_FIRMWARE_ACPI1C3 = 0x2
const PPM_FIRMWARE_ACPI1TSTATES = 0x4
const PPM_FIRMWARE_CPC = 0x40000
const PPM_FIRMWARE_CSD = 0x10
const PPM_FIRMWARE_CST = 0x8
const PPM_FIRMWARE_OSC = 0x10000
const PPM_FIRMWARE_PCCH = 0x4000
const PPM_FIRMWARE_PCCP = 0x8000
const PPM_FIRMWARE_PCT = 0x20
const PPM_FIRMWARE_PDC = 0x20000
const PPM_FIRMWARE_PPC = 0x100
const PPM_FIRMWARE_PSD = 0x200
const PPM_FIRMWARE_PSS = 0x40
const PPM_FIRMWARE_PTC = 0x400
const PPM_FIRMWARE_TPC = 0x1000
const PPM_FIRMWARE_TSD = 0x2000
const PPM_FIRMWARE_TSS = 0x800
const PPM_FIRMWARE_XPSS = 0x80
const PPM_IDLE_IMPLEMENTATION_CSTATES = 0x1
const PPM_IDLE_IMPLEMENTATION_NONE = 0x0
const PPM_IDLE_IMPLEMENTATION_PEP = 0x2
const PPM_PERFORMANCE_IMPLEMENTATION_CPPC = 3
const PPM_PERFORMANCE_IMPLEMENTATION_NONE = 0
const PPM_PERFORMANCE_IMPLEMENTATION_PCCV1 = 2
const PPM_PERFORMANCE_IMPLEMENTATION_PEP = 4
const PPM_PERFORMANCE_IMPLEMENTATION_PSTATES = 1
const PP_DISPLAYERRORS = 0x01
const PRAGMA_DEPRECATED_DDK = 0
const PRINTRATEUNIT_CPS = 2
const PRINTRATEUNIT_IPM = 4
const PRINTRATEUNIT_LPM = 3
const PRINTRATEUNIT_PPM = 1
const PRIVATE_NAMESPACE_FLAG_DESTROY = 0x1
const PRIVILEGE_SET_ALL_NECESSARY = 1
const PROCESSOR_ALPHA_21064 = 21064
const PROCESSOR_AMD_X8664 = 8664
const PROCESSOR_ARCHITECTURE_ALPHA = 2
const PROCESSOR_ARCHITECTURE_ALPHA64 = 7
const PROCESSOR_ARCHITECTURE_AMD64 = 9
const PROCESSOR_ARCHITECTURE_ARM = 5
const PROCESSOR_ARCHITECTURE_ARM32_ON_WIN64 = 13
const PROCESSOR_ARCHITECTURE_ARM64 = 12
const PROCESSOR_ARCHITECTURE_IA32_ON_ARM64 = 14
const PROCESSOR_ARCHITECTURE_IA32_ON_WIN64 = 10
const PROCESSOR_ARCHITECTURE_IA64 = 6
const PROCESSOR_ARCHITECTURE_INTEL = 0
const PROCESSOR_ARCHITECTURE_MIPS = 1
const PROCESSOR_ARCHITECTURE_MSIL = 8
const PROCESSOR_ARCHITECTURE_NEUTRAL = 11
const PROCESSOR_ARCHITECTURE_PPC = 3
const PROCESSOR_ARCHITECTURE_SHX = 4
const PROCESSOR_ARCHITECTURE_UNKNOWN = 0xffff
const PROCESSOR_ARM720 = 1824
const PROCESSOR_ARM820 = 2080
const PROCESSOR_ARM920 = 2336
const PROCESSOR_ARM_7TDMI = 70001
const PROCESSOR_HITACHI_SH3 = 10003
const PROCESSOR_HITACHI_SH3E = 10004
const PROCESSOR_HITACHI_SH4 = 10005
const PROCESSOR_IDLESTATE_POLICY_COUNT = 3
const PROCESSOR_INTEL_386 = 386
const PROCESSOR_INTEL_486 = 486
const PROCESSOR_INTEL_IA64 = 2200
const PROCESSOR_INTEL_PENTIUM = 586
const PROCESSOR_MIPS_R4000 = 4000
const PROCESSOR_MOTOROLA_821 = 821
const PROCESSOR_OPTIL = 0x494f
const PROCESSOR_PERF_BOOST_MODE_AGGRESSIVE = 2
const PROCESSOR_PERF_BOOST_MODE_DISABLED = 0
const PROCESSOR_PERF_BOOST_MODE_EFFICIENT_AGGRESSIVE = 4
const PROCESSOR_PERF_BOOST_MODE_EFFICIENT_ENABLED = 3
const PROCESSOR_PERF_BOOST_MODE_ENABLED = 1
const PROCESSOR_PERF_BOOST_MODE_MAX = "PROCESSOR_PERF_BOOST_MODE_EFFICIENT_AGGRESSIVE"
const PROCESSOR_PERF_BOOST_POLICY_DISABLED = 0
const PROCESSOR_PERF_BOOST_POLICY_MAX = 100
const PROCESSOR_PPC_601 = 601
const PROCESSOR_PPC_603 = 603
const PROCESSOR_PPC_604 = 604
const PROCESSOR_PPC_620 = 620
const PROCESSOR_SHx_SH3 = 103
const PROCESSOR_SHx_SH4 = 104
const PROCESSOR_STRONGARM = 2577
const PROCESS_CREATE_PROCESS = 0x0080
const PROCESS_CREATE_THREAD = 0x0002
const PROCESS_DEP_DISABLE_ATL_THUNK_EMULATION = 0x00000002
const PROCESS_DEP_ENABLE = 0x00000001
const PROCESS_DUP_HANDLE = 0x0040
const PROCESS_HEAP_ENTRY_BUSY = 0x4
const PROCESS_HEAP_ENTRY_DDESHARE = 0x20
const PROCESS_HEAP_ENTRY_MOVEABLE = 0x10
const PROCESS_HEAP_REGION = 0x1
const PROCESS_HEAP_SEG_ALLOC = 0x8
const PROCESS_HEAP_UNCOMMITTED_RANGE = 0x2
const PROCESS_LEAP_SECOND_INFO_FLAG_ENABLE_SIXTY_SECOND = 0x1
const PROCESS_LEAP_SECOND_INFO_VALID_FLAGS = "PROCESS_LEAP_SECOND_INFO_FLAG_ENABLE_SIXTY_SECOND"
const PROCESS_MODE_BACKGROUND_BEGIN = 0x100000
const PROCESS_MODE_BACKGROUND_END = 0x200000
const PROCESS_NAME_NATIVE = 0x00000001
const PROCESS_POWER_THROTTLING_CURRENT_VERSION = 1
const PROCESS_POWER_THROTTLING_EXECUTION_SPEED = 0x1
const PROCESS_POWER_THROTTLING_IGNORE_TIMER_RESOLUTION = 0x4
const PROCESS_QUERY_INFORMATION = 0x0400
const PROCESS_QUERY_LIMITED_INFORMATION = 0x1000
const PROCESS_SET_INFORMATION = 0x0200
const PROCESS_SET_LIMITED_INFORMATION = 0x2000
const PROCESS_SET_QUOTA = 0x0100
const PROCESS_SET_SESSIONID = 0x0004
const PROCESS_SUSPEND_RESUME = 0x0800
const PROCESS_TERMINATE = 0x0001
const PROCESS_VM_OPERATION = 0x0008
const PROCESS_VM_READ = 0x0010
const PROCESS_VM_WRITE = 0x0020
const PROC_IDLE_BUCKET_COUNT = 6
const PROC_IDLE_BUCKET_COUNT_EX = 16
const PROC_THREAD_ATTRIBUTE_ADDITIVE = 0x00040000
const PROC_THREAD_ATTRIBUTE_INPUT = 0x00020000
const PROC_THREAD_ATTRIBUTE_NUMBER = 0x0000ffff
const PROC_THREAD_ATTRIBUTE_REPLACE_VALUE = 0x00000001
const PROC_THREAD_ATTRIBUTE_THREAD = 0x00010000
const PRODUCT_ARM64_SERVER = 0x78
const PRODUCT_AZURESTACKHCI_SERVER_CORE = 0x196
const PRODUCT_AZURE_NANO_SERVER = 0xA9
const PRODUCT_AZURE_SERVER_AGENTBRIDGE = 0xD0
const PRODUCT_AZURE_SERVER_CLOUDHOST = 0xC7
const PRODUCT_AZURE_SERVER_CLOUDMOS = 0xC8
const PRODUCT_AZURE_SERVER_CORE = 0xA8
const PRODUCT_AZURE_SERVER_NANOHOST = 0xD1
const PRODUCT_BUSINESS = 0x6
const PRODUCT_BUSINESS_N = 0x10
const PRODUCT_CLOUD = 0xB2
const PRODUCT_CLOUDE = 0xB7
const PRODUCT_CLOUDEDITION = 0xCB
const PRODUCT_CLOUDEDITIONN = 0xCA
const PRODUCT_CLOUDEN = 0xBA
const PRODUCT_CLOUDN = 0xB3
const PRODUCT_CLOUD_HOST_INFRASTRUCTURE_SERVER = 0x7C
const PRODUCT_CLOUD_STORAGE_SERVER = 0x6E
const PRODUCT_CLUSTER_SERVER = 0x12
const PRODUCT_CLUSTER_SERVER_V = 0x40
const PRODUCT_CONNECTED_CAR = 0x75
const PRODUCT_CORE = 0x65
const PRODUCT_CORE_ARM = 0x61
const PRODUCT_CORE_CONNECTED = 0x6F
const PRODUCT_CORE_CONNECTED_COUNTRYSPECIFIC = 0x74
const PRODUCT_CORE_CONNECTED_N = 0x71
const PRODUCT_CORE_CONNECTED_SINGLELANGUAGE = 0x73
const PRODUCT_CORE_COUNTRYSPECIFIC = 0x63
const PRODUCT_CORE_LANGUAGESPECIFIC = 0x64
const PRODUCT_CORE_N = 0x62
const PRODUCT_CORE_SINGLELANGUAGE = 0x64
const PRODUCT_DATACENTER_A_SERVER_CORE = 0x91
const PRODUCT_DATACENTER_EVALUATION_SERVER = 0x50
const PRODUCT_DATACENTER_EVALUATION_SERVER_CORE = 0x9F
const PRODUCT_DATACENTER_NANO_SERVER = 0x8F
const PRODUCT_DATACENTER_SERVER = 0x8
const PRODUCT_DATACENTER_SERVER_AZURE_EDITION = 0x197
const PRODUCT_DATACENTER_SERVER_CORE = 0xc
const PRODUCT_DATACENTER_SERVER_CORE_AZURE_EDITION = 0x198
const PRODUCT_DATACENTER_SERVER_CORE_V = 0x27
const PRODUCT_DATACENTER_SERVER_V = 0x25
const PRODUCT_DATACENTER_WS_SERVER_CORE = 0x93
const PRODUCT_EDUCATION = 0x79
const PRODUCT_EDUCATION_N = 0x7a
const PRODUCT_EMBEDDED = 0x41
const PRODUCT_EMBEDDED_A = 0x58
const PRODUCT_EMBEDDED_AUTOMOTIVE = 0x55
const PRODUCT_EMBEDDED_E = 0x5A
const PRODUCT_EMBEDDED_EVAL = 0x6B
const PRODUCT_EMBEDDED_E_EVAL = 0x6C
const PRODUCT_EMBEDDED_INDUSTRY = 0x59
const PRODUCT_EMBEDDED_INDUSTRY_A = 0x56
const PRODUCT_EMBEDDED_INDUSTRY_A_E = 0x5C
const PRODUCT_EMBEDDED_INDUSTRY_E = 0x5B
const PRODUCT_EMBEDDED_INDUSTRY_EVAL = 0x69
const PRODUCT_EMBEDDED_INDUSTRY_E_EVAL = 0x6A
const PRODUCT_ENTERPRISE = 0x4
const PRODUCT_ENTERPRISEG = 0xAB
const PRODUCT_ENTERPRISEGN = 0xAC
const PRODUCT_ENTERPRISE_E = 0x46
const PRODUCT_ENTERPRISE_EVALUATION = 0x48
const PRODUCT_ENTERPRISE_N = 0x1b
const PRODUCT_ENTERPRISE_N_EVALUATION = 0x54
const PRODUCT_ENTERPRISE_S = 0x7D
const PRODUCT_ENTERPRISE_SERVER = 0xa
const PRODUCT_ENTERPRISE_SERVER_CORE = 0xe
const PRODUCT_ENTERPRISE_SERVER_CORE_V = 0x29
const PRODUCT_ENTERPRISE_SERVER_IA64 = 0xf
const PRODUCT_ENTERPRISE_SERVER_V = 0x26
const PRODUCT_ENTERPRISE_SUBSCRIPTION = 0x8C
const PRODUCT_ENTERPRISE_SUBSCRIPTION_N = 0x8D
const PRODUCT_ENTERPRISE_S_EVALUATION = 0x81
const PRODUCT_ENTERPRISE_S_N = 0x7E
const PRODUCT_ENTERPRISE_S_N_EVALUATION = 0x82
const PRODUCT_ESSENTIALBUSINESS_SERVER_ADDL = 0x3C
const PRODUCT_ESSENTIALBUSINESS_SERVER_ADDLSVC = 0x3E
const PRODUCT_ESSENTIALBUSINESS_SERVER_MGMT = 0x3B
const PRODUCT_ESSENTIALBUSINESS_SERVER_MGMTSVC = 0x3D
const PRODUCT_HOLOGRAPHIC = 0x87
const PRODUCT_HOLOGRAPHIC_BUSINESS = 0x88
const PRODUCT_HOME_BASIC = 0x2
const PRODUCT_HOME_BASIC_E = 0x43
const PRODUCT_HOME_BASIC_N = 0x5
const PRODUCT_HOME_PREMIUM = 0x3
const PRODUCT_HOME_PREMIUM_E = 0x44
const PRODUCT_HOME_PREMIUM_N = 0x1a
const PRODUCT_HOME_PREMIUM_SERVER = 0x22
const PRODUCT_HOME_SERVER = 0x13
const PRODUCT_HUBOS = 0xB4
const PRODUCT_HYPERV = 0x2a
const PRODUCT_INDUSTRY_HANDHELD = 0x76
const PRODUCT_IOTEDGEOS = 0xBB
const PRODUCT_IOTENTERPRISE = 0xBC
const PRODUCT_IOTENTERPRISEK = 0xCE
const PRODUCT_IOTENTERPRISES = 0xBF
const PRODUCT_IOTENTERPRISESEVAL = 0xCF
const PRODUCT_IOTENTERPRISESK = 0xCD
const PRODUCT_IOTOS = 0xB9
const PRODUCT_IOTUAP = 0x7B
const PRODUCT_LITE = 0xBD
const PRODUCT_MEDIUMBUSINESS_SERVER_MANAGEMENT = 0x1e
const PRODUCT_MEDIUMBUSINESS_SERVER_MESSAGING = 0x20
const PRODUCT_MEDIUMBUSINESS_SERVER_SECURITY = 0x1f
const PRODUCT_MOBILE_CORE = 0x68
const PRODUCT_MOBILE_ENTERPRISE = 0x85
const PRODUCT_MULTIPOINT_PREMIUM_SERVER = 0x4D
const PRODUCT_MULTIPOINT_STANDARD_SERVER = 0x4C
const PRODUCT_NANO_SERVER = 0x6D
const PRODUCT_ONECOREUPDATEOS = 0xB6
const PRODUCT_PPI_PRO = 0x77
const PRODUCT_PROFESSIONAL = 0x30
const PRODUCT_PROFESSIONAL_E = 0x45
const PRODUCT_PROFESSIONAL_EMBEDDED = 0x3A
const PRODUCT_PROFESSIONAL_N = 0x31
const PRODUCT_PROFESSIONAL_S = 0x7F
const PRODUCT_PROFESSIONAL_STUDENT = 0x70
const PRODUCT_PROFESSIONAL_STUDENT_N = 0x72
const PRODUCT_PROFESSIONAL_S_N = 0x80
const PRODUCT_PROFESSIONAL_WMC = 0x67
const PRODUCT_PRO_CHINA = 0x8B
const PRODUCT_PRO_FOR_EDUCATION = 0xA4
const PRODUCT_PRO_FOR_EDUCATION_N = 0xA5
const PRODUCT_PRO_SINGLE_LANGUAGE = 0x8A
const PRODUCT_PRO_WORKSTATION = 0xA1
const PRODUCT_PRO_WORKSTATION_N = 0xA2
const PRODUCT_SB_SOLUTION_SERVER = 0x32
const PRODUCT_SB_SOLUTION_SERVER_EM = 0x36
const PRODUCT_SERVERRDSH = 0xAF
const PRODUCT_SERVER_FOR_SB_SOLUTIONS = 0x33
const PRODUCT_SERVER_FOR_SB_SOLUTIONS_EM = 0x37
const PRODUCT_SERVER_FOR_SMALLBUSINESS = 0x18
const PRODUCT_SERVER_FOR_SMALLBUSINESS_V = 0x23
const PRODUCT_SERVER_FOUNDATION = 0x21
const PRODUCT_SERVER_V = 0x25
const PRODUCT_SMALLBUSINESS_SERVER = 0x9
const PRODUCT_SMALLBUSINESS_SERVER_PREMIUM = 0x19
const PRODUCT_SMALLBUSINESS_SERVER_PREMIUM_CORE = 0x3f
const PRODUCT_SOLUTION_EMBEDDEDSERVER = 0x38
const PRODUCT_SOLUTION_EMBEDDEDSERVER_CORE = 0x39
const PRODUCT_STANDARD_A_SERVER_CORE = 0x92
const PRODUCT_STANDARD_EVALUATION_SERVER = 0x4F
const PRODUCT_STANDARD_EVALUATION_SERVER_CORE = 0xA0
const PRODUCT_STANDARD_NANO_SERVER = 0x90
const PRODUCT_STANDARD_SERVER = 0x7
const PRODUCT_STANDARD_SERVER_CORE = 0xd
const PRODUCT_STANDARD_SERVER_CORE_V = 0x28
const PRODUCT_STANDARD_SERVER_SOLUTIONS = 0x34
const PRODUCT_STANDARD_SERVER_SOLUTIONS_CORE = 0x35
const PRODUCT_STANDARD_SERVER_V = 0x24
const PRODUCT_STANDARD_WS_SERVER_CORE = 0x94
const PRODUCT_STARTER = 0xb
const PRODUCT_STARTER_E = 0x42
const PRODUCT_STARTER_N = 0x2f
const PRODUCT_STORAGE_ENTERPRISE_SERVER = 0x17
const PRODUCT_STORAGE_ENTERPRISE_SERVER_CORE = 0x2e
const PRODUCT_STORAGE_EXPRESS_SERVER = 0x14
const PRODUCT_STORAGE_EXPRESS_SERVER_CORE = 0x2b
const PRODUCT_STORAGE_STANDARD_EVALUATION_SERVER = 0x60
const PRODUCT_STORAGE_STANDARD_SERVER = 0x15
const PRODUCT_STORAGE_STANDARD_SERVER_CORE = 0x2c
const PRODUCT_STORAGE_WORKGROUP_EVALUATION_SERVER = 0x5F
const PRODUCT_STORAGE_WORKGROUP_SERVER = 0x16
const PRODUCT_STORAGE_WORKGROUP_SERVER_CORE = 0x2d
const PRODUCT_THINPC = 0x57
const PRODUCT_ULTIMATE = 0x1
const PRODUCT_ULTIMATE_E = 0x47
const PRODUCT_ULTIMATE_N = 0x1c
const PRODUCT_UNDEFINED = 0x0
const PRODUCT_UNLICENSED = 0xabcdabcd
const PRODUCT_UTILITY_VM = 0x95
const PRODUCT_VALIDATION = 0xCC
const PRODUCT_WEB_SERVER = 0x11
const PRODUCT_WEB_SERVER_CORE = 0x1d
const PRODUCT_WNC = 0xD2
const PRODUCT_XBOX_DURANGOHOSTOS = 0xC4
const PRODUCT_XBOX_ERAOS = 0xC3
const PRODUCT_XBOX_GAMEOS = 0xC2
const PRODUCT_XBOX_KEYSTONE = 0xC6
const PRODUCT_XBOX_NATIVEOS = 0xC1
const PRODUCT_XBOX_SCARLETTHOSTOS = 0xC5
const PRODUCT_XBOX_SYSTEMOS = 0xC0
const PROFILE_KERNEL = 0x20000000
const PROFILE_SERVER = 0x40000000
const PROFILE_USER = 0x10000000
const PROGRESS_CANCEL = 1
const PROGRESS_CONTINUE = 0
const PROGRESS_QUIET = 3
const PROGRESS_STOP = 2
const PROOF_QUALITY = 2
const PROTECTION_LEVEL_ANTIMALWARE_LIGHT = 0x00000003
const PROTECTION_LEVEL_AUTHENTICODE = 0x00000007
const PROTECTION_LEVEL_CODEGEN_LIGHT = 0x00000006
const PROTECTION_LEVEL_LSA_LIGHT = 0x00000004
const PROTECTION_LEVEL_NONE = 0xfffffffe
const PROTECTION_LEVEL_PPL_APP = 0x00000008
const PROTECTION_LEVEL_SAME = 0xffffffff
const PROTECTION_LEVEL_WINDOWS = 0x00000001
const PROTECTION_LEVEL_WINDOWS_LIGHT = 0x00000002
const PROTECTION_LEVEL_WINTCB = 0x00000005
const PROTECTION_LEVEL_WINTCB_LIGHT = 0x00000000
const PROVIDER_KEEPS_VALUE_LENGTH = 0x1
const PR_JOBSTATUS = 0x0000
const PSIDENT_GDICENTRIC = 0
const PSIDENT_PSCENTRIC = 1
const PSINJECT_BEGINDEFAULTS = 12
const PSINJECT_BEGINPAGESETUP = 101
const PSINJECT_BEGINPROLOG = 14
const PSINJECT_BEGINSETUP = 16
const PSINJECT_BEGINSTREAM = 1
const PSINJECT_BOUNDINGBOX = 9
const PSINJECT_COMMENTS = 11
const PSINJECT_DLFONT = 0xdddddddd
const PSINJECT_DOCNEEDEDRES = 5
const PSINJECT_DOCSUPPLIEDRES = 6
const PSINJECT_DOCUMENTPROCESSCOLORS = 10
const PSINJECT_DOCUMENTPROCESSCOLORSATEND = 21
const PSINJECT_ENDDEFAULTS = 13
const PSINJECT_ENDPAGECOMMENTS = 107
const PSINJECT_ENDPAGESETUP = 102
const PSINJECT_ENDPROLOG = 15
const PSINJECT_ENDSETUP = 17
const PSINJECT_ENDSTREAM = 20
const PSINJECT_EOF = 19
const PSINJECT_ORIENTATION = 8
const PSINJECT_PAGEBBOX = 106
const PSINJECT_PAGENUMBER = 100
const PSINJECT_PAGEORDER = 7
const PSINJECT_PAGES = 4
const PSINJECT_PAGESATEND = 3
const PSINJECT_PAGETRAILER = 103
const PSINJECT_PLATECOLOR = 104
const PSINJECT_PSADOBE = 2
const PSINJECT_SHOWPAGE = 105
const PSINJECT_TRAILER = 18
const PSINJECT_VMRESTORE = 201
const PSINJECT_VMSAVE = 200
const PSPROTOCOL_ASCII = 0
const PSPROTOCOL_BCP = 1
const PSPROTOCOL_BINARY = 3
const PSPROTOCOL_TBCP = 2
const PS_ALTERNATE = 8
const PS_COSMETIC = 0x00000000
const PS_DASH = 1
const PS_DASHDOT = 3
const PS_DASHDOTDOT = 4
const PS_DOT = 2
const PS_ENDCAP_FLAT = 0x00000200
const PS_ENDCAP_MASK = 0x00000F00
const PS_ENDCAP_ROUND = 0x00000000
const PS_ENDCAP_SQUARE = 0x00000100
const PS_GEOMETRIC = 0x00010000
const PS_INSIDEFRAME = 6
const PS_JOIN_BEVEL = 0x00001000
const PS_JOIN_MASK = 0x0000F000
const PS_JOIN_MITER = 0x00002000
const PS_JOIN_ROUND = 0x00000000
const PS_NULL = 5
const PS_SOLID = 0
const PS_STYLE_MASK = 0x0000000F
const PS_TYPE_MASK = 0x000F0000
const PS_USERSTYLE = 7
const PTRDIFF_MAX = "INT64_MAX"
const PTRDIFF_MIN = "INT64_MIN"
const PT_BEZIERTO = 0x04
const PT_CLOSEFIGURE = 0x01
const PT_LINETO = 0x02
const PT_MOVETO = 0x06
const PURGE_RXABORT = 0x2
const PURGE_RXCLEAR = 0x8
const PURGE_TXABORT = 0x1
const PURGE_TXCLEAR = 0x4
const PWR_CRITICALRESUME = 3
const PWR_OK = 1
const PWR_SUSPENDREQUEST = 1
const PWR_SUSPENDRESUME = 2
const PW_CLIENTONLY = 0x00000001
const P_DETACH = "_P_DETACH"
const P_NOWAIT = "_P_NOWAIT"
const P_NOWAITO = "_P_NOWAITO"
const P_OVERLAY = "_P_OVERLAY"
const P_WAIT = "_P_WAIT"
const P_tmpdir = "_P_tmpdir"
const PrivilegedServiceAuditAlarm = "PrivilegedServiceAuditAlarmA"
const QDI_DIBTOSCREEN = 4
const QDI_GETDIBITS = 2
const QDI_SETDIBITS = 1
const QDI_STRETCHDIB = 8
const QS_ALLPOSTMESSAGE = 0x0100
const QS_HOTKEY = 0x0080
const QS_KEY = 0x0001
const QS_MOUSEBUTTON = 0x0004
const QS_MOUSEMOVE = 0x0002
const QS_PAINT = 0x0020
const QS_POSTMESSAGE = 0x0008
const QS_RAWINPUT = 0x0400
const QS_SENDMESSAGE = 0x0040
const QS_TIMER = 0x0010
const QUERYDIBSUPPORT = 3073
const QUERYESCSUPPORT = 8
const QUERY_ACTCTX_FLAG_ACTCTX_IS_ADDRESS = 0x00000010
const QUERY_ACTCTX_FLAG_ACTCTX_IS_HMODULE = 0x00000008
const QUERY_ACTCTX_FLAG_NO_ADDREF = 0x80000000
const QUERY_ACTCTX_FLAG_USE_ACTIVE_ACTCTX = 0x00000004
const QUOTA_LIMITS_HARDWS_MAX_DISABLE = 0x00000008
const QUOTA_LIMITS_HARDWS_MAX_ENABLE = 0x00000004
const QUOTA_LIMITS_HARDWS_MIN_DISABLE = 0x00000002
const QUOTA_LIMITS_HARDWS_MIN_ENABLE = 0x00000001
const QUOTA_LIMITS_USE_DEFAULT_LIMITS = 0x00000010
const QueryDosDevice = "QueryDosDeviceA"
const R2_BLACK = 1
const R2_COPYPEN = 13
const R2_LAST = 16
const R2_MASKNOTPEN = 3
const R2_MASKPEN = 9
const R2_MASKPENNOT = 5
const R2_MERGENOTPEN = 12
const R2_MERGEPEN = 15
const R2_MERGEPENNOT = 14
const R2_NOP = 11
const R2_NOT = 6
const R2_NOTCOPYPEN = 4
const R2_NOTMASKPEN = 8
const R2_NOTMERGEPEN = 2
const R2_NOTXORPEN = 10
const R2_WHITE = 16
const R2_XORPEN = 7
const RAND_MAX = 0x7fff
const RANK_POSITION_MAX_COUNT_LOG = 32
const RANK_POSITION_TABLE_SIZE = 192
const RASTERCAPS = 38
const RASTER_FONTTYPE = 0x0001
const RC_BANDING = 2
const RC_BIGFONT = 0x0400
const RC_BITBLT = 1
const RC_BITMAP64 = 8
const RC_DEVBITS = 0x8000
const RC_DIBTODEV = 0x0200
const RC_DI_BITMAP = 0x0080
const RC_FLOODFILL = 0x1000
const RC_GDI20_OUTPUT = 0x0010
const RC_GDI20_STATE = 0x0020
const RC_OP_DX_OUTPUT = 0x4000
const RC_PALETTE = 0x0100
const RC_SAVEBITMAP = 0x0040
const RC_SCALING = 4
const RC_STRETCHBLT = 0x0800
const RC_STRETCHDIB = 0x2000
const RDH_RECTANGLES = 1
const RDW_ALLCHILDREN = 0x0080
const RDW_ERASE = 0x0004
const RDW_ERASENOW = 0x0200
const RDW_FRAME = 0x0400
const RDW_INTERNALPAINT = 0x0002
const RDW_INVALIDATE = 0x0001
const RDW_NOCHILDREN = 0x0040
const RDW_NOERASE = 0x0020
const RDW_NOFRAME = 0x0800
const RDW_NOINTERNALPAINT = 0x0010
const RDW_UPDATENOW = 0x0100
const RDW_VALIDATE = 0x0008
const READ_THREAD_PROFILING_FLAG_DISPATCHING = 0x00000001
const READ_THREAD_PROFILING_FLAG_HARDWARE_COUNTERS = 0x00000002
const REALTIME_PRIORITY_CLASS = 0x100
const REASON_LEGACY_API = "SHTDN_REASON_LEGACY_API"
const REASON_PLANNED_FLAG = "SHTDN_REASON_FLAG_PLANNED"
const REASON_UNKNOWN = "SHTDN_REASON_UNKNOWN"
const RECOVERY_DEFAULT_PING_INTERVAL = 5000
const REG_BINARY = 3
const REG_DWORD = 4
const REG_DWORD_BIG_ENDIAN = 5
const REG_DWORD_LITTLE_ENDIAN = 4
const REG_EXPAND_SZ = 2
const REG_FORCE_UNLOAD = 1
const REG_FULL_RESOURCE_DESCRIPTOR = 9
const REG_LATEST_FORMAT = 2
const REG_LINK = 6
const REG_MUI_STRING_TRUNCATE = 0x00000001
const REG_MULTI_SZ = 7
const REG_NONE = 0
const REG_NO_COMPRESSION = 4
const REG_PROCESS_APPKEY = 0x00000001
const REG_QWORD = 11
const REG_QWORD_LITTLE_ENDIAN = 11
const REG_RESOURCE_LIST = 8
const REG_RESOURCE_REQUIREMENTS_LIST = 10
const REG_SECURE_CONNECTION = 1
const REG_STANDARD_FORMAT = 1
const REG_SZ = 1
const RELATIVE = 2
const REMOTE_NAME_INFO_LEVEL = 0x00000002
const REMOTE_PROTOCOL_INFO_FLAG_LOOPBACK = 0x00000001
const REMOTE_PROTOCOL_INFO_FLAG_OFFLINE = 0x00000002
const REPLACEFILE_IGNORE_ACL_ERRORS = 0x4
const REPLACEFILE_IGNORE_MERGE_ERRORS = 0x2
const REPLACEFILE_WRITE_THROUGH = 0x1
const RESETDEV = 7
const RESOURCEDISPLAYTYPE_DIRECTORY = 0x00000009
const RESOURCEDISPLAYTYPE_DOMAIN = 0x00000001
const RESOURCEDISPLAYTYPE_FILE = 0x00000004
const RESOURCEDISPLAYTYPE_GENERIC = 0x00000000
const RESOURCEDISPLAYTYPE_GROUP = 0x00000005
const RESOURCEDISPLAYTYPE_NDSCONTAINER = 0x0000000b
const RESOURCEDISPLAYTYPE_NETWORK = 0x00000006
const RESOURCEDISPLAYTYPE_ROOT = 0x00000007
const RESOURCEDISPLAYTYPE_SERVER = 0x00000002
const RESOURCEDISPLAYTYPE_SHARE = 0x00000003
const RESOURCEDISPLAYTYPE_SHAREADMIN = 0x00000008
const RESOURCEDISPLAYTYPE_TREE = 0x0000000a
const RESOURCEMANAGER_COMPLETE_PROPAGATION = 0x0040
const RESOURCEMANAGER_ENLIST = 0x0008
const RESOURCEMANAGER_GET_NOTIFICATION = 0x0010
const RESOURCEMANAGER_QUERY_INFORMATION = 0x0001
const RESOURCEMANAGER_RECOVER = 0x0004
const RESOURCEMANAGER_REGISTER_PROTOCOL = 0x0020
const RESOURCEMANAGER_SET_INFORMATION = 0x0002
const RESOURCETYPE_ANY = 0x00000000
const RESOURCETYPE_DISK = 0x00000001
const RESOURCETYPE_PRINT = 0x00000002
const RESOURCETYPE_RESERVED = 0x00000008
const RESOURCETYPE_UNKNOWN = 0xFFFFFFFF
const RESOURCEUSAGE_ATTACHED = 0x00000010
const RESOURCEUSAGE_CONNECTABLE = 0x00000001
const RESOURCEUSAGE_CONTAINER = 0x00000002
const RESOURCEUSAGE_NOLOCALDEVICE = 0x00000004
const RESOURCEUSAGE_RESERVED = 0x80000000
const RESOURCEUSAGE_SIBLING = 0x00000008
const RESOURCE_CONNECTED = 0x00000001
const RESOURCE_CONTEXT = 0x00000005
const RESOURCE_ENUM_LN = 0x0001
const RESOURCE_ENUM_MODULE_EXACT = 0x0010
const RESOURCE_ENUM_MUI = 0x0002
const RESOURCE_ENUM_MUI_SYSTEM = 0x0004
const RESOURCE_ENUM_VALIDATE = 0x0008
const RESOURCE_GLOBALNET = 0x00000002
const RESOURCE_MANAGER_COMMUNICATION = 0x00000002
const RESOURCE_MANAGER_MAXIMUM_OPTION = 0x00000003
const RESOURCE_MANAGER_OBJECT_PATH = "\\\\ResourceManager\\\\"
const RESOURCE_MANAGER_VOLATILE = 0x00000001
const RESOURCE_RECENT = 0x00000004
const RESOURCE_REMEMBERED = 0x00000003
const RESTART_MAX_CMD_LINE = 1024
const RESTART_NO_CRASH = 1
const RESTART_NO_HANG = 2
const RESTART_NO_PATCH = 4
const RESTART_NO_REBOOT = 8
const RESTORE_CTM = 4100
const RES_CURSOR = 2
const RES_ICON = 1
const RGN_AND = 1
const RGN_COPY = 5
const RGN_DIFF = 4
const RGN_ERROR = "ERROR"
const RGN_MAX = "RGN_COPY"
const RGN_MIN = "RGN_AND"
const RGN_OR = 2
const RGN_XOR = 3
const RIDEV_APPKEYS = 0x00000400
const RIDEV_CAPTUREMOUSE = 0x00000200
const RIDEV_DEVNOTIFY = 0x00002000
const RIDEV_EXCLUDE = 0x00000010
const RIDEV_EXINPUTSINK = 0x00001000
const RIDEV_EXMODEMASK = 0x000000F0
const RIDEV_INPUTSINK = 0x00000100
const RIDEV_NOHOTKEYS = 0x00000200
const RIDEV_NOLEGACY = 0x00000030
const RIDEV_PAGEONLY = 0x00000020
const RIDEV_REMOVE = 0x00000001
const RIDI_DEVICEINFO = 0x2000000b
const RIDI_DEVICENAME = 0x20000007
const RIDI_PREPARSEDDATA = 0x20000005
const RID_HEADER = 0x10000005
const RID_INPUT = 0x10000003
const RIGHTMOST_BUTTON_PRESSED = 0x0002
const RIGHT_ALT_PRESSED = 0x0001
const RIGHT_CTRL_PRESSED = 0x0004
const RIM_INPUT = 0
const RIM_INPUTSINK = 1
const RIM_TYPEHID = 2
const RIM_TYPEKEYBOARD = 1
const RIM_TYPEMAX = 2
const RIM_TYPEMOUSE = 0
const RIP_EVENT = 9
const RI_KEY_BREAK = 1
const RI_KEY_E0 = 2
const RI_KEY_E1 = 4
const RI_KEY_MAKE = 0
const RI_KEY_TERMSRV_SET_LED = 8
const RI_KEY_TERMSRV_SHADOW = 0x10
const RI_MOUSE_BUTTON_1_DOWN = "RI_MOUSE_LEFT_BUTTON_DOWN"
const RI_MOUSE_BUTTON_1_UP = "RI_MOUSE_LEFT_BUTTON_UP"
const RI_MOUSE_BUTTON_2_DOWN = "RI_MOUSE_RIGHT_BUTTON_DOWN"
const RI_MOUSE_BUTTON_2_UP = "RI_MOUSE_RIGHT_BUTTON_UP"
const RI_MOUSE_BUTTON_3_DOWN = "RI_MOUSE_MIDDLE_BUTTON_DOWN"
const RI_MOUSE_BUTTON_3_UP = "RI_MOUSE_MIDDLE_BUTTON_UP"
const RI_MOUSE_BUTTON_4_DOWN = 0x0040
const RI_MOUSE_BUTTON_4_UP = 0x0080
const RI_MOUSE_BUTTON_5_DOWN = 0x0100
const RI_MOUSE_BUTTON_5_UP = 0x0200
const RI_MOUSE_HWHEEL = 0x0800
const RI_MOUSE_LEFT_BUTTON_DOWN = 0x0001
const RI_MOUSE_LEFT_BUTTON_UP = 0x0002
const RI_MOUSE_MIDDLE_BUTTON_DOWN = 0x0010
const RI_MOUSE_MIDDLE_BUTTON_UP = 0x0020
const RI_MOUSE_RIGHT_BUTTON_DOWN = 0x0004
const RI_MOUSE_RIGHT_BUTTON_UP = 0x0008
const RI_MOUSE_WHEEL = 0x0400
const RPI_SMB2_SHAREFLAG_COMPRESS_DATA = 0x00000002
const RPI_SMB2_SHAREFLAG_ENCRYPT_DATA = 0x00000001
const RP_INIFILE = 0x02
const RP_LOGON = 0x01
const RRF_NOEXPAND = 0x10000000
const RRF_RT_ANY = 0x0000ffff
const RRF_RT_REG_BINARY = 0x00000008
const RRF_RT_REG_DWORD = 0x00000010
const RRF_RT_REG_EXPAND_SZ = 0x00000004
const RRF_RT_REG_MULTI_SZ = 0x00000020
const RRF_RT_REG_NONE = 0x00000001
const RRF_RT_REG_QWORD = 0x00000040
const RRF_RT_REG_SZ = 0x00000002
const RRF_ZEROONFAILURE = 0x20000000
const RSYNC_LENGTH = 32
const RSYNC_MIN_BLOCK_LOG = "ZSTD_BLOCKSIZELOG_MAX"
const RTL_CONDITION_VARIABLE_LOCKMODE_SHARED = 0x1
const RTL_CRITICAL_SECTION_ALL_FLAG_BITS = 0xff000000
const RTL_CRITICAL_SECTION_DEBUG_FLAG_STATIC_INIT = 0x00000001
const RTL_CRITICAL_SECTION_FLAG_DYNAMIC_SPIN = 0x02000000
const RTL_CRITICAL_SECTION_FLAG_FORCE_DEBUG_INFO = 0x10000000
const RTL_CRITICAL_SECTION_FLAG_NO_DEBUG_INFO = 0x01000000
const RTL_CRITICAL_SECTION_FLAG_RESOURCE_TYPE = 0x08000000
const RTL_CRITICAL_SECTION_FLAG_STATIC_INIT = 0x04000000
const RTL_CRITSECT_TYPE = 0
const RTL_RESOURCE_TYPE = 1
const RTL_RUN_ONCE_CTX_RESERVED_BITS = 2
const RTL_UMS_VERSION = 0x0100
const RTL_VRF_FLG_APPCOMPAT_CHECKS = 0x00000010
const RTL_VRF_FLG_COM_CHECKS = 0x00000100
const RTL_VRF_FLG_DANGEROUS_APIS = 0x00000200
const RTL_VRF_FLG_DEADLOCK_CHECKS = 0x00000800
const RTL_VRF_FLG_DIRTY_STACKS = 0x00000040
const RTL_VRF_FLG_ENABLED_SYSTEM_WIDE = 0x00020000
const RTL_VRF_FLG_ENABLE_LOGGING = 0x00004000
const RTL_VRF_FLG_FAST_FILL_HEAP = 0x00008000
const RTL_VRF_FLG_FIRST_CHANCE_EXCEPTION_CHECKS = 0x00001000
const RTL_VRF_FLG_FULL_PAGE_HEAP = 0x00000001
const RTL_VRF_FLG_HANDLE_CHECKS = 0x00000004
const RTL_VRF_FLG_LOCK_CHECKS = 0x00040000
const RTL_VRF_FLG_MISCELLANEOUS_CHECKS = 0x00020000
const RTL_VRF_FLG_RACE_CHECKS = 0x00000400
const RTL_VRF_FLG_RESERVED_DONOTUSE = 0x00000002
const RTL_VRF_FLG_RPC_CHECKS = 0x00000080
const RTL_VRF_FLG_STACK_CHECKS = 0x00000008
const RTL_VRF_FLG_TLS_CHECKS = 0x00000020
const RTL_VRF_FLG_VIRTUAL_MEM_CHECKS = 0x00002000
const RTL_VRF_FLG_VIRTUAL_SPACE_TRACKING = 0x00010000
const RTS_CONTROL_DISABLE = 0x0
const RTS_CONTROL_ENABLE = 0x1
const RTS_CONTROL_HANDSHAKE = 0x2
const RTS_CONTROL_TOGGLE = 0x3
const RUNTIME_FUNCTION_INDIRECT = 0x1
const RUSSIAN_CHARSET = 204
const ReadMxCsr = "_mm_getcsr"
const RotateLeft16 = "_rotl16"
const RotateLeft32 = "_rotl"
const RotateLeft64 = "_rotl64"
const RotateLeft8 = "_rotl8"
const RotateRight16 = "_rotr16"
const RotateRight32 = "_rotr"
const RotateRight64 = "_rotr64"
const RotateRight8 = "_rotr8"
const SANDBOX_INERT = 0x2
const SAVE_CTM = 4101
const SBM_ENABLE_ARROWS = 0x00E4
const SBM_GETPOS = 0x00E1
const SBM_GETRANGE = 0x00E3
const SBM_GETSCROLLBARINFO = 0x00EB
const SBM_GETSCROLLINFO = 0x00EA
const SBM_SETPOS = 0x00E0
const SBM_SETRANGE = 0x00E2
const SBM_SETRANGEREDRAW = 0x00E6
const SBM_SETSCROLLINFO = 0x00E9
const SB_BOTH = 3
const SB_BOTTOM = 7
const SB_CONST_ALPHA = 0x00000001
const SB_CTL = 2
const SB_ENDSCROLL = 8
const SB_GRAD_RECT = 0x00000010
const SB_GRAD_TRI = 0x00000020
const SB_HORZ = 0
const SB_LEFT = 6
const SB_LINEDOWN = 1
const SB_LINELEFT = 0
const SB_LINERIGHT = 1
const SB_LINEUP = 0
const SB_NONE = 0x00000000
const SB_PAGEDOWN = 3
const SB_PAGELEFT = 2
const SB_PAGERIGHT = 3
const SB_PAGEUP = 2
const SB_PIXEL_ALPHA = 0x00000002
const SB_PREMULT_ALPHA = 0x00000004
const SB_RIGHT = 7
const SB_THUMBPOSITION = 4
const SB_THUMBTRACK = 5
const SB_TOP = 6
const SB_VERT = 1
const SCALINGFACTORX = 114
const SCALINGFACTORY = 115
const SCARD_S_SUCCESS = "NO_ERROR"
const SCF_ISSECURE = 0x00000001
const SCROLLLOCK_ON = 0x0040
const SCS_32BIT_BINARY = 0
const SCS_64BIT_BINARY = 6
const SCS_CAP_COMPSTR = 0x00000001
const SCS_CAP_MAKEREAD = 0x00000002
const SCS_CAP_SETRECONVERTSTRING = 0x00000004
const SCS_DOS_BINARY = 1
const SCS_OS216_BINARY = 5
const SCS_PIF_BINARY = 3
const SCS_POSIX_BINARY = 4
const SCS_QUERYRECONVERTSTRING = 0x00020000
const SCS_SETRECONVERTSTRING = 0x00010000
const SCS_THIS_PLATFORM_BINARY = "SCS_64BIT_BINARY"
const SCS_WOW_BINARY = 2
const SC_ARRANGE = 0xF110
const SC_CLOSE = 0xF060
const SC_CONTEXTHELP = 0xF180
const SC_DEFAULT = 0xF160
const SC_GROUP_IDENTIFIERA = '+'
const SC_GROUP_IDENTIFIERW = '+'
const SC_HOTKEY = 0xF150
const SC_HSCROLL = 0xF080
const SC_ICON = "SC_MINIMIZE"
const SC_KEYMENU = 0xF100
const SC_MANAGER_CONNECT = 0x0001
const SC_MANAGER_CREATE_SERVICE = 0x0002
const SC_MANAGER_ENUMERATE_SERVICE = 0x0004
const SC_MANAGER_LOCK = 0x0008
const SC_MANAGER_MODIFY_BOOT_CONFIG = 0x0020
const SC_MANAGER_QUERY_LOCK_STATUS = 0x0010
const SC_MAXIMIZE = 0xF030
const SC_MINIMIZE = 0xF020
const SC_MONITORPOWER = 0xF170
const SC_MOUSEMENU = 0xF090
const SC_MOVE = 0xF010
const SC_NEXTWINDOW = 0xF040
const SC_PREVWINDOW = 0xF050
const SC_RESTORE = 0xF120
const SC_SCREENSAVE = 0xF140
const SC_SEPARATOR = 0xF00F
const SC_SIZE = 0xF000
const SC_TASKLIST = 0xF130
const SC_VSCROLL = 0xF070
const SC_ZOOM = "SC_MAXIMIZE"
const SECTION_EXTEND_SIZE = 0x0010
const SECTION_MAP_EXECUTE = 0x0008
const SECTION_MAP_EXECUTE_EXPLICIT = 0x0020
const SECTION_MAP_READ = 0x0004
const SECTION_MAP_WRITE = 0x0002
const SECTION_QUERY = 0x0001
const SECURITY_CONTEXT_TRACKING = 0x40000
const SECURITY_DESCRIPTOR_REVISION = 1
const SECURITY_DESCRIPTOR_REVISION1 = 1
const SECURITY_DYNAMIC_TRACKING = "TRUE"
const SECURITY_EFFECTIVE_ONLY = 0x80000
const SECURITY_INSTALLER_CAPABILITY_RID_COUNT = 10
const SECURITY_INSTALLER_GROUP_CAPABILITY_BASE = 0x20
const SECURITY_INSTALLER_GROUP_CAPABILITY_RID_COUNT = 9
const SECURITY_MANDATORY_MAXIMUM_USER_RID = "SECURITY_MANDATORY_SYSTEM_RID"
const SECURITY_MAX_IMPERSONATION_LEVEL = "SecurityDelegation"
const SECURITY_MIN_IMPERSONATION_LEVEL = "SecurityAnonymous"
const SECURITY_PARENT_PACKAGE_RID_COUNT = "SECURITY_APP_PACKAGE_RID_COUNT"
const SECURITY_SERVER_LOGON_RID = "SECURITY_ENTERPRISE_CONTROLLERS_RID"
const SECURITY_SQOS_PRESENT = 0x100000
const SECURITY_STATIC_TRACKING = "FALSE"
const SECURITY_TRUSTED_INSTALLER_RID1 = 956008885
const SECURITY_TRUSTED_INSTALLER_RID2 = 3418522649
const SECURITY_TRUSTED_INSTALLER_RID3 = 1831038044
const SECURITY_TRUSTED_INSTALLER_RID4 = 1853292631
const SECURITY_TRUSTED_INSTALLER_RID5 = 2271478464
const SECURITY_VALID_SQOS_FLAGS = 0x1f0000
const SEC_64K_PAGES = 0x80000
const SEC_COMMIT = 0x8000000
const SEC_E_NOT_SUPPORTED = "SEC_E_UNSUPPORTED_FUNCTION"
const SEC_E_NO_SPM = "SEC_E_INTERNAL_ERROR"
const SEC_FILE = 0x800000
const SEC_HUGE_PAGES = 0x20000
const SEC_IMAGE = 0x1000000
const SEC_LARGE_PAGES = 0x80000000
const SEC_NOCACHE = 0x10000000
const SEC_PARTITION_OWNER_HANDLE = 0x40000
const SEC_PROTECTED_IMAGE = 0x2000000
const SEC_RESERVE = 0x4000000
const SEC_WRITECOMBINE = 0x40000000
const SEEK_CUR = 1
const SEEK_END = 2
const SEEK_SET = 0
const SEF_AVOID_OWNER_CHECK = 0x10
const SEF_AVOID_OWNER_RESTRICTION = 0x1000
const SEF_AVOID_PRIVILEGE_CHECK = 0x08
const SEF_DACL_AUTO_INHERIT = 0x01
const SEF_DEFAULT_DESCRIPTOR_FOR_OBJECT = 0x04
const SEF_DEFAULT_GROUP_FROM_PARENT = 0x40
const SEF_DEFAULT_OWNER_FROM_PARENT = 0x20
const SEF_MACL_NO_EXECUTE_UP = 0x400
const SEF_MACL_NO_READ_UP = 0x200
const SEF_MACL_NO_WRITE_UP = 0x100
const SEF_SACL_AUTO_INHERIT = 0x02
const SEGMENT_SIZE = 512
const SELECTPAPERSOURCE = 18
const SELECT_CAP_CONVERSION = 0x00000001
const SELECT_CAP_SENTENCE = 0x00000002
const SEMAPHORE_MODIFY_STATE = 0x0002
const SEM_FAILCRITICALERRORS = 0x0001
const SEM_NOALIGNMENTFAULTEXCEPT = 0x0004
const SEM_NOGPFAULTERRORBOX = 0x0002
const SEM_NOOPENFILEERRORBOX = 0x8000
const SERKF_AVAILABLE = 0x00000002
const SERKF_INDICATOR = 0x00000004
const SERKF_SERIALKEYSON = 0x00000001
const SERVICES_ACTIVE_DATABASEA = "ServicesActive"
const SERVICES_ACTIVE_DATABASEW = "ServicesActive"
const SERVICES_FAILED_DATABASEA = "ServicesFailed"
const SERVICES_FAILED_DATABASEW = "ServicesFailed"
const SERVICE_ACCEPT_HARDWAREPROFILECHANGE = 0x00000020
const SERVICE_ACCEPT_LOWRESOURCES = 0x00002000
const SERVICE_ACCEPT_NETBINDCHANGE = 0x00000010
const SERVICE_ACCEPT_PARAMCHANGE = 0x00000008
const SERVICE_ACCEPT_PAUSE_CONTINUE = 0x00000002
const SERVICE_ACCEPT_POWEREVENT = 0x00000040
const SERVICE_ACCEPT_PRESHUTDOWN = 0x00000100
const SERVICE_ACCEPT_SESSIONCHANGE = 0x00000080
const SERVICE_ACCEPT_SHUTDOWN = 0x00000004
const SERVICE_ACCEPT_STOP = 0x00000001
const SERVICE_ACCEPT_SYSTEMLOWRESOURCES = 0x00004000
const SERVICE_ACCEPT_TIMECHANGE = 0x00000200
const SERVICE_ACCEPT_TRIGGEREVENT = 0x00000400
const SERVICE_ACCEPT_USER_LOGOFF = 0x00000800
const SERVICE_ACTIVE = 0x00000001
const SERVICE_ADAPTER = 4
const SERVICE_AUTO_START = 2
const SERVICE_BOOT_START = 0
const SERVICE_CHANGE_CONFIG = 0x0002
const SERVICE_CONFIG_DELAYED_AUTO_START_INFO = 3
const SERVICE_CONFIG_DESCRIPTION = 1
const SERVICE_CONFIG_FAILURE_ACTIONS = 2
const SERVICE_CONFIG_FAILURE_ACTIONS_FLAG = 4
const SERVICE_CONFIG_LAUNCH_PROTECTED = 12
const SERVICE_CONFIG_PREFERRED_NODE = 9
const SERVICE_CONFIG_PRESHUTDOWN_INFO = 7
const SERVICE_CONFIG_REQUIRED_PRIVILEGES_INFO = 6
const SERVICE_CONFIG_SERVICE_SID_INFO = 5
const SERVICE_CONFIG_TRIGGER_INFO = 8
const SERVICE_CONTINUE_PENDING = 0x00000005
const SERVICE_CONTROL_CONTINUE = 0x00000003
const SERVICE_CONTROL_DEVICEEVENT = 0x0000000B
const SERVICE_CONTROL_HARDWAREPROFILECHANGE = 0x0000000C
const SERVICE_CONTROL_INTERROGATE = 0x00000004
const SERVICE_CONTROL_LOWRESOURCES = 0x00000060
const SERVICE_CONTROL_NETBINDADD = 0x00000007
const SERVICE_CONTROL_NETBINDDISABLE = 0x0000000A
const SERVICE_CONTROL_NETBINDENABLE = 0x00000009
const SERVICE_CONTROL_NETBINDREMOVE = 0x00000008
const SERVICE_CONTROL_PARAMCHANGE = 0x00000006
const SERVICE_CONTROL_PAUSE = 0x00000002
const SERVICE_CONTROL_POWEREVENT = 0x0000000D
const SERVICE_CONTROL_PRESHUTDOWN = 0x0000000F
const SERVICE_CONTROL_SESSIONCHANGE = 0x0000000E
const SERVICE_CONTROL_SHUTDOWN = 0x00000005
const SERVICE_CONTROL_STATUS_REASON_INFO = 1
const SERVICE_CONTROL_STOP = 0x00000001
const SERVICE_CONTROL_SYSTEMLOWRESOURCES = 0x00000061
const SERVICE_CONTROL_TIMECHANGE = 0x00000010
const SERVICE_CONTROL_TRIGGEREVENT = 0x00000020
const SERVICE_CONTROL_USER_LOGOFF = 0x00000011
const SERVICE_DEMAND_START = 3
const SERVICE_DISABLED = 4
const SERVICE_DYNAMIC_INFORMATION_LEVEL_START_REASON = 1
const SERVICE_ENUMERATE_DEPENDENTS = 0x0008
const SERVICE_ERROR_CRITICAL = 3
const SERVICE_ERROR_IGNORE = 0
const SERVICE_ERROR_NORMAL = 1
const SERVICE_ERROR_SEVERE = 2
const SERVICE_FILE_SYSTEM_DRIVER = 2
const SERVICE_INACTIVE = 0x00000002
const SERVICE_INTERACTIVE_PROCESS = 0x00000100
const SERVICE_INTERROGATE = 0x0080
const SERVICE_KERNEL_DRIVER = 1
const SERVICE_LAUNCH_PROTECTED_ANTIMALWARE_LIGHT = 3
const SERVICE_LAUNCH_PROTECTED_NONE = 0
const SERVICE_LAUNCH_PROTECTED_WINDOWS = 1
const SERVICE_LAUNCH_PROTECTED_WINDOWS_LIGHT = 2
const SERVICE_NOTIFY_CONTINUE_PENDING = 0x00000010
const SERVICE_NOTIFY_CREATED = 0x00000080
const SERVICE_NOTIFY_DELETED = 0x00000100
const SERVICE_NOTIFY_DELETE_PENDING = 0x00000200
const SERVICE_NOTIFY_PAUSED = 0x00000040
const SERVICE_NOTIFY_PAUSE_PENDING = 0x00000020
const SERVICE_NOTIFY_RUNNING = 0x00000008
const SERVICE_NOTIFY_START_PENDING = 0x00000002
const SERVICE_NOTIFY_STATUS_CHANGE = "SERVICE_NOTIFY_STATUS_CHANGE_2"
const SERVICE_NOTIFY_STATUS_CHANGE_1 = 1
const SERVICE_NOTIFY_STATUS_CHANGE_2 = 2
const SERVICE_NOTIFY_STOPPED = 0x00000001
const SERVICE_NOTIFY_STOP_PENDING = 0x00000004
const SERVICE_NO_CHANGE = 0xffffffff
const SERVICE_PAUSED = 0x00000007
const SERVICE_PAUSE_CONTINUE = 0x0040
const SERVICE_PAUSE_PENDING = 0x00000006
const SERVICE_QUERY_CONFIG = 0x0001
const SERVICE_QUERY_STATUS = 0x0004
const SERVICE_RECOGNIZER_DRIVER = 8
const SERVICE_RUNNING = 0x00000004
const SERVICE_RUNS_IN_SYSTEM_PROCESS = 0x00000001
const SERVICE_SID_TYPE_NONE = 0x00000000
const SERVICE_SID_TYPE_RESTRICTED = 0x00000003
const SERVICE_SID_TYPE_UNRESTRICTED = 0x00000001
const SERVICE_START = 0x0010
const SERVICE_START_PENDING = 0x00000002
const SERVICE_START_REASON_AUTO = 0x00000002
const SERVICE_START_REASON_DELAYEDAUTO = 0x00000010
const SERVICE_START_REASON_DEMAND = 0x00000001
const SERVICE_START_REASON_RESTART_ON_FAILURE = 0x00000008
const SERVICE_START_REASON_TRIGGER = 0x00000004
const SERVICE_STOP = 0x0020
const SERVICE_STOPPED = 0x00000001
const SERVICE_STOP_PENDING = 0x00000003
const SERVICE_STOP_REASON_FLAG_CUSTOM = 0x20000000
const SERVICE_STOP_REASON_FLAG_MAX = 0x80000000
const SERVICE_STOP_REASON_FLAG_MIN = 0x00000000
const SERVICE_STOP_REASON_FLAG_PLANNED = 0x40000000
const SERVICE_STOP_REASON_FLAG_UNPLANNED = 0x10000000
const SERVICE_STOP_REASON_MAJOR_APPLICATION = 0x00050000
const SERVICE_STOP_REASON_MAJOR_HARDWARE = 0x00020000
const SERVICE_STOP_REASON_MAJOR_MAX = 0x00070000
const SERVICE_STOP_REASON_MAJOR_MAX_CUSTOM = 0x00ff0000
const SERVICE_STOP_REASON_MAJOR_MIN = 0x00000000
const SERVICE_STOP_REASON_MAJOR_MIN_CUSTOM = 0x00400000
const SERVICE_STOP_REASON_MAJOR_NONE = 0x00060000
const SERVICE_STOP_REASON_MAJOR_OPERATINGSYSTEM = 0x00030000
const SERVICE_STOP_REASON_MAJOR_OTHER = 0x00010000
const SERVICE_STOP_REASON_MAJOR_SOFTWARE = 0x00040000
const SERVICE_STOP_REASON_MINOR_DISK = 0x00000008
const SERVICE_STOP_REASON_MINOR_ENVIRONMENT = 0x0000000a
const SERVICE_STOP_REASON_MINOR_HARDWARE_DRIVER = 0x0000000b
const SERVICE_STOP_REASON_MINOR_HUNG = 0x00000006
const SERVICE_STOP_REASON_MINOR_INSTALLATION = 0x00000003
const SERVICE_STOP_REASON_MINOR_MAINTENANCE = 0x00000002
const SERVICE_STOP_REASON_MINOR_MAX = 0x00000019
const SERVICE_STOP_REASON_MINOR_MAX_CUSTOM = 0x0000FFFF
const SERVICE_STOP_REASON_MINOR_MEMOTYLIMIT = 0x00000018
const SERVICE_STOP_REASON_MINOR_MIN = 0x00000000
const SERVICE_STOP_REASON_MINOR_MIN_CUSTOM = 0x00000100
const SERVICE_STOP_REASON_MINOR_MMC = 0x00000016
const SERVICE_STOP_REASON_MINOR_NETWORKCARD = 0x00000009
const SERVICE_STOP_REASON_MINOR_NETWORK_CONNECTIVITY = 0x00000011
const SERVICE_STOP_REASON_MINOR_NONE = 0x00000017
const SERVICE_STOP_REASON_MINOR_OTHER = 0x00000001
const SERVICE_STOP_REASON_MINOR_OTHERDRIVER = 0x0000000c
const SERVICE_STOP_REASON_MINOR_RECONFIG = 0x00000005
const SERVICE_STOP_REASON_MINOR_SECURITY = 0x00000010
const SERVICE_STOP_REASON_MINOR_SECURITYFIX = 0x0000000f
const SERVICE_STOP_REASON_MINOR_SECURITYFIX_UNINSTALL = 0x00000015
const SERVICE_STOP_REASON_MINOR_SERVICEPACK = 0x0000000d
const SERVICE_STOP_REASON_MINOR_SERVICEPACK_UNINSTALL = 0x00000013
const SERVICE_STOP_REASON_MINOR_SOFTWARE_UPDATE = 0x0000000e
const SERVICE_STOP_REASON_MINOR_SOFTWARE_UPDATE_UNINSTALL = 0x00000014
const SERVICE_STOP_REASON_MINOR_UNSTABLE = 0x00000007
const SERVICE_STOP_REASON_MINOR_UPGRADE = 0x00000004
const SERVICE_STOP_REASON_MINOR_WMI = 0x00000012
const SERVICE_SYSTEM_START = 1
const SERVICE_TRIGGER_DATA_TYPE_BINARY = 1
const SERVICE_TRIGGER_DATA_TYPE_KEYWORD_ALL = 5
const SERVICE_TRIGGER_DATA_TYPE_KEYWORD_ANY = 4
const SERVICE_TRIGGER_DATA_TYPE_LEVEL = 3
const SERVICE_TRIGGER_DATA_TYPE_STRING = 2
const SERVICE_TRIGGER_TYPE_AGGREGATE = 30
const SERVICE_TRIGGER_TYPE_CUSTOM = 20
const SERVICE_TRIGGER_TYPE_CUSTOM_SYSTEM_STATE_CHANGE = 7
const SERVICE_TRIGGER_TYPE_DEVICE_INTERFACE_ARRIVAL = 1
const SERVICE_TRIGGER_TYPE_DOMAIN_JOIN = 3
const SERVICE_TRIGGER_TYPE_FIREWALL_PORT_EVENT = 4
const SERVICE_TRIGGER_TYPE_GROUP_POLICY = 5
const SERVICE_TRIGGER_TYPE_IP_ADDRESS_AVAILABILITY = 2
const SERVICE_TRIGGER_TYPE_NETWORK_ENDPOINT = 6
const SERVICE_USER_DEFINED_CONTROL = 0x0100
const SERVICE_WIN32_OWN_PROCESS = 16
const SERVICE_WIN32_SHARE_PROCESS = 32
const SESSION_MODIFY_ACCESS = 0x2
const SESSION_QUERY_ACCESS = 0x1
const SETABORTPROC = 9
const SETALLJUSTVALUES = 771
const SETBREAK = 8
const SETCHARSET = 772
const SETCOLORTABLE = 4
const SETCOPYCOUNT = 17
const SETDIBSCALING = 32
const SETDTR = 5
const SETICMPROFILE_EMBEDED = 0x00000001
const SETKERNTRACK = 770
const SETLINECAP = 21
const SETLINEJOIN = 22
const SETMITERLIMIT = 23
const SETRTS = 3
const SETXOFF = 1
const SETXON = 2
const SET_ARC_DIRECTION = 4102
const SET_BACKGROUND_COLOR = 4103
const SET_BOUNDS = 4109
const SET_CLIP_BOX = 4108
const SET_MIRROR_MODE = 4110
const SET_POLY_MODE = 4104
const SET_SCREEN_ANGLE = 4105
const SET_SPREAD = 4106
const SET_TAPE_DRIVE_INFORMATION = 1
const SET_TAPE_MEDIA_INFORMATION = 0
const SEVERITY_ERROR = 1
const SEVERITY_SUCCESS = 0
const SE_ACCESS_CHECK_FLAG_NO_LEARNING_MODE_LOGGING = 0x00000008
const SE_ACCESS_CHECK_VALID_FLAGS = 0x00000008
const SE_ACTIVATE_AS_USER_CAPABILITY = "activateAsUser"
const SE_APP_SILO_PRINT_CAPABILITY = "isolatedWin32-print"
const SE_APP_SILO_PROFILES_ROOT_MINIMAL_CAPABILITY = "isolatedWin32-profilesRootMinimal"
const SE_APP_SILO_USER_PROFILE_MINIMAL_CAPABILITY = "isolatedWin32-userProfileMinimal"
const SE_APP_SILO_VOLUME_ROOT_MINIMAL_CAPABILITY = "isolatedWin32-volumeRootMinimal"
const SE_CONSTRAINED_IMPERSONATION_CAPABILITY = "constrainedImpersonation"
const SE_DACL_AUTO_INHERITED = 0x0400
const SE_DACL_AUTO_INHERIT_REQ = 0x0100
const SE_DACL_DEFAULTED = 0x0008
const SE_DACL_PRESENT = 0x0004
const SE_DACL_PROTECTED = 0x1000
const SE_DEVELOPMENT_MODE_NETWORK_CAPABILITY = "developmentModeNetwork"
const SE_GROUP_DEFAULTED = 0x0002
const SE_LEARNING_MODE_FLAG_PERMISSIVE = 0x00000001
const SE_LEARNING_MODE_LOGGING_CAPABILITY = "learningModeLogging"
const SE_MUMA_CAPABILITY = "muma"
const SE_OWNER_DEFAULTED = 0x0001
const SE_PERMISSIVE_LEARNING_MODE_CAPABILITY = "permissiveLearningMode"
const SE_PRIVILEGE_REMOVED = 0x00000004
const SE_RM_CONTROL_VALID = 0x4000
const SE_SACL_AUTO_INHERITED = 0x0800
const SE_SACL_AUTO_INHERIT_REQ = 0x0200
const SE_SACL_DEFAULTED = 0x0020
const SE_SACL_PRESENT = 0x0010
const SE_SACL_PROTECTED = 0x2000
const SE_SECURITY_DESCRIPTOR_FLAG_NO_ACCESS_FILTER_ACE = 0x00000004
const SE_SECURITY_DESCRIPTOR_FLAG_NO_LABEL_ACE = 0x00000002
const SE_SECURITY_DESCRIPTOR_FLAG_NO_OWNER_ACE = 0x00000001
const SE_SECURITY_DESCRIPTOR_VALID_FLAGS = 0x00000007
const SE_SELF_RELATIVE = 0x8000
const SE_SESSION_IMPERSONATION_CAPABILITY = "sessionImpersonation"
const SE_SIGNING_LEVEL_ANTIMALWARE = "SE_SIGNING_LEVEL_CUSTOM_3"
const SE_SIGNING_LEVEL_AUTHENTICODE = 0x00000004
const SE_SIGNING_LEVEL_CUSTOM_1 = 0x00000003
const SE_SIGNING_LEVEL_CUSTOM_2 = 0x00000005
const SE_SIGNING_LEVEL_CUSTOM_3 = 0x00000007
const SE_SIGNING_LEVEL_CUSTOM_4 = 0x00000009
const SE_SIGNING_LEVEL_CUSTOM_5 = 0x0000000A
const SE_SIGNING_LEVEL_CUSTOM_6 = 0x0000000F
const SE_SIGNING_LEVEL_CUSTOM_7 = 0x0000000D
const SE_SIGNING_LEVEL_DEVELOPER = "SE_SIGNING_LEVEL_CUSTOM_1"
const SE_SIGNING_LEVEL_DYNAMIC_CODEGEN = 0x0000000B
const SE_SIGNING_LEVEL_ENTERPRISE = 0x00000002
const SE_SIGNING_LEVEL_MICROSOFT = 0x00000008
const SE_SIGNING_LEVEL_STORE = 0x00000006
const SE_SIGNING_LEVEL_UNCHECKED = 0x00000000
const SE_SIGNING_LEVEL_UNSIGNED = 0x00000001
const SE_SIGNING_LEVEL_WINDOWS = 0x0000000C
const SE_SIGNING_LEVEL_WINDOWS_TCB = 0x0000000E
const SHADEBLENDCAPS = 120
const SHIFTJIS_CHARSET = 128
const SHIFT_PRESSED = 0x0010
const SHOW_FULLSCREEN = 3
const SHOW_ICONWINDOW = 2
const SHOW_OPENNOACTIVATE = 4
const SHOW_OPENWINDOW = 1
const SHTDN_REASON_FLAG_CLEAN_UI = 0x04000000
const SHTDN_REASON_FLAG_COMMENT_REQUIRED = 0x01000000
const SHTDN_REASON_FLAG_DIRTY_PROBLEM_ID_REQUIRED = 0x02000000
const SHTDN_REASON_FLAG_DIRTY_UI = 0x08000000
const SHTDN_REASON_FLAG_PLANNED = 0x80000000
const SHTDN_REASON_FLAG_USER_DEFINED = 0x40000000
const SHTDN_REASON_MAJOR_APPLICATION = 0x00040000
const SHTDN_REASON_MAJOR_HARDWARE = 0x00010000
const SHTDN_REASON_MAJOR_LEGACY_API = 0x00070000
const SHTDN_REASON_MAJOR_NONE = 0x00000000
const SHTDN_REASON_MAJOR_OPERATINGSYSTEM = 0x00020000
const SHTDN_REASON_MAJOR_OTHER = 0x00000000
const SHTDN_REASON_MAJOR_POWER = 0x00060000
const SHTDN_REASON_MAJOR_SOFTWARE = 0x00030000
const SHTDN_REASON_MAJOR_SYSTEM = 0x00050000
const SHTDN_REASON_MINOR_BLUESCREEN = 0x0000000F
const SHTDN_REASON_MINOR_CORDUNPLUGGED = 0x0000000b
const SHTDN_REASON_MINOR_DC_DEMOTION = 0x00000022
const SHTDN_REASON_MINOR_DC_PROMOTION = 0x00000021
const SHTDN_REASON_MINOR_DISK = 0x00000007
const SHTDN_REASON_MINOR_ENVIRONMENT = 0x0000000c
const SHTDN_REASON_MINOR_HARDWARE_DRIVER = 0x0000000d
const SHTDN_REASON_MINOR_HOTFIX = 0x00000011
const SHTDN_REASON_MINOR_HOTFIX_UNINSTALL = 0x00000017
const SHTDN_REASON_MINOR_HUNG = 0x00000005
const SHTDN_REASON_MINOR_INSTALLATION = 0x00000002
const SHTDN_REASON_MINOR_MAINTENANCE = 0x00000001
const SHTDN_REASON_MINOR_MMC = 0x00000019
const SHTDN_REASON_MINOR_NETWORKCARD = 0x00000009
const SHTDN_REASON_MINOR_NETWORK_CONNECTIVITY = 0x00000014
const SHTDN_REASON_MINOR_NONE = 0x000000ff
const SHTDN_REASON_MINOR_OTHER = 0x00000000
const SHTDN_REASON_MINOR_OTHERDRIVER = 0x0000000e
const SHTDN_REASON_MINOR_POWER_SUPPLY = 0x0000000a
const SHTDN_REASON_MINOR_PROCESSOR = 0x00000008
const SHTDN_REASON_MINOR_RECONFIG = 0x00000004
const SHTDN_REASON_MINOR_SECURITY = 0x00000013
const SHTDN_REASON_MINOR_SECURITYFIX = 0x00000012
const SHTDN_REASON_MINOR_SECURITYFIX_UNINSTALL = 0x00000018
const SHTDN_REASON_MINOR_SERVICEPACK = 0x00000010
const SHTDN_REASON_MINOR_SERVICEPACK_UNINSTALL = 0x00000016
const SHTDN_REASON_MINOR_SYSTEMRESTORE = 0x0000001a
const SHTDN_REASON_MINOR_TERMSRV = 0x00000020
const SHTDN_REASON_MINOR_UNSTABLE = 0x00000006
const SHTDN_REASON_MINOR_UPGRADE = 0x00000003
const SHTDN_REASON_MINOR_WMI = 0x00000015
const SHTDN_REASON_UNKNOWN = "SHTDN_REASON_MINOR_NONE"
const SHTDN_REASON_VALID_BIT_MASK = 0xc0ffffff
const SHUTDOWN_ARSO = 0x00002000
const SHUTDOWN_FORCE_OTHERS = 0x00000001
const SHUTDOWN_FORCE_SELF = 0x00000002
const SHUTDOWN_GRACE_OVERRIDE = 0x00000020
const SHUTDOWN_HYBRID = 0x00000200
const SHUTDOWN_INSTALL_UPDATES = 0x00000040
const SHUTDOWN_MOBILE_UI = 0x00001000
const SHUTDOWN_NOREBOOT = 0x00000010
const SHUTDOWN_NORETRY = 0x1
const SHUTDOWN_POWEROFF = 0x00000008
const SHUTDOWN_RESTART = 0x00000004
const SHUTDOWN_RESTARTAPPS = 0x00000080
const SHUTDOWN_RESTART_BOOTOPTIONS = 0x00000400
const SHUTDOWN_SKIP_SVC_PRESHUTDOWN = 0x00000100
const SHUTDOWN_SOFT_REBOOT = 0x00000800
const SHUTDOWN_TYPE_LEN = 32
const SID_HASH_SIZE = 32
const SID_MAX_SUB_AUTHORITIES = 15
const SID_RECOMMENDED_SUB_AUTHORITIES = 1
const SID_REVISION = 1
const SIF_DISABLENOSCROLL = 0x0008
const SIF_PAGE = 0x0002
const SIF_POS = 0x0004
const SIF_RANGE = 0x0001
const SIF_TRACKPOS = 0x0010
const SIG_ATOMIC_MAX = "INT32_MAX"
const SIG_ATOMIC_MIN = "INT32_MIN"
const SIMPLEREGION = 2
const SIZEFULLSCREEN = "SIZE_MAXIMIZED"
const SIZEICONIC = "SIZE_MINIMIZED"
const SIZENORMAL = "SIZE_RESTORED"
const SIZEOF_RFPO_DATA = 16
const SIZEPALETTE = 104
const SIZEZOOMHIDE = "SIZE_MAXHIDE"
const SIZEZOOMSHOW = "SIZE_MAXSHOW"
const SIZE_MAX = "_UI64_MAX"
const SIZE_MAXHIDE = 4
const SIZE_MAXIMIZED = 2
const SIZE_MAXSHOW = 3
const SIZE_MINIMIZED = 1
const SIZE_RESTORED = 0
const SKF_AUDIBLEFEEDBACK = 0x00000040
const SKF_AVAILABLE = 0x00000002
const SKF_CONFIRMHOTKEY = 0x00000008
const SKF_HOTKEYACTIVE = 0x00000004
const SKF_HOTKEYSOUND = 0x00000010
const SKF_INDICATOR = 0x00000020
const SKF_LALTLATCHED = 0x10000000
const SKF_LALTLOCKED = 0x00100000
const SKF_LCTLLATCHED = 0x04000000
const SKF_LCTLLOCKED = 0x00040000
const SKF_LSHIFTLATCHED = 0x01000000
const SKF_LSHIFTLOCKED = 0x00010000
const SKF_LWINLATCHED = 0x40000000
const SKF_LWINLOCKED = 0x00400000
const SKF_RALTLATCHED = 0x20000000
const SKF_RALTLOCKED = 0x00200000
const SKF_RCTLLATCHED = 0x08000000
const SKF_RCTLLOCKED = 0x00080000
const SKF_RSHIFTLATCHED = 0x02000000
const SKF_RSHIFTLOCKED = 0x00020000
const SKF_RWINLATCHED = 0x80000000
const SKF_RWINLOCKED = 0x00800000
const SKF_STICKYKEYSON = 0x00000001
const SKF_TRISTATE = 0x00000080
const SKF_TWOKEYSOFF = 0x00000100
const SLE_ERROR = 0x00000001
const SLE_MINORERROR = 0x00000002
const SLE_WARNING = 0x00000003
const SMTO_ABORTIFHUNG = 0x0002
const SMTO_BLOCK = 0x0001
const SMTO_ERRORONEXIT = 0x0020
const SMTO_NORMAL = 0x0000
const SMTO_NOTIMEOUTIFNOTHUNG = 0x0008
const SM_ARRANGE = 56
const SM_CARETBLINKINGENABLED = 0x2002
const SM_CLEANBOOT = 67
const SM_CMETRICS = 93
const SM_CMONITORS = 80
const SM_CMOUSEBUTTONS = 43
const SM_CXBORDER = 5
const SM_CXCURSOR = 13
const SM_CXDLGFRAME = 7
const SM_CXDOUBLECLK = 36
const SM_CXDRAG = 68
const SM_CXEDGE = 45
const SM_CXFIXEDFRAME = "SM_CXDLGFRAME"
const SM_CXFOCUSBORDER = 83
const SM_CXFRAME = 32
const SM_CXFULLSCREEN = 16
const SM_CXHSCROLL = 21
const SM_CXHTHUMB = 10
const SM_CXICON = 11
const SM_CXICONSPACING = 38
const SM_CXMAXIMIZED = 61
const SM_CXMAXTRACK = 59
const SM_CXMENUCHECK = 71
const SM_CXMENUSIZE = 54
const SM_CXMIN = 28
const SM_CXMINIMIZED = 57
const SM_CXMINSPACING = 47
const SM_CXMINTRACK = 34
const SM_CXPADDEDBORDER = 92
const SM_CXSCREEN = 0
const SM_CXSIZE = 30
const SM_CXSIZEFRAME = "SM_CXFRAME"
const SM_CXSMICON = 49
const SM_CXSMSIZE = 52
const SM_CXVIRTUALSCREEN = 78
const SM_CXVSCROLL = 2
const SM_CYBORDER = 6
const SM_CYCAPTION = 4
const SM_CYCURSOR = 14
const SM_CYDLGFRAME = 8
const SM_CYDOUBLECLK = 37
const SM_CYDRAG = 69
const SM_CYEDGE = 46
const SM_CYFIXEDFRAME = "SM_CYDLGFRAME"
const SM_CYFOCUSBORDER = 84
const SM_CYFRAME = 33
const SM_CYFULLSCREEN = 17
const SM_CYHSCROLL = 3
const SM_CYICON = 12
const SM_CYICONSPACING = 39
const SM_CYKANJIWINDOW = 18
const SM_CYMAXIMIZED = 62
const SM_CYMAXTRACK = 60
const SM_CYMENU = 15
const SM_CYMENUCHECK = 72
const SM_CYMENUSIZE = 55
const SM_CYMIN = 29
const SM_CYMINIMIZED = 58
const SM_CYMINSPACING = 48
const SM_CYMINTRACK = 35
const SM_CYSCREEN = 1
const SM_CYSIZE = 31
const SM_CYSIZEFRAME = "SM_CYFRAME"
const SM_CYSMCAPTION = 51
const SM_CYSMICON = 50
const SM_CYSMSIZE = 53
const SM_CYVIRTUALSCREEN = 79
const SM_CYVSCROLL = 20
const SM_CYVTHUMB = 9
const SM_DBCSENABLED = 42
const SM_DEBUG = 22
const SM_IMMENABLED = 82
const SM_MEDIACENTER = 87
const SM_MENUDROPALIGNMENT = 40
const SM_MIDEASTENABLED = 74
const SM_MOUSEHORIZONTALWHEELPRESENT = 91
const SM_MOUSEPRESENT = 19
const SM_MOUSEWHEELPRESENT = 75
const SM_NETWORK = 63
const SM_PENWINDOWS = 41
const SM_REMOTECONTROL = 0x2001
const SM_REMOTESESSION = 0x1000
const SM_RESERVED1 = 24
const SM_RESERVED2 = 25
const SM_RESERVED3 = 26
const SM_RESERVED4 = 27
const SM_SAMEDISPLAYFORMAT = 81
const SM_SECURE = 44
const SM_SERVERR2 = 89
const SM_SHOWSOUNDS = 70
const SM_SHUTTINGDOWN = 0x2000
const SM_SLOWMACHINE = 73
const SM_STARTER = 88
const SM_SWAPBUTTON = 23
const SM_TABLETPC = 86
const SM_XVIRTUALSCREEN = 76
const SM_YVIRTUALSCREEN = 77
const SNAPSHOT_POLICY_ALWAYS = 1
const SNAPSHOT_POLICY_NEVER = 0
const SNAPSHOT_POLICY_UNPLANNED = 2
const SOFTKEYBOARD_TYPE_C1 = 0x0002
const SOFTKEYBOARD_TYPE_T1 = 0x0001
const SORT_CHINESE_BIG5 = 0x0
const SORT_CHINESE_BOPOMOFO = 0x3
const SORT_CHINESE_PRC = 0x2
const SORT_CHINESE_PRCP = 0x0
const SORT_CHINESE_RADICALSTROKE = 0x4
const SORT_CHINESE_UNICODE = 0x1
const SORT_DEFAULT = 0x0
const SORT_GEORGIAN_MODERN = 0x1
const SORT_GEORGIAN_TRADITIONAL = 0x0
const SORT_GERMAN_PHONE_BOOK = 0x1
const SORT_HUNGARIAN_DEFAULT = 0x0
const SORT_HUNGARIAN_TECHNICAL = 0x1
const SORT_INVARIANT_MATH = 0x1
const SORT_JAPANESE_RADICALSTROKE = 0x4
const SORT_JAPANESE_UNICODE = 0x1
const SORT_JAPANESE_XJIS = 0x0
const SORT_KOREAN_KSC = 0x0
const SORT_KOREAN_UNICODE = 0x1
const SORT_STRINGSORT = 0x00001000
const SOUND_SYSTEM_APPEND = 14
const SOUND_SYSTEM_APPSTART = 12
const SOUND_SYSTEM_BEEP = 3
const SOUND_SYSTEM_ERROR = 4
const SOUND_SYSTEM_FAULT = 13
const SOUND_SYSTEM_INFORMATION = 7
const SOUND_SYSTEM_MAXIMIZE = 8
const SOUND_SYSTEM_MENUCOMMAND = 15
const SOUND_SYSTEM_MENUPOPUP = 16
const SOUND_SYSTEM_MINIMIZE = 9
const SOUND_SYSTEM_QUESTION = 5
const SOUND_SYSTEM_RESTOREDOWN = 11
const SOUND_SYSTEM_RESTOREUP = 10
const SOUND_SYSTEM_SHUTDOWN = 2
const SOUND_SYSTEM_STARTUP = 1
const SOUND_SYSTEM_WARNING = 6
const SPACEPARITY = 4
const SPCLPASSTHROUGH2 = 4568
const SPIF_SENDCHANGE = "SPIF_SENDWININICHANGE"
const SPIF_SENDWININICHANGE = 0x0002
const SPIF_UPDATEINIFILE = 0x0001
const SPI_GETACCESSTIMEOUT = 0x003C
const SPI_GETACTIVEWINDOWTRACKING = 0x1000
const SPI_GETACTIVEWNDTRKTIMEOUT = 0x2002
const SPI_GETACTIVEWNDTRKZORDER = 0x100C
const SPI_GETANIMATION = 0x0048
const SPI_GETAUDIODESCRIPTION = 0x0074
const SPI_GETBEEP = 0x0001
const SPI_GETBLOCKSENDINPUTRESETS = 0x1026
const SPI_GETBORDER = 0x0005
const SPI_GETCARETWIDTH = 0x2006
const SPI_GETCLEARTYPE = 0x1048
const SPI_GETCLIENTAREAANIMATION = 0x1042
const SPI_GETCOMBOBOXANIMATION = 0x1004
const SPI_GETCURSORSHADOW = 0x101A
const SPI_GETDEFAULTINPUTLANG = 0x0059
const SPI_GETDESKWALLPAPER = 0x0073
const SPI_GETDISABLEOVERLAPPEDCONTENT = 0x1040
const SPI_GETDRAGFULLWINDOWS = 0x0026
const SPI_GETDROPSHADOW = 0x1024
const SPI_GETFASTTASKSWITCH = 0x0023
const SPI_GETFILTERKEYS = 0x0032
const SPI_GETFLATMENU = 0x1022
const SPI_GETFOCUSBORDERHEIGHT = 0x2010
const SPI_GETFOCUSBORDERWIDTH = 0x200E
const SPI_GETFONTSMOOTHING = 0x004A
const SPI_GETFONTSMOOTHINGCONTRAST = 0x200C
const SPI_GETFONTSMOOTHINGORIENTATION = 0x2012
const SPI_GETFONTSMOOTHINGTYPE = 0x200A
const SPI_GETFOREGROUNDFLASHCOUNT = 0x2004
const SPI_GETFOREGROUNDLOCKTIMEOUT = 0x2000
const SPI_GETGRADIENTCAPTIONS = 0x1008
const SPI_GETGRIDGRANULARITY = 0x0012
const SPI_GETHIGHCONTRAST = 0x0042
const SPI_GETHOTTRACKING = 0x100E
const SPI_GETICONMETRICS = 0x002D
const SPI_GETICONTITLELOGFONT = 0x001F
const SPI_GETICONTITLEWRAP = 0x0019
const SPI_GETKEYBOARDCUES = 0x100A
const SPI_GETKEYBOARDDELAY = 0x0016
const SPI_GETKEYBOARDPREF = 0x0044
const SPI_GETKEYBOARDSPEED = 0x000A
const SPI_GETLISTBOXSMOOTHSCROLLING = 0x1006
const SPI_GETLOWPOWERACTIVE = 0x0053
const SPI_GETLOWPOWERTIMEOUT = 0x004F
const SPI_GETMENUANIMATION = 0x1002
const SPI_GETMENUDROPALIGNMENT = 0x001B
const SPI_GETMENUFADE = 0x1012
const SPI_GETMENUSHOWDELAY = 0x006A
const SPI_GETMENUUNDERLINES = "SPI_GETKEYBOARDCUES"
const SPI_GETMESSAGEDURATION = 0x2016
const SPI_GETMINIMIZEDMETRICS = 0x002B
const SPI_GETMINIMUMHITRADIUS = 0x2014
const SPI_GETMOUSE = 0x0003
const SPI_GETMOUSECLICKLOCK = 0x101E
const SPI_GETMOUSECLICKLOCKTIME = 0x2008
const SPI_GETMOUSEHOVERHEIGHT = 0x0064
const SPI_GETMOUSEHOVERTIME = 0x0066
const SPI_GETMOUSEHOVERWIDTH = 0x0062
const SPI_GETMOUSEKEYS = 0x0036
const SPI_GETMOUSESONAR = 0x101C
const SPI_GETMOUSESPEED = 0x0070
const SPI_GETMOUSETRAILS = 0x005E
const SPI_GETMOUSEVANISH = 0x1020
const SPI_GETNONCLIENTMETRICS = 0x0029
const SPI_GETPOWEROFFACTIVE = 0x0054
const SPI_GETPOWEROFFTIMEOUT = 0x0050
const SPI_GETSCREENREADER = 0x0046
const SPI_GETSCREENSAVEACTIVE = 0x0010
const SPI_GETSCREENSAVERRUNNING = 0x0072
const SPI_GETSCREENSAVESECURE = 0x0076
const SPI_GETSCREENSAVETIMEOUT = 0x000E
const SPI_GETSELECTIONFADE = 0x1014
const SPI_GETSERIALKEYS = 0x003E
const SPI_GETSHOWIMEUI = 0x006E
const SPI_GETSHOWSOUNDS = 0x0038
const SPI_GETSNAPTODEFBUTTON = 0x005F
const SPI_GETSOUNDSENTRY = 0x0040
const SPI_GETSPEECHRECOGNITION = 0x104a
const SPI_GETSTICKYKEYS = 0x003A
const SPI_GETTOGGLEKEYS = 0x0034
const SPI_GETTOOLTIPANIMATION = 0x1016
const SPI_GETTOOLTIPFADE = 0x1018
const SPI_GETUIEFFECTS = 0x103E
const SPI_GETWHEELSCROLLCHARS = 0x006C
const SPI_GETWHEELSCROLLLINES = 0x0068
const SPI_GETWINDOWSEXTENSION = 0x005C
const SPI_GETWORKAREA = 0x0030
const SPI_ICONHORIZONTALSPACING = 0x000D
const SPI_ICONVERTICALSPACING = 0x0018
const SPI_LANGDRIVER = 0x000C
const SPI_SCREENSAVERRUNNING = "SPI_SETSCREENSAVERRUNNING"
const SPI_SETACCESSTIMEOUT = 0x003D
const SPI_SETACTIVEWINDOWTRACKING = 0x1001
const SPI_SETACTIVEWNDTRKTIMEOUT = 0x2003
const SPI_SETACTIVEWNDTRKZORDER = 0x100D
const SPI_SETANIMATION = 0x0049
const SPI_SETAUDIODESCRIPTION = 0x0075
const SPI_SETBEEP = 0x0002
const SPI_SETBLOCKSENDINPUTRESETS = 0x1027
const SPI_SETBORDER = 0x0006
const SPI_SETCARETWIDTH = 0x2007
const SPI_SETCLEARTYPE = 0x1049
const SPI_SETCLIENTAREAANIMATION = 0x1043
const SPI_SETCOMBOBOXANIMATION = 0x1005
const SPI_SETCURSORS = 0x0057
const SPI_SETCURSORSHADOW = 0x101B
const SPI_SETDEFAULTINPUTLANG = 0x005A
const SPI_SETDESKPATTERN = 0x0015
const SPI_SETDESKWALLPAPER = 0x0014
const SPI_SETDISABLEOVERLAPPEDCONTENT = 0x1041
const SPI_SETDOUBLECLICKTIME = 0x0020
const SPI_SETDOUBLECLKHEIGHT = 0x001E
const SPI_SETDOUBLECLKWIDTH = 0x001D
const SPI_SETDRAGFULLWINDOWS = 0x0025
const SPI_SETDRAGHEIGHT = 0x004D
const SPI_SETDRAGWIDTH = 0x004C
const SPI_SETDROPSHADOW = 0x1025
const SPI_SETFASTTASKSWITCH = 0x0024
const SPI_SETFILTERKEYS = 0x0033
const SPI_SETFLATMENU = 0x1023
const SPI_SETFOCUSBORDERHEIGHT = 0x2011
const SPI_SETFOCUSBORDERWIDTH = 0x200F
const SPI_SETFONTSMOOTHING = 0x004B
const SPI_SETFONTSMOOTHINGCONTRAST = 0x200D
const SPI_SETFONTSMOOTHINGORIENTATION = 0x2013
const SPI_SETFONTSMOOTHINGTYPE = 0x200B
const SPI_SETFOREGROUNDFLASHCOUNT = 0x2005
const SPI_SETFOREGROUNDLOCKTIMEOUT = 0x2001
const SPI_SETGRADIENTCAPTIONS = 0x1009
const SPI_SETGRIDGRANULARITY = 0x0013
const SPI_SETHANDHELD = 0x004E
const SPI_SETHIGHCONTRAST = 0x0043
const SPI_SETHOTTRACKING = 0x100F
const SPI_SETICONMETRICS = 0x002E
const SPI_SETICONS = 0x0058
const SPI_SETICONTITLELOGFONT = 0x0022
const SPI_SETICONTITLEWRAP = 0x001A
const SPI_SETKEYBOARDCUES = 0x100B
const SPI_SETKEYBOARDDELAY = 0x0017
const SPI_SETKEYBOARDPREF = 0x0045
const SPI_SETKEYBOARDSPEED = 0x000B
const SPI_SETLANGTOGGLE = 0x005B
const SPI_SETLISTBOXSMOOTHSCROLLING = 0x1007
const SPI_SETLOWPOWERACTIVE = 0x0055
const SPI_SETLOWPOWERTIMEOUT = 0x0051
const SPI_SETMENUANIMATION = 0x1003
const SPI_SETMENUDROPALIGNMENT = 0x001C
const SPI_SETMENUFADE = 0x1013
const SPI_SETMENUSHOWDELAY = 0x006B
const SPI_SETMENUUNDERLINES = "SPI_SETKEYBOARDCUES"
const SPI_SETMESSAGEDURATION = 0x2017
const SPI_SETMINIMIZEDMETRICS = 0x002C
const SPI_SETMINIMUMHITRADIUS = 0x2015
const SPI_SETMOUSE = 0x0004
const SPI_SETMOUSEBUTTONSWAP = 0x0021
const SPI_SETMOUSECLICKLOCK = 0x101F
const SPI_SETMOUSECLICKLOCKTIME = 0x2009
const SPI_SETMOUSEHOVERHEIGHT = 0x0065
const SPI_SETMOUSEHOVERTIME = 0x0067
const SPI_SETMOUSEHOVERWIDTH = 0x0063
const SPI_SETMOUSEKEYS = 0x0037
const SPI_SETMOUSESONAR = 0x101D
const SPI_SETMOUSESPEED = 0x0071
const SPI_SETMOUSETRAILS = 0x005D
const SPI_SETMOUSEVANISH = 0x1021
const SPI_SETNONCLIENTMETRICS = 0x002A
const SPI_SETPENWINDOWS = 0x0031
const SPI_SETPOWEROFFACTIVE = 0x0056
const SPI_SETPOWEROFFTIMEOUT = 0x0052
const SPI_SETSCREENREADER = 0x0047
const SPI_SETSCREENSAVEACTIVE = 0x0011
const SPI_SETSCREENSAVERRUNNING = 0x0061
const SPI_SETSCREENSAVESECURE = 0x0077
const SPI_SETSCREENSAVETIMEOUT = 0x000F
const SPI_SETSELECTIONFADE = 0x1015
const SPI_SETSERIALKEYS = 0x003F
const SPI_SETSHOWIMEUI = 0x006F
const SPI_SETSHOWSOUNDS = 0x0039
const SPI_SETSNAPTODEFBUTTON = 0x0060
const SPI_SETSOUNDSENTRY = 0x0041
const SPI_SETSPEECHRECOGNITION = 0x104b
const SPI_SETSTICKYKEYS = 0x003B
const SPI_SETTOGGLEKEYS = 0x0035
const SPI_SETTOOLTIPANIMATION = 0x1017
const SPI_SETTOOLTIPFADE = 0x1019
const SPI_SETUIEFFECTS = 0x103F
const SPI_SETWHEELSCROLLCHARS = 0x006D
const SPI_SETWHEELSCROLLLINES = 0x0069
const SPI_SETWORKAREA = 0x002F
const SPVERSION_MASK = 0x0000FF00
const SP_NOTREPORTED = 0x4000
const SRWLOCK_INIT = "RTL_SRWLOCK_INIT"
const SSF_AVAILABLE = 0x00000002
const SSF_INDICATOR = 0x00000004
const SSF_SOUNDSENTRYON = 0x00000001
const SSGF_DISPLAY = 3
const SSGF_NONE = 0
const SSIZE_MAX = "_I64_MAX"
const SSTF_BORDER = 2
const SSTF_CHARS = 1
const SSTF_DISPLAY = 3
const SSTF_NONE = 0
const SSWF_CUSTOM = 4
const SSWF_DISPLAY = 3
const SSWF_NONE = 0
const SSWF_TITLE = 1
const SSWF_WINDOW = 2
const SS_BLOCKSIZE = 1024
const SS_INSERTIONSORT_THRESHOLD = 8
const SS_MISORT_STACKSIZE = 16
const SS_SMERGE_STACKSIZE = 32
const STACK_SIZE_PARAM_IS_A_RESERVATION = 0x10000
const STANDARD_RIGHTS_EXECUTE = "READ_CONTROL"
const STANDARD_RIGHTS_READ = "READ_CONTROL"
const STANDARD_RIGHTS_WRITE = "READ_CONTROL"
const STARTDOC = 10
const STARTF_FORCEOFFFEEDBACK = 0x00000080
const STARTF_FORCEONFEEDBACK = 0x00000040
const STARTF_PREVENTPINNING = 0x00002000
const STARTF_RUNFULLSCREEN = 0x00000020
const STARTF_TITLEISAPPID = 0x00001000
const STARTF_TITLEISLINKNAME = 0x00000800
const STARTF_UNTRUSTEDSOURCE = 0x00008000
const STARTF_USECOUNTCHARS = 0x00000008
const STARTF_USEFILLATTRIBUTE = 0x00000010
const STARTF_USEHOTKEY = 0x00000200
const STARTF_USEPOSITION = 0x00000004
const STARTF_USESHOWWINDOW = 0x00000001
const STARTF_USESIZE = 0x00000002
const STARTF_USESTDHANDLES = 0x00000100
const STATE_SYSTEM_ALERT_HIGH = 0x10000000
const STATE_SYSTEM_ALERT_LOW = 0x04000000
const STATE_SYSTEM_ALERT_MEDIUM = 0x08000000
const STATE_SYSTEM_ANIMATED = 0x00004000
const STATE_SYSTEM_BUSY = 0x00000800
const STATE_SYSTEM_CHECKED = 0x00000010
const STATE_SYSTEM_COLLAPSED = 0x00000400
const STATE_SYSTEM_DEFAULT = 0x00000100
const STATE_SYSTEM_EXPANDED = 0x00000200
const STATE_SYSTEM_EXTSELECTABLE = 0x02000000
const STATE_SYSTEM_FLOATING = 0x00001000
const STATE_SYSTEM_FOCUSABLE = 0x00100000
const STATE_SYSTEM_FOCUSED = 0x00000004
const STATE_SYSTEM_HOTTRACKED = 0x00000080
const STATE_SYSTEM_INDETERMINATE = "STATE_SYSTEM_MIXED"
const STATE_SYSTEM_INVISIBLE = 0x00008000
const STATE_SYSTEM_LINKED = 0x00400000
const STATE_SYSTEM_MARQUEED = 0x00002000
const STATE_SYSTEM_MIXED = 0x00000020
const STATE_SYSTEM_MOVEABLE = 0x00040000
const STATE_SYSTEM_MULTISELECTABLE = 0x01000000
const STATE_SYSTEM_OFFSCREEN = 0x00010000
const STATE_SYSTEM_PRESSED = 0x00000008
const STATE_SYSTEM_PROTECTED = 0x20000000
const STATE_SYSTEM_READONLY = 0x00000040
const STATE_SYSTEM_SELECTABLE = 0x00200000
const STATE_SYSTEM_SELECTED = 0x00000002
const STATE_SYSTEM_SELFVOICING = 0x00080000
const STATE_SYSTEM_SIZEABLE = 0x00020000
const STATE_SYSTEM_TRAVERSED = 0x00800000
const STATE_SYSTEM_UNAVAILABLE = 0x00000001
const STATE_SYSTEM_VALID = 0x3FFFFFFF
const STATIC_BMI2 = 0
const STDAPICALLTYPE = "WINAPI"
const STDAPIVCALLTYPE = "__cdecl"
const STDERR_FILENO = 2
const STDIN_FILENO = 0
const STDMETHODCALLTYPE = "WINAPI"
const STDMETHODVCALLTYPE = "__cdecl"
const STDOUT_FILENO = 1
const STILL_ACTIVE = "STATUS_PENDING"
const STM_GETICON = 0x0171
const STM_GETIMAGE = 0x0173
const STM_MSGMAX = 0x0174
const STM_SETICON = 0x0170
const STM_SETIMAGE = 0x0172
const STN_CLICKED = 0
const STN_DBLCLK = 1
const STN_DISABLE = 3
const STN_ENABLE = 2
const STOCK_LAST = 19
const STORED_SEQS = 8
const STREAM_ACCUMULATOR_MIN_32 = 25
const STREAM_ACCUMULATOR_MIN_64 = 57
const STREAM_CONTAINS_GHOSTED_FILE_EXTENTS = 0x00000010
const STREAM_CONTAINS_PROPERTIES = 0x00000004
const STREAM_CONTAINS_SECURITY = 0x00000002
const STREAM_MODIFIED_WHEN_READ = 0x00000001
const STREAM_NORMAL_ATTRIBUTE = 0x00000000
const STREAM_SPARSE_ATTRIBUTE = 0x00000008
const STRETCHBLT = 2048
const STRETCH_ANDSCANS = "BLACKONWHITE"
const STRETCH_DELETESCANS = "COLORONCOLOR"
const STRETCH_HALFTONE = "HALFTONE"
const STRETCH_ORSCANS = "WHITEONBLACK"
const STRICT = 1
const STRUNCATE = 80
const STYLE_DESCRIPTION_SIZE = 32
const SUBLANG_AFRIKAANS_SOUTH_AFRICA = 0x01
const SUBLANG_ALBANIAN_ALBANIA = 0x01
const SUBLANG_ALSATIAN_FRANCE = 0x01
const SUBLANG_AMHARIC_ETHIOPIA = 0x01
const SUBLANG_ARABIC_ALGERIA = 0x05
const SUBLANG_ARABIC_BAHRAIN = 0x0f
const SUBLANG_ARABIC_EGYPT = 0x03
const SUBLANG_ARABIC_IRAQ = 0x02
const SUBLANG_ARABIC_JORDAN = 0x0b
const SUBLANG_ARABIC_KUWAIT = 0x0d
const SUBLANG_ARABIC_LEBANON = 0x0c
const SUBLANG_ARABIC_LIBYA = 0x04
const SUBLANG_ARABIC_MOROCCO = 0x06
const SUBLANG_ARABIC_OMAN = 0x08
const SUBLANG_ARABIC_QATAR = 0x10
const SUBLANG_ARABIC_SAUDI_ARABIA = 0x01
const SUBLANG_ARABIC_SYRIA = 0x0a
const SUBLANG_ARABIC_TUNISIA = 0x07
const SUBLANG_ARABIC_UAE = 0x0e
const SUBLANG_ARABIC_YEMEN = 0x09
const SUBLANG_ARMENIAN_ARMENIA = 0x01
const SUBLANG_ASSAMESE_INDIA = 0x01
const SUBLANG_AZERBAIJANI_AZERBAIJAN_CYRILLIC = 0x02
const SUBLANG_AZERBAIJANI_AZERBAIJAN_LATIN = 0x01
const SUBLANG_AZERI_CYRILLIC = 0x02
const SUBLANG_AZERI_LATIN = 0x01
const SUBLANG_BANGLA_BANGLADESH = 0x02
const SUBLANG_BANGLA_INDIA = 0x01
const SUBLANG_BASHKIR_RUSSIA = 0x01
const SUBLANG_BASQUE_BASQUE = 0x01
const SUBLANG_BELARUSIAN_BELARUS = 0x01
const SUBLANG_BENGALI_BANGLADESH = 0x02
const SUBLANG_BENGALI_INDIA = 0x01
const SUBLANG_BOSNIAN_BOSNIA_HERZEGOVINA_CYRILLIC = 0x08
const SUBLANG_BOSNIAN_BOSNIA_HERZEGOVINA_LATIN = 0x05
const SUBLANG_BRETON_FRANCE = 0x01
const SUBLANG_BULGARIAN_BULGARIA = 0x01
const SUBLANG_CATALAN_CATALAN = 0x01
const SUBLANG_CENTRAL_KURDISH_IRAQ = 0x01
const SUBLANG_CHEROKEE_CHEROKEE = 0x01
const SUBLANG_CHINESE_HONGKONG = 0x03
const SUBLANG_CHINESE_MACAU = 0x05
const SUBLANG_CHINESE_SIMPLIFIED = 0x02
const SUBLANG_CHINESE_SINGAPORE = 0x04
const SUBLANG_CHINESE_TRADITIONAL = 0x01
const SUBLANG_CORSICAN_FRANCE = 0x01
const SUBLANG_CROATIAN_BOSNIA_HERZEGOVINA_LATIN = 0x04
const SUBLANG_CROATIAN_CROATIA = 0x01
const SUBLANG_CUSTOM_DEFAULT = 0x03
const SUBLANG_CUSTOM_UNSPECIFIED = 0x04
const SUBLANG_CZECH_CZECH_REPUBLIC = 0x01
const SUBLANG_DANISH_DENMARK = 0x01
const SUBLANG_DARI_AFGHANISTAN = 0x01
const SUBLANG_DEFAULT = 0x01
const SUBLANG_DIVEHI_MALDIVES = 0x01
const SUBLANG_DUTCH = 0x01
const SUBLANG_DUTCH_BELGIAN = 0x02
const SUBLANG_ENGLISH_AUS = 0x03
const SUBLANG_ENGLISH_BELIZE = 0x0a
const SUBLANG_ENGLISH_CAN = 0x04
const SUBLANG_ENGLISH_CARIBBEAN = 0x09
const SUBLANG_ENGLISH_EIRE = 0x06
const SUBLANG_ENGLISH_INDIA = 0x10
const SUBLANG_ENGLISH_IRELAND = 0x06
const SUBLANG_ENGLISH_JAMAICA = 0x08
const SUBLANG_ENGLISH_MALAYSIA = 0x11
const SUBLANG_ENGLISH_NZ = 0x05
const SUBLANG_ENGLISH_PHILIPPINES = 0x0d
const SUBLANG_ENGLISH_SINGAPORE = 0x12
const SUBLANG_ENGLISH_SOUTH_AFRICA = 0x07
const SUBLANG_ENGLISH_TRINIDAD = 0x0b
const SUBLANG_ENGLISH_UK = 0x02
const SUBLANG_ENGLISH_US = 0x01
const SUBLANG_ENGLISH_ZIMBABWE = 0x0c
const SUBLANG_ESTONIAN_ESTONIA = 0x01
const SUBLANG_FAEROESE_FAROE_ISLANDS = 0x01
const SUBLANG_FILIPINO_PHILIPPINES = 0x01
const SUBLANG_FINNISH_FINLAND = 0x01
const SUBLANG_FRENCH = 0x01
const SUBLANG_FRENCH_BELGIAN = 0x02
const SUBLANG_FRENCH_CANADIAN = 0x03
const SUBLANG_FRENCH_LUXEMBOURG = 0x05
const SUBLANG_FRENCH_MONACO = 0x06
const SUBLANG_FRENCH_SWISS = 0x04
const SUBLANG_FRISIAN_NETHERLANDS = 0x01
const SUBLANG_FULAH_SENEGAL = 0x02
const SUBLANG_GALICIAN_GALICIAN = 0x01
const SUBLANG_GEORGIAN_GEORGIA = 0x01
const SUBLANG_GERMAN = 0x01
const SUBLANG_GERMAN_AUSTRIAN = 0x03
const SUBLANG_GERMAN_LIECHTENSTEIN = 0x05
const SUBLANG_GERMAN_LUXEMBOURG = 0x04
const SUBLANG_GERMAN_SWISS = 0x02
const SUBLANG_GREEK_GREECE = 0x01
const SUBLANG_GREENLANDIC_GREENLAND = 0x01
const SUBLANG_GUJARATI_INDIA = 0x01
const SUBLANG_HAUSA_NIGERIA = "SUBLANG_HAUSA_NIGERIA_LATIN"
const SUBLANG_HAUSA_NIGERIA_LATIN = 0x01
const SUBLANG_HAWAIIAN_US = 0x01
const SUBLANG_HEBREW_ISRAEL = 0x01
const SUBLANG_HINDI_INDIA = 0x01
const SUBLANG_HUNGARIAN_HUNGARY = 0x01
const SUBLANG_ICELANDIC_ICELAND = 0x01
const SUBLANG_IGBO_NIGERIA = 0x01
const SUBLANG_INDONESIAN_INDONESIA = 0x01
const SUBLANG_INUKTITUT_CANADA = 0x01
const SUBLANG_INUKTITUT_CANADA_LATIN = 0x02
const SUBLANG_IRISH_IRELAND = 0x02
const SUBLANG_ITALIAN = 0x01
const SUBLANG_ITALIAN_SWISS = 0x02
const SUBLANG_JAPANESE_JAPAN = 0x01
const SUBLANG_KANNADA_INDIA = 0x01
const SUBLANG_KASHMIRI_INDIA = 0x02
const SUBLANG_KASHMIRI_SASIA = 0x02
const SUBLANG_KAZAK_KAZAKHSTAN = 0x01
const SUBLANG_KHMER_CAMBODIA = 0x01
const SUBLANG_KICHE_GUATEMALA = 0x01
const SUBLANG_KINYARWANDA_RWANDA = 0x01
const SUBLANG_KONKANI_INDIA = 0x01
const SUBLANG_KOREAN = 0x01
const SUBLANG_KYRGYZ_KYRGYZSTAN = 0x01
const SUBLANG_LAO_LAO = 0x01
const SUBLANG_LAO_LAO_PDR = "SUBLANG_LAO_LAO"
const SUBLANG_LATVIAN_LATVIA = 0x01
const SUBLANG_LITHUANIAN = 0x01
const SUBLANG_LITHUANIAN_LITHUANIA = 0x01
const SUBLANG_LOWER_SORBIAN_GERMANY = 0x02
const SUBLANG_LUXEMBOURGISH_LUXEMBOURG = 0x01
const SUBLANG_MACEDONIAN_MACEDONIA = 0x01
const SUBLANG_MALAYALAM_INDIA = 0x01
const SUBLANG_MALAY_BRUNEI_DARUSSALAM = 0x02
const SUBLANG_MALAY_MALAYSIA = 0x01
const SUBLANG_MALTESE_MALTA = 0x01
const SUBLANG_MAORI_NEW_ZEALAND = 0x01
const SUBLANG_MAPUDUNGUN_CHILE = 0x01
const SUBLANG_MARATHI_INDIA = 0x01
const SUBLANG_MOHAWK_MOHAWK = 0x01
const SUBLANG_MONGOLIAN_CYRILLIC_MONGOLIA = 0x01
const SUBLANG_MONGOLIAN_PRC = 0x02
const SUBLANG_NEPALI_INDIA = 0x02
const SUBLANG_NEPALI_NEPAL = 0x01
const SUBLANG_NEUTRAL = 0x00
const SUBLANG_NORWEGIAN_BOKMAL = 0x01
const SUBLANG_NORWEGIAN_NYNORSK = 0x02
const SUBLANG_OCCITAN_FRANCE = 0x01
const SUBLANG_ODIA_INDIA = 0x01
const SUBLANG_ORIYA_INDIA = 0x01
const SUBLANG_PASHTO_AFGHANISTAN = 0x01
const SUBLANG_PERSIAN_IRAN = 0x01
const SUBLANG_POLISH_POLAND = 0x01
const SUBLANG_PORTUGUESE = 0x02
const SUBLANG_PORTUGUESE_BRAZILIAN = 0x01
const SUBLANG_PORTUGUESE_PORTUGAL = 0x02
const SUBLANG_PULAR_SENEGAL = 0x02
const SUBLANG_PUNJABI_INDIA = 0x01
const SUBLANG_PUNJABI_PAKISTAN = 0x02
const SUBLANG_QUECHUA_BOLIVIA = 0x01
const SUBLANG_QUECHUA_ECUADOR = 0x02
const SUBLANG_QUECHUA_PERU = 0x03
const SUBLANG_ROMANIAN_ROMANIA = 0x01
const SUBLANG_ROMANSH_SWITZERLAND = 0x01
const SUBLANG_RUSSIAN_RUSSIA = 0x01
const SUBLANG_SAKHA_RUSSIA = 0x01
const SUBLANG_SAMI_INARI_FINLAND = 0x09
const SUBLANG_SAMI_LULE_NORWAY = 0x04
const SUBLANG_SAMI_LULE_SWEDEN = 0x05
const SUBLANG_SAMI_NORTHERN_FINLAND = 0x03
const SUBLANG_SAMI_NORTHERN_NORWAY = 0x01
const SUBLANG_SAMI_NORTHERN_SWEDEN = 0x02
const SUBLANG_SAMI_SKOLT_FINLAND = 0x08
const SUBLANG_SAMI_SOUTHERN_NORWAY = 0x06
const SUBLANG_SAMI_SOUTHERN_SWEDEN = 0x07
const SUBLANG_SANSKRIT_INDIA = 0x01
const SUBLANG_SCOTTISH_GAELIC = 0x01
const SUBLANG_SERBIAN_BOSNIA_HERZEGOVINA_CYRILLIC = 0x07
const SUBLANG_SERBIAN_BOSNIA_HERZEGOVINA_LATIN = 0x06
const SUBLANG_SERBIAN_CROATIA = 0x01
const SUBLANG_SERBIAN_CYRILLIC = 0x03
const SUBLANG_SERBIAN_LATIN = 0x02
const SUBLANG_SERBIAN_MONTENEGRO_CYRILLIC = 0x0c
const SUBLANG_SERBIAN_MONTENEGRO_LATIN = 0x0b
const SUBLANG_SERBIAN_SERBIA_CYRILLIC = 0x0a
const SUBLANG_SERBIAN_SERBIA_LATIN = 0x09
const SUBLANG_SINDHI_AFGHANISTAN = 0x02
const SUBLANG_SINDHI_INDIA = 0x01
const SUBLANG_SINDHI_PAKISTAN = 0x02
const SUBLANG_SINHALESE_SRI_LANKA = 0x01
const SUBLANG_SLOVAK_SLOVAKIA = 0x01
const SUBLANG_SLOVENIAN_SLOVENIA = 0x01
const SUBLANG_SOTHO_NORTHERN_SOUTH_AFRICA = 0x01
const SUBLANG_SPANISH = 0x01
const SUBLANG_SPANISH_ARGENTINA = 0x0b
const SUBLANG_SPANISH_BOLIVIA = 0x10
const SUBLANG_SPANISH_CHILE = 0x0d
const SUBLANG_SPANISH_COLOMBIA = 0x09
const SUBLANG_SPANISH_COSTA_RICA = 0x05
const SUBLANG_SPANISH_DOMINICAN_REPUBLIC = 0x07
const SUBLANG_SPANISH_ECUADOR = 0x0c
const SUBLANG_SPANISH_EL_SALVADOR = 0x11
const SUBLANG_SPANISH_GUATEMALA = 0x04
const SUBLANG_SPANISH_HONDURAS = 0x12
const SUBLANG_SPANISH_MEXICAN = 0x02
const SUBLANG_SPANISH_MODERN = 0x03
const SUBLANG_SPANISH_NICARAGUA = 0x13
const SUBLANG_SPANISH_PANAMA = 0x06
const SUBLANG_SPANISH_PARAGUAY = 0x0f
const SUBLANG_SPANISH_PERU = 0x0a
const SUBLANG_SPANISH_PUERTO_RICO = 0x14
const SUBLANG_SPANISH_URUGUAY = 0x0e
const SUBLANG_SPANISH_US = 0x15
const SUBLANG_SPANISH_VENEZUELA = 0x08
const SUBLANG_SWAHILI_KENYA = 0x01
const SUBLANG_SWEDISH = 0x01
const SUBLANG_SWEDISH_FINLAND = 0x02
const SUBLANG_SWEDISH_SWEDEN = 0x01
const SUBLANG_SYRIAC = 0x01
const SUBLANG_SYRIAC_SYRIA = "SUBLANG_SYRIAC"
const SUBLANG_SYS_DEFAULT = 0x02
const SUBLANG_TAJIK_TAJIKISTAN = 0x01
const SUBLANG_TAMAZIGHT_ALGERIA_LATIN = 0x02
const SUBLANG_TAMAZIGHT_MOROCCO_TIFINAGH = 0x04
const SUBLANG_TAMIL_INDIA = 0x01
const SUBLANG_TAMIL_SRI_LANKA = 0x02
const SUBLANG_TATAR_RUSSIA = 0x01
const SUBLANG_TELUGU_INDIA = 0x01
const SUBLANG_THAI_THAILAND = 0x01
const SUBLANG_TIBETAN_BHUTAN = 0x02
const SUBLANG_TIBETAN_PRC = 0x01
const SUBLANG_TIGRIGNA_ERITREA = 0x02
const SUBLANG_TIGRINYA_ERITREA = 0x02
const SUBLANG_TIGRINYA_ETHIOPIA = 0x01
const SUBLANG_TSWANA_BOTSWANA = 0x02
const SUBLANG_TSWANA_SOUTH_AFRICA = 0x01
const SUBLANG_TURKISH_TURKEY = 0x01
const SUBLANG_TURKMEN_TURKMENISTAN = 0x01
const SUBLANG_UIGHUR_PRC = 0x01
const SUBLANG_UI_CUSTOM_DEFAULT = 0x05
const SUBLANG_UKRAINIAN_UKRAINE = 0x01
const SUBLANG_UPPER_SORBIAN_GERMANY = 0x01
const SUBLANG_URDU_INDIA = 0x02
const SUBLANG_URDU_PAKISTAN = 0x01
const SUBLANG_UZBEK_CYRILLIC = 0x02
const SUBLANG_UZBEK_LATIN = 0x01
const SUBLANG_VALENCIAN_VALENCIA = 0x02
const SUBLANG_VIETNAMESE_VIETNAM = 0x01
const SUBLANG_WELSH_UNITED_KINGDOM = 0x01
const SUBLANG_WOLOF_SENEGAL = 0x01
const SUBLANG_XHOSA_SOUTH_AFRICA = 0x01
const SUBLANG_YAKUT_RUSSIA = 0x01
const SUBLANG_YI_PRC = 0x01
const SUBLANG_YORUBA_NIGERIA = 0x01
const SUBLANG_ZULU_SOUTH_AFRICA = 0x01
const SUBVERSION_MASK = 0x000000FF
const SUCCESSFUL_ACCESS_ACE_FLAG = 0x40
const SUPPORT_LANG_NUMBER = 32
const SUSPECT_INCOMPRESSIBLE_SAMPLE_RATIO = 10
const SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE = 4096
const SUSPECT_UNCOMPRESSIBLE_LITERAL_RATIO = 20
const SWP_ASYNCWINDOWPOS = 0x4000
const SWP_DEFERERASE = 0x2000
const SWP_DRAWFRAME = "SWP_FRAMECHANGED"
const SWP_FRAMECHANGED = 0x0020
const SWP_HIDEWINDOW = 0x0080
const SWP_NOACTIVATE = 0x0010
const SWP_NOCOPYBITS = 0x0100
const SWP_NOMOVE = 0x0002
const SWP_NOOWNERZORDER = 0x0200
const SWP_NOREDRAW = 0x0008
const SWP_NOREPOSITION = "SWP_NOOWNERZORDER"
const SWP_NOSENDCHANGING = 0x0400
const SWP_NOSIZE = 0x0001
const SWP_NOZORDER = 0x0004
const SWP_SHOWWINDOW = 0x0040
const SW_ERASE = 0x0004
const SW_FORCEMINIMIZE = 11
const SW_HIDE = 0
const SW_INVALIDATE = 0x0002
const SW_MAX = 11
const SW_MAXIMIZE = 3
const SW_MINIMIZE = 6
const SW_NORMAL = 1
const SW_OTHERUNZOOM = 4
const SW_OTHERZOOM = 2
const SW_PARENTCLOSING = 1
const SW_PARENTOPENING = 3
const SW_RESTORE = 9
const SW_SCROLLCHILDREN = 0x0001
const SW_SHOW = 5
const SW_SHOWDEFAULT = 10
const SW_SHOWMAXIMIZED = 3
const SW_SHOWMINIMIZED = 2
const SW_SHOWMINNOACTIVE = 7
const SW_SHOWNA = 8
const SW_SHOWNOACTIVATE = 4
const SW_SHOWNORMAL = 1
const SW_SMOOTHSCROLL = 0x0010
const SYMBOLIC_LINK_FLAG_ALLOW_UNPRIVILEGED_CREATE = 0x2
const SYMBOLIC_LINK_FLAG_DIRECTORY = 0x1
const SYMBOL_CHARSET = 2
const SYMLINK_FLAG_RELATIVE = 1
const SYNCHRONIZATION_BARRIER_FLAGS_BLOCK_ONLY = 0x02
const SYNCHRONIZATION_BARRIER_FLAGS_NO_DELETE = 0x04
const SYNCHRONIZATION_BARRIER_FLAGS_SPIN_ONLY = 0x01
const SYSPAL_ERROR = 0
const SYSPAL_NOSTATIC = 2
const SYSPAL_NOSTATIC256 = 3
const SYSPAL_STATIC = 1
const SYSRGN = 4
const SYSTEM_ACCESS_FILTER_ACE_TYPE = 0x15
const SYSTEM_ACCESS_FILTER_NOCONSTRAINT_MASK = 0xffffffff
const SYSTEM_ACCESS_FILTER_VALID_MASK = 0x00ffffff
const SYSTEM_ALARM_ACE_TYPE = 0x3
const SYSTEM_ALARM_CALLBACK_ACE_TYPE = 0xE
const SYSTEM_ALARM_CALLBACK_OBJECT_ACE_TYPE = 0x10
const SYSTEM_ALARM_OBJECT_ACE_TYPE = 0x8
const SYSTEM_AUDIT_ACE_TYPE = 0x2
const SYSTEM_AUDIT_CALLBACK_ACE_TYPE = 0xD
const SYSTEM_AUDIT_CALLBACK_OBJECT_ACE_TYPE = 0xF
const SYSTEM_AUDIT_OBJECT_ACE_TYPE = 0x7
const SYSTEM_CACHE_ALIGNMENT_SIZE = "X86_CACHE_ALIGNMENT_SIZE"
const SYSTEM_CPU_SET_INFORMATION_ALLOCATED = 0x2
const SYSTEM_CPU_SET_INFORMATION_ALLOCATED_TO_TARGET_PROCESS = 0x4
const SYSTEM_CPU_SET_INFORMATION_PARKED = 0x1
const SYSTEM_CPU_SET_INFORMATION_REALTIME = 0x8
const SYSTEM_FIXED_FONT = 16
const SYSTEM_FONT = 13
const SYSTEM_MANDATORY_LABEL_ACE_TYPE = 0x11
const SYSTEM_MANDATORY_LABEL_NO_EXECUTE_UP = 0x4
const SYSTEM_MANDATORY_LABEL_NO_READ_UP = 0x2
const SYSTEM_MANDATORY_LABEL_NO_WRITE_UP = 0x1
const SYSTEM_PROCESS_TRUST_LABEL_ACE_TYPE = 0x14
const SYSTEM_PROCESS_TRUST_LABEL_VALID_MASK = 0x00ffffff
const SYSTEM_PROCESS_TRUST_NOCONSTRAINT_MASK = 0xffffffff
const SYSTEM_RESOURCE_ATTRIBUTE_ACE_TYPE = 0x12
const SYSTEM_SCOPED_POLICY_ID_ACE_TYPE = 0x13
const SYSTEM_STATUS_FLAG_POWER_SAVING_ON = 0x01
const SYS_OPEN = "_SYS_OPEN"
const S_ALLTHRESHOLD = 2
const S_LEGATO = 1
const S_NORMAL = 0
const S_PERIOD1024 = 1
const S_PERIOD2048 = 2
const S_PERIOD512 = 0
const S_PERIODVOICE = 3
const S_QUEUEEMPTY = 0
const S_STACCATO = 2
const S_THRESHOLD = 1
const S_WHITE1024 = 5
const S_WHITE2048 = 6
const S_WHITE512 = 4
const S_WHITEVOICE = 7
const SecureZeroMemory = "RtlSecureZeroMemory"
const SetComputerNameEx = "SetComputerNameExA"
const SetEnvironmentStrings = "SetEnvironmentStringsA"
const SetFileSecurity = "SetFileSecurityA"
const ShiftLeft128 = "__shiftleft128"
const ShiftRight128 = "__shiftright128"
const StoreFence = "_mm_sfence"
const TAPE_DRIVE_ABSOLUTE_BLK = 0x80001000
const TAPE_DRIVE_ABS_BLK_IMMED = 0x80002000
const TAPE_DRIVE_CLEAN_REQUESTS = 0x02000000
const TAPE_DRIVE_COMPRESSION = 0x00020000
const TAPE_DRIVE_ECC = 0x00010000
const TAPE_DRIVE_EJECT_MEDIA = 0x01000000
const TAPE_DRIVE_END_OF_DATA = 0x80010000
const TAPE_DRIVE_EOT_WZ_SIZE = 0x00002000
const TAPE_DRIVE_ERASE_BOP_ONLY = 0x00000040
const TAPE_DRIVE_ERASE_IMMEDIATE = 0x00000080
const TAPE_DRIVE_ERASE_LONG = 0x00000020
const TAPE_DRIVE_ERASE_SHORT = 0x00000010
const TAPE_DRIVE_FILEMARKS = 0x80040000
const TAPE_DRIVE_FIXED = 0x00000001
const TAPE_DRIVE_FIXED_BLOCK = 0x00000400
const TAPE_DRIVE_FORMAT = 0xA0000000
const TAPE_DRIVE_FORMAT_IMMEDIATE = 0xC0000000
const TAPE_DRIVE_GET_ABSOLUTE_BLK = 0x00100000
const TAPE_DRIVE_GET_LOGICAL_BLK = 0x00200000
const TAPE_DRIVE_HIGH_FEATURES = 0x80000000
const TAPE_DRIVE_INITIATOR = 0x00000004
const TAPE_DRIVE_LOAD_UNLD_IMMED = 0x80000020
const TAPE_DRIVE_LOAD_UNLOAD = 0x80000001
const TAPE_DRIVE_LOCK_UNLK_IMMED = 0x80000080
const TAPE_DRIVE_LOCK_UNLOCK = 0x80000004
const TAPE_DRIVE_LOGICAL_BLK = 0x80004000
const TAPE_DRIVE_LOG_BLK_IMMED = 0x80008000
const TAPE_DRIVE_PADDING = 0x00040000
const TAPE_DRIVE_RELATIVE_BLKS = 0x80020000
const TAPE_DRIVE_REPORT_SMKS = 0x00080000
const TAPE_DRIVE_RESERVED_BIT = 0x80000000
const TAPE_DRIVE_REVERSE_POSITION = 0x80400000
const TAPE_DRIVE_REWIND_IMMEDIATE = 0x80000008
const TAPE_DRIVE_SELECT = 0x00000002
const TAPE_DRIVE_SEQUENTIAL_FMKS = 0x80080000
const TAPE_DRIVE_SEQUENTIAL_SMKS = 0x80200000
const TAPE_DRIVE_SETMARKS = 0x80100000
const TAPE_DRIVE_SET_BLOCK_SIZE = 0x80000010
const TAPE_DRIVE_SET_CMP_BOP_ONLY = 0x04000000
const TAPE_DRIVE_SET_COMPRESSION = 0x80000200
const TAPE_DRIVE_SET_ECC = 0x80000100
const TAPE_DRIVE_SET_EOT_WZ_SIZE = 0x00400000
const TAPE_DRIVE_SET_PADDING = 0x80000400
const TAPE_DRIVE_SET_REPORT_SMKS = 0x80000800
const TAPE_DRIVE_SPACE_IMMEDIATE = 0x80800000
const TAPE_DRIVE_TAPE_CAPACITY = 0x00000100
const TAPE_DRIVE_TAPE_REMAINING = 0x00000200
const TAPE_DRIVE_TENSION = 0x80000002
const TAPE_DRIVE_TENSION_IMMED = 0x80000040
const TAPE_DRIVE_VARIABLE_BLOCK = 0x00000800
const TAPE_DRIVE_WRITE_FILEMARKS = 0x82000000
const TAPE_DRIVE_WRITE_LONG_FMKS = 0x88000000
const TAPE_DRIVE_WRITE_MARK_IMMED = 0x90000000
const TAPE_DRIVE_WRITE_PROTECT = 0x00001000
const TAPE_DRIVE_WRITE_SETMARKS = 0x81000000
const TAPE_DRIVE_WRITE_SHORT_FMKS = 0x84000000
const TA_BASELINE = 24
const TA_BOTTOM = 8
const TA_CENTER = 6
const TA_LEFT = 0
const TA_NOUPDATECP = 0
const TA_RIGHT = 2
const TA_RTLREADING = 256
const TA_TOP = 0
const TA_UPDATECP = 1
const TCI_SRCCHARSET = 1
const TCI_SRCCODEPAGE = 2
const TCI_SRCFONTSIG = 3
const TCI_SRCLOCALE = 0x1000
const TC_CP_STROKE = 0x00000004
const TC_CR_90 = 0x00000008
const TC_CR_ANY = 0x00000010
const TC_EA_DOUBLE = 0x00000200
const TC_GP_TRAP = 2
const TC_HARDERR = 1
const TC_IA_ABLE = 0x00000400
const TC_NORMAL = 0
const TC_OP_CHARACTER = 0x00000001
const TC_OP_STROKE = 0x00000002
const TC_RA_ABLE = 0x00002000
const TC_RESERVED = 0x00008000
const TC_SA_CONTIN = 0x00000100
const TC_SA_DOUBLE = 0x00000040
const TC_SA_INTEGER = 0x00000080
const TC_SCROLLBLT = 0x00010000
const TC_SF_X_YINDEP = 0x00000020
const TC_SIGNAL = 3
const TC_SO_ABLE = 0x00001000
const TC_UA_ABLE = 0x00000800
const TC_VA_ABLE = 0x00004000
const TECHNOLOGY = 2
const TEXTCAPS = 34
const THAI_CHARSET = 222
const THREAD_BASE_PRIORITY_LOWRT = 15
const THREAD_BASE_PRIORITY_MAX = 2
const THREAD_DIRECT_IMPERSONATION = 0x0200
const THREAD_DYNAMIC_CODE_ALLOW = 1
const THREAD_GET_CONTEXT = 0x0008
const THREAD_IMPERSONATE = 0x0100
const THREAD_MODE_BACKGROUND_BEGIN = 0x00010000
const THREAD_MODE_BACKGROUND_END = 0x00020000
const THREAD_POWER_THROTTLING_CURRENT_VERSION = 1
const THREAD_POWER_THROTTLING_EXECUTION_SPEED = 0x1
const THREAD_POWER_THROTTLING_VALID_FLAGS = "THREAD_POWER_THROTTLING_EXECUTION_SPEED"
const THREAD_PRIORITY_ERROR_RETURN = "MAXLONG"
const THREAD_PRIORITY_HIGHEST = "THREAD_BASE_PRIORITY_MAX"
const THREAD_PRIORITY_IDLE = "THREAD_BASE_PRIORITY_IDLE"
const THREAD_PRIORITY_LOWEST = "THREAD_BASE_PRIORITY_MIN"
const THREAD_PRIORITY_NORMAL = 0
const THREAD_PRIORITY_TIME_CRITICAL = "THREAD_BASE_PRIORITY_LOWRT"
const THREAD_PROFILING_FLAG_DISPATCH = 0x1
const THREAD_QUERY_INFORMATION = 0x0040
const THREAD_QUERY_LIMITED_INFORMATION = 0x0800
const THREAD_RESUME = 0x1000
const THREAD_SET_CONTEXT = 0x0010
const THREAD_SET_INFORMATION = 0x0020
const THREAD_SET_LIMITED_INFORMATION = 0x0400
const THREAD_SET_THREAD_TOKEN = 0x0080
const THREAD_SUSPEND_RESUME = 0x0002
const THREAD_TERMINATE = 0x0001
const THRESHOLD_PENALTY = 3
const THRESHOLD_PENALTY_RATE = 16
const TIMEFMT_ENUMPROC = "TIMEFMT_ENUMPROCA"
const TIMER_ABSTIME = 1
const TIMER_MODIFY_STATE = 0x0002
const TIMER_QUERY_STATE = 0x0001
const TIME_FORCE24HOURFORMAT = 0x00000008
const TIME_NOMINUTESORSECONDS = 0x00000001
const TIME_NOSECONDS = 0x00000002
const TIME_NOTIMEMARKER = 0x00000004
const TIME_ZONE_ID_DAYLIGHT = 2
const TIME_ZONE_ID_STANDARD = 1
const TIME_ZONE_ID_UNKNOWN = 0
const TKF_AVAILABLE = 0x00000002
const TKF_CONFIRMHOTKEY = 0x00000008
const TKF_HOTKEYACTIVE = 0x00000004
const TKF_HOTKEYSOUND = 0x00000010
const TKF_INDICATOR = 0x00000020
const TKF_TOGGLEKEYSON = 0x00000001
const TLS_MINIMUM_AVAILABLE = 64
const TME_CANCEL = 0x80000000
const TME_HOVER = 0x00000001
const TME_LEAVE = 0x00000002
const TME_NONCLIENT = 0x00000010
const TME_QUERY = 0x40000000
const TMPF_DEVICE = 0x08
const TMPF_FIXED_PITCH = 0x01
const TMPF_TRUETYPE = 0x04
const TMPF_VECTOR = 0x02
const TMP_MAX = 32767
const TMP_MAX_S = "TMP_MAX"
const TOKEN_ADJUST_DEFAULT = 0x0080
const TOKEN_ADJUST_GROUPS = 0x0040
const TOKEN_ADJUST_PRIVILEGES = 0x0020
const TOKEN_ADJUST_SESSIONID = 0x0100
const TOKEN_ASSIGN_PRIMARY = 0x0001
const TOKEN_DUPLICATE = 0x0002
const TOKEN_EXECUTE = "STANDARD_RIGHTS_EXECUTE"
const TOKEN_IMPERSONATE = 0x0004
const TOKEN_MANDATORY_POLICY_NEW_PROCESS_MIN = 0x2
const TOKEN_MANDATORY_POLICY_NO_WRITE_UP = 0x1
const TOKEN_MANDATORY_POLICY_OFF = 0x0
const TOKEN_QUERY = 0x0008
const TOKEN_QUERY_SOURCE = 0x0010
const TOKEN_SOURCE_LENGTH = 8
const TRANSACTIONMANAGER_BIND_TRANSACTION = 0x00020
const TRANSACTIONMANAGER_CREATE_RM = 0x00010
const TRANSACTIONMANAGER_GENERIC_EXECUTE = "STANDARD_RIGHTS_EXECUTE"
const TRANSACTIONMANAGER_OBJECT_PATH = "\\\\TransactionManager\\\\"
const TRANSACTIONMANAGER_QUERY_INFORMATION = 0x00001
const TRANSACTIONMANAGER_RECOVER = 0x00004
const TRANSACTIONMANAGER_RENAME = 0x00008
const TRANSACTIONMANAGER_SET_INFORMATION = 0x00002
const TRANSACTION_COMMIT = 0x0008
const TRANSACTION_DO_NOT_PROMOTE = 0x00000001
const TRANSACTION_ENLIST = 0x0004
const TRANSACTION_MANAGER_COMMIT_DEFAULT = 0x00000000
const TRANSACTION_MANAGER_COMMIT_LOWEST = 0x00000008
const TRANSACTION_MANAGER_COMMIT_SYSTEM_HIVES = 0x00000004
const TRANSACTION_MANAGER_COMMIT_SYSTEM_VOLUME = 0x00000002
const TRANSACTION_MANAGER_CORRUPT_FOR_PROGRESS = 0x00000020
const TRANSACTION_MANAGER_CORRUPT_FOR_RECOVERY = 0x00000010
const TRANSACTION_MANAGER_MAXIMUM_OPTION = 0x0000003f
const TRANSACTION_MANAGER_VOLATILE = 0x00000001
const TRANSACTION_MAXIMUM_OPTION = 0x00000001
const TRANSACTION_NOTIFICATION_TM_ONLINE_FLAG_IS_CLUSTERED = 0x1
const TRANSACTION_NOTIFY_COMMIT = 0x00000004
const TRANSACTION_NOTIFY_COMMIT_COMPLETE = 0x00000040
const TRANSACTION_NOTIFY_COMMIT_FINALIZE = 0x40000000
const TRANSACTION_NOTIFY_COMMIT_REQUEST = 0x04000000
const TRANSACTION_NOTIFY_DELEGATE_COMMIT = 0x00000400
const TRANSACTION_NOTIFY_ENLIST_MASK = 0x00040000
const TRANSACTION_NOTIFY_ENLIST_PREPREPARE = 0x00001000
const TRANSACTION_NOTIFY_INDOUBT = 0x00004000
const TRANSACTION_NOTIFY_LAST_RECOVER = 0x00002000
const TRANSACTION_NOTIFY_MARSHAL = 0x00020000
const TRANSACTION_NOTIFY_MASK = 0x3fffffff
const TRANSACTION_NOTIFY_PREPARE = 0x00000002
const TRANSACTION_NOTIFY_PREPARE_COMPLETE = 0x00000020
const TRANSACTION_NOTIFY_PREPREPARE = 0x00000001
const TRANSACTION_NOTIFY_PREPREPARE_COMPLETE = 0x00000010
const TRANSACTION_NOTIFY_PROMOTE = 0x08000000
const TRANSACTION_NOTIFY_PROMOTE_NEW = 0x10000000
const TRANSACTION_NOTIFY_PROPAGATE_PULL = 0x00008000
const TRANSACTION_NOTIFY_PROPAGATE_PUSH = 0x00010000
const TRANSACTION_NOTIFY_RECOVER = 0x00000100
const TRANSACTION_NOTIFY_RECOVER_QUERY = 0x00000800
const TRANSACTION_NOTIFY_REQUEST_OUTCOME = 0x20000000
const TRANSACTION_NOTIFY_RM_DISCONNECTED = 0x01000000
const TRANSACTION_NOTIFY_ROLLBACK = 0x00000008
const TRANSACTION_NOTIFY_ROLLBACK_COMPLETE = 0x00000080
const TRANSACTION_NOTIFY_SINGLE_PHASE_COMMIT = 0x00000200
const TRANSACTION_NOTIFY_TM_ONLINE = 0x02000000
const TRANSACTION_OBJECT_PATH = "\\\\Transaction\\\\"
const TRANSACTION_PROPAGATE = 0x0020
const TRANSACTION_QUERY_INFORMATION = 0x0001
const TRANSACTION_RIGHT_RESERVED1 = 0x0040
const TRANSACTION_ROLLBACK = 0x0010
const TRANSACTION_SET_INFORMATION = 0x0002
const TRANSFORM_CTM = 4107
const TRANSPARENT = 1
const TREE_CONNECT_ATTRIBUTE_GLOBAL = 0x00000004
const TREE_CONNECT_ATTRIBUTE_INTEGRITY = 0x00008000
const TREE_CONNECT_ATTRIBUTE_PINNED = 0x00000002
const TREE_CONNECT_ATTRIBUTE_PRIVACY = 0x00004000
const TRUE = 1
const TRUETYPE_FONTTYPE = 0x004
const TRUNCATE_EXISTING = 5
const TRUST_PROTECTED_FILTER_ACE_FLAG = 0x40
const TR_INSERTIONSORT_THRESHOLD = 8
const TR_STACKSIZE = 64
const TT_AVAILABLE = 0x0001
const TT_ENABLED = 0x0002
const TT_POLYGON_TYPE = 24
const TT_PRIM_CSPLINE = 3
const TT_PRIM_LINE = 1
const TT_PRIM_QSPLINE = 2
const TURKISH_CHARSET = 162
const TWOSTOPBITS = 2
const UCLEANUI = "SHTDN_REASON_FLAG_CLEAN_UI"
const UCSCHAR_INVALID_CHARACTER = 0xffffffff
const UDIRTYUI = "SHTDN_REASON_FLAG_DIRTY_UI"
const UILANGUAGE_ENUMPROC = "UILANGUAGE_ENUMPROCA"
const UINT16_MAX = 65535
const UINT32_MAX = 0xffffffff
const UINT64_MAX = "0xffffffffffffffffU"
const UINT8_MAX = 255
const UINTMAX_MAX = "UINT64_MAX"
const UINTPTR_MAX = "UINT64_MAX"
const UINT_FAST16_MAX = "UINT16_MAX"
const UINT_FAST32_MAX = "UINT32_MAX"
const UINT_FAST64_MAX = "UINT64_MAX"
const UINT_FAST8_MAX = "UINT8_MAX"
const UINT_LEAST16_MAX = "UINT16_MAX"
const UINT_LEAST32_MAX = "UINT32_MAX"
const UINT_LEAST64_MAX = "UINT64_MAX"
const UINT_LEAST8_MAX = "UINT8_MAX"
const UISF_ACTIVE = 0x4
const UISF_HIDEACCEL = 0x2
const UISF_HIDEFOCUS = 0x1
const UIS_CLEAR = 2
const UIS_INITIALIZE = 3
const UIS_SET = 1
const UI_CAP_2700 = 0x00000001
const UI_CAP_ROT90 = 0x00000002
const UI_CAP_ROTANY = 0x00000004
const ULW_ALPHA = 0x00000002
const ULW_COLORKEY = 0x00000001
const ULW_EX_NORESIZE = 0x00000008
const ULW_OPAQUE = 0x00000004
const UMS_VERSION = "RTL_UMS_VERSION"
const UNICODE_NOCHAR = 0xFFFF
const UNICODE_STRING_MAX_CHARS = 32767
const UNIVERSAL_NAME_INFO_LEVEL = 0x00000001
const UNLOAD_DLL_DEBUG_EVENT = 7
const UNWIND_HISTORY_TABLE_GLOBAL = 1
const UNWIND_HISTORY_TABLE_LOCAL = 2
const UNWIND_HISTORY_TABLE_NONE = 0
const UNWIND_HISTORY_TABLE_SIZE = 12
const UNW_FLAG_CHAININFO = 0x4
const UNW_FLAG_EHANDLER = 0x1
const UNW_FLAG_NHANDLER = 0x0
const UNW_FLAG_UHANDLER = 0x2
const UOI_FLAGS = 1
const UOI_HEAPSIZE = 5
const UOI_IO = 6
const UOI_NAME = 2
const UOI_TIMERPROC_EXCEPTION_SUPPRESSION = 7
const UOI_TYPE = 3
const UOI_USER_SID = 4
const USER_CET_ENVIRONMENT_SGX2_ENCLAVE = 0x00000002
const USER_CET_ENVIRONMENT_VBS_BASIC_ENCLAVE = 0x00000011
const USER_CET_ENVIRONMENT_VBS_ENCLAVE = 0x00000010
const USER_CET_ENVIRONMENT_WIN32_PROCESS = 0x00000000
const USER_DEFAULT_SCREEN_DPI = 96
const USER_TIMER_MAXIMUM = 0x7FFFFFFF
const USER_TIMER_MINIMUM = 0x0000000A
const USE___UUIDOF = 0
const UnsignedMultiply128 = "_umul128"
const UnsignedMultiplyHigh = "__umulh"
const VALID_INHERIT_FLAGS = 0x1F
const VALID_SYMBOLIC_LINK_FLAGS = "SYMBOLIC_LINK_FLAG_DIRECTORY"
const VARIABLE_PITCH = 2
const VBS_BASIC_PAGE_MEASURED_DATA = 0x00000001
const VBS_BASIC_PAGE_SYSTEM_CALL = 0x00000005
const VBS_BASIC_PAGE_THREAD_DESCRIPTOR = 0x00000004
const VBS_BASIC_PAGE_UNMEASURED_DATA = 0x00000002
const VBS_BASIC_PAGE_ZERO_FILL = 0x00000003
const VERTRES = 10
const VERTSIZE = 6
const VER_AND = 6
const VER_BUILDNUMBER = 0x0000004
const VER_CONDITION_MASK = 7
const VER_EQUAL = 1
const VER_GREATER = 2
const VER_GREATER_EQUAL = 3
const VER_LESS = 4
const VER_LESS_EQUAL = 5
const VER_MAJORVERSION = 0x0000002
const VER_MINORVERSION = 0x0000001
const VER_NT_DOMAIN_CONTROLLER = 0x0000002
const VER_NT_SERVER = 0x0000003
const VER_NT_WORKSTATION = 0x0000001
const VER_NUM_BITS_PER_CONDITION_MASK = 3
const VER_OR = 7
const VER_PLATFORMID = 0x0000008
const VER_PLATFORM_WIN32_NT = 2
const VER_PLATFORM_WIN32_WINDOWS = 1
const VER_PLATFORM_WIN32s = 0
const VER_PRODUCT_TYPE = 0x0000080
const VER_SERVER_NT = 0x80000000
const VER_SERVICEPACKMAJOR = 0x0000020
const VER_SERVICEPACKMINOR = 0x0000010
const VER_SUITENAME = 0x0000040
const VER_SUITE_BACKOFFICE = 0x00000004
const VER_SUITE_BLADE = 0x00000400
const VER_SUITE_COMMUNICATIONS = 0x00000008
const VER_SUITE_COMPUTE_SERVER = 0x00004000
const VER_SUITE_DATACENTER = 0x00000080
const VER_SUITE_EMBEDDEDNT = 0x00000040
const VER_SUITE_EMBEDDED_RESTRICTED = 0x00000800
const VER_SUITE_ENTERPRISE = 0x00000002
const VER_SUITE_MULTIUSERTS = 0x00020000
const VER_SUITE_PERSONAL = 0x00000200
const VER_SUITE_SECURITY_APPLIANCE = 0x00001000
const VER_SUITE_SINGLEUSERTS = 0x00000100
const VER_SUITE_SMALLBUSINESS = 0x00000001
const VER_SUITE_SMALLBUSINESS_RESTRICTED = 0x00000020
const VER_SUITE_STORAGE_SERVER = 0x00002000
const VER_SUITE_TERMINAL = 0x00000010
const VER_SUITE_WH_SERVER = 0x00008000
const VER_WORKSTATION_NT = 0x40000000
const VFFF_ISSHAREDFILE = 0x0001
const VFF_BUFFTOOSMALL = 0x0004
const VFF_CURNEDEST = 0x0001
const VFF_FILEINUSE = 0x0002
const VIETNAMESE_CHARSET = 163
const VIFF_DONTDELETEOLD = 0x0002
const VIFF_FORCEINSTALL = 0x0001
const VK_ACCEPT = 0x1E
const VK_ADD = 0x6B
const VK_APPS = 0x5D
const VK_ATTN = 0xF6
const VK_BACK = 0x08
const VK_BROWSER_BACK = 0xA6
const VK_BROWSER_FAVORITES = 0xAB
const VK_BROWSER_FORWARD = 0xA7
const VK_BROWSER_HOME = 0xAC
const VK_BROWSER_REFRESH = 0xA8
const VK_BROWSER_SEARCH = 0xAA
const VK_BROWSER_STOP = 0xA9
const VK_CANCEL = 0x03
const VK_CAPITAL = 0x14
const VK_CLEAR = 0x0C
const VK_CONTROL = 0x11
const VK_CONVERT = 0x1C
const VK_CRSEL = 0xF7
const VK_DECIMAL = 0x6E
const VK_DELETE = 0x2E
const VK_DIVIDE = 0x6F
const VK_DOWN = 0x28
const VK_END = 0x23
const VK_EREOF = 0xF9
const VK_ESCAPE = 0x1B
const VK_EXECUTE = 0x2B
const VK_EXSEL = 0xF8
const VK_F1 = 0x70
const VK_F10 = 0x79
const VK_F11 = 0x7A
const VK_F12 = 0x7B
const VK_F13 = 0x7C
const VK_F14 = 0x7D
const VK_F15 = 0x7E
const VK_F16 = 0x7F
const VK_F17 = 0x80
const VK_F18 = 0x81
const VK_F19 = 0x82
const VK_F2 = 0x71
const VK_F20 = 0x83
const VK_F21 = 0x84
const VK_F22 = 0x85
const VK_F23 = 0x86
const VK_F24 = 0x87
const VK_F3 = 0x72
const VK_F4 = 0x73
const VK_F5 = 0x74
const VK_F6 = 0x75
const VK_F7 = 0x76
const VK_F8 = 0x77
const VK_F9 = 0x78
const VK_FINAL = 0x18
const VK_HANGEUL = 0x15
const VK_HANGUL = 0x15
const VK_HANJA = 0x19
const VK_HELP = 0x2F
const VK_HOME = 0x24
const VK_ICO_00 = 0xE4
const VK_ICO_CLEAR = 0xE6
const VK_ICO_HELP = 0xE3
const VK_IME_OFF = 0x1A
const VK_IME_ON = 0x16
const VK_INSERT = 0x2D
const VK_JUNJA = 0x17
const VK_KANA = 0x15
const VK_KANJI = 0x19
const VK_LAUNCH_APP1 = 0xB6
const VK_LAUNCH_APP2 = 0xB7
const VK_LAUNCH_MAIL = 0xB4
const VK_LAUNCH_MEDIA_SELECT = 0xB5
const VK_LBUTTON = 0x01
const VK_LCONTROL = 0xA2
const VK_LEFT = 0x25
const VK_LMENU = 0xA4
const VK_LSHIFT = 0xA0
const VK_LWIN = 0x5B
const VK_MBUTTON = 0x04
const VK_MEDIA_NEXT_TRACK = 0xB0
const VK_MEDIA_PLAY_PAUSE = 0xB3
const VK_MEDIA_PREV_TRACK = 0xB1
const VK_MEDIA_STOP = 0xB2
const VK_MENU = 0x12
const VK_MODECHANGE = 0x1F
const VK_MULTIPLY = 0x6A
const VK_NEXT = 0x22
const VK_NONAME = 0xFC
const VK_NONCONVERT = 0x1D
const VK_NUMLOCK = 0x90
const VK_NUMPAD0 = 0x60
const VK_NUMPAD1 = 0x61
const VK_NUMPAD2 = 0x62
const VK_NUMPAD3 = 0x63
const VK_NUMPAD4 = 0x64
const VK_NUMPAD5 = 0x65
const VK_NUMPAD6 = 0x66
const VK_NUMPAD7 = 0x67
const VK_NUMPAD8 = 0x68
const VK_NUMPAD9 = 0x69
const VK_OEM_1 = 0xBA
const VK_OEM_102 = 0xE2
const VK_OEM_2 = 0xBF
const VK_OEM_3 = 0xC0
const VK_OEM_4 = 0xDB
const VK_OEM_5 = 0xDC
const VK_OEM_6 = 0xDD
const VK_OEM_7 = 0xDE
const VK_OEM_8 = 0xDF
const VK_OEM_ATTN = 0xF0
const VK_OEM_AUTO = 0xF3
const VK_OEM_AX = 0xE1
const VK_OEM_BACKTAB = 0xF5
const VK_OEM_CLEAR = 0xFE
const VK_OEM_COMMA = 0xBC
const VK_OEM_COPY = 0xF2
const VK_OEM_CUSEL = 0xEF
const VK_OEM_ENLW = 0xF4
const VK_OEM_FINISH = 0xF1
const VK_OEM_FJ_JISHO = 0x92
const VK_OEM_FJ_LOYA = 0x95
const VK_OEM_FJ_MASSHOU = 0x93
const VK_OEM_FJ_ROYA = 0x96
const VK_OEM_FJ_TOUROKU = 0x94
const VK_OEM_JUMP = 0xEA
const VK_OEM_MINUS = 0xBD
const VK_OEM_NEC_EQUAL = 0x92
const VK_OEM_PA1 = 0xEB
const VK_OEM_PA2 = 0xEC
const VK_OEM_PA3 = 0xED
const VK_OEM_PERIOD = 0xBE
const VK_OEM_PLUS = 0xBB
const VK_OEM_RESET = 0xE9
const VK_OEM_WSCTRL = 0xEE
const VK_PA1 = 0xFD
const VK_PACKET = 0xE7
const VK_PAUSE = 0x13
const VK_PLAY = 0xFA
const VK_PRINT = 0x2A
const VK_PRIOR = 0x21
const VK_PROCESSKEY = 0xE5
const VK_RBUTTON = 0x02
const VK_RCONTROL = 0xA3
const VK_RETURN = 0x0D
const VK_RIGHT = 0x27
const VK_RMENU = 0xA5
const VK_RSHIFT = 0xA1
const VK_RWIN = 0x5C
const VK_SCROLL = 0x91
const VK_SELECT = 0x29
const VK_SEPARATOR = 0x6C
const VK_SHIFT = 0x10
const VK_SLEEP = 0x5F
const VK_SNAPSHOT = 0x2C
const VK_SPACE = 0x20
const VK_SUBTRACT = 0x6D
const VK_TAB = 0x09
const VK_UP = 0x26
const VK_VOLUME_DOWN = 0xAE
const VK_VOLUME_MUTE = 0xAD
const VK_VOLUME_UP = 0xAF
const VK_XBUTTON1 = 0x05
const VK_XBUTTON2 = 0x06
const VK_ZOOM = 0xFB
const VOID = "void"
const VOLUME_NAME_DOS = 0x0
const VOLUME_NAME_GUID = 0x1
const VOLUME_NAME_NONE = 0x4
const VOLUME_NAME_NT = 0x2
const VP_COMMAND_GET = 0x0001
const VP_COMMAND_SET = 0x0002
const VP_CP_CMD_ACTIVATE = 0x0001
const VP_CP_CMD_CHANGE = 0x0004
const VP_CP_CMD_DEACTIVATE = 0x0002
const VP_CP_TYPE_APS_TRIGGER = 0x0001
const VP_CP_TYPE_MACROVISION = 0x0002
const VP_FLAGS_BRIGHTNESS = 0x0040
const VP_FLAGS_CONTRAST = 0x0080
const VP_FLAGS_COPYPROTECT = 0x0100
const VP_FLAGS_FLICKER = 0x0004
const VP_FLAGS_MAX_UNSCALED = 0x0010
const VP_FLAGS_OVERSCAN = 0x0008
const VP_FLAGS_POSITION = 0x0020
const VP_FLAGS_TV_MODE = 0x0001
const VP_FLAGS_TV_STANDARD = 0x0002
const VP_MODE_TV_PLAYBACK = 0x0002
const VP_MODE_WIN_GRAPHICS = 0x0001
const VP_TV_STANDARD_NTSC_433 = 0x00010000
const VP_TV_STANDARD_NTSC_M = 0x0001
const VP_TV_STANDARD_NTSC_M_J = 0x0002
const VP_TV_STANDARD_PAL_60 = 0x00040000
const VP_TV_STANDARD_PAL_B = 0x0004
const VP_TV_STANDARD_PAL_D = 0x0008
const VP_TV_STANDARD_PAL_G = 0x00020000
const VP_TV_STANDARD_PAL_H = 0x0010
const VP_TV_STANDARD_PAL_I = 0x0020
const VP_TV_STANDARD_PAL_M = 0x0040
const VP_TV_STANDARD_PAL_N = 0x0080
const VP_TV_STANDARD_SECAM_B = 0x0100
const VP_TV_STANDARD_SECAM_D = 0x0200
const VP_TV_STANDARD_SECAM_G = 0x0400
const VP_TV_STANDARD_SECAM_H = 0x0800
const VP_TV_STANDARD_SECAM_K = 0x1000
const VP_TV_STANDARD_SECAM_K1 = 0x2000
const VP_TV_STANDARD_SECAM_L = 0x4000
const VP_TV_STANDARD_SECAM_L1 = 0x00080000
const VP_TV_STANDARD_WIN_VGA = 0x8000
const VREFRESH = 116
const VS_ALLOW_LATIN = 0x0001
const VS_FILE_INFO = "RT_VERSION"
const VS_USER_DEFINED = 100
const VS_VERSION_INFO = 1
const VTA_BASELINE = "TA_BASELINE"
const VTA_BOTTOM = "TA_RIGHT"
const VTA_CENTER = "TA_CENTER"
const VTA_LEFT = "TA_BOTTOM"
const VTA_RIGHT = "TA_TOP"
const VTA_TOP = "TA_LEFT"
const WAIT_CHILD = "_WAIT_CHILD"
const WAIT_GRANDCHILD = "_WAIT_GRANDCHILD"
const WAIT_IO_COMPLETION = "STATUS_USER_APC"
const WA_ACTIVE = 1
const WA_CLICKACTIVE = 2
const WA_INACTIVE = 0
const WB_ISDELIMITER = 2
const WB_LEFT = 0
const WB_RIGHT = 1
const WCHAR_MAX = 0xffff
const WCHAR_MIN = 0
const WC_COMPOSITECHECK = 0x00000200
const WC_DEFAULTCHAR = 0x00000040
const WC_DISCARDNS = 0x00000010
const WC_ERR_INVALID_CHARS = 0x00000080
const WC_NO_BEST_FIT_CHARS = 0x00000400
const WC_SEPCHARS = 0x00000020
const WDK_NTDDI_VERSION = "NTDDI_WIN11_GE"
const WGL_FONT_LINES = 0
const WGL_FONT_POLYGONS = 1
const WGL_SWAPMULTIPLE_MAX = 16
const WGL_SWAP_MAIN_PLANE = 0x00000001
const WGL_SWAP_OVERLAY1 = 0x00000002
const WGL_SWAP_OVERLAY10 = 0x00000400
const WGL_SWAP_OVERLAY11 = 0x00000800
const WGL_SWAP_OVERLAY12 = 0x00001000
const WGL_SWAP_OVERLAY13 = 0x00002000
const WGL_SWAP_OVERLAY14 = 0x00004000
const WGL_SWAP_OVERLAY15 = 0x00008000
const WGL_SWAP_OVERLAY2 = 0x00000004
const WGL_SWAP_OVERLAY3 = 0x00000008
const WGL_SWAP_OVERLAY4 = 0x00000010
const WGL_SWAP_OVERLAY5 = 0x00000020
const WGL_SWAP_OVERLAY6 = 0x00000040
const WGL_SWAP_OVERLAY7 = 0x00000080
const WGL_SWAP_OVERLAY8 = 0x00000100
const WGL_SWAP_OVERLAY9 = 0x00000200
const WGL_SWAP_UNDERLAY1 = 0x00010000
const WGL_SWAP_UNDERLAY10 = 0x02000000
const WGL_SWAP_UNDERLAY11 = 0x04000000
const WGL_SWAP_UNDERLAY12 = 0x08000000
const WGL_SWAP_UNDERLAY13 = 0x10000000
const WGL_SWAP_UNDERLAY14 = 0x20000000
const WGL_SWAP_UNDERLAY15 = 0x40000000
const WGL_SWAP_UNDERLAY2 = 0x00020000
const WGL_SWAP_UNDERLAY3 = 0x00040000
const WGL_SWAP_UNDERLAY4 = 0x00080000
const WGL_SWAP_UNDERLAY5 = 0x00100000
const WGL_SWAP_UNDERLAY6 = 0x00200000
const WGL_SWAP_UNDERLAY7 = 0x00400000
const WGL_SWAP_UNDERLAY8 = 0x00800000
const WGL_SWAP_UNDERLAY9 = 0x01000000
const WHEEL_DELTA = 120
const WHEEL_PAGESCROLL = "UINT_MAX"
const WHITEONBLACK = 2
const WHITE_BRUSH = 0
const WHITE_PEN = 6
const WH_CALLWNDPROC = 4
const WH_CALLWNDPROCRET = 12
const WH_CBT = 5
const WH_DEBUG = 9
const WH_FOREGROUNDIDLE = 11
const WH_GETMESSAGE = 3
const WH_HARDWARE = 8
const WH_JOURNALPLAYBACK = 1
const WH_JOURNALRECORD = 0
const WH_KEYBOARD = 2
const WH_KEYBOARD_LL = 13
const WH_MAX = 14
const WH_MAXHOOK = "WH_MAX"
const WH_MINHOOK = "WH_MIN"
const WH_MOUSE = 7
const WH_MOUSE_LL = 14
const WH_SHELL = 10
const WH_SYSMSGFILTER = 6
const WILDCOPY_OVERLENGTH = 32
const WILDCOPY_VECLEN = 16
const WIN31_CLASS = "NULL"
const WINABLEAPI = "DECLSPEC_IMPORT"
const WINADVAPI = "DECLSPEC_IMPORT"
const WINAPI = "__stdcall"
const WINAPIV = "__cdecl"
const WINAPI_FAMILY = "WINAPI_FAMILY_DESKTOP_APP"
const WINAPI_FAMILY_APP = "WINAPI_PARTITION_APP"
const WINAPI_INLINE = "WINAPI"
const WINAPI_PARTITION_APP = 0x2
const WINAPI_PARTITION_DESKTOP = 0x1
const WINBASEAPI = "DECLSPEC_IMPORT"
const WINCFGMGR32API = "DECLSPEC_IMPORT"
const WINDEVQUERYAPI = "DECLSPEC_IMPORT"
const WINDING = 2
const WINDOW_BUFFER_SIZE_EVENT = 0x0004
const WINEVENT_INCONTEXT = 0x0004
const WINEVENT_OUTOFCONTEXT = 0x0000
const WINEVENT_SKIPOWNPROCESS = 0x0002
const WINEVENT_SKIPOWNTHREAD = 0x0001
const WINGDIAPI = "DECLSPEC_IMPORT"
const WINNORMALIZEAPI = "DECLSPEC_IMPORT"
const WINPATHCCHAPI = "WINBASEAPI"
const WINPTHREADS_INLINE = "__inline__"
const WINPTHREADS_TIME_BITS = 64
const WINSPOOLAPI = "DECLSPEC_IMPORT"
const WINSTORAGEAPI = "DECLSPEC_IMPORT"
const WINSWDEVICEAPI = "DECLSPEC_IMPORT"
const WINT_MAX = 0xffff
const WINT_MIN = 0
const WINUSERAPI = "DECLSPEC_IMPORT"
const WINVER = 0x0600
const WMSZ_BOTTOM = 6
const WMSZ_BOTTOMLEFT = 7
const WMSZ_BOTTOMRIGHT = 8
const WMSZ_LEFT = 1
const WMSZ_RIGHT = 2
const WMSZ_TOP = 3
const WMSZ_TOPLEFT = 4
const WMSZ_TOPRIGHT = 5
const WM_ACTIVATE = 0x0006
const WM_ACTIVATEAPP = 0x001C
const WM_AFXFIRST = 0x0360
const WM_AFXLAST = 0x037F
const WM_APP = 0x8000
const WM_APPCOMMAND = 0x0319
const WM_ASKCBFORMATNAME = 0x030C
const WM_CANCELJOURNAL = 0x004B
const WM_CANCELMODE = 0x001F
const WM_CAPTURECHANGED = 0x0215
const WM_CHANGECBCHAIN = 0x030D
const WM_CHANGEUISTATE = 0x0127
const WM_CHAR = 0x0102
const WM_CHARTOITEM = 0x002F
const WM_CHILDACTIVATE = 0x0022
const WM_CLEAR = 0x0303
const WM_CLIPBOARDUPDATE = 0x031d
const WM_CLOSE = 0x0010
const WM_COMMAND = 0x0111
const WM_COMMNOTIFY = 0x0044
const WM_COMPACTING = 0x0041
const WM_COMPAREITEM = 0x0039
const WM_CONTEXTMENU = 0x007B
const WM_COPY = 0x0301
const WM_COPYDATA = 0x004A
const WM_CREATE = 0x0001
const WM_CTLCOLORBTN = 0x0135
const WM_CTLCOLORDLG = 0x0136
const WM_CTLCOLOREDIT = 0x0133
const WM_CTLCOLORLISTBOX = 0x0134
const WM_CTLCOLORMSGBOX = 0x0132
const WM_CTLCOLORSCROLLBAR = 0x0137
const WM_CTLCOLORSTATIC = 0x0138
const WM_CUT = 0x0300
const WM_DEADCHAR = 0x0103
const WM_DELETEITEM = 0x002D
const WM_DESTROY = 0x0002
const WM_DESTROYCLIPBOARD = 0x0307
const WM_DEVICECHANGE = 0x0219
const WM_DEVMODECHANGE = 0x001B
const WM_DISPLAYCHANGE = 0x007E
const WM_DRAWCLIPBOARD = 0x0308
const WM_DRAWITEM = 0x002B
const WM_DROPFILES = 0x0233
const WM_DWMCOLORIZATIONCOLORCHANGED = 0x0320
const WM_DWMCOMPOSITIONCHANGED = 0x031e
const WM_DWMNCRENDERINGCHANGED = 0x031f
const WM_DWMWINDOWMAXIMIZEDCHANGE = 0x0321
const WM_ENABLE = 0x000A
const WM_ENDSESSION = 0x0016
const WM_ENTERIDLE = 0x0121
const WM_ENTERMENULOOP = 0x0211
const WM_ENTERSIZEMOVE = 0x0231
const WM_ERASEBKGND = 0x0014
const WM_EXITMENULOOP = 0x0212
const WM_EXITSIZEMOVE = 0x0232
const WM_FONTCHANGE = 0x001D
const WM_GETDLGCODE = 0x0087
const WM_GETFONT = 0x0031
const WM_GETHOTKEY = 0x0033
const WM_GETICON = 0x007F
const WM_GETMINMAXINFO = 0x0024
const WM_GETOBJECT = 0x003D
const WM_GETTEXT = 0x000D
const WM_GETTEXTLENGTH = 0x000E
const WM_GETTITLEBARINFOEX = 0x033f
const WM_HANDHELDFIRST = 0x0358
const WM_HANDHELDLAST = 0x035F
const WM_HELP = 0x0053
const WM_HOTKEY = 0x0312
const WM_HSCROLL = 0x0114
const WM_HSCROLLCLIPBOARD = 0x030E
const WM_ICONERASEBKGND = 0x0027
const WM_IME_CHAR = 0x0286
const WM_IME_COMPOSITION = 0x010F
const WM_IME_COMPOSITIONFULL = 0x0284
const WM_IME_CONTROL = 0x0283
const WM_IME_ENDCOMPOSITION = 0x010E
const WM_IME_KEYDOWN = 0x0290
const WM_IME_KEYLAST = 0x010F
const WM_IME_KEYUP = 0x0291
const WM_IME_NOTIFY = 0x0282
const WM_IME_REQUEST = 0x0288
const WM_IME_SELECT = 0x0285
const WM_IME_SETCONTEXT = 0x0281
const WM_IME_STARTCOMPOSITION = 0x010D
const WM_INITDIALOG = 0x0110
const WM_INITMENU = 0x0116
const WM_INITMENUPOPUP = 0x0117
const WM_INPUT = 0x00FF
const WM_INPUTLANGCHANGE = 0x0051
const WM_INPUTLANGCHANGEREQUEST = 0x0050
const WM_INPUT_DEVICE_CHANGE = 0x00fe
const WM_KEYDOWN = 0x0100
const WM_KEYFIRST = 0x0100
const WM_KEYLAST = 0x0109
const WM_KEYUP = 0x0101
const WM_KILLFOCUS = 0x0008
const WM_LBUTTONDBLCLK = 0x0203
const WM_LBUTTONDOWN = 0x0201
const WM_LBUTTONUP = 0x0202
const WM_MBUTTONDBLCLK = 0x0209
const WM_MBUTTONDOWN = 0x0207
const WM_MBUTTONUP = 0x0208
const WM_MDIACTIVATE = 0x0222
const WM_MDICASCADE = 0x0227
const WM_MDICREATE = 0x0220
const WM_MDIDESTROY = 0x0221
const WM_MDIGETACTIVE = 0x0229
const WM_MDIICONARRANGE = 0x0228
const WM_MDIMAXIMIZE = 0x0225
const WM_MDINEXT = 0x0224
const WM_MDIREFRESHMENU = 0x0234
const WM_MDIRESTORE = 0x0223
const WM_MDISETMENU = 0x0230
const WM_MDITILE = 0x0226
const WM_MEASUREITEM = 0x002C
const WM_MENUCHAR = 0x0120
const WM_MENUCOMMAND = 0x0126
const WM_MENUDRAG = 0x0123
const WM_MENUGETOBJECT = 0x0124
const WM_MENURBUTTONUP = 0x0122
const WM_MENUSELECT = 0x011F
const WM_MOUSEACTIVATE = 0x0021
const WM_MOUSEFIRST = 0x0200
const WM_MOUSEHOVER = 0x02A1
const WM_MOUSEHWHEEL = 0x020e
const WM_MOUSELAST = 0x020e
const WM_MOUSELEAVE = 0x02A3
const WM_MOUSEMOVE = 0x0200
const WM_MOUSEWHEEL = 0x020A
const WM_MOVE = 0x0003
const WM_MOVING = 0x0216
const WM_NCACTIVATE = 0x0086
const WM_NCCALCSIZE = 0x0083
const WM_NCCREATE = 0x0081
const WM_NCDESTROY = 0x0082
const WM_NCHITTEST = 0x0084
const WM_NCLBUTTONDBLCLK = 0x00A3
const WM_NCLBUTTONDOWN = 0x00A1
const WM_NCLBUTTONUP = 0x00A2
const WM_NCMBUTTONDBLCLK = 0x00A9
const WM_NCMBUTTONDOWN = 0x00A7
const WM_NCMBUTTONUP = 0x00A8
const WM_NCMOUSEHOVER = 0x02A0
const WM_NCMOUSELEAVE = 0x02A2
const WM_NCMOUSEMOVE = 0x00A0
const WM_NCPAINT = 0x0085
const WM_NCRBUTTONDBLCLK = 0x00A6
const WM_NCRBUTTONDOWN = 0x00A4
const WM_NCRBUTTONUP = 0x00A5
const WM_NCXBUTTONDBLCLK = 0x00AD
const WM_NCXBUTTONDOWN = 0x00AB
const WM_NCXBUTTONUP = 0x00AC
const WM_NEXTDLGCTL = 0x0028
const WM_NEXTMENU = 0x0213
const WM_NOTIFY = 0x004E
const WM_NOTIFYFORMAT = 0x0055
const WM_NULL = 0x0000
const WM_PAINT = 0x000F
const WM_PAINTCLIPBOARD = 0x0309
const WM_PAINTICON = 0x0026
const WM_PALETTECHANGED = 0x0311
const WM_PALETTEISCHANGING = 0x0310
const WM_PARENTNOTIFY = 0x0210
const WM_PASTE = 0x0302
const WM_PENWINFIRST = 0x0380
const WM_PENWINLAST = 0x038F
const WM_POWER = 0x0048
const WM_POWERBROADCAST = 0x0218
const WM_PRINT = 0x0317
const WM_PRINTCLIENT = 0x0318
const WM_QUERYDRAGICON = 0x0037
const WM_QUERYENDSESSION = 0x0011
const WM_QUERYNEWPALETTE = 0x030F
const WM_QUERYOPEN = 0x0013
const WM_QUERYUISTATE = 0x0129
const WM_QUEUESYNC = 0x0023
const WM_QUIT = 0x0012
const WM_RBUTTONDBLCLK = 0x0206
const WM_RBUTTONDOWN = 0x0204
const WM_RBUTTONUP = 0x0205
const WM_RENDERALLFORMATS = 0x0306
const WM_RENDERFORMAT = 0x0305
const WM_SETCURSOR = 0x0020
const WM_SETFOCUS = 0x0007
const WM_SETFONT = 0x0030
const WM_SETHOTKEY = 0x0032
const WM_SETICON = 0x0080
const WM_SETREDRAW = 0x000B
const WM_SETTEXT = 0x000C
const WM_SETTINGCHANGE = "WM_WININICHANGE"
const WM_SHOWWINDOW = 0x0018
const WM_SIZE = 0x0005
const WM_SIZECLIPBOARD = 0x030B
const WM_SIZING = 0x0214
const WM_SPOOLERSTATUS = 0x002A
const WM_STYLECHANGED = 0x007D
const WM_STYLECHANGING = 0x007C
const WM_SYNCPAINT = 0x0088
const WM_SYSCHAR = 0x0106
const WM_SYSCOLORCHANGE = 0x0015
const WM_SYSCOMMAND = 0x0112
const WM_SYSDEADCHAR = 0x0107
const WM_SYSKEYDOWN = 0x0104
const WM_SYSKEYUP = 0x0105
const WM_TABLET_FIRST = 0x02c0
const WM_TABLET_LAST = 0x02df
const WM_TCARD = 0x0052
const WM_THEMECHANGED = 0x031A
const WM_TIMECHANGE = 0x001E
const WM_TIMER = 0x0113
const WM_UNDO = 0x0304
const WM_UNICHAR = 0x0109
const WM_UNINITMENUPOPUP = 0x0125
const WM_UPDATEUISTATE = 0x0128
const WM_USER = 0x0400
const WM_USERCHANGED = 0x0054
const WM_VKEYTOITEM = 0x002E
const WM_VSCROLL = 0x0115
const WM_VSCROLLCLIPBOARD = 0x030A
const WM_WINDOWPOSCHANGED = 0x0047
const WM_WINDOWPOSCHANGING = 0x0046
const WM_WININICHANGE = 0x001A
const WM_WTSSESSION_CHANGE = 0x02B1
const WM_XBUTTONDBLCLK = 0x020D
const WM_XBUTTONDOWN = 0x020B
const WM_XBUTTONUP = 0x020C
const WNCON_DYNAMIC = 0x00000008
const WNCON_FORNETCARD = 0x00000001
const WNCON_NOTROUTED = 0x00000002
const WNCON_SLOWLINK = 0x00000004
const WNFMT_ABBREVIATED = 0x02
const WNFMT_CONNECTION = 0x20
const WNFMT_INENUM = 0x10
const WNFMT_MULTILINE = 0x01
const WNNC_CRED_MANAGER = 0xffff0000
const WNNC_NET_10NET = 0x00050000
const WNNC_NET_3IN1 = 0x00270000
const WNNC_NET_9TILES = 0x00090000
const WNNC_NET_APPLETALK = 0x00130000
const WNNC_NET_AS400 = 0x000b0000
const WNNC_NET_AVID = 0x001a0000
const WNNC_NET_AVID1 = 0x003a0000
const WNNC_NET_BMC = 0x00180000
const WNNC_NET_BWNFS = 0x00100000
const WNNC_NET_CLEARCASE = 0x00160000
const WNNC_NET_COGENT = 0x00110000
const WNNC_NET_CSC = 0x00260000
const WNNC_NET_DAV = 0x002e0000
const WNNC_NET_DCE = 0x00190000
const WNNC_NET_DECORB = 0x00200000
const WNNC_NET_DFS = 0x003b0000
const WNNC_NET_DISTINCT = 0x00230000
const WNNC_NET_DOCUSPACE = 0x001b0000
const WNNC_NET_DRIVEONWEB = 0x003e0000
const WNNC_NET_EXIFS = 0x002d0000
const WNNC_NET_EXTENDNET = 0x00290000
const WNNC_NET_FARALLON = 0x00120000
const WNNC_NET_FJ_REDIR = 0x00220000
const WNNC_NET_FOXBAT = 0x002b0000
const WNNC_NET_FRONTIER = 0x00170000
const WNNC_NET_FTP_NFS = 0x000c0000
const WNNC_NET_GOOGLE = 0x00430000
const WNNC_NET_HOB_NFS = 0x00320000
const WNNC_NET_IBMAL = 0x00340000
const WNNC_NET_INTERGRAPH = 0x00140000
const WNNC_NET_KNOWARE = 0x002f0000
const WNNC_NET_KWNP = 0x003c0000
const WNNC_NET_LANMAN = "WNNC_NET_SMB"
const WNNC_NET_LANSTEP = 0x00080000
const WNNC_NET_LANTASTIC = 0x000a0000
const WNNC_NET_LIFENET = 0x000e0000
const WNNC_NET_LOCK = 0x00350000
const WNNC_NET_LOCUS = 0x00060000
const WNNC_NET_MANGOSOFT = 0x001c0000
const WNNC_NET_MASFAX = 0x00310000
const WNNC_NET_MFILES = 0x00410000
const WNNC_NET_MSNET = 0x00010000
const WNNC_NET_MS_NFS = 0x00420000
const WNNC_NET_NDFS = 0x00440000
const WNNC_NET_NETWARE = 0x00030000
const WNNC_NET_OBJECT_DIRE = 0x00300000
const WNNC_NET_OPENAFS = 0x00390000
const WNNC_NET_PATHWORKS = 0x000d0000
const WNNC_NET_POWERLAN = 0x000f0000
const WNNC_NET_PROTSTOR = 0x00210000
const WNNC_NET_QUINCY = 0x00380000
const WNNC_NET_RDR2SAMPLE = 0x00250000
const WNNC_NET_RIVERFRONT1 = 0x001e0000
const WNNC_NET_RIVERFRONT2 = 0x001f0000
const WNNC_NET_RSFX = 0x00400000
const WNNC_NET_SERNET = 0x001d0000
const WNNC_NET_SHIVA = 0x00330000
const WNNC_NET_SMB = 0x00020000
const WNNC_NET_SRT = 0x00370000
const WNNC_NET_STAC = 0x002a0000
const WNNC_NET_SUN_PC_NFS = 0x00070000
const WNNC_NET_SYMFONET = 0x00150000
const WNNC_NET_TERMSRV = 0x00360000
const WNNC_NET_TWINS = 0x00240000
const WNNC_NET_VINES = 0x00040000
const WNNC_NET_VMWARE = 0x003f0000
const WNNC_NET_YAHOO = 0x002c0000
const WNNC_NET_ZENWORKS = 0x003d0000
const WN_ACCESS_DENIED = "ERROR_ACCESS_DENIED"
const WN_ALREADY_CONNECTED = "ERROR_ALREADY_ASSIGNED"
const WN_BAD_DEV_TYPE = "ERROR_BAD_DEV_TYPE"
const WN_BAD_HANDLE = "ERROR_INVALID_HANDLE"
const WN_BAD_LEVEL = "ERROR_INVALID_LEVEL"
const WN_BAD_LOCALNAME = "ERROR_BAD_DEVICE"
const WN_BAD_NETNAME = "ERROR_BAD_NET_NAME"
const WN_BAD_PASSWORD = "ERROR_INVALID_PASSWORD"
const WN_BAD_POINTER = "ERROR_INVALID_ADDRESS"
const WN_BAD_PROFILE = "ERROR_BAD_PROFILE"
const WN_BAD_PROVIDER = "ERROR_BAD_PROVIDER"
const WN_BAD_USER = "ERROR_BAD_USERNAME"
const WN_BAD_VALUE = "ERROR_INVALID_PARAMETER"
const WN_CANCEL = "ERROR_CANCELLED"
const WN_CANNOT_OPEN_PROFILE = "ERROR_CANNOT_OPEN_PROFILE"
const WN_CONNECTED_OTHER_PASSWORD = "ERROR_CONNECTED_OTHER_PASSWORD"
const WN_CONNECTED_OTHER_PASSWORD_DEFAULT = "ERROR_CONNECTED_OTHER_PASSWORD_DEFAULT"
const WN_CONNECTION_CLOSED = "ERROR_CONNECTION_UNAVAIL"
const WN_DEVICE_ALREADY_REMEMBERED = "ERROR_DEVICE_ALREADY_REMEMBERED"
const WN_DEVICE_ERROR = "ERROR_GEN_FAILURE"
const WN_DEVICE_IN_USE = "ERROR_DEVICE_IN_USE"
const WN_EXTENDED_ERROR = "ERROR_EXTENDED_ERROR"
const WN_FUNCTION_BUSY = "ERROR_BUSY"
const WN_MORE_DATA = "ERROR_MORE_DATA"
const WN_NET_ERROR = "ERROR_UNEXP_NET_ERR"
const WN_NOT_AUTHENTICATED = "ERROR_NOT_AUTHENTICATED"
const WN_NOT_CONNECTED = "ERROR_NOT_CONNECTED"
const WN_NOT_CONTAINER = "ERROR_NOT_CONTAINER"
const WN_NOT_INITIALIZING = "ERROR_ALREADY_INITIALIZED"
const WN_NOT_LOGGED_ON = "ERROR_NOT_LOGGED_ON"
const WN_NOT_SUPPORTED = "ERROR_NOT_SUPPORTED"
const WN_NOT_VALIDATED = "ERROR_NO_LOGON_SERVERS"
const WN_NO_ERROR = "NO_ERROR"
const WN_NO_MORE_DEVICES = "ERROR_NO_MORE_DEVICES"
const WN_NO_MORE_ENTRIES = "ERROR_NO_MORE_ITEMS"
const WN_NO_NETWORK = "ERROR_NO_NETWORK"
const WN_NO_NET_OR_BAD_PATH = "ERROR_NO_NET_OR_BAD_PATH"
const WN_OPEN_FILES = "ERROR_OPEN_FILES"
const WN_OUT_OF_MEMORY = "ERROR_NOT_ENOUGH_MEMORY"
const WN_RETRY = "ERROR_RETRY"
const WN_SUCCESS = "NO_ERROR"
const WN_WINDOWS_ERROR = "ERROR_UNEXP_NET_ERR"
const WOW64_CONTEXT_EXCEPTION_ACTIVE = 0x08000000
const WOW64_CONTEXT_EXCEPTION_REPORTING = 0x80000000
const WOW64_CONTEXT_EXCEPTION_REQUEST = 0x40000000
const WOW64_CONTEXT_SERVICE_ACTIVE = 0x10000000
const WOW64_CONTEXT_i386 = 0x00010000
const WOW64_CONTEXT_i486 = 0x00010000
const WOW64_MAXIMUM_SUPPORTED_EXTENSION = 512
const WOW64_SIZE_OF_80387_REGISTERS = 80
const WPF_ASYNCWINDOWPLACEMENT = 0x0004
const WPF_RESTORETOMAXIMIZED = 0x0002
const WPF_SETMINPOSITION = 0x0001
const WRITE_RESTRICTED = 0x8
const WRITE_WATCH_FLAG_RESET = 0x01
const WSABASEERR = 10000
const WSA_QOS_EUNKOWNPSOBJ = "WSA_QOS_EUNKNOWNPSOBJ"
const WS_ACTIVECAPTION = 0x0001
const WS_CHILDWINDOW = "WS_CHILD"
const WS_EX_LAYERED = 0x00080000
const WS_ICONIC = "WS_MINIMIZE"
const WS_SIZEBOX = "WS_THICKFRAME"
const WS_TILED = "WS_OVERLAPPED"
const WS_TILEDWINDOW = "WS_OVERLAPPEDWINDOW"
const WTS_CONSOLE_CONNECT = 0x1
const WTS_CONSOLE_DISCONNECT = 0x2
const WTS_REMOTE_CONNECT = 0x3
const WTS_REMOTE_DISCONNECT = 0x4
const WTS_SESSION_CREATE = 0xa
const WTS_SESSION_LOCK = 0x7
const WTS_SESSION_LOGOFF = 0x6
const WTS_SESSION_LOGON = 0x5
const WTS_SESSION_REMOTE_CONTROL = 0x9
const WTS_SESSION_TERMINATE = 0xb
const WTS_SESSION_UNLOCK = 0x8
const WT_EXECUTEDEFAULT = 0x00000000
const WT_EXECUTEDELETEWAIT = 0x00000008
const WT_EXECUTEINIOTHREAD = 0x00000001
const WT_EXECUTEINLONGTHREAD = 0x00000010
const WT_EXECUTEINPERSISTENTIOTHREAD = 0x00000040
const WT_EXECUTEINPERSISTENTTHREAD = 0x00000080
const WT_EXECUTEINTIMERTHREAD = 0x00000020
const WT_EXECUTEINUITHREAD = 0x00000002
const WT_EXECUTEINWAITTHREAD = 0x00000004
const WT_EXECUTELONGFUNCTION = 0x00000010
const WT_EXECUTEONLYONCE = 0x00000008
const WT_TRANSFER_IMPERSONATION = 0x00000100
const WVR_ALIGNBOTTOM = 0x0040
const WVR_ALIGNLEFT = 0x0020
const WVR_ALIGNRIGHT = 0x0080
const WVR_ALIGNTOP = 0x0010
const WVR_HREDRAW = 0x0100
const WVR_VALIDRECTS = 0x0400
const WVR_VREDRAW = 0x0200
const WaitNamedPipe = "WaitNamedPipeA"
const WriteMxCsr = "_mm_setcsr"
const X3_BTYPE_QP_INST_VAL_POS_X = 0
const X3_BTYPE_QP_INST_WORD_POS_X = 23
const X3_BTYPE_QP_INST_WORD_X = 2
const X3_BTYPE_QP_SIZE_X = 9
const X3_D_WH_INST_WORD_POS_X = 24
const X3_D_WH_INST_WORD_X = 3
const X3_D_WH_SIGN_VAL_POS_X = 0
const X3_D_WH_SIZE_X = 3
const X3_EMPTY_INST_VAL_POS_X = 0
const X3_EMPTY_INST_WORD_POS_X = 14
const X3_EMPTY_INST_WORD_X = 1
const X3_EMPTY_SIZE_X = 2
const X3_IMM20_INST_WORD_POS_X = 4
const X3_IMM20_INST_WORD_X = 3
const X3_IMM20_SIGN_VAL_POS_X = 0
const X3_IMM20_SIZE_X = 20
const X3_IMM39_1_INST_WORD_POS_X = 0
const X3_IMM39_1_INST_WORD_X = 2
const X3_IMM39_1_SIGN_VAL_POS_X = 36
const X3_IMM39_1_SIZE_X = 23
const X3_IMM39_2_INST_WORD_POS_X = 16
const X3_IMM39_2_INST_WORD_X = 1
const X3_IMM39_2_SIGN_VAL_POS_X = 20
const X3_IMM39_2_SIZE_X = 16
const X3_I_INST_WORD_POS_X = 27
const X3_I_INST_WORD_X = 3
const X3_I_SIGN_VAL_POS_X = 59
const X3_I_SIZE_X = 1
const X3_OPCODE_INST_WORD_POS_X = 28
const X3_OPCODE_INST_WORD_X = 3
const X3_OPCODE_SIGN_VAL_POS_X = 0
const X3_OPCODE_SIZE_X = 4
const X3_P_INST_WORD_POS_X = 0
const X3_P_INST_WORD_X = 3
const X3_P_SIGN_VAL_POS_X = 0
const X3_P_SIZE_X = 4
const X3_TMPLT_INST_WORD_POS_X = 0
const X3_TMPLT_INST_WORD_X = 0
const X3_TMPLT_SIGN_VAL_POS_X = 0
const X3_TMPLT_SIZE_X = 4
const X86_CACHE_ALIGNMENT_SIZE = 64
const XACT_E_FIRST = 0x8004D000
const XACT_E_LAST = 0x8004D029
const XACT_S_FIRST = 0x0004D000
const XACT_S_LAST = 0x0004D010
const XBUTTON1 = 0x0001
const XBUTTON2 = 0x0002
const XSTATE_ALIGN_BIT = 1
const XSTATE_AMX_TILE_CONFIG = 17
const XSTATE_AMX_TILE_DATA = 18
const XSTATE_AVX = "XSTATE_GSSE"
const XSTATE_AVX512_KMASK = 5
const XSTATE_AVX512_ZMM = 7
const XSTATE_AVX512_ZMM_H = 6
const XSTATE_CET_S = 12
const XSTATE_CET_U = 11
const XSTATE_COMPACTION_ENABLE = 63
const XSTATE_CONTEXT_FLAG_LOOKASIDE = 0x1
const XSTATE_CONTROLFLAG_XFD_MASK = 4
const XSTATE_CONTROLFLAG_XSAVEC_MASK = 2
const XSTATE_CONTROLFLAG_XSAVEOPT_MASK = 1
const XSTATE_GSSE = 2
const XSTATE_IPT = 8
const XSTATE_LEGACY_FLOATING_POINT = 0
const XSTATE_LEGACY_SSE = 1
const XSTATE_LWP = 62
const XSTATE_MASK_AVX = "XSTATE_MASK_GSSE"
const XSTATE_MASK_LARGE_FEATURES = "XSTATE_MASK_AMX_TILE_DATA"
const XSTATE_MASK_USER_VISIBLE_SUPERVISOR = "XSTATE_MASK_CET_U"
const XSTATE_MPX_BNDCSR = 4
const XSTATE_MPX_BNDREGS = 3
const XSTATE_PASID = 10
const XSTATE_XFD_BIT = 2
const XXH32_ENDJMP = 0
const XXH3_INLINE_SECRET = 0
const XXH3_WITH_SECRET_INLINE = "XXH_NO_INLINE"
const XXH_C23_VN = 201711
const XXH_CPU_LITTLE_ENDIAN = 1
const XXH_DEBUGLEVEL = "DEBUGLEVEL"
const XXH_FORCE_ALIGN_CHECK = 0
const XXH_FORCE_MEMORY_ACCESS = 1
const XXH_NO_INLINE = "static"
const XXH_NO_INLINE_HINTS = 1
const XXH_PRIME32_1 = 2654435761
const XXH_PRIME32_2 = 2246822519
const XXH_PRIME32_3 = 3266489917
const XXH_PRIME32_4 = 668265263
const XXH_PRIME32_5 = 374761393
const XXH_PRIME64_1 = 11400714785074694791
const XXH_PRIME64_2 = 14029467366897019727
const XXH_PRIME64_3 = 1609587929392839161
const XXH_PRIME64_4 = 9650029242287828579
const XXH_PRIME64_5 = 2870177450012600261
const XXH_SIZE_OPT = 0
const XXH_VERSION_MAJOR = 0
const XXH_VERSION_MINOR = 8
const XXH_VERSION_RELEASE = 2
const XXH_swap32 = "__builtin_bswap32"
const XXH_swap64 = "__builtin_bswap64"
const YieldProcessor = "_mm_pause"
const ZAWPROXYAPI = "DECLSPEC_IMPORT"
const ZDICTLIB_API = "ZDICTLIB_VISIBLE"
const ZDICTLIB_STATIC_API = "ZDICTLIB_VISIBLE"
const ZDICT_CONTENTSIZE_MIN = 128
const ZDICT_DICTSIZE_MIN = 256
const ZSTDERRORLIB_API = "ZSTDERRORLIB_VISIBLE"
const ZSTDLIB_API = "ZSTDLIB_VISIBLE"
const ZSTDLIB_STATIC_API = "ZSTDLIB_VISIBLE"
const ZSTD_ADDRESS_SANITIZER = 0
const ZSTD_ASAN_DONT_POISON_WORKSPACE = 1
const ZSTD_ASM_SUPPORTED = 1
const ZSTD_BLOCKHEADERSIZE = 3
const ZSTD_BLOCKSIZELOG_MAX = 17
const ZSTD_BLOCKSPLITTER_LEVEL_MAX = 6
const ZSTD_CHAINLOG_MAX_32 = 29
const ZSTD_CHAINLOG_MAX_64 = 30
const ZSTD_CHAINLOG_MIN = "ZSTD_HASHLOG_MIN"
const ZSTD_CLEVEL_DEFAULT = 3
const ZSTD_COMPRESSBLOCK_BTLAZY2 = "ZSTD_compressBlock_btlazy2"
const ZSTD_COMPRESSBLOCK_BTLAZY2_DICTMATCHSTATE = "ZSTD_compressBlock_btlazy2_dictMatchState"
const ZSTD_COMPRESSBLOCK_BTLAZY2_EXTDICT = "ZSTD_compressBlock_btlazy2_extDict"
const ZSTD_COMPRESSBLOCK_BTOPT = "ZSTD_compressBlock_btopt"
const ZSTD_COMPRESSBLOCK_BTOPT_DICTMATCHSTATE = "ZSTD_compressBlock_btopt_dictMatchState"
const ZSTD_COMPRESSBLOCK_BTOPT_EXTDICT = "ZSTD_compressBlock_btopt_extDict"
const ZSTD_COMPRESSBLOCK_BTULTRA = "ZSTD_compressBlock_btultra"
const ZSTD_COMPRESSBLOCK_BTULTRA2 = "ZSTD_compressBlock_btultra2"
const ZSTD_COMPRESSBLOCK_BTULTRA_DICTMATCHSTATE = "ZSTD_compressBlock_btultra_dictMatchState"
const ZSTD_COMPRESSBLOCK_BTULTRA_EXTDICT = "ZSTD_compressBlock_btultra_extDict"
const ZSTD_COMPRESSBLOCK_DOUBLEFAST = "ZSTD_compressBlock_doubleFast"
const ZSTD_COMPRESSBLOCK_DOUBLEFAST_DICTMATCHSTATE = "ZSTD_compressBlock_doubleFast_dictMatchState"
const ZSTD_COMPRESSBLOCK_DOUBLEFAST_EXTDICT = "ZSTD_compressBlock_doubleFast_extDict"
const ZSTD_COMPRESSBLOCK_GREEDY = "ZSTD_compressBlock_greedy"
const ZSTD_COMPRESSBLOCK_GREEDY_DEDICATEDDICTSEARCH = "ZSTD_compressBlock_greedy_dedicatedDictSearch"
const ZSTD_COMPRESSBLOCK_GREEDY_DEDICATEDDICTSEARCH_ROW = "ZSTD_compressBlock_greedy_dedicatedDictSearch_row"
const ZSTD_COMPRESSBLOCK_GREEDY_DICTMATCHSTATE = "ZSTD_compressBlock_greedy_dictMatchState"
const ZSTD_COMPRESSBLOCK_GREEDY_DICTMATCHSTATE_ROW = "ZSTD_compressBlock_greedy_dictMatchState_row"
const ZSTD_COMPRESSBLOCK_GREEDY_EXTDICT = "ZSTD_compressBlock_greedy_extDict"
const ZSTD_COMPRESSBLOCK_GREEDY_EXTDICT_ROW = "ZSTD_compressBlock_greedy_extDict_row"
const ZSTD_COMPRESSBLOCK_GREEDY_ROW = "ZSTD_compressBlock_greedy_row"
const ZSTD_COMPRESSBLOCK_LAZY = "ZSTD_compressBlock_lazy"
const ZSTD_COMPRESSBLOCK_LAZY2 = "ZSTD_compressBlock_lazy2"
const ZSTD_COMPRESSBLOCK_LAZY2_DEDICATEDDICTSEARCH = "ZSTD_compressBlock_lazy2_dedicatedDictSearch"
const ZSTD_COMPRESSBLOCK_LAZY2_DEDICATEDDICTSEARCH_ROW = "ZSTD_compressBlock_lazy2_dedicatedDictSearch_row"
const ZSTD_COMPRESSBLOCK_LAZY2_DICTMATCHSTATE = "ZSTD_compressBlock_lazy2_dictMatchState"
const ZSTD_COMPRESSBLOCK_LAZY2_DICTMATCHSTATE_ROW = "ZSTD_compressBlock_lazy2_dictMatchState_row"
const ZSTD_COMPRESSBLOCK_LAZY2_EXTDICT = "ZSTD_compressBlock_lazy2_extDict"
const ZSTD_COMPRESSBLOCK_LAZY2_EXTDICT_ROW = "ZSTD_compressBlock_lazy2_extDict_row"
const ZSTD_COMPRESSBLOCK_LAZY2_ROW = "ZSTD_compressBlock_lazy2_row"
const ZSTD_COMPRESSBLOCK_LAZY_DEDICATEDDICTSEARCH = "ZSTD_compressBlock_lazy_dedicatedDictSearch"
const ZSTD_COMPRESSBLOCK_LAZY_DEDICATEDDICTSEARCH_ROW = "ZSTD_compressBlock_lazy_dedicatedDictSearch_row"
const ZSTD_COMPRESSBLOCK_LAZY_DICTMATCHSTATE = "ZSTD_compressBlock_lazy_dictMatchState"
const ZSTD_COMPRESSBLOCK_LAZY_DICTMATCHSTATE_ROW = "ZSTD_compressBlock_lazy_dictMatchState_row"
const ZSTD_COMPRESSBLOCK_LAZY_EXTDICT = "ZSTD_compressBlock_lazy_extDict"
const ZSTD_COMPRESSBLOCK_LAZY_EXTDICT_ROW = "ZSTD_compressBlock_lazy_extDict_row"
const ZSTD_COMPRESSBLOCK_LAZY_ROW = "ZSTD_compressBlock_lazy_row"
const ZSTD_COMPRESS_HEAPMODE = 0
const ZSTD_CWKSP_ALIGNMENT_BYTES = 64
const ZSTD_CWKSP_ASAN_REDZONE_SIZE = 128
const ZSTD_DATAFLOW_SANITIZER = 0
const ZSTD_DISABLE_ASM = 1
const ZSTD_DUBT_UNSORTED_MARK = 1
const ZSTD_ENABLE_ASM_X86_64_BMI2 = 0
const ZSTD_FRAMECHECKSUMSIZE = 4
const ZSTD_FRAMEHEADERSIZE_MAX = 18
const ZSTD_FRAMEIDSIZE = 4
const ZSTD_HASHLOG3_MAX = 17
const ZSTD_HASHLOG_MIN = 6
const ZSTD_HAVE_WEAK_SYMBOLS = 0
const ZSTD_HEAPMODE = 1
const ZSTD_HUFFDTABLE_CAPACITY_LOG = 12
const ZSTD_LAZY_DDSS_BUCKET_LOG = 2
const ZSTD_LBMIN = 64
const ZSTD_LDM_BUCKETSIZELOG_MAX = 8
const ZSTD_LDM_BUCKETSIZELOG_MIN = 1
const ZSTD_LDM_DEFAULT_WINDOW_LOG = "ZSTD_WINDOWLOG_LIMIT_DEFAULT"
const ZSTD_LDM_HASHLOG_MAX = "ZSTD_HASHLOG_MAX"
const ZSTD_LDM_HASHLOG_MIN = "ZSTD_HASHLOG_MIN"
const ZSTD_LDM_HASHRATELOG_MIN = 0
const ZSTD_LDM_MINMATCH_MAX = 4096
const ZSTD_LDM_MINMATCH_MIN = 4
const ZSTD_LEGACY_SUPPORT = 0
const ZSTD_LITFREQ_ADD = 2
const ZSTD_MAGICNUMBER = 4247762216
const ZSTD_MAGIC_DICTIONARY = 3962610743
const ZSTD_MAGIC_SKIPPABLE_MASK = 4294967280
const ZSTD_MAGIC_SKIPPABLE_START = 407710288
const ZSTD_MAX_CLEVEL = 22
const ZSTD_MAX_HUF_HEADER_SIZE = 128
const ZSTD_MAX_NB_BLOCK_SPLITS = 196
const ZSTD_MEMORY_SANITIZER = 0
const ZSTD_MINMATCH_MAX = 7
const ZSTD_MINMATCH_MIN = 3
const ZSTD_MSAN_DONT_POISON_WORKSPACE = 1
const ZSTD_NO_CLEVEL = 0
const ZSTD_NO_FORWARD_PROGRESS_MAX = 16
const ZSTD_NO_INTRINSICS = 1
const ZSTD_OVERLAPLOG_MAX = 9
const ZSTD_OVERLAPLOG_MIN = 0
const ZSTD_PREDEF_THRESHOLD = 8
const ZSTD_REP_NUM = 3
const ZSTD_RESIZE_SEQPOOL = 0
const ZSTD_ROLL_HASH_CHAR_OFFSET = 10
const ZSTD_ROWSIZE = 16
const ZSTD_ROW_HASH_CACHE_SIZE = 8
const ZSTD_ROW_HASH_MAX_ENTRIES = 64
const ZSTD_ROW_HASH_TAG_BITS = 8
const ZSTD_SEARCHLOG_MIN = 1
const ZSTD_SEARCH_FN_ATTRS = "FORCE_NOINLINE"
const ZSTD_SHORT_CACHE_TAG_BITS = 8
const ZSTD_SKIPPABLEHEADERSIZE = 8
const ZSTD_SLIPBLOCK_WORKSPACESIZE = 8208
const ZSTD_SRCSIZEHINT_MAX = "INT_MAX"
const ZSTD_SRCSIZEHINT_MIN = 0
const ZSTD_STRATEGY_MAX = 9
const ZSTD_STRATEGY_MIN = 1
const ZSTD_TARGETCBLOCKSIZE_MAX = "ZSTD_BLOCKSIZE_MAX"
const ZSTD_TARGETCBLOCKSIZE_MIN = 1340
const ZSTD_TARGETLENGTH_MAX = "ZSTD_BLOCKSIZE_MAX"
const ZSTD_TARGETLENGTH_MIN = 0
const ZSTD_TRACE = 0
const ZSTD_USE_CDICT_PARAMS_DICTSIZE_MULTIPLIER = 6
const ZSTD_VERSION_MAJOR = 1
const ZSTD_VERSION_MINOR = 5
const ZSTD_VERSION_RELEASE = 7
const ZSTD_WINDOWLOG_ABSOLUTEMIN = 10
const ZSTD_WINDOWLOG_LIMIT_DEFAULT = 27
const ZSTD_WINDOWLOG_MAX_32 = 30
const ZSTD_WINDOWLOG_MAX_64 = 31
const ZSTD_WINDOWLOG_MIN = 10
const ZSTD_WINDOW_OVERFLOW_CORRECT_FREQUENTLY = 0
const ZSTD_WINDOW_START_INDEX = 2
const ZSTD_WORKSPACETOOLARGE_FACTOR = 3
const ZSTD_WORKSPACETOOLARGE_MAXDURATION = 128
const ZSTD_c_blockDelimiters = 1008
const ZSTD_c_blockSplitterLevel = 1017
const ZSTD_c_deterministicRefPrefix = 1012
const ZSTD_c_enableDedicatedDictSearch = 1005
const ZSTD_c_enableSeqProducerFallback = 1014
const ZSTD_c_forceAttachDict = 1001
const ZSTD_c_forceMaxWindow = 1000
const ZSTD_c_format = 10
const ZSTD_c_literalCompressionMode = 1002
const ZSTD_c_maxBlockSize = 1015
const ZSTD_c_prefetchCDictTables = 1013
const ZSTD_c_repcodeResolution = 1016
const ZSTD_c_rsyncable = 500
const ZSTD_c_searchForExternalRepcodes = "ZSTD_c_experimentalParam19"
const ZSTD_c_splitAfterSequences = 1010
const ZSTD_c_srcSizeHint = 1004
const ZSTD_c_stableInBuffer = 1006
const ZSTD_c_stableOutBuffer = 1007
const ZSTD_c_useRowMatchFinder = 1011
const ZSTD_c_validateSequences = 1009
const ZSTD_d_disableHuffmanAssembly = 1004
const ZSTD_d_forceIgnoreChecksum = 1002
const ZSTD_d_format = 1000
const ZSTD_d_maxBlockSize = 1005
const ZSTD_d_refMultipleDDicts = 1003
const ZSTD_d_stableOutBuffer = 1001
const ZSTD_frameHeader = "ZSTD_FrameHeader"
const ZSTD_frameType_e = "ZSTD_FrameType_e"
const ZSTD_paramSwitch_e = "ZSTD_ParamSwitch_e"
const ZSTD_pthread_cond_t = "CONDITION_VARIABLE"
const ZSTD_pthread_mutex_t = "CRITICAL_SECTION"
const ZSTD_sequenceFormat_e = "ZSTD_SequenceFormat_e"
const ZeroMemory = "RtlZeroMemory"
const _ALLOCA_S_HEAP_MARKER = 56797
const _ALLOCA_S_MARKER_SIZE = 16
const _ALLOCA_S_STACK_MARKER = 0xCCCC
const _ALLOCA_S_THRESHOLD = 1024
const _ANONYMOUS_STRUCT = "__MINGW_EXTENSION"
const _ANONYMOUS_UNION = "__MINGW_EXTENSION"
const _ARGMAX = 100
const _ASSEMBLY_DLL_REDIRECTION_DETAILED_INFORMATION = "_ASSEMBLY_FILE_DETAILED_INFORMATION"
const _BLANK = 0x40
const _CALL_REPORTFAULT = 0x2
const _CMP_EQ_OQ = 0x00
const _CMP_LE_OS = 0x02
const _CMP_LT_OS = 0x01
const _CMP_NEQ_UQ = 0x04
const _CMP_NLE_US = 0x06
const _CMP_NLT_US = 0x05
const _CMP_ORD_Q = 0x07
const _CMP_UNORD_Q = 0x03
const _CONTROL = 0x20
const _CRTIMP2 = "_CRTIMP"
const _CRTIMP_ALTERNATIVE = "_CRTIMP"
const _CRTIMP_NOIA64 = "_CRTIMP"
const _CRTIMP_PURE = "_CRTIMP"
const _CRT_INTERNAL_PRINTF_LEGACY_MSVCRT_COMPATIBILITY = "0x0008U"
const _CRT_INTERNAL_PRINTF_LEGACY_THREE_DIGIT_EXPONENTS = "0x0010U"
const _CRT_INTERNAL_PRINTF_LEGACY_VSPRINTF_NULL_TERMINATION = "0x0001U"
const _CRT_INTERNAL_PRINTF_LEGACY_WIDE_SPECIFIERS = "0x0004U"
const _CRT_INTERNAL_PRINTF_STANDARD_ROUNDING = "0x0020U"
const _CRT_INTERNAL_PRINTF_STANDARD_SNPRINTF_BEHAVIOR = "0x0002U"
const _CRT_INTERNAL_SCANF_LEGACY_MSVCRT_COMPATIBILITY = "0x0004U"
const _CRT_INTERNAL_SCANF_LEGACY_WIDE_SPECIFIERS = "0x0002U"
const _CRT_INTERNAL_SCANF_SECURECRT = "0x0001U"
const _Check_return_ = "__checkReturn"
const _DIGIT = 0x4
const _DIVSUFSORT_H = 1
const _FILE_OFFSET_BITS = 64
const _FMA4INTRIN_H_INCLUDED = 1
const _FREEENTRY = 0
const _HEAP_MAXREQ = 0xFFFFFFFFFFFFFFE0
const _HEX = 0x80
const _I16_MAX = 32767
const _I32_MAX = 2147483647
const _I64_MAX = "9223372036854775807ll"
const _I8_MAX = 127
const _IMMINTRIN_H_INCLUDED = 1
const _INC_CRT_UNICODE_MACROS = 2
const _INTEGRAL_MAX_BITS = 64
const _IOB_ENTRIES = 20
const _IOEOF = 0x0010
const _IOERR = 0x0020
const _IOFBF = 0x0000
const _IOLBF = 0x0040
const _IOMYBUF = 0x0008
const _IONBF = 0x0004
const _IOREAD = 0x0001
const _IORW = 0x0080
const _IOSTRG = 0x0040
const _IOWRT = 0x0002
const _LEADBYTE = 0x8000
const _LOWER = 0x2
const _MAX_DIR = 256
const _MAX_DRIVE = 3
const _MAX_ENV = 32767
const _MAX_EXT = 256
const _MAX_FNAME = 256
const _MAX_PATH = 260
const _MAX_WAIT_MALLOC_CRT = 60000
const _MCRTIMP = "_CRTIMP"
const _MM_EXCEPT_DENORM = 0x0002
const _MM_EXCEPT_DIV_ZERO = 0x0004
const _MM_EXCEPT_INEXACT = 0x0020
const _MM_EXCEPT_INVALID = 0x0001
const _MM_EXCEPT_MASK = 63
const _MM_EXCEPT_OVERFLOW = 0x0008
const _MM_EXCEPT_UNDERFLOW = 0x0010
const _MM_FLUSH_ZERO_MASK = 32768
const _MM_FLUSH_ZERO_OFF = 0x0000
const _MM_FLUSH_ZERO_ON = 0x8000
const _MM_MASK_DENORM = 0x0100
const _MM_MASK_DIV_ZERO = 0x0200
const _MM_MASK_INEXACT = 0x1000
const _MM_MASK_INVALID = 0x0080
const _MM_MASK_MASK = 8064
const _MM_MASK_OVERFLOW = 0x0400
const _MM_MASK_UNDERFLOW = 0x0800
const _MM_ROUND_DOWN = 0x2000
const _MM_ROUND_MASK = 24576
const _MM_ROUND_NEAREST = 0x0000
const _MM_ROUND_TOWARD_ZERO = 0x6000
const _MM_ROUND_UP = 0x4000
const _MRTIMP2 = "_CRTIMP"
const _M_AMD64 = 100
const _M_X64 = 100
const _NFILE = "_NSTREAM_"
const _NLSCMPERROR = 2147483647
const _NSTREAM_ = 512
const _OLD_P_OVERLAY = 2
const _OUT_TO_DEFAULT = 0
const _OUT_TO_MSGBOX = 2
const _OUT_TO_STDERR = 1
const _POSIX_CPUTIME = 200809
const _POSIX_MONOTONIC_CLOCK = 200809
const _POSIX_THREAD_CPUTIME = 200809
const _POSIX_TIMERS = 200809
const _PUNCT = 0x10
const _P_DETACH = 4
const _P_NOWAIT = 1
const _P_NOWAITO = 3
const _P_OVERLAY = 2
const _P_WAIT = 0
const _P_tmpdir = "\\\\"
const _REENTRANT = 1
const _REPORT_ERRMODE = 3
const _RTL_RUN_ONCE_DEF = 1
const _ReadBarrier = "_ReadWriteBarrier"
const _SECURECRT_FILL_BUFFER_PATTERN = 0xFD
const _SPACE = 0x8
const _STRALIGN_USE_SECURE_CRT = 0
const _SYS_OPEN = 20
const _TWO_DIGIT_EXPONENT = 0x1
const _UI16_MAX = "0xffffu"
const _UI32_MAX = "0xffffffffu"
const _UI64_MAX = "0xffffffffffffffffull"
const _UI8_MAX = "0xffu"
const _UPPER = 0x1
const _USEDENTRY = 1
const _WAIT_CHILD = 0
const _WAIT_GRANDCHILD = 1
const _WConst_return = "_CONST_RETURN"
const _WIN32 = 1
const _WIN32_IE = "_WIN32_IE_LONGHORN"
const _WIN32_IE_IE100 = 0x0a00
const _WIN32_IE_IE110 = 0x0A00
const _WIN32_IE_IE20 = 0x0200
const _WIN32_IE_IE30 = 0x0300
const _WIN32_IE_IE302 = 0x0302
const _WIN32_IE_IE40 = 0x0400
const _WIN32_IE_IE401 = 0x0401
const _WIN32_IE_IE50 = 0x0500
const _WIN32_IE_IE501 = 0x0501
const _WIN32_IE_IE55 = 0x0550
const _WIN32_IE_IE60 = 0x0600
const _WIN32_IE_IE60SP1 = 0x0601
const _WIN32_IE_IE60SP2 = 0x0603
const _WIN32_IE_IE70 = 0x0700
const _WIN32_IE_IE80 = 0x0800
const _WIN32_IE_IE90 = 0x0900
const _WIN32_IE_LONGHORN = "_WIN32_IE_IE70"
const _WIN32_IE_NT4 = "_WIN32_IE_IE20"
const _WIN32_IE_NT4SP1 = "_WIN32_IE_IE20"
const _WIN32_IE_NT4SP2 = "_WIN32_IE_IE20"
const _WIN32_IE_NT4SP3 = "_WIN32_IE_IE302"
const _WIN32_IE_NT4SP4 = "_WIN32_IE_IE401"
const _WIN32_IE_NT4SP5 = "_WIN32_IE_IE401"
const _WIN32_IE_NT4SP6 = "_WIN32_IE_IE50"
const _WIN32_IE_WIN10 = "_WIN32_IE_IE110"
const _WIN32_IE_WIN2K = "_WIN32_IE_IE501"
const _WIN32_IE_WIN2KSP1 = "_WIN32_IE_IE501"
const _WIN32_IE_WIN2KSP2 = "_WIN32_IE_IE501"
const _WIN32_IE_WIN2KSP3 = "_WIN32_IE_IE501"
const _WIN32_IE_WIN2KSP4 = "_WIN32_IE_IE501"
const _WIN32_IE_WIN6 = "_WIN32_IE_IE70"
const _WIN32_IE_WIN7 = "_WIN32_IE_IE80"
const _WIN32_IE_WIN8 = "_WIN32_IE_IE100"
const _WIN32_IE_WIN98 = "_WIN32_IE_IE401"
const _WIN32_IE_WIN98SE = "_WIN32_IE_IE50"
const _WIN32_IE_WINBLUE = "_WIN32_IE_IE100"
const _WIN32_IE_WINME = "_WIN32_IE_IE55"
const _WIN32_IE_WINTHRESHOLD = "_WIN32_IE_IE110"
const _WIN32_IE_WS03 = 0x0602
const _WIN32_IE_WS03SP1 = "_WIN32_IE_IE60SP2"
const _WIN32_IE_XP = "_WIN32_IE_IE60"
const _WIN32_IE_XPSP1 = "_WIN32_IE_IE60SP1"
const _WIN32_IE_XPSP2 = "_WIN32_IE_IE60SP2"
const _WIN32_WINNT_LONGHORN = 0x0600
const _WIN32_WINNT_NT4 = 0x0400
const _WIN32_WINNT_VISTA = 0x0600
const _WIN32_WINNT_WIN10 = 0x0A00
const _WIN32_WINNT_WIN2K = 0x0500
const _WIN32_WINNT_WIN6 = 0x0600
const _WIN32_WINNT_WIN7 = 0x0601
const _WIN32_WINNT_WIN8 = 0x0602
const _WIN32_WINNT_WINBLUE = 0x0603
const _WIN32_WINNT_WINTHRESHOLD = 0x0A00
const _WIN32_WINNT_WINXP = 0x0501
const _WIN32_WINNT_WS03 = 0x0502
const _WIN32_WINNT_WS08 = 0x0600
const _WIN64 = 1
const _WRITE_ABORT_MSG = 0x1
const _WriteBarrier = "_ReadWriteBarrier"
const _XOPMMINTRIN_H_INCLUDED = 1
const __ATOMIC_ACQUIRE = 2
const __ATOMIC_ACQ_REL = 4
const __ATOMIC_CONSUME = 1
const __ATOMIC_HLE_ACQUIRE = 65536
const __ATOMIC_HLE_RELEASE = 131072
const __ATOMIC_RELAXED = 0
const __ATOMIC_RELEASE = 3
const __ATOMIC_SEQ_CST = 5
const __BFLT16_DECIMAL_DIG__ = 4
const __BFLT16_DENORM_MIN__ = "9.18354961579912115600575419704879436e-41B"
const __BFLT16_DIG__ = 2
const __BFLT16_EPSILON__ = "7.81250000000000000000000000000000000e-3B"
const __BFLT16_HAS_DENORM__ = 1
const __BFLT16_HAS_INFINITY__ = 1
const __BFLT16_HAS_QUIET_NAN__ = 1
const __BFLT16_IS_IEC_60559__ = 0
const __BFLT16_MANT_DIG__ = 8
const __BFLT16_MAX_10_EXP__ = 38
const __BFLT16_MAX_EXP__ = 128
const __BFLT16_MAX__ = "3.38953138925153547590470800371487867e+38B"
const __BFLT16_MIN__ = "1.17549435082228750796873653722224568e-38B"
const __BFLT16_NORM_MAX__ = "3.38953138925153547590470800371487867e+38B"
const __BIGGEST_ALIGNMENT__ = 16
const __BITINT_MAXWIDTH__ = 65535
const __BYTE_ORDER__ = "__ORDER_LITTLE_ENDIAN__"
const __C89_NAMELESS = "__MINGW_EXTENSION"
const __CCGO__ = 1
const __CHAR_BIT__ = 8
const __CLRCALL_OR_CDECL = "__cdecl"
const __CRTDECL = "__cdecl"
const __DBL_DECIMAL_DIG__ = 17
const __DBL_DIG__ = 15
const __DBL_HAS_DENORM__ = 1
const __DBL_HAS_INFINITY__ = 1
const __DBL_HAS_QUIET_NAN__ = 1
const __DBL_IS_IEC_60559__ = 1
const __DBL_MANT_DIG__ = 53
const __DBL_MAX_10_EXP__ = 308
const __DBL_MAX_EXP__ = 1024
const __DEC128_EPSILON__ = 1e-33
const __DEC128_MANT_DIG__ = 34
const __DEC128_MAX_EXP__ = 6145
const __DEC128_MAX__ = "9.999999999999999999999999999999999E6144"
const __DEC128_MIN__ = 1e-6143
const __DEC128_SUBNORMAL_MIN__ = 0.000000000000000000000000000000001e-6143
const __DEC32_EPSILON__ = 1e-6
const __DEC32_MANT_DIG__ = 7
const __DEC32_MAX_EXP__ = 97
const __DEC32_MAX__ = 9.999999e96
const __DEC32_MIN__ = 1e-95
const __DEC32_SUBNORMAL_MIN__ = 0.000001e-95
const __DEC64X_EPSILON__ = "1E-33D64x"
const __DEC64X_MANT_DIG__ = 34
const __DEC64X_MAX_EXP__ = 6145
const __DEC64X_MAX__ = "9.999999999999999999999999999999999E6144D64x"
const __DEC64X_MIN__ = "1E-6143D64x"
const __DEC64X_SUBNORMAL_MIN__ = "0.000000000000000000000000000000001E-6143D64x"
const __DEC64_EPSILON__ = 1e-15
const __DEC64_MANT_DIG__ = 16
const __DEC64_MAX_EXP__ = 385
const __DEC64_MAX__ = "9.999999999999999E384"
const __DEC64_MIN__ = 1e-383
const __DEC64_SUBNORMAL_MIN__ = 0.000000000000001e-383
const __DECIMAL_BID_FORMAT__ = 1
const __DECIMAL_DIG__ = 17
const __DEC_EVAL_METHOD__ = 2
const __FINITE_MATH_ONLY__ = 0
const __FLOAT_WORD_ORDER__ = "__ORDER_LITTLE_ENDIAN__"
const __FLT128_DECIMAL_DIG__ = 36
const __FLT128_DENORM_MIN__ = 6.47517511943802511092443895822764655e-4966
const __FLT128_DIG__ = 33
const __FLT128_EPSILON__ = 1.92592994438723585305597794258492732e-34
const __FLT128_HAS_DENORM__ = 1
const __FLT128_HAS_INFINITY__ = 1
const __FLT128_HAS_QUIET_NAN__ = 1
const __FLT128_IS_IEC_60559__ = 1
const __FLT128_MANT_DIG__ = 113
const __FLT128_MAX_10_EXP__ = 4932
const __FLT128_MAX_EXP__ = 16384
const __FLT128_MAX__ = "1.18973149535723176508575932662800702e+4932"
const __FLT128_MIN__ = 3.36210314311209350626267781732175260e-4932
const __FLT128_NORM_MAX__ = "1.18973149535723176508575932662800702e+4932"
const __FLT16_DECIMAL_DIG__ = 5
const __FLT16_DENORM_MIN__ = 5.96046447753906250000000000000000000e-8
const __FLT16_DIG__ = 3
const __FLT16_EPSILON__ = 9.76562500000000000000000000000000000e-4
const __FLT16_HAS_DENORM__ = 1
const __FLT16_HAS_INFINITY__ = 1
const __FLT16_HAS_QUIET_NAN__ = 1
const __FLT16_IS_IEC_60559__ = 1
const __FLT16_MANT_DIG__ = 11
const __FLT16_MAX_10_EXP__ = 4
const __FLT16_MAX_EXP__ = 16
const __FLT16_MAX__ = 6.55040000000000000000000000000000000e+4
const __FLT16_MIN__ = 6.10351562500000000000000000000000000e-5
const __FLT16_NORM_MAX__ = 6.55040000000000000000000000000000000e+4
const __FLT32X_DECIMAL_DIG__ = 17
const __FLT32X_DENORM_MIN__ = 4.94065645841246544176568792868221372e-324
const __FLT32X_DIG__ = 15
const __FLT32X_EPSILON__ = 2.22044604925031308084726333618164062e-16
const __FLT32X_HAS_DENORM__ = 1
const __FLT32X_HAS_INFINITY__ = 1
const __FLT32X_HAS_QUIET_NAN__ = 1
const __FLT32X_IS_IEC_60559__ = 1
const __FLT32X_MANT_DIG__ = 53
const __FLT32X_MAX_10_EXP__ = 308
const __FLT32X_MAX_EXP__ = 1024
const __FLT32X_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT32X_MIN__ = 2.22507385850720138309023271733240406e-308
const __FLT32X_NORM_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT32_DECIMAL_DIG__ = 9
const __FLT32_DENORM_MIN__ = 1.40129846432481707092372958328991613e-45
const __FLT32_DIG__ = 6
const __FLT32_EPSILON__ = 1.19209289550781250000000000000000000e-7
const __FLT32_HAS_DENORM__ = 1
const __FLT32_HAS_INFINITY__ = 1
const __FLT32_HAS_QUIET_NAN__ = 1
const __FLT32_IS_IEC_60559__ = 1
const __FLT32_MANT_DIG__ = 24
const __FLT32_MAX_10_EXP__ = 38
const __FLT32_MAX_EXP__ = 128
const __FLT32_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT32_MIN__ = 1.17549435082228750796873653722224568e-38
const __FLT32_NORM_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT64X_DECIMAL_DIG__ = 36
const __FLT64X_DENORM_MIN__ = 6.47517511943802511092443895822764655e-4966
const __FLT64X_DIG__ = 33
const __FLT64X_EPSILON__ = 1.92592994438723585305597794258492732e-34
const __FLT64X_HAS_DENORM__ = 1
const __FLT64X_HAS_INFINITY__ = 1
const __FLT64X_HAS_QUIET_NAN__ = 1
const __FLT64X_IS_IEC_60559__ = 1
const __FLT64X_MANT_DIG__ = 113
const __FLT64X_MAX_10_EXP__ = 4932
const __FLT64X_MAX_EXP__ = 16384
const __FLT64X_MAX__ = "1.18973149535723176508575932662800702e+4932"
const __FLT64X_MIN__ = 3.36210314311209350626267781732175260e-4932
const __FLT64X_NORM_MAX__ = "1.18973149535723176508575932662800702e+4932"
const __FLT64_DECIMAL_DIG__ = 17
const __FLT64_DENORM_MIN__ = 4.94065645841246544176568792868221372e-324
const __FLT64_DIG__ = 15
const __FLT64_EPSILON__ = 2.22044604925031308084726333618164062e-16
const __FLT64_HAS_DENORM__ = 1
const __FLT64_HAS_INFINITY__ = 1
const __FLT64_HAS_QUIET_NAN__ = 1
const __FLT64_IS_IEC_60559__ = 1
const __FLT64_MANT_DIG__ = 53
const __FLT64_MAX_10_EXP__ = 308
const __FLT64_MAX_EXP__ = 1024
const __FLT64_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT64_MIN__ = 2.22507385850720138309023271733240406e-308
const __FLT64_NORM_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT_DECIMAL_DIG__ = 9
const __FLT_DENORM_MIN__ = 1.40129846432481707092372958328991613e-45
const __FLT_DIG__ = 6
const __FLT_EPSILON__ = 1.19209289550781250000000000000000000e-7
const __FLT_EVAL_METHOD_TS_18661_3__ = 2
const __FLT_EVAL_METHOD__ = 2
const __FLT_HAS_DENORM__ = 1
const __FLT_HAS_INFINITY__ = 1
const __FLT_HAS_QUIET_NAN__ = 1
const __FLT_IS_IEC_60559__ = 1
const __FLT_MANT_DIG__ = 24
const __FLT_MAX_10_EXP__ = 38
const __FLT_MAX_EXP__ = 128
const __FLT_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT_MIN__ = 1.17549435082228750796873653722224568e-38
const __FLT_NORM_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT_RADIX__ = 2
const __FUNCTION__ = "__func__"
const __FXSR__ = 1
const __GCC_ASM_FLAG_OUTPUTS__ = 1
const __GCC_ATOMIC_BOOL_LOCK_FREE = 2
const __GCC_ATOMIC_CHAR16_T_LOCK_FREE = 2
const __GCC_ATOMIC_CHAR32_T_LOCK_FREE = 2
const __GCC_ATOMIC_CHAR_LOCK_FREE = 2
const __GCC_ATOMIC_INT_LOCK_FREE = 2
const __GCC_ATOMIC_LLONG_LOCK_FREE = 2
const __GCC_ATOMIC_LONG_LOCK_FREE = 2
const __GCC_ATOMIC_POINTER_LOCK_FREE = 2
const __GCC_ATOMIC_SHORT_LOCK_FREE = 2
const __GCC_ATOMIC_TEST_AND_SET_TRUEVAL = 1
const __GCC_ATOMIC_WCHAR_T_LOCK_FREE = 2
const __GCC_CONSTRUCTIVE_SIZE = 64
const __GCC_DESTRUCTIVE_SIZE = 64
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 = 1
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 = 1
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 = 1
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 = 1
const __GCC_IEC_559 = 2
const __GCC_IEC_559_COMPLEX = 2
const __GNUC_EXECUTION_CHARSET_NAME = "UTF-8"
const __GNUC_MINOR__ = 1
const __GNUC_PATCHLEVEL__ = 0
const __GNUC_STDC_INLINE__ = 1
const __GNUC_WIDE_EXECUTION_CHARSET_NAME = "UTF-16LE"
const __GNUC__ = 15
const __GNU_EXTENSION = "__MINGW_EXTENSION"
const __GOT_SECURE_LIB__ = "__STDC_SECURE_LIB__"
const __GXX_ABI_VERSION = 1020
const __GXX_MERGED_TYPEINFO_NAMES = 0
const __GXX_TYPEINFO_EQUALITY_INLINE = 0
const __HAVE_SPECULATION_SAFE_VALUE = 1
const __INT16_MAX__ = 0x7fff
const __INT32_MAX__ = 0x7fffffff
const __INT32_TYPE__ = "int"
const __INT64_MAX__ = 0x7fffffffffffffff
const __INT8_MAX__ = 0x7f
const __INTMAX_MAX__ = 0x7fffffffffffffff
const __INTMAX_WIDTH__ = 64
const __INTPTR_MAX__ = 0x7fffffffffffffff
const __INTPTR_WIDTH__ = 64
const __INT_FAST16_MAX__ = 0x7fff
const __INT_FAST16_WIDTH__ = 16
const __INT_FAST32_MAX__ = 0x7fffffff
const __INT_FAST32_TYPE__ = "int"
const __INT_FAST32_WIDTH__ = 32
const __INT_FAST64_MAX__ = 0x7fffffffffffffff
const __INT_FAST64_WIDTH__ = 64
const __INT_FAST8_MAX__ = 0x7f
const __INT_FAST8_WIDTH__ = 8
const __INT_LEAST16_MAX__ = 0x7fff
const __INT_LEAST16_WIDTH__ = 16
const __INT_LEAST32_MAX__ = 0x7fffffff
const __INT_LEAST32_TYPE__ = "int"
const __INT_LEAST32_WIDTH__ = 32
const __INT_LEAST64_MAX__ = 0x7fffffffffffffff
const __INT_LEAST64_WIDTH__ = 64
const __INT_LEAST8_MAX__ = 0x7f
const __INT_LEAST8_WIDTH__ = 8
const __INT_MAX__ = 2147483647
const __INT_WIDTH__ = 32
const __LDBL_DECIMAL_DIG__ = 17
const __LDBL_DENORM_MIN__ = 4.94065645841246544176568792868221372e-324
const __LDBL_DIG__ = 15
const __LDBL_EPSILON__ = 2.22044604925031308084726333618164062e-16
const __LDBL_HAS_DENORM__ = 1
const __LDBL_HAS_INFINITY__ = 1
const __LDBL_HAS_QUIET_NAN__ = 1
const __LDBL_IS_IEC_60559__ = 1
const __LDBL_MANT_DIG__ = 53
const __LDBL_MAX_10_EXP__ = 308
const __LDBL_MAX_EXP__ = 1024
const __LDBL_MAX__ = 1.79769313486231570814527423731704357e+308
const __LDBL_MIN__ = 2.22507385850720138309023271733240406e-308
const __LDBL_NORM_MAX__ = 1.79769313486231570814527423731704357e+308
const __LONG32 = "long"
const __LONG_DOUBLE_64__ = 1
const __LONG_LONG_MAX__ = 0x7fffffffffffffff
const __LONG_LONG_WIDTH__ = 64
const __LONG_MAX__ = 0x7fffffff
const __LONG_WIDTH__ = 32
const __MIDL_CONST = "const"
const __MINGW32_MAJOR_VERSION = 3
const __MINGW32_MINOR_VERSION = 11
const __MINGW32__ = 1
const __MINGW64_VERSION_BUGFIX = 0
const __MINGW64_VERSION_MAJOR = 13
const __MINGW64_VERSION_MINOR = 0
const __MINGW64_VERSION_RC = 0
const __MINGW64_VERSION_STATE = "alpha"
const __MINGW64__ = 1
const __MINGW_DEBUGBREAK_IMPL = 1
const __MINGW_FASTFAIL_IMPL = 1
const __MINGW_FORTIFY_LEVEL = 0
const __MINGW_FORTIFY_VA_ARG = 0
const __MINGW_HAVE_ANSI_C99_PRINTF = 1
const __MINGW_HAVE_ANSI_C99_SCANF = 1
const __MINGW_HAVE_WIDE_C99_PRINTF = 1
const __MINGW_HAVE_WIDE_C99_SCANF = 1
const __MINGW_MSVC2005_DEPREC_STR = "This POSIX function is deprecated beginning in Visual C++ 2005, use _CRT_NONSTDC_NO_DEPRECATE to disable deprecation"
const __MINGW_PREFETCH_IMPL = 1
const __MINGW_PROCNAMEEXT_AW = "A"
const __MINGW_SEC_WARN_STR = "This function or variable may be unsafe, use _CRT_SECURE_NO_WARNINGS to disable deprecation"
const __MINGW_USE_UNDERSCORE_PREFIX = 0
const __MSVCRT_VERSION__ = 0x600
const __MSVCRT__ = 1
const __NO_INLINE__ = 1
const __ORDER_BIG_ENDIAN__ = 4321
const __ORDER_LITTLE_ENDIAN__ = 1234
const __ORDER_PDP_ENDIAN__ = 3412
const __PIC__ = 1
const __PRAGMA_REDEFINE_EXTNAME = 1
const __PRETTY_FUNCTION__ = "__func__"
const __PTRDIFF_MAX__ = 0x7fffffffffffffff
const __PTRDIFF_WIDTH__ = 64
const __SCHAR_MAX__ = 0x7f
const __SCHAR_WIDTH__ = 8
const __SEG_FS = 1
const __SEG_GS = 1
const __SEH__ = 1
const __SHRT_MAX__ = 0x7fff
const __SHRT_WIDTH__ = 16
const __SIG_ATOMIC_MAX__ = 0x7fffffff
const __SIG_ATOMIC_TYPE__ = "int"
const __SIG_ATOMIC_WIDTH__ = 32
const __SIZEOF_DOUBLE__ = 8
const __SIZEOF_FLOAT128__ = 16
const __SIZEOF_FLOAT80__ = 16
const __SIZEOF_FLOAT__ = 4
const __SIZEOF_INT128__ = 16
const __SIZEOF_INT__ = 4
const __SIZEOF_LONG_DOUBLE__ = 8
const __SIZEOF_LONG_LONG__ = 8
const __SIZEOF_LONG__ = 4
const __SIZEOF_POINTER__ = 8
const __SIZEOF_PTRDIFF_T__ = 8
const __SIZEOF_SHORT__ = 2
const __SIZEOF_SIZE_T__ = 8
const __SIZEOF_WCHAR_T__ = 2
const __SIZEOF_WINT_T__ = 2
const __SIZE_MAX__ = "0xffffffffffffffffU"
const __SIZE_WIDTH__ = 64
const __STDC_EMBED_EMPTY__ = 2
const __STDC_EMBED_FOUND__ = 1
const __STDC_EMBED_NOT_FOUND__ = 0
const __STDC_HOSTED__ = 1
const __STDC_SECURE_LIB__ = 200411
const __STDC_UTF_16__ = 1
const __STDC_UTF_32__ = 1
const __STDC_VERSION__ = 201710
const __STDC_WANT_SECURE_LIB__ = 0
const __STDC__ = 1
const __STRICT_ANSI__ = 1
const __UA_WCSLEN = "ua_wcslen"
const __UINT16_MAX__ = 0xffff
const __UINT32_MAX__ = 0xffffffff
const __UINT64_MAX__ = "0xffffffffffffffffU"
const __UINT8_MAX__ = 0xff
const __UINTMAX_MAX__ = "0xffffffffffffffffU"
const __UINTPTR_MAX__ = "0xffffffffffffffffU"
const __UINT_FAST16_MAX__ = 0xffff
const __UINT_FAST32_MAX__ = 0xffffffff
const __UINT_FAST64_MAX__ = "0xffffffffffffffffU"
const __UINT_FAST8_MAX__ = 0xff
const __UINT_LEAST16_MAX__ = 0xffff
const __UINT_LEAST32_MAX__ = 0xffffffff
const __UINT_LEAST64_MAX__ = "0xffffffffffffffffU"
const __UINT_LEAST8_MAX__ = 0xff
const __USE_MINGW_ANSI_STDIO = 1
const __USE_MINGW_STRTOX = 1
const __USING_POSIXTHREAD__ = 1
const __VERSION__ = "15.1.0"
const __WCHAR_MAX__ = 0xffff
const __WCHAR_MIN__ = 0
const __WCHAR_WIDTH__ = 16
const __WIN32 = 1
const __WIN32__ = 1
const __WIN64 = 1
const __WIN64__ = 1
const __WINNT = 1
const __WINNT__ = 1
const __WINT_MAX__ = 0xffff
const __WINT_MIN__ = 0
const __WINT_WIDTH__ = 16
const __amd64 = 1
const __amd64__ = 1
const __builtin_vsnprintf1 = "__mingw_vsnprintf"
const __builtin_vsprintf = "__mingw_vsprintf"
const __checkReturn = "__inner_checkReturn"
const __clockid_t_defined = 1
const __code_model_medium__ = 1
const __int16 = "short"
const __int32 = "int"
const __int3264 = "__int64"
const __int8 = "char"
const __k8 = 1
const __k8__ = 1
const __mingw_bos_ovr = "__mingw_ovr"
const __pic__ = 1
const __x86_64 = 1
const __x86_64__ = 1
const _ftime1 = "_ftime64"
const _ftime_s = "_ftime64_s"
const _inline = "__inline"
const _timeb = "__timeb64"
const _wP_tmpdir = "\\\\"
const abnormal_termination = "_abnormal_termination"
const environ1 = "_environ"
const exception_code = "_exception_code"
const isascii1 = "__isascii"
const iscsym = "__iscsym"
const iscsymf = "__iscsymf"
const kLazySkippingStep = 8
const kSearchStrength = 8
const onexit_t = "_onexit_t"
const pclose1 = "_pclose"
const popen1 = "_popen"
const strcasecmp1 = "_stricmp"
const strncasecmp = "_strnicmp"
const sys_errlist = "_sys_errlist"
const sys_nerr = "_sys_nerr"
const toascii = "__toascii"
const ua_CharUpper = "CharUpperA"
const ua_lstrcmp = "lstrcmpA"
const ua_lstrcmpi = "lstrcmpiA"
const ua_lstrlen = "lstrlenA"
const ua_tcscpy = "strcpy"
const wcswcs = "wcsstr"
const wpopen = "_wpopen"

type __builtin_va_list = uintptr

type __predefined_size_t = uint64

type __predefined_wchar_t = uint16

type __predefined_ptrdiff_t = int64

type __gnuc_va_list = uintptr

type va_list = uintptr

type size_t = uint64

type ssize_t = int64

type rsize_t = uint64

type intptr_t = int64

type uintptr_t = uint64

type ptrdiff_t = int64

type wchar_t = uint16

type wint_t = uint16

type wctype_t = uint16

type errno_t = int32

type __time32_t = int32

type __time64_t = int64

type time_t = int64

type threadlocaleinfostruct = struct {
	Frefcount      int32
	Flc_codepage   uint32
	Flc_collate_cp uint32
	Flc_handle     [6]uint32
	Flc_id         [6]LC_ID
	Flc_category   [6]struct {
		Flocale    uintptr
		Fwlocale   uintptr
		Frefcount  uintptr
		Fwrefcount uintptr
	}
	Flc_clike            int32
	Fmb_cur_max          int32
	Flconv_intl_refcount uintptr
	Flconv_num_refcount  uintptr
	Flconv_mon_refcount  uintptr
	Flconv               uintptr
	Fctype1_refcount     uintptr
	Fctype1              uintptr
	Fpctype              uintptr
	Fpclmap              uintptr
	Fpcumap              uintptr
	Flc_time_curr        uintptr
}

type pthreadlocinfo = uintptr

type pthreadmbcinfo = uintptr

type _locale_tstruct = struct {
	Flocinfo pthreadlocinfo
	Fmbcinfo pthreadmbcinfo
}

type localeinfo_struct = _locale_tstruct

type _locale_t = uintptr

type LC_ID = struct {
	FwLanguage uint16
	FwCountry  uint16
	FwCodePage uint16
}

type tagLC_ID = LC_ID

type LPLC_ID = uintptr

type threadlocinfo = struct {
	Frefcount      int32
	Flc_codepage   uint32
	Flc_collate_cp uint32
	Flc_handle     [6]uint32
	Flc_id         [6]LC_ID
	Flc_category   [6]struct {
		Flocale    uintptr
		Fwlocale   uintptr
		Frefcount  uintptr
		Fwrefcount uintptr
	}
	Flc_clike            int32
	Fmb_cur_max          int32
	Flconv_intl_refcount uintptr
	Flconv_num_refcount  uintptr
	Flconv_mon_refcount  uintptr
	Flconv               uintptr
	Fctype1_refcount     uintptr
	Fctype1              uintptr
	Fpctype              uintptr
	Fpclmap              uintptr
	Fpcumap              uintptr
	Flc_time_curr        uintptr
}

type max_align_t = struct {
	F__max_align_ll int64
	F__max_align_ld float64
}

type _onexit_t = uintptr

type div_t = struct {
	Fquot int32
	Frem  int32
}

type _div_t = div_t

type ldiv_t = struct {
	Fquot int32
	Frem  int32
}

type _ldiv_t = ldiv_t

type _LDOUBLE = struct {
	Fld [10]uint8
}

type _CRT_DOUBLE = struct {
	Fx float64
}

type _CRT_FLOAT = struct {
	Ff float32
}

type _LONGDOUBLE = struct {
	Fx float64
}

type _LDBL12 = struct {
	Fld12 [12]uint8
}

type _purecall_handler = uintptr

type _invalid_parameter_handler = uintptr

type lldiv_t = struct {
	Fquot int64
	Frem  int64
}

type _HEAPINFO = struct {
	F_pentry  uintptr
	F_size    size_t
	F_useflag int32
}

type _heapinfo = _HEAPINFO

/**** ended inlining common/debug.c ****/
/**** start inlining common/entropy_common.c ****/
/* ******************************************************************
 * Common functions of New Generation Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* *************************************
*  Dependencies
***************************************/
/**** start inlining mem.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-****************************************
*  Dependencies
******************************************/
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/*
 * ISO C Standard:  7.17  Common definitions  <stddef.h>
 */

/* Copyright (C) 1989-2025 Free Software Foundation, Inc.

This file is part of GCC.

GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.

GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.

You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */

/*
 * ISO C Standard:  7.17  Common definitions  <stddef.h>
 */
/**** start inlining compiler.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/*
 * ISO C Standard:  7.17  Common definitions  <stddef.h>
 */

/* Copyright (C) 1989-2025 Free Software Foundation, Inc.

This file is part of GCC.

GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.

GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.

You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */

/*
 * ISO C Standard:  7.17  Common definitions  <stddef.h>
 */

/**** start inlining portability_macros.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**
 * This header file contains macro definitions to support portability.
 * This header is shared between C and ASM code, so it MUST only
 * contain macro definitions. It MUST not contain any C code.
 *
 * This header ONLY defines macros to detect platforms/feature support.
 *
 */

/* compat. with non-clang compilers */

/* compat. with non-clang compilers */

/* compat. with non-clang compilers */

/* detects whether we are being compiled under msan */

/* detects whether we are being compiled under asan */

/* detects whether we are being compiled under dfsan */

/* Mark the internal assembly functions as hidden  */

/* Compile time determination of BMI2 support */

/* Enable runtime BMI2 dispatch based on the CPU.
 * Enabled for clang & gcc >=4.8 on x86 when BMI2 isn't enabled by default.
 */

/**
 * Only enable assembly for GNU C compatible compilers,
 * because other platforms may not support GAS assembly syntax.
 *
 * Only enable assembly for Linux / MacOS / Win32, other platforms may
 * work, but they haven't been tested. This could likely be
 * extended to BSD systems.
 *
 * Disable assembly when MSAN is enabled, because MSAN requires
 * 100% of code to be instrumented to work.
 */

/**
 * Determines whether we should enable assembly for x86-64
 * with BMI2.
 *
 * Enable if all of the following conditions hold:
 * - ASM hasn't been explicitly disabled by defining ZSTD_DISABLE_ASM
 * - Assembly is supported
 * - We are compiling for x86-64 and either:
 *   - DYNAMIC_BMI2 is enabled
 *   - BMI2 is supported at compile time
 */

/*
 * For x86 ELF targets, add .note.gnu.property section for Intel CET in
 * assembly sources when CET is enabled.
 *
 * Additionally, any function that may be called indirectly must begin
 * with ZSTD_CET_ENDBRANCH.
 */

/**** ended inlining portability_macros.h ****/

/*-*******************************************************
*  Compiler specifics
*********************************************************/
/* force inlining */

/**
  On MSVC qsort requires that functions passed into it use the __cdecl calling conversion(CC).
  This explicitly marks such functions as __cdecl so that the code will still compile
  if a CC other than __cdecl has been made the default.
*/

/* UNUSED_ATTR tells the compiler it is okay if the function is unused. */

/**
 * FORCE_INLINE_TEMPLATE is used to define C "templates", which take constant
 * parameters. They must be inlined for the compiler to eliminate the constant
 * branches.
 */
/**
 * HINT_INLINE is used to help the compiler generate better code. It is *not*
 * used for "templates", so it can be tweaked based on the compilers
 * performance.
 *
 * gcc-4.8 and gcc-4.9 have been shown to benefit from leaving off the
 * always_inline attribute.
 *
 * clang up to 5.0.0 (trunk) benefit tremendously from the always_inline
 * attribute.
 */

/* "soft" inline :
 * The compiler is free to select if it's a good idea to inline or not.
 * The main objective is to silence compiler warnings
 * when a defined function in included but not used.
 *
 * Note : this macro is prefixed `MEM_` because it used to be provided by `mem.h` unit.
 * Updating the prefix is probably preferable, but requires a fairly large codemod,
 * since this name is used everywhere.
 */

/* force no inlining */

/* target attribute */

/* Target attribute for BMI2 dynamic dispatch.
 * Enable lzcnt, bmi, and bmi2.
 * We test for bmi1 & bmi2. lzcnt is included in bmi1.
 */

/* prefetch
 * can be disabled, by declaring NO_PREFETCH build macro */

/* vectorization
 * older GCC (pre gcc-4.3 picked as the cutoff) uses a different syntax,
 * and some compilers, like Intel ICC and MCST LCC, do not support it at all. */

/* Tell the compiler that a branch is likely or unlikely.
 * Only use these macros if it causes the compiler to generate better code.
 * If you can remove a LIKELY/UNLIKELY annotation without speed changes in gcc
 * and clang, please do.
 */

/* disable warnings */

/* compile time determination of SIMD support */

/* C-language Attributes are added in C23. */

/* Only use C++ attributes in C++. Some compilers report support for C++
 * attributes when compiling with C.
 */

/* Define ZSTD_FALLTHROUGH macro for annotating switch case with the 'fallthrough' attribute.
 * - C23: https://en.cppreference.com/w/c/language/attributes/fallthrough
 * - CPP17: https://en.cppreference.com/w/cpp/language/attributes/fallthrough
 * - Else: __attribute__((__fallthrough__))
 */

/*-**************************************************************
*  Alignment
*****************************************************************/

// C documentation
//
//	/* @return 1 if @u is a 2^n value, 0 otherwise
//	 * useful to check a value is valid for alignment restrictions */
func ZSTD_isPower2(tls *libc.TLS, u size_t) (r int32) {
	return libc.BoolInt32(u&(u-uint64(1)) == uint64(0))
}

/* this test was initially positioned in mem.h,
 * but this file is removed (or replaced) for linux kernel
 * so it's now hosted in compiler.h,
 * which remains valid for both user & kernel spaces.
 */

/* covers gcc, clang & MSVC */
/* note : this section must come first, before C11,
 * due to a limitation in the kernel source generator */

/* C90-compatible alignment macro (GCC/Clang). Adjust for other compilers if needed. */

/*-**************************************************************
*  Sanitizer
*****************************************************************/

/**
 * Zstd relies on pointer overflow in its decompressor.
 * We add this attribute to functions that rely on pointer overflow.
 */

// C documentation
//
//	/**
//	 * Helper function to perform a wrapped pointer difference without triggering
//	 * UBSAN.
//	 *
//	 * @returns lhs - rhs with wrapping
//	 */
func ZSTD_wrappedPtrDiff(tls *libc.TLS, lhs uintptr, rhs uintptr) (r ptrdiff_t) {
	return int64(lhs) - int64(rhs)
}

// C documentation
//
//	/**
//	 * Helper function to perform a wrapped pointer add without triggering UBSAN.
//	 *
//	 * @return ptr + add with wrapping
//	 */
func ZSTD_wrappedPtrAdd(tls *libc.TLS, ptr uintptr, add ptrdiff_t) (r uintptr) {
	return ptr + uintptr(add)
}

// C documentation
//
//	/**
//	 * Helper function to perform a wrapped pointer subtraction without triggering
//	 * UBSAN.
//	 *
//	 * @return ptr - sub with wrapping
//	 */
func ZSTD_wrappedPtrSub(tls *libc.TLS, ptr uintptr, sub ptrdiff_t) (r uintptr) {
	return ptr - uintptr(sub)
}

// C documentation
//
//	/**
//	 * Helper function to add to a pointer that works around C's undefined behavior
//	 * of adding 0 to NULL.
//	 *
//	 * @returns `ptr + add` except it defines `NULL + 0 == NULL`.
//	 */
func ZSTD_maybeNullPtrAdd(tls *libc.TLS, ptr uintptr, add ptrdiff_t) (r uintptr) {
	var v1 uintptr
	_ = v1
	if add > 0 {
		v1 = ptr + uintptr(add)
	} else {
		v1 = ptr
	}
	return v1
}

type int8_t = int8

type uint8_t = uint8

type int16_t = int16

type uint16_t = uint16

type int32_t = int32

type uint32_t = uint32

type int64_t = int64

type uint64_t = uint64

type int_least8_t = int8

type uint_least8_t = uint8

type int_least16_t = int16

type uint_least16_t = uint16

type int_least32_t = int32

type uint_least32_t = uint32

type int_least64_t = int64

type uint_least64_t = uint64

type int_fast8_t = int8

type uint_fast8_t = uint8

type int_fast16_t = int16

type uint_fast16_t = uint16

type int_fast32_t = int32

type uint_fast32_t = uint32

type int_fast64_t = int64

type uint_fast64_t = uint64

type intmax_t = int64

type uintmax_t = uint64

/* 7.18.2  Limits of specified-width integer types */

/* 7.18.2.1  Limits of exact-width integer types */

/* 7.18.2.2  Limits of minimum-width integer types */

/* 7.18.2.3  Limits of fastest minimum-width integer types */

/* 7.18.2.4  Limits of integer types capable of holding
   object pointers */

/* 7.18.2.5  Limits of greatest-width integer types */

/* 7.18.3  Limits of other integer types */

/*
 * wint_t is unsigned short for compatibility with MS runtime
 */

/* 7.18.4  Macros for integer constants */

/* 7.18.4.1  Macros for minimum-width integer constants

    Accoding to Douglas Gwyn <gwyn@arl.mil>:
	"This spec was changed in ISO/IEC 9899:1999 TC1; in ISO/IEC
	9899:1999 as initially published, the expansion was required
	to be an integer constant of precisely matching type, which
	is impossible to accomplish for the shorter types on most
	platforms, because C99 provides no standard way to designate
	an integer constant with width less than that of type int.
	TC1 changed this to require just an integer constant
	*expression* with *promoted* type."

	The trick used here is from Clive D W Feather.
*/

/*  The 'trick' doesn't work in C89 for long long because, without
    suffix, (val) will be evaluated as int, not intmax_t */

/* 7.18.4.2  Macros for greatest-width integer constants */
type BYTE = uint8

type U8 = uint8

type S8 = int8

type U16 = uint16

type S16 = int16

type U32 = uint32

type S32 = int32

type U64 = uint64

type S64 = int64

/*-**************************************************************
*  Memory I/O Implementation
*****************************************************************/
/* MEM_FORCE_MEMORY_ACCESS : For accessing unaligned memory:
 * Method 0 : always use `memcpy()`. Safe and portable.
 * Method 1 : Use compiler extension to set unaligned access.
 * Method 2 : direct access. This method is portable but violate C standard.
 *            It can generate buggy code on targets depending on alignment.
 * Default  : method 1 if supported, else method 0
 */
func MEM_32bits(tls *libc.TLS) (r uint32) {
	return libc.BoolUint32(uint64(8) == uint64(4))
}

func MEM_64bits(tls *libc.TLS) (r uint32) {
	return libc.BoolUint32(uint64(8) == uint64(8))
}

func MEM_isLittleEndian(tls *libc.TLS) (r uint32) {
	return uint32(1)
}

type unalign16 = uint16

type unalign32 = uint32

type unalign64 = uint64

type unalignArch = uint64

func MEM_read16(tls *libc.TLS, ptr uintptr) (r U16) {
	return *(*unalign16)(unsafe.Pointer(ptr))
}

func MEM_read32(tls *libc.TLS, ptr uintptr) (r U32) {
	return *(*unalign32)(unsafe.Pointer(ptr))
}

func MEM_read64(tls *libc.TLS, ptr uintptr) (r U64) {
	return *(*unalign64)(unsafe.Pointer(ptr))
}

func MEM_readST(tls *libc.TLS, ptr uintptr) (r size_t) {
	return *(*unalignArch)(unsafe.Pointer(ptr))
}

func MEM_write16(tls *libc.TLS, memPtr uintptr, value U16) {
	*(*unalign16)(unsafe.Pointer(memPtr)) = value
}

func MEM_write32(tls *libc.TLS, memPtr uintptr, value U32) {
	*(*unalign32)(unsafe.Pointer(memPtr)) = value
}

func MEM_write64(tls *libc.TLS, memPtr uintptr, value U64) {
	*(*unalign64)(unsafe.Pointer(memPtr)) = value
}

func MEM_swap32_fallback(tls *libc.TLS, in U32) (r U32) {
	return in<<libc.Int32FromInt32(24)&uint32(0xff000000) | in<<libc.Int32FromInt32(8)&uint32(0x00ff0000) | in>>libc.Int32FromInt32(8)&uint32(0x0000ff00) | in>>libc.Int32FromInt32(24)&uint32(0x000000ff)
}

func MEM_swap32(tls *libc.TLS, in U32) (r U32) {
	return libc.X__builtin_bswap32(tls, in)
}

func MEM_swap64_fallback(tls *libc.TLS, in U64) (r U64) {
	return in<<libc.Int32FromInt32(56)&uint64(0xff00000000000000) | in<<libc.Int32FromInt32(40)&uint64(0x00ff000000000000) | in<<libc.Int32FromInt32(24)&uint64(0x0000ff0000000000) | in<<libc.Int32FromInt32(8)&uint64(0x000000ff00000000) | in>>libc.Int32FromInt32(8)&uint64(0x00000000ff000000) | in>>libc.Int32FromInt32(24)&uint64(0x0000000000ff0000) | in>>libc.Int32FromInt32(40)&uint64(0x000000000000ff00) | in>>libc.Int32FromInt32(56)&uint64(0x00000000000000ff)
}

func MEM_swap64(tls *libc.TLS, in U64) (r U64) {
	return libc.X__builtin_bswap64(tls, in)
}

func MEM_swapST(tls *libc.TLS, in size_t) (r size_t) {
	if MEM_32bits(tls) != 0 {
		return uint64(MEM_swap32(tls, uint32(in)))
	} else {
		return MEM_swap64(tls, in)
	}
	return r
}

/*=== Little endian r/w ===*/
func MEM_readLE16(tls *libc.TLS, memPtr uintptr) (r U16) {
	var p uintptr
	_ = p
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_read16(tls, memPtr)
	} else {
		p = memPtr
		return uint16(int32(*(*BYTE)(unsafe.Pointer(p))) + int32(*(*BYTE)(unsafe.Pointer(p + 1)))<<libc.Int32FromInt32(8))
	}
	return r
}

func MEM_writeLE16(tls *libc.TLS, memPtr uintptr, val U16) {
	var p uintptr
	_ = p
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write16(tls, memPtr, val)
	} else {
		p = memPtr
		*(*BYTE)(unsafe.Pointer(p)) = uint8(val)
		*(*BYTE)(unsafe.Pointer(p + 1)) = uint8(int32(val) >> libc.Int32FromInt32(8))
	}
}

func MEM_readLE24(tls *libc.TLS, memPtr uintptr) (r U32) {
	return uint32(MEM_readLE16(tls, memPtr)) + uint32(*(*BYTE)(unsafe.Pointer(memPtr + 2)))<<libc.Int32FromInt32(16)
}

func MEM_writeLE24(tls *libc.TLS, memPtr uintptr, val U32) {
	MEM_writeLE16(tls, memPtr, uint16(val))
	*(*BYTE)(unsafe.Pointer(memPtr + 2)) = uint8(val >> libc.Int32FromInt32(16))
}

func MEM_readLE32(tls *libc.TLS, memPtr uintptr) (r U32) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_read32(tls, memPtr)
	} else {
		return MEM_swap32(tls, MEM_read32(tls, memPtr))
	}
	return r
}

func MEM_writeLE32(tls *libc.TLS, memPtr uintptr, val32 U32) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write32(tls, memPtr, val32)
	} else {
		MEM_write32(tls, memPtr, MEM_swap32(tls, val32))
	}
}

func MEM_readLE64(tls *libc.TLS, memPtr uintptr) (r U64) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_read64(tls, memPtr)
	} else {
		return MEM_swap64(tls, MEM_read64(tls, memPtr))
	}
	return r
}

func MEM_writeLE64(tls *libc.TLS, memPtr uintptr, val64 U64) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write64(tls, memPtr, val64)
	} else {
		MEM_write64(tls, memPtr, MEM_swap64(tls, val64))
	}
}

func MEM_readLEST(tls *libc.TLS, memPtr uintptr) (r size_t) {
	if MEM_32bits(tls) != 0 {
		return uint64(MEM_readLE32(tls, memPtr))
	} else {
		return MEM_readLE64(tls, memPtr)
	}
	return r
}

func MEM_writeLEST(tls *libc.TLS, memPtr uintptr, val size_t) {
	if MEM_32bits(tls) != 0 {
		MEM_writeLE32(tls, memPtr, uint32(val))
	} else {
		MEM_writeLE64(tls, memPtr, val)
	}
}

/*=== Big endian r/w ===*/
func MEM_readBE32(tls *libc.TLS, memPtr uintptr) (r U32) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_swap32(tls, MEM_read32(tls, memPtr))
	} else {
		return MEM_read32(tls, memPtr)
	}
	return r
}

func MEM_writeBE32(tls *libc.TLS, memPtr uintptr, val32 U32) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write32(tls, memPtr, MEM_swap32(tls, val32))
	} else {
		MEM_write32(tls, memPtr, val32)
	}
}

func MEM_readBE64(tls *libc.TLS, memPtr uintptr) (r U64) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_swap64(tls, MEM_read64(tls, memPtr))
	} else {
		return MEM_read64(tls, memPtr)
	}
	return r
}

func MEM_writeBE64(tls *libc.TLS, memPtr uintptr, val64 U64) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write64(tls, memPtr, MEM_swap64(tls, val64))
	} else {
		MEM_write64(tls, memPtr, val64)
	}
}

func MEM_readBEST(tls *libc.TLS, memPtr uintptr) (r size_t) {
	if MEM_32bits(tls) != 0 {
		return uint64(MEM_readBE32(tls, memPtr))
	} else {
		return MEM_readBE64(tls, memPtr)
	}
	return r
}

func MEM_writeBEST(tls *libc.TLS, memPtr uintptr, val size_t) {
	if MEM_32bits(tls) != 0 {
		MEM_writeBE32(tls, memPtr, uint32(val))
	} else {
		MEM_writeBE64(tls, memPtr, val)
	}
}

// C documentation
//
//	/* code only tested on 32 and 64 bits systems */
func MEM_check(tls *libc.TLS) {
	_ = libc.Uint64FromInt64(1)
}

/**** ended inlining mem.h ****/
/**** start inlining error_private.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* Note : this module is expected to remain private, do not expose it */

/* ****************************************
*  Dependencies
******************************************/
/**** start inlining ../zstd_errors.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* =====   ZSTDERRORLIB_API : control library symbols visibility   ===== */
/* Backwards compatibility with old macro name */

// C documentation
//
//	/*-*********************************************
//	 *  Error codes list
//	 *-*********************************************
//	 *  Error codes _values_ are pinned down since v1.3.1 only.
//	 *  Therefore, don't rely on values if you may link to any version < v1.3.1.
//	 *
//	 *  Only values < 100 are considered stable.
//	 *
//	 *  note 1 : this API shall be used with static linking only.
//	 *           dynamic linking is not yet officially supported.
//	 *  note 2 : Prefer relying on the enum than on its value whenever possible
//	 *           This is the only supported way to use the error list < v1.3.1
//	 *  note 3 : ZSTD_isError() is always correct, whatever the library version.
//	 **********************************************/
type ZSTD_ErrorCode = int32

const ZSTD_error_no_error = 0
const ZSTD_error_GENERIC = 1
const ZSTD_error_prefix_unknown = 10
const ZSTD_error_version_unsupported = 12
const ZSTD_error_frameParameter_unsupported = 14
const ZSTD_error_frameParameter_windowTooLarge = 16
const ZSTD_error_corruption_detected = 20
const ZSTD_error_checksum_wrong = 22
const ZSTD_error_literals_headerWrong = 24
const ZSTD_error_dictionary_corrupted = 30
const ZSTD_error_dictionary_wrong = 32
const ZSTD_error_dictionaryCreation_failed = 34
const ZSTD_error_parameter_unsupported = 40
const ZSTD_error_parameter_combination_unsupported = 41
const ZSTD_error_parameter_outOfBound = 42
const ZSTD_error_tableLog_tooLarge = 44
const ZSTD_error_maxSymbolValue_tooLarge = 46
const ZSTD_error_maxSymbolValue_tooSmall = 48
const ZSTD_error_cannotProduce_uncompressedBlock = 49
const ZSTD_error_stabilityCondition_notRespected = 50
const ZSTD_error_stage_wrong = 60
const ZSTD_error_init_missing = 62
const ZSTD_error_memory_allocation = 64
const ZSTD_error_workSpace_tooSmall = 66
const ZSTD_error_dstSize_tooSmall = 70
const ZSTD_error_srcSize_wrong = 72
const ZSTD_error_dstBuffer_null = 74
const ZSTD_error_noForwardProgress_destFull = 80
const ZSTD_error_noForwardProgress_inputEmpty = 82
const
/* following error codes are __NOT STABLE__, they can be removed or changed in future versions */
ZSTD_error_frameIndex_tooLarge = 100
const ZSTD_error_seekableIO = 102
const ZSTD_error_dstBuffer_wrong = 104
const ZSTD_error_srcBuffer_wrong = 105
const ZSTD_error_sequenceProducer_failed = 106
const ZSTD_error_externalSequences_invalid = 107
const ZSTD_error_maxCode = 120

/**< Same as ZSTD_getErrorName, but using a `ZSTD_ErrorCode` enum argument */

/**** ended inlining ../zstd_errors.h ****/
/**** skipping file: compiler.h ****/
/**** skipping file: debug.h ****/
/**** skipping file: zstd_deps.h ****/

/* ****************************************
*  Compiler-specific
******************************************/

// C documentation
//
//	/*-****************************************
//	*  Customization (error_public.h)
//	******************************************/
type ERR_enum = int32

/*-****************************************
*  Error codes handling
******************************************/
func ERR_isError(tls *libc.TLS, code size_t) (r uint32) {
	return libc.BoolUint32(code > uint64(-int32(ZSTD_error_maxCode)))
}

func ERR_getErrorCode(tls *libc.TLS, code size_t) (r ERR_enum) {
	if !(ERR_isError(tls, code) != 0) {
		return libc.Int32FromInt32(0)
	}
	return int32(libc.Uint64FromInt32(0) - code)
}

/* error_private.c */
func ERR_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorString(tls, ERR_getErrorCode(tls, code))
}

// C documentation
//
//	/**
//	 * Ignore: this is an internal helper.
//	 *
//	 * This is a helper function to help force C99-correctness during compilation.
//	 * Under strict compilation modes, variadic macro arguments can't be empty.
//	 * However, variadic function arguments can be. Using a function therefore lets
//	 * us statically check that at least one (string) argument was passed,
//	 * independent of the compilation flags.
//	 */
func _force_has_format_string(tls *libc.TLS, format uintptr, va uintptr) {
	_ = format
}

// C documentation
//
//	/*! Constructor and Destructor of FSE_CTable.
//	    Note that FSE_CTable size depends on 'tableLog' and 'maxSymbolValue' */
type FSE_CTable = uint32

type FSE_DTable = uint32 /* don't allocate that. It's just a way to be more restrictive than void* */

/*!
Tutorial :
----------
(Note : these functions only decompress FSE-compressed blocks.
 If block is uncompressed, use memcpy() instead
 If block is a single repeated byte, use memset() instead )

The first step is to obtain the normalized frequencies of symbols.
This can be performed by FSE_readNCount() if it was saved using FSE_writeNCount().
'normalizedCounter' must be already allocated, and have at least 'maxSymbolValuePtr[0]+1' cells of signed short.
In practice, that means it's necessary to know 'maxSymbolValue' beforehand,
or size the table to handle worst case situations (typically 256).
FSE_readNCount() will provide 'tableLog' and 'maxSymbolValue'.
The result of FSE_readNCount() is the number of bytes read from 'rBuffer'.
Note that 'rBufferSize' must be at least 4 bytes, even if useful information is less than that.
If there is an error, the function will return an error code, which can be tested using FSE_isError().

The next step is to build the decompression tables 'FSE_DTable' from 'normalizedCounter'.
This is performed by the function FSE_buildDTable().
The space required by 'FSE_DTable' must be already allocated using FSE_createDTable().
If there is an error, the function will return an error code, which can be tested using FSE_isError().

`FSE_DTable` can then be used to decompress `cSrc`, with FSE_decompress_usingDTable().
`cSrcSize` must be strictly correct, otherwise decompression will fail.
FSE_decompress_usingDTable() result will tell how many bytes were regenerated (<=`dstCapacity`).
If there is an error, the function will return an error code, which can be tested using FSE_isError(). (ex: dst buffer too small)
*/

/**** start inlining bitstream.h ****/
/* ******************************************************************
 * bitstream
 * Part of FSE library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * You can contact the author at :
 * - Source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/*
*  This API consists of small unitary functions, which must be inlined for best performance.
*  Since link-time-optimization is not available for all compilers,
*  these functions are defined into a .h to be included.
 */

/*-****************************************
*  Dependencies
******************************************/
/**** skipping file: mem.h ****/
/**** skipping file: compiler.h ****/
/**** skipping file: debug.h ****/
/**** skipping file: error_private.h ****/
/**** start inlining bits.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: mem.h ****/
func ZSTD_countTrailingZeros32_fallback(tls *libc.TLS, val U32) (r uint32) {
	return DeBruijnBytePos[val&uint32(-int32(val))*libc.Uint32FromUint32(0x077CB531)>>int32(27)]
	return r
}

var DeBruijnBytePos = [32]U32{
	1:  uint32(1),
	2:  uint32(28),
	3:  uint32(2),
	4:  uint32(29),
	5:  uint32(14),
	6:  uint32(24),
	7:  uint32(3),
	8:  uint32(30),
	9:  uint32(22),
	10: uint32(20),
	11: uint32(15),
	12: uint32(25),
	13: uint32(17),
	14: uint32(4),
	15: uint32(8),
	16: uint32(31),
	17: uint32(27),
	18: uint32(13),
	19: uint32(23),
	20: uint32(21),
	21: uint32(19),
	22: uint32(16),
	23: uint32(7),
	24: uint32(26),
	25: uint32(12),
	26: uint32(18),
	27: uint32(6),
	28: uint32(11),
	29: uint32(5),
	30: uint32(10),
	31: uint32(9),
}

func ZSTD_countTrailingZeros32(tls *libc.TLS, val U32) (r uint32) {
	return uint32(libc.X__builtin_ctz(tls, val))
}

func ZSTD_countLeadingZeros32_fallback(tls *libc.TLS, val U32) (r uint32) {
	val = val | val>>int32(1)
	val = val | val>>int32(2)
	val = val | val>>int32(4)
	val = val | val>>int32(8)
	val = val | val>>int32(16)
	return uint32(31) - DeBruijnClz[val*uint32(0x07C4ACDD)>>int32(27)]
	return r
}

var DeBruijnClz = [32]U32{
	1:  uint32(9),
	2:  uint32(1),
	3:  uint32(10),
	4:  uint32(13),
	5:  uint32(21),
	6:  uint32(2),
	7:  uint32(29),
	8:  uint32(11),
	9:  uint32(14),
	10: uint32(16),
	11: uint32(18),
	12: uint32(22),
	13: uint32(25),
	14: uint32(3),
	15: uint32(30),
	16: uint32(8),
	17: uint32(12),
	18: uint32(20),
	19: uint32(28),
	20: uint32(15),
	21: uint32(17),
	22: uint32(24),
	23: uint32(7),
	24: uint32(19),
	25: uint32(27),
	26: uint32(23),
	27: uint32(6),
	28: uint32(26),
	29: uint32(5),
	30: uint32(4),
	31: uint32(31),
}

func ZSTD_countLeadingZeros32(tls *libc.TLS, val U32) (r uint32) {
	return uint32(libc.X__builtin_clz(tls, val))
}

func ZSTD_countTrailingZeros64(tls *libc.TLS, val U64) (r uint32) {
	var leastSignificantWord, mostSignificantWord U32
	_, _ = leastSignificantWord, mostSignificantWord
	mostSignificantWord = uint32(val >> libc.Int32FromInt32(32))
	leastSignificantWord = uint32(val)
	if leastSignificantWord == uint32(0) {
		return uint32(32) + ZSTD_countTrailingZeros32(tls, mostSignificantWord)
	} else {
		return ZSTD_countTrailingZeros32(tls, leastSignificantWord)
	}
	return r
}

func ZSTD_countLeadingZeros64(tls *libc.TLS, val U64) (r uint32) {
	return uint32(libc.X__builtin_clzll(tls, val))
}

func ZSTD_NbCommonBytes(tls *libc.TLS, val size_t) (r uint32) {
	if MEM_isLittleEndian(tls) != 0 {
		if MEM_64bits(tls) != 0 {
			return ZSTD_countTrailingZeros64(tls, val) >> int32(3)
		} else {
			return ZSTD_countTrailingZeros32(tls, uint32(val)) >> int32(3)
		}
	} else { /* Big Endian CPU */
		if MEM_64bits(tls) != 0 {
			return ZSTD_countLeadingZeros64(tls, val) >> int32(3)
		} else {
			return ZSTD_countLeadingZeros32(tls, uint32(val)) >> int32(3)
		}
	}
	return r
}

func ZSTD_highbit32(tls *libc.TLS, val U32) (r uint32) {
	/* compress, dictBuilder, decodeCorpus */
	return uint32(31) - ZSTD_countLeadingZeros32(tls, val)
}

// C documentation
//
//	/* ZSTD_rotateRight_*():
//	 * Rotates a bitfield to the right by "count" bits.
//	 * https://en.wikipedia.org/w/index.php?title=Circular_shift&oldid=991635599#Implementing_circular_shifts
//	 */
func ZSTD_rotateRight_U64(tls *libc.TLS, value U64, count U32) (r U64) {
	count = count & uint32(0x3F) /* for fickle pattern recognition */
	return value>>count | value<<((libc.Uint32FromUint32(0)-count)&libc.Uint32FromInt32(0x3F))
}

func ZSTD_rotateRight_U32(tls *libc.TLS, value U32, count U32) (r U32) {
	count = count & uint32(0x1F) /* for fickle pattern recognition */
	return value>>count | value<<((libc.Uint32FromUint32(0)-count)&libc.Uint32FromInt32(0x1F))
}

func ZSTD_rotateRight_U16(tls *libc.TLS, value U16, count U32) (r U16) {
	count = count & uint32(0x0F) /* for fickle pattern recognition */
	return uint16(int32(value)>>count | int32(uint16(int32(value)<<((libc.Uint32FromUint32(0)-count)&libc.Uint32FromInt32(0x0F)))))
}

/**** ended inlining bits.h ****/

/*=========================================
*  Target specific
=========================================*/

// C documentation
//
//	/*-******************************************
//	*  bitStream encoding API (write forward)
//	********************************************/
type BitContainerType = uint64

// C documentation
//
//	/* bitStream can mix input from multiple sources.
//	 * A critical property of these streams is that they encode and decode in **reverse** direction.
//	 * So the first bit sequence you add will be the last to be read, like a LIFO stack.
//	 */
type BIT_CStream_t = struct {
	FbitContainer BitContainerType
	FbitPos       uint32
	FstartPtr     uintptr
	Fptr          uintptr
	FendPtr       uintptr
}

/* Start with initCStream, providing the size of buffer to write into.
*  bitStream will never write outside of this buffer.
*  `dstCapacity` must be >= sizeof(bitD->bitContainer), otherwise @return will be an error code.
*
*  bits are first added to a local register.
*  Local register is BitContainerType, 64-bits on 64-bits systems, or 32-bits on 32-bits systems.
*  Writing data into memory is an explicit operation, performed by the flushBits function.
*  Hence keep track how many bits are potentially stored into local register to avoid register overflow.
*  After a flushBits, a maximum of 7 bits might still be stored into local register.
*
*  Avoid storing elements of more than 24 bits if you want compatibility with 32-bits bitstream readers.
*
*  Last operation is to close the bitStream.
*  The function returns the final size of CStream in bytes.
*  If data couldn't fit into `dstBuffer`, it will return a 0 ( == not storable)
 */

// C documentation
//
//	/*-********************************************
//	*  bitStream decoding API (read backward)
//	**********************************************/
type BIT_DStream_t = struct {
	FbitContainer BitContainerType
	FbitsConsumed uint32
	Fptr          uintptr
	Fstart        uintptr
	FlimitPtr     uintptr
}

type BIT_DStream_status = int32

const BIT_DStream_unfinished = 0
const /* fully refilled */
BIT_DStream_endOfBuffer = 1
const /* still some bits left in bitstream */
BIT_DStream_completed = 2
const /* bitstream entirely consumed, bit-exact */
BIT_DStream_overflow = 3

/* faster, but works only if nbBits >= 1 */

// C documentation
//
//	/*=====    Local Constants   =====*/
var BIT_mask = [32]uint32{
	1:  uint32(1),
	2:  uint32(3),
	3:  uint32(7),
	4:  uint32(0xF),
	5:  uint32(0x1F),
	6:  uint32(0x3F),
	7:  uint32(0x7F),
	8:  uint32(0xFF),
	9:  uint32(0x1FF),
	10: uint32(0x3FF),
	11: uint32(0x7FF),
	12: uint32(0xFFF),
	13: uint32(0x1FFF),
	14: uint32(0x3FFF),
	15: uint32(0x7FFF),
	16: uint32(0xFFFF),
	17: uint32(0x1FFFF),
	18: uint32(0x3FFFF),
	19: uint32(0x7FFFF),
	20: uint32(0xFFFFF),
	21: uint32(0x1FFFFF),
	22: uint32(0x3FFFFF),
	23: uint32(0x7FFFFF),
	24: uint32(0xFFFFFF),
	25: uint32(0x1FFFFFF),
	26: uint32(0x3FFFFFF),
	27: uint32(0x7FFFFFF),
	28: uint32(0xFFFFFFF),
	29: uint32(0x1FFFFFFF),
	30: uint32(0x3FFFFFFF),
	31: uint32(0x7FFFFFFF),
} /* up to 31 bits */

// C documentation
//
//	/*-**************************************************************
//	*  bitStream encoding
//	****************************************************************/
//	/*! BIT_initCStream() :
//	 *  `dstCapacity` must be > sizeof(size_t)
//	 *  @return : 0 if success,
//	 *            otherwise an error code (can be tested using ERR_isError()) */
func BIT_initCStream(tls *libc.TLS, bitC uintptr, startPtr uintptr, dstCapacity size_t) (r size_t) {
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitContainer = uint64(0)
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos = uint32(0)
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr = startPtr
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr = (*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr + uintptr(dstCapacity) - uintptr(8)
	if dstCapacity <= uint64(8) {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	return uint64(0)
}

func BIT_getLowerBits(tls *libc.TLS, bitContainer BitContainerType, nbBits U32) (r BitContainerType) {
	return bitContainer & uint64(BIT_mask[nbBits])
}

// C documentation
//
//	/*! BIT_addBits() :
//	 *  can add up to 31 bits into `bitC`.
//	 *  Note : does not check for register overflow ! */
func BIT_addBits(tls *libc.TLS, bitC uintptr, value BitContainerType, nbBits uint32) {
	_ = libc.Uint64FromInt64(1)
	*(*BitContainerType)(unsafe.Pointer(bitC)) |= BIT_getLowerBits(tls, value, nbBits) << (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos
	*(*uint32)(unsafe.Pointer(bitC + 8)) += nbBits
}

// C documentation
//
//	/*! BIT_addBitsFast() :
//	 *  works only if `value` is _clean_,
//	 *  meaning all high bits above nbBits are 0 */
func BIT_addBitsFast(tls *libc.TLS, bitC uintptr, value BitContainerType, nbBits uint32) {
	*(*BitContainerType)(unsafe.Pointer(bitC)) |= value << (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos
	*(*uint32)(unsafe.Pointer(bitC + 8)) += nbBits
}

// C documentation
//
//	/*! BIT_flushBitsFast() :
//	 *  assumption : bitContainer has not overflowed
//	 *  unsafe version; does not check buffer overflow */
func BIT_flushBitsFast(tls *libc.TLS, bitC uintptr) {
	var nbBytes size_t
	_ = nbBytes
	nbBytes = uint64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos >> int32(3))
	MEM_writeLEST(tls, (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr, (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitContainer)
	*(*uintptr)(unsafe.Pointer(bitC + 24)) += uintptr(nbBytes)
	*(*uint32)(unsafe.Pointer(bitC + 8)) &= uint32(7)
	*(*BitContainerType)(unsafe.Pointer(bitC)) >>= nbBytes * uint64(8)
}

// C documentation
//
//	/*! BIT_flushBits() :
//	 *  assumption : bitContainer has not overflowed
//	 *  safe version; check for buffer overflow, and prevents it.
//	 *  note : does not signal buffer overflow.
//	 *  overflow will be revealed later on using BIT_closeCStream() */
func BIT_flushBits(tls *libc.TLS, bitC uintptr) {
	var nbBytes size_t
	_ = nbBytes
	nbBytes = uint64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos >> int32(3))
	MEM_writeLEST(tls, (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr, (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitContainer)
	*(*uintptr)(unsafe.Pointer(bitC + 24)) += uintptr(nbBytes)
	if (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr > (*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		(*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr
	}
	*(*uint32)(unsafe.Pointer(bitC + 8)) &= uint32(7)
	*(*BitContainerType)(unsafe.Pointer(bitC)) >>= nbBytes * uint64(8)
}

// C documentation
//
//	/*! BIT_closeCStream() :
//	 *  @return : size of CStream, in bytes,
//	 *            or 0 if it could not fit into dstBuffer */
func BIT_closeCStream(tls *libc.TLS, bitC uintptr) (r size_t) {
	BIT_addBitsFast(tls, bitC, uint64(1), uint32(1)) /* endMark */
	BIT_flushBits(tls, bitC)
	if (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr >= (*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		return uint64(0)
	} /* overflow detected */
	return uint64(int64((*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr)-int64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr)) + libc.BoolUint64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos > libc.Uint32FromInt32(0))
}

// C documentation
//
//	/*-********************************************************
//	*  bitStream decoding
//	**********************************************************/
//	/*! BIT_initDStream() :
//	 *  Initialize a BIT_DStream_t.
//	 * `bitD` : a pointer to an already allocated BIT_DStream_t structure.
//	 * `srcSize` must be the *exact* size of the bitStream, in bytes.
//	 * @return : size of stream (== srcSize), or an errorCode if a problem is detected
//	 */
func BIT_initDStream(tls *libc.TLS, bitD uintptr, srcBuffer uintptr, srcSize size_t) (r size_t) {
	var lastByte, lastByte1 BYTE
	var v1 uint32
	_, _, _ = lastByte, lastByte1, v1
	if srcSize < uint64(1) {
		libc.Xmemset(tls, bitD, 0, libc.Uint64FromInt64(40))
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart = srcBuffer
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).FlimitPtr = (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart + uintptr(8)
	if srcSize >= uint64(8) { /* normal case */
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr = srcBuffer + uintptr(srcSize) - uintptr(8)
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = MEM_readLEST(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr)
		lastByte = *(*BYTE)(unsafe.Pointer(srcBuffer + uintptr(srcSize-uint64(1))))
		if lastByte != 0 {
			v1 = uint32(8) - ZSTD_highbit32(tls, uint32(lastByte))
		} else {
			v1 = uint32(0)
		}
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed = v1 /* ensures bitsConsumed is always set */
		if int32(lastByte) == 0 {
			return uint64(-int32(ZSTD_error_GENERIC))
		} /* endMark not present */
	} else {
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr = (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = uint64(*(*BYTE)(unsafe.Pointer((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart)))
		switch srcSize {
		case uint64(7):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 6))) << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(16))
			fallthrough
		case uint64(6):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 5))) << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(24))
			fallthrough
		case uint64(5):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 4))) << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(32))
			fallthrough
		case uint64(4):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 3))) << int32(24)
			fallthrough
		case uint64(3):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 2))) << int32(16)
			fallthrough
		case uint64(2):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 1))) << int32(8)
			fallthrough
		default:
			break
		}
		lastByte1 = *(*BYTE)(unsafe.Pointer(srcBuffer + uintptr(srcSize-uint64(1))))
		if lastByte1 != 0 {
			v1 = uint32(8) - ZSTD_highbit32(tls, uint32(lastByte1))
		} else {
			v1 = uint32(0)
		}
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed = v1
		if int32(lastByte1) == 0 {
			return uint64(-int32(ZSTD_error_corruption_detected))
		} /* endMark not present */
		*(*uint32)(unsafe.Pointer(bitD + 8)) += uint32(libc.Uint64FromInt64(8)-srcSize) * uint32(8)
	}
	return srcSize
}

func BIT_getUpperBits(tls *libc.TLS, bitContainer BitContainerType, start U32) (r BitContainerType) {
	return bitContainer >> start
}

func BIT_getMiddleBits(tls *libc.TLS, bitContainer BitContainerType, start U32, nbBits U32) (r BitContainerType) {
	var regMask U32
	_ = regMask
	regMask = uint32(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(1))
	/* if start > regMask, bitstream is corrupted, and result is undefined */
	/* x86 transform & ((1 << nbBits) - 1) to bzhi instruction, it is better
	 * than accessing memory. When bmi2 instruction is not present, we consider
	 * such cpus old (pre-Haswell, 2013) and their performance is not of that
	 * importance.
	 */
	return bitContainer >> (start & regMask) & (libc.Uint64FromInt32(1)<<nbBits - uint64(1))
}

// C documentation
//
//	/*! BIT_lookBits() :
//	 *  Provides next n bits from local register.
//	 *  local register is not modified.
//	 *  On 32-bits, maxNbBits==24.
//	 *  On 64-bits, maxNbBits==56.
//	 * @return : value extracted */
func BIT_lookBits(tls *libc.TLS, bitD uintptr, nbBits U32) (r BitContainerType) {
	/* arbitrate between double-shift and shift+mask */
	/* if bitD->bitsConsumed + nbBits > sizeof(bitD->bitContainer)*8,
	 * bitstream is likely corrupted, and result is undefined */
	return BIT_getMiddleBits(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer, uint32(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8)-uint64((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed)-uint64(nbBits)), nbBits)
}

// C documentation
//
//	/*! BIT_lookBitsFast() :
//	 *  unsafe version; only works if nbBits >= 1 */
func BIT_lookBitsFast(tls *libc.TLS, bitD uintptr, nbBits U32) (r BitContainerType) {
	var regMask U32
	_ = regMask
	regMask = uint32(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(1))
	return (*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer << ((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed & regMask) >> ((regMask + uint32(1) - nbBits) & regMask)
}

func BIT_skipBits(tls *libc.TLS, bitD uintptr, nbBits U32) {
	*(*uint32)(unsafe.Pointer(bitD + 8)) += nbBits
}

// C documentation
//
//	/*! BIT_readBits() :
//	 *  Read (consume) next n bits from local register and update.
//	 *  Pay attention to not read more than nbBits contained into local register.
//	 * @return : extracted value. */
func BIT_readBits(tls *libc.TLS, bitD uintptr, nbBits uint32) (r BitContainerType) {
	var value BitContainerType
	_ = value
	value = BIT_lookBits(tls, bitD, nbBits)
	BIT_skipBits(tls, bitD, nbBits)
	return value
}

// C documentation
//
//	/*! BIT_readBitsFast() :
//	 *  unsafe version; only works if nbBits >= 1 */
func BIT_readBitsFast(tls *libc.TLS, bitD uintptr, nbBits uint32) (r BitContainerType) {
	var value BitContainerType
	_ = value
	value = BIT_lookBitsFast(tls, bitD, nbBits)
	BIT_skipBits(tls, bitD, nbBits)
	return value
}

// C documentation
//
//	/*! BIT_reloadDStream_internal() :
//	 *  Simple variant of BIT_reloadDStream(), with two conditions:
//	 *  1. bitstream is valid : bitsConsumed <= sizeof(bitD->bitContainer)*8
//	 *  2. look window is valid after shifted down : bitD->ptr >= bitD->start
//	 */
func BIT_reloadDStream_internal(tls *libc.TLS, bitD uintptr) (r BIT_DStream_status) {
	*(*uintptr)(unsafe.Pointer(bitD + 16)) -= uintptr((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed >> int32(3))
	*(*uint32)(unsafe.Pointer(bitD + 8)) &= uint32(7)
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = MEM_readLEST(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr)
	return int32(BIT_DStream_unfinished)
}

// C documentation
//
//	/*! BIT_reloadDStreamFast() :
//	 *  Similar to BIT_reloadDStream(), but with two differences:
//	 *  1. bitsConsumed <= sizeof(bitD->bitContainer)*8 must hold!
//	 *  2. Returns BIT_DStream_overflow when bitD->ptr < bitD->limitPtr, at this
//	 *     point you must use BIT_reloadDStream() to reload.
//	 */
func BIT_reloadDStreamFast(tls *libc.TLS, bitD uintptr) (r BIT_DStream_status) {
	if libc.BoolInt32((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr < (*BIT_DStream_t)(unsafe.Pointer(bitD)).FlimitPtr) != 0 {
		return int32(BIT_DStream_overflow)
	}
	return BIT_reloadDStream_internal(tls, bitD)
}

// C documentation
//
//	/*! BIT_reloadDStream() :
//	 *  Refill `bitD` from buffer previously set in BIT_initDStream() .
//	 *  This function is safe, it guarantees it will not never beyond src buffer.
//	 * @return : status of `BIT_DStream_t` internal register.
//	 *           when status == BIT_DStream_unfinished, internal register is filled with at least 25 or 57 bits */
func BIT_reloadDStream(tls *libc.TLS, bitD uintptr) (r BIT_DStream_status) {
	var nbBytes U32
	var result BIT_DStream_status
	_, _ = nbBytes, result
	/* note : once in overflow mode, a bitstream remains in this mode until it's reset */
	if libc.BoolInt32(uint64((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8)) != 0 {
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr = uintptr(unsafe.Pointer(&zeroFilled)) /* aliasing is allowed for char */
		/* overflow detected, erroneous scenario or end of stream: no update */
		return int32(BIT_DStream_overflow)
	}
	if (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr >= (*BIT_DStream_t)(unsafe.Pointer(bitD)).FlimitPtr {
		return BIT_reloadDStream_internal(tls, bitD)
	}
	if (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr == (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart {
		/* reached end of bitStream => no update */
		if uint64((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed) < libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) {
			return int32(BIT_DStream_endOfBuffer)
		}
		return int32(BIT_DStream_completed)
	}
	/* start < ptr < limitPtr => cautious update */
	nbBytes = (*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed >> int32(3)
	result = int32(BIT_DStream_unfinished)
	if (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr-uintptr(nbBytes) < (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart {
		nbBytes = uint32(int64((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr) - int64((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart)) /* ptr > start */
		result = int32(BIT_DStream_endOfBuffer)
	}
	*(*uintptr)(unsafe.Pointer(bitD + 16)) -= uintptr(nbBytes)
	*(*uint32)(unsafe.Pointer(bitD + 8)) -= nbBytes * uint32(8)
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = MEM_readLEST(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr) /* reminder : srcSize > sizeof(bitD->bitContainer), otherwise bitD->ptr == bitD->start */
	return result
	return r
}

var zeroFilled BitContainerType

// C documentation
//
//	/*! BIT_endOfDStream() :
//	 * @return : 1 if DStream has _exactly_ reached its end (all bits consumed).
//	 */
func BIT_endOfDStream(tls *libc.TLS, DStream uintptr) (r uint32) {
	return libc.BoolUint32((*BIT_DStream_t)(unsafe.Pointer(DStream)).Fptr == (*BIT_DStream_t)(unsafe.Pointer(DStream)).Fstart && uint64((*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed) == libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8))
}

/**< same as FSE_decompress(), using an externally allocated `workSpace` produced with `FSE_DECOMPRESS_WKSP_SIZE_U32(maxLog, maxSymbolValue)`.
 * Set bmi2 to 1 if your CPU supports BMI2 or 0 if it doesn't */

type FSE_repeat = int32

const FSE_repeat_none = 0
const /**< Cannot use the previous table */
FSE_repeat_check = 1
const /**< Can use the previous table but it must be checked */
FSE_repeat_valid = 2

// C documentation
//
//	/* *****************************************
//	*  FSE symbol compression API
//	*******************************************/
//	/*!
//	   This API consists of small unitary functions, which highly benefit from being inlined.
//	   Hence their body are included in next section.
//	*/
type FSE_CState_t = struct {
	Fvalue      ptrdiff_t
	FstateTable uintptr
	FsymbolTT   uintptr
	FstateLog   uint32
}

/**<
These functions are inner components of FSE_compress_usingCTable().
They allow the creation of custom streams, mixing multiple tables and bit sources.

A key property to keep in mind is that encoding and decoding are done **in reverse direction**.
So the first symbol you will encode is the last you will decode, like a LIFO stack.

You will need a few variables to track your CStream. They are :

FSE_CTable    ct;         // Provided by FSE_buildCTable()
BIT_CStream_t bitStream;  // bitStream tracking structure
FSE_CState_t  state;      // State tracking structure (can have several)


The first thing to do is to init bitStream and state.
    size_t errorCode = BIT_initCStream(&bitStream, dstBuffer, maxDstSize);
    FSE_initCState(&state, ct);

Note that BIT_initCStream() can produce an error code, so its result should be tested, using FSE_isError();
You can then encode your input data, byte after byte.
FSE_encodeSymbol() outputs a maximum of 'tableLog' bits at a time.
Remember decoding will be done in reverse direction.
    FSE_encodeByte(&bitStream, &state, symbol);

At any time, you can also add any bit sequence.
Note : maximum allowed nbBits is 25, for compatibility with 32-bits decoders
    BIT_addBits(&bitStream, bitField, nbBits);

The above methods don't commit data to memory, they just store it into local register, for speed.
Local register size is 64-bits on 64-bits systems, 32-bits on 32-bits systems (size_t).
Writing data to memory is a manual operation, performed by the flushBits function.
    BIT_flushBits(&bitStream);

Your last FSE encoding operation shall be to flush your last state value(s).
    FSE_flushState(&bitStream, &state);

Finally, you must close the bitStream.
The function returns the size of CStream in bytes.
If data couldn't fit into dstBuffer, it will return a 0 ( == not compressible)
If there is an error, it returns an errorCode (which can be tested using FSE_isError()).
    size_t size = BIT_closeCStream(&bitStream);
*/

// C documentation
//
//	/* *****************************************
//	*  FSE symbol decompression API
//	*******************************************/
type FSE_DState_t = struct {
	Fstate size_t
	Ftable uintptr
}

/* faster, but works only if nbBits is always >= 1 (otherwise, result will be corrupted) */

// C documentation
//
//	/* *****************************************
//	*  Implementation of inlined functions
//	*******************************************/
type FSE_symbolCompressionTransform = struct {
	FdeltaFindState int32
	FdeltaNbBits    U32
} /* total 8 bytes */
func FSE_initCState(tls *libc.TLS, statePtr uintptr, ct uintptr) {
	var ptr, u16ptr uintptr
	var tableLog U32
	var v1 int32
	_, _, _, _ = ptr, tableLog, u16ptr, v1
	ptr = ct
	u16ptr = ptr
	tableLog = uint32(MEM_read16(tls, ptr))
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = libc.Int64FromInt32(1) << tableLog
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateTable = u16ptr + uintptr(2)*2
	if tableLog != 0 {
		v1 = int32(1) << (tableLog - uint32(1))
	} else {
		v1 = int32(1)
	}
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).FsymbolTT = ct + uintptr(1)*4 + uintptr(v1)*4
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateLog = tableLog
}

// C documentation
//
//	/*! FSE_initCState2() :
//	*   Same as FSE_initCState(), but the first symbol to include (which will be the last to be read)
//	*   uses the smallest state value possible, saving the cost of this symbol */
func FSE_initCState2(tls *libc.TLS, statePtr uintptr, ct uintptr, symbol U32) {
	var nbBitsOut U32
	var stateTable uintptr
	var symbolTT FSE_symbolCompressionTransform
	_, _, _ = nbBitsOut, stateTable, symbolTT
	FSE_initCState(tls, statePtr, ct)
	symbolTT = *(*FSE_symbolCompressionTransform)(unsafe.Pointer((*FSE_CState_t)(unsafe.Pointer(statePtr)).FsymbolTT + uintptr(symbol)*8))
	stateTable = (*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateTable
	nbBitsOut = (symbolTT.FdeltaNbBits + uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(15))) >> libc.Int32FromInt32(16)
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = int64(nbBitsOut<<libc.Int32FromInt32(16) - symbolTT.FdeltaNbBits)
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = int64(*(*U16)(unsafe.Pointer(stateTable + uintptr((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue>>nbBitsOut+int64(symbolTT.FdeltaFindState))*2)))
}

func FSE_encodeSymbol(tls *libc.TLS, bitC uintptr, statePtr uintptr, symbol uint32) {
	var nbBitsOut U32
	var stateTable uintptr
	var symbolTT FSE_symbolCompressionTransform
	_, _, _ = nbBitsOut, stateTable, symbolTT
	symbolTT = *(*FSE_symbolCompressionTransform)(unsafe.Pointer((*FSE_CState_t)(unsafe.Pointer(statePtr)).FsymbolTT + uintptr(symbol)*8))
	stateTable = (*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateTable
	nbBitsOut = uint32(((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue + int64(symbolTT.FdeltaNbBits)) >> libc.Int32FromInt32(16))
	BIT_addBits(tls, bitC, uint64((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue), nbBitsOut)
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = int64(*(*U16)(unsafe.Pointer(stateTable + uintptr((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue>>nbBitsOut+int64(symbolTT.FdeltaFindState))*2)))
}

func FSE_flushCState(tls *libc.TLS, bitC uintptr, statePtr uintptr) {
	BIT_addBits(tls, bitC, uint64((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue), (*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateLog)
	BIT_flushBits(tls, bitC)
}

// C documentation
//
//	/* FSE_getMaxNbBits() :
//	 * Approximate maximum cost of a symbol, in bits.
//	 * Fractional get rounded up (i.e. a symbol with a normalized frequency of 3 gives the same result as a frequency of 2)
//	 * note 1 : assume symbolValue is valid (<= maxSymbolValue)
//	 * note 2 : if freq[symbolValue]==0, @return a fake cost of tableLog+1 bits */
func FSE_getMaxNbBits(tls *libc.TLS, symbolTTPtr uintptr, symbolValue U32) (r U32) {
	var symbolTT uintptr
	_ = symbolTT
	symbolTT = symbolTTPtr
	return ((*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits + uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)-libc.Int32FromInt32(1))) >> int32(16)
}

// C documentation
//
//	/* FSE_bitCost() :
//	 * Approximate symbol cost, as fractional value, using fixed-point format (accuracyLog fractional bits)
//	 * note 1 : assume symbolValue is valid (<= maxSymbolValue)
//	 * note 2 : if freq[symbolValue]==0, @return a fake cost of tableLog+1 bits */
func FSE_bitCost(tls *libc.TLS, symbolTTPtr uintptr, tableLog U32, symbolValue U32, accuracyLog U32) (r U32) {
	var bitMultiplier, deltaFromThreshold, minNbBits, normalizedDeltaFromThreshold, tableSize, threshold U32
	var symbolTT uintptr
	_, _, _, _, _, _, _ = bitMultiplier, deltaFromThreshold, minNbBits, normalizedDeltaFromThreshold, symbolTT, tableSize, threshold
	symbolTT = symbolTTPtr
	minNbBits = (*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits >> int32(16)
	threshold = (minNbBits + uint32(1)) << int32(16)
	/* ensure enough room for renormalization double shift */
	tableSize = uint32(int32(1) << tableLog)
	deltaFromThreshold = threshold - ((*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits + tableSize)
	normalizedDeltaFromThreshold = deltaFromThreshold << accuracyLog >> tableLog /* linear interpolation (very approximate) */
	bitMultiplier = uint32(int32(1) << accuracyLog)
	return (minNbBits+uint32(1))*bitMultiplier - normalizedDeltaFromThreshold
	return r
}

/* ======    Decompression    ====== */

type FSE_DTableHeader = struct {
	FtableLog U16
	FfastMode U16
}

/* sizeof U32 */

type FSE_decode_t = struct {
	FnewState uint16
	Fsymbol   uint8
	FnbBits   uint8
} /* size == U32 */
func FSE_initDState(tls *libc.TLS, DStatePtr uintptr, bitD uintptr, dt uintptr) {
	var DTableH, ptr uintptr
	_, _ = DTableH, ptr
	ptr = dt
	DTableH = ptr
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = BIT_readBits(tls, bitD, uint32((*FSE_DTableHeader)(unsafe.Pointer(DTableH)).FtableLog))
	BIT_reloadDStream(tls, bitD)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable = dt + uintptr(1)*4
}

func FSE_peekSymbol(tls *libc.TLS, DStatePtr uintptr) (r BYTE) {
	var DInfo FSE_decode_t
	_ = DInfo
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	return DInfo.Fsymbol
}

func FSE_updateState(tls *libc.TLS, DStatePtr uintptr, bitD uintptr) {
	var DInfo FSE_decode_t
	var lowBits size_t
	var nbBits U32
	_, _, _ = DInfo, lowBits, nbBits
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	nbBits = uint32(DInfo.FnbBits)
	lowBits = BIT_readBits(tls, bitD, nbBits)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = uint64(DInfo.FnewState) + lowBits
}

func FSE_decodeSymbol(tls *libc.TLS, DStatePtr uintptr, bitD uintptr) (r BYTE) {
	var DInfo FSE_decode_t
	var lowBits size_t
	var nbBits U32
	var symbol BYTE
	_, _, _, _ = DInfo, lowBits, nbBits, symbol
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	nbBits = uint32(DInfo.FnbBits)
	symbol = DInfo.Fsymbol
	lowBits = BIT_readBits(tls, bitD, nbBits)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = uint64(DInfo.FnewState) + lowBits
	return symbol
}

// C documentation
//
//	/*! FSE_decodeSymbolFast() :
//	    unsafe, only works if no symbol has a probability > 50% */
func FSE_decodeSymbolFast(tls *libc.TLS, DStatePtr uintptr, bitD uintptr) (r BYTE) {
	var DInfo FSE_decode_t
	var lowBits size_t
	var nbBits U32
	var symbol BYTE
	_, _, _, _ = DInfo, lowBits, nbBits, symbol
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	nbBits = uint32(DInfo.FnbBits)
	symbol = DInfo.Fsymbol
	lowBits = BIT_readBitsFast(tls, bitD, nbBits)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = uint64(DInfo.FnewState) + lowBits
	return symbol
}

func FSE_endOfDState(tls *libc.TLS, DStatePtr uintptr) (r uint32) {
	return libc.BoolUint32((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate == uint64(0))
}

/**< provides error code string (useful for debugging) */

/* *** Constants *** */

/* ****************************************
*  Static allocation
******************************************/
/* HUF buffer bounds */

// C documentation
//
//	/* static allocation of HUF's Compression Table */
//	/* this is a private definition, just exposed for allocation and strict aliasing purpose. never EVER access its members directly */
type HUF_CElt = uint64

/* consider it an incomplete type */

// C documentation
//
//	/* static allocation of HUF's DTable */
type HUF_DTable = uint32

/* ****************************************
*  Advanced decompression functions
******************************************/

// C documentation
//
//	/**
//	 * Huffman flags bitset.
//	 * For all flags, 0 is the default value.
//	 */
type HUF_flags_e = int32

const
/**
 * If compiled with DYNAMIC_BMI2: Set flag only if the CPU supports BMI2 at runtime.
 * Otherwise: Ignored.
 */
HUF_flags_bmi2 = 1
const
/**
 * If set: Test possible table depths to find the one that produces the smallest header + encoded size.
 * If unset: Use heuristic to find the table depth.
 */
HUF_flags_optimalDepth = 2
const
/**
 * If set: If the previous table can encode the input, always reuse the previous table.
 * If unset: If the previous table can encode the input, reuse the previous table if it results in a smaller output.
 */
HUF_flags_preferRepeat = 4
const
/**
 * If set: Sample the input and check if the sample is uncompressible, if it is then don't attempt to compress.
 * If unset: Always histogram the entire input.
 */
HUF_flags_suspectUncompressible = 8
const
/**
 * If set: Don't use assembly implementations
 * If unset: Allow using assembly implementations
 */
HUF_flags_disableAsm = 16
const
/**
 * If set: Don't use the fast decoding loop, always use the fallback decoding loop.
 * If unset: Use the fast decoding loop when possible.
 */
HUF_flags_disableFast = 32

type HUF_repeat = int32

const HUF_repeat_none = 0
const /**< Cannot use the previous table */
HUF_repeat_check = 1
const /**< Can use the previous table but it must be checked. Note : The previous table must have been constructed by HUF_compress{1, 4}X_repeat */
HUF_repeat_valid = 2

type HUF_CTableHeader = struct {
	FtableLog       BYTE
	FmaxSymbolValue BYTE
	Funused         [6]BYTE
}

/**** ended inlining huf.h ****/
/**** skipping file: bits.h ****/

// C documentation
//
//	/*===   Version   ===*/
func FSE_versionNumber(tls *libc.TLS) (r uint32) {
	return uint32(libc.Int32FromInt32(FSE_VERSION_MAJOR)*libc.Int32FromInt32(100)*libc.Int32FromInt32(100) + libc.Int32FromInt32(FSE_VERSION_MINOR)*libc.Int32FromInt32(100) + libc.Int32FromInt32(FSE_VERSION_RELEASE))
}

// C documentation
//
//	/*===   Error Management   ===*/
func FSE_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

func FSE_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorName(tls, code)
}

func HUF_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

func HUF_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorName(tls, code)
}

// C documentation
//
//	/*-**************************************************************
//	*  FSE NCount encoding-decoding
//	****************************************************************/
func FSE_readNCount_body(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var bitCount, count, max, nbBits, previous0, remaining, repeats, threshold int32
	var bitStream U32
	var charnum, maxSV1, v2 uint32
	var countSize size_t
	var iend, ip, istart uintptr
	var _ /* buffer at bp+0 */ [8]int8
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bitCount, bitStream, charnum, count, countSize, iend, ip, istart, max, maxSV1, nbBits, previous0, remaining, repeats, threshold, v2
	istart = headerBuffer
	iend = istart + uintptr(hbSize)
	ip = istart
	charnum = uint32(0)
	maxSV1 = *(*uint32)(unsafe.Pointer(maxSVPtr)) + uint32(1)
	previous0 = 0
	if hbSize < uint64(8) {
		/* This function only works when hbSize >= 8 */
		*(*[8]int8)(unsafe.Pointer(bp)) = [8]int8{}
		libc.Xmemcpy(tls, bp, headerBuffer, hbSize)
		countSize = FSE_readNCount(tls, normalizedCounter, maxSVPtr, tableLogPtr, bp, uint64(8))
		if FSE_isError(tls, countSize) != 0 {
			return countSize
		}
		if countSize > hbSize {
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		return countSize
	}
	/* init */
	libc.Xmemset(tls, normalizedCounter, 0, uint64(*(*uint32)(unsafe.Pointer(maxSVPtr))+libc.Uint32FromInt32(1))*libc.Uint64FromInt64(2)) /* all symbols not present in NCount have a frequency of 0 */
	bitStream = MEM_readLE32(tls, ip)
	nbBits = int32(bitStream&uint32(0xF) + uint32(FSE_MIN_TABLELOG)) /* extract tableLog */
	if nbBits > int32(FSE_TABLELOG_ABSOLUTE_MAX) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	bitStream = bitStream >> uint32(4)
	bitCount = int32(4)
	*(*uint32)(unsafe.Pointer(tableLogPtr)) = uint32(nbBits)
	remaining = int32(1)<<nbBits + int32(1)
	threshold = int32(1) << nbBits
	nbBits = nbBits + 1
	for {
		if previous0 != 0 {
			/* Count the number of repeats. Each time the
			 * 2-bit repeat code is 0b11 there is another
			 * repeat.
			 * Avoid UB by setting the high bit to 1.
			 */
			repeats = int32(ZSTD_countTrailingZeros32(tls, ^bitStream|uint32(0x80000000)) >> int32(1))
			for repeats >= int32(12) {
				charnum = charnum + uint32(libc.Int32FromInt32(3)*libc.Int32FromInt32(12))
				if libc.BoolInt32(ip <= iend-libc.UintptrFromInt32(7)) != 0 {
					ip = ip + uintptr(3)
				} else {
					bitCount = bitCount - int32(libc.Int64FromInt32(8)*(int64(iend-libc.UintptrFromInt32(7))-int64(ip)))
					bitCount = bitCount & int32(31)
					ip = iend - uintptr(4)
				}
				bitStream = MEM_readLE32(tls, ip) >> bitCount
				repeats = int32(ZSTD_countTrailingZeros32(tls, ^bitStream|uint32(0x80000000)) >> int32(1))
			}
			charnum = charnum + uint32(int32(3)*repeats)
			bitStream = bitStream >> uint32(int32(2)*repeats)
			bitCount = bitCount + int32(2)*repeats
			/* Add the final repeat which isn't 0b11. */
			charnum = charnum + bitStream&uint32(3)
			bitCount = bitCount + int32(2)
			/* This is an error, but break and return an error
			 * at the end, because returning out of a loop makes
			 * it harder for the compiler to optimize.
			 */
			if charnum >= maxSV1 {
				break
			}
			/* We don't need to set the normalized count to 0
			 * because we already memset the whole buffer to 0.
			 */
			if libc.BoolInt32(ip <= iend-libc.UintptrFromInt32(7)) != 0 || ip+uintptr(bitCount>>libc.Int32FromInt32(3)) <= iend-uintptr(4) {
				/* For first condition to work */
				ip = ip + uintptr(bitCount>>int32(3))
				bitCount = bitCount & int32(7)
			} else {
				bitCount = bitCount - int32(libc.Int64FromInt32(8)*(int64(iend-libc.UintptrFromInt32(4))-int64(ip)))
				bitCount = bitCount & int32(31)
				ip = iend - uintptr(4)
			}
			bitStream = MEM_readLE32(tls, ip) >> bitCount
		}
		max = int32(2)*threshold - int32(1) - remaining
		if bitStream&uint32(threshold-libc.Int32FromInt32(1)) < uint32(max) {
			count = int32(bitStream & uint32(threshold-libc.Int32FromInt32(1)))
			bitCount = bitCount + (nbBits - int32(1))
		} else {
			count = int32(bitStream & uint32(libc.Int32FromInt32(2)*threshold-libc.Int32FromInt32(1)))
			if count >= threshold {
				count = count - max
			}
			bitCount = bitCount + nbBits
		}
		count = count - 1 /* extra accuracy */
		/* When it matters (small blocks), this is a
		 * predictable branch, because we don't use -1.
		 */
		if count >= 0 {
			remaining = remaining - count
		} else {
			remaining = remaining + count
		}
		v2 = charnum
		charnum = charnum + 1
		*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(v2)*2)) = int16(count)
		previous0 = libc.BoolInt32(!(count != 0))
		if remaining < threshold {
			/* This branch can be folded into the
			 * threshold update condition because we
			 * know that threshold > 1.
			 */
			if remaining <= int32(1) {
				break
			}
			nbBits = int32(ZSTD_highbit32(tls, uint32(remaining)) + uint32(1))
			threshold = int32(1) << (nbBits - int32(1))
		}
		if charnum >= maxSV1 {
			break
		}
		if libc.BoolInt32(ip <= iend-libc.UintptrFromInt32(7)) != 0 || ip+uintptr(bitCount>>libc.Int32FromInt32(3)) <= iend-uintptr(4) {
			ip = ip + uintptr(bitCount>>int32(3))
			bitCount = bitCount & int32(7)
		} else {
			bitCount = bitCount - int32(libc.Int64FromInt32(8)*(int64(iend-libc.UintptrFromInt32(4))-int64(ip)))
			bitCount = bitCount & int32(31)
			ip = iend - uintptr(4)
		}
		bitStream = MEM_readLE32(tls, ip) >> bitCount
		goto _1
	_1:
	}
	if remaining != int32(1) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* Only possible when there are too many zeros. */
	if charnum > maxSV1 {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooSmall))
	}
	if bitCount > int32(32) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	*(*uint32)(unsafe.Pointer(maxSVPtr)) = charnum - uint32(1)
	ip = ip + uintptr((bitCount+int32(7))>>int32(3))
	return uint64(int64(ip) - int64(istart))
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func FSE_readNCount_body_default(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	return FSE_readNCount_body(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
}

func FSE_readNCount_body_bmi2(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	return FSE_readNCount_body(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
}

func FSE_readNCount_bmi2(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t, bmi2 int32) (r size_t) {
	if bmi2 != 0 {
		return FSE_readNCount_body_bmi2(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
	}
	_ = bmi2
	return FSE_readNCount_body_default(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
}

func FSE_readNCount(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	return FSE_readNCount_bmi2(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize, 0)
}

// C documentation
//
//	/*! HUF_readStats() :
//	    Read compact Huffman tree, saved by HUF_writeCTable().
//	    `huffWeight` is destination buffer.
//	    `rankStats` is assumed to be a table of at least HUF_TABLELOG_MAX U32.
//	    @return : size read from `src` , or an error Code .
//	    Note : Needed by HUF_readCTable() and HUF_readDTableX?() .
//	*/
func HUF_readStats(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(880)
	defer tls.Free(880)
	var _ /* wksp at bp+0 */ [219]U32
	return HUF_readStats_wksp(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, bp, uint64(876), 0)
}

func HUF_readStats_body(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	var iSize, oSize size_t
	var ip uintptr
	var lastWeight, n, n1, rest, tableLog, total, verif, weightTotal U32
	_, _, _, _, _, _, _, _, _, _, _ = iSize, ip, lastWeight, n, n1, oSize, rest, tableLog, total, verif, weightTotal
	ip = src
	if !(srcSize != 0) {
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	iSize = uint64(*(*BYTE)(unsafe.Pointer(ip)))
	/* ZSTD_memset(huffWeight, 0, hwSize);   */ /* is not necessary, even though some analyzer complain ... */
	if iSize >= uint64(128) {                   /* special header */
		oSize = iSize - uint64(127)
		iSize = (oSize + libc.Uint64FromInt32(1)) / libc.Uint64FromInt32(2)
		if iSize+uint64(1) > srcSize {
			return uint64(-int32(ZSTD_error_srcSize_wrong))
		}
		if oSize >= hwSize {
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		ip = ip + uintptr(1)
		n = uint32(0)
		for {
			if !(uint64(n) < oSize) {
				break
			}
			*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n))) = uint8(int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n/uint32(2))))) >> int32(4))
			*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n+uint32(1)))) = uint8(int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n/uint32(2))))) & int32(15))
			goto _1
		_1:
			;
			n = n + uint32(2)
		}
	} else { /* header compressed with FSE (normal case) */
		if iSize+uint64(1) > srcSize {
			return uint64(-int32(ZSTD_error_srcSize_wrong))
		}
		/* max (hwSize-1) values decoded, as last one is implied */
		oSize = FSE_decompress_wksp_bmi2(tls, huffWeight, hwSize-uint64(1), ip+uintptr(1), iSize, uint32(6), workSpace, wkspSize, bmi2)
		if FSE_isError(tls, oSize) != 0 {
			return oSize
		}
	}
	/* collect weight stats */
	libc.Xmemset(tls, rankStats, 0, uint64(libc.Int32FromInt32(HUF_TABLELOG_MAX)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
	weightTotal = uint32(0)
	n1 = uint32(0)
	for {
		if !(uint64(n1) < oSize) {
			break
		}
		if int32(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1)))) > int32(HUF_TABLELOG_MAX) {
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		*(*U32)(unsafe.Pointer(rankStats + uintptr(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1))))*4)) = *(*U32)(unsafe.Pointer(rankStats + uintptr(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1))))*4)) + 1
		weightTotal = weightTotal + uint32(int32(1)<<*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1)))>>int32(1))
		goto _2
	_2:
		;
		n1 = n1 + 1
	}
	if weightTotal == uint32(0) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* get last non-null symbol weight (implied, total must be 2^n) */
	tableLog = ZSTD_highbit32(tls, weightTotal) + uint32(1)
	if tableLog > uint32(HUF_TABLELOG_MAX) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	*(*U32)(unsafe.Pointer(tableLogPtr)) = tableLog
	/* determine last weight */
	total = uint32(int32(1) << tableLog)
	rest = total - weightTotal
	verif = uint32(int32(1) << ZSTD_highbit32(tls, rest))
	lastWeight = ZSTD_highbit32(tls, rest) + uint32(1)
	if verif != rest {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* last value must be a clean power of 2 */
	*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(oSize))) = uint8(lastWeight)
	*(*U32)(unsafe.Pointer(rankStats + uintptr(lastWeight)*4)) = *(*U32)(unsafe.Pointer(rankStats + uintptr(lastWeight)*4)) + 1
	/* check tree construction validity */
	if *(*U32)(unsafe.Pointer(rankStats + 1*4)) < uint32(2) || *(*U32)(unsafe.Pointer(rankStats + 1*4))&uint32(1) != 0 {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* by construction : at least 2 elts of rank 1, must be even */
	/* results */
	*(*U32)(unsafe.Pointer(nbSymbolsPtr)) = uint32(oSize + libc.Uint64FromInt32(1))
	return iSize + uint64(1)
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func HUF_readStats_body_default(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t) (r size_t) {
	return HUF_readStats_body(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize, 0)
}

func HUF_readStats_body_bmi2(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t) (r size_t) {
	return HUF_readStats_body(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize, int32(1))
}

func HUF_readStats_wksp(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_readStats_body_bmi2(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize)
	}
	_ = flags
	return HUF_readStats_body_default(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize)
}

/**** ended inlining common/entropy_common.c ****/
/**** start inlining common/error_private.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* The purpose of this file is to have a single list of error strings embedded in binary */

/**** skipping file: error_private.h ****/

func ERR_getErrorString(tls *libc.TLS, code ERR_enum) (r uintptr) {
	switch code {
	case int32(ZSTD_error_no_error):
		return __ccgo_ts + 23
	case int32(ZSTD_error_GENERIC):
		return __ccgo_ts + 41
	case int32(ZSTD_error_prefix_unknown):
		return __ccgo_ts + 57
	case int32(ZSTD_error_version_unsupported):
		return __ccgo_ts + 82
	case int32(ZSTD_error_frameParameter_unsupported):
		return __ccgo_ts + 104
	case int32(ZSTD_error_frameParameter_windowTooLarge):
		return __ccgo_ts + 132
	case int32(ZSTD_error_corruption_detected):
		return __ccgo_ts + 176
	case int32(ZSTD_error_checksum_wrong):
		return __ccgo_ts + 201
	case int32(ZSTD_error_literals_headerWrong):
		return __ccgo_ts + 238
	case int32(ZSTD_error_parameter_unsupported):
		return __ccgo_ts + 301
	case int32(ZSTD_error_parameter_combination_unsupported):
		return __ccgo_ts + 323
	case int32(ZSTD_error_parameter_outOfBound):
		return __ccgo_ts + 361
	case int32(ZSTD_error_init_missing):
		return __ccgo_ts + 387
	case int32(ZSTD_error_memory_allocation):
		return __ccgo_ts + 416
	case int32(ZSTD_error_workSpace_tooSmall):
		return __ccgo_ts + 453
	case int32(ZSTD_error_stage_wrong):
		return __ccgo_ts + 490
	case int32(ZSTD_error_tableLog_tooLarge):
		return __ccgo_ts + 543
	case int32(ZSTD_error_maxSymbolValue_tooLarge):
		return __ccgo_ts + 591
	case int32(ZSTD_error_maxSymbolValue_tooSmall):
		return __ccgo_ts + 632
	case int32(ZSTD_error_cannotProduce_uncompressedBlock):
		return __ccgo_ts + 670
	case int32(ZSTD_error_stabilityCondition_notRespected):
		return __ccgo_ts + 718
	case int32(ZSTD_error_dictionary_corrupted):
		return __ccgo_ts + 770
	case int32(ZSTD_error_dictionary_wrong):
		return __ccgo_ts + 794
	case int32(ZSTD_error_dictionaryCreation_failed):
		return __ccgo_ts + 814
	case int32(ZSTD_error_dstSize_tooSmall):
		return __ccgo_ts + 861
	case int32(ZSTD_error_srcSize_wrong):
		return __ccgo_ts + 893
	case int32(ZSTD_error_dstBuffer_null):
		return __ccgo_ts + 915
	case int32(ZSTD_error_noForwardProgress_destFull):
		return __ccgo_ts + 952
	case int32(ZSTD_error_noForwardProgress_inputEmpty):
		return __ccgo_ts + 1032
		/* following error codes are not stable and may be removed or changed in a future version */
		fallthrough
	case int32(ZSTD_error_frameIndex_tooLarge):
		return __ccgo_ts + 1105
	case int32(ZSTD_error_seekableIO):
		return __ccgo_ts + 1130
	case int32(ZSTD_error_dstBuffer_wrong):
		return __ccgo_ts + 1173
	case int32(ZSTD_error_srcBuffer_wrong):
		return __ccgo_ts + 1201
	case int32(ZSTD_error_sequenceProducer_failed):
		return __ccgo_ts + 1224
	case int32(ZSTD_error_externalSequences_invalid):
		return __ccgo_ts + 1286
	case int32(ZSTD_error_maxCode):
		fallthrough
	default:
		return notErrorCode
	}
	return r
}

var notErrorCode = __ccgo_ts

/**** ended inlining common/error_private.c ****/
/**** start inlining common/fse_decompress.c ****/
/* ******************************************************************
 * FSE : Finite State Entropy decoder
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Includes
****************************************************************/
/**** skipping file: debug.h ****/
/**** skipping file: bitstream.h ****/
/**** skipping file: compiler.h ****/
/**** skipping file: fse.h ****/
/**** skipping file: error_private.h ****/
/**** skipping file: zstd_deps.h ****/
/**** skipping file: bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

/* **************************************************************
*  Templates
****************************************************************/
/*
  designed to be included
  for type-specific functions (template emulation in C)
  Objective is to write these functions only once, for improved maintenance
*/

/* safety checks */

/* Function names */

func FSE_buildDTable_internal(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var add, sv U64
	var highThreshold, maxSV1, nextState, position1, s, s1, s3, step1, tableMask1, tableSize, u1, v2 U32
	var i, i1, n int32
	var largeLimit S16
	var pos, position, s2, step, tableMask, u, uPosition, unroll size_t
	var spread, symbolNext, tableDecode, tdPtr, v11 uintptr
	var symbol BYTE
	var v10 U16
	var _ /* DTableH at bp+0 */ FSE_DTableHeader
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = add, highThreshold, i, i1, largeLimit, maxSV1, n, nextState, pos, position, position1, s, s1, s2, s3, spread, step, step1, sv, symbol, symbolNext, tableDecode, tableMask, tableMask1, tableSize, tdPtr, u, u1, uPosition, unroll, v10, v11, v2
	tdPtr = dt + uintptr(1)*4 /* because *dt is unsigned, 32-bits aligned on 32-bits */
	tableDecode = tdPtr
	symbolNext = workSpace
	spread = symbolNext + uintptr(maxSymbolValue)*2 + libc.UintptrFromInt32(1)*2
	maxSV1 = maxSymbolValue + uint32(1)
	tableSize = uint32(int32(1) << tableLog)
	highThreshold = tableSize - uint32(1)
	/* Sanity Checks */
	if uint64(2)*uint64(maxSymbolValue+libc.Uint32FromInt32(1))+uint64(1)<<tableLog+uint64(8) > wkspSize {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	if maxSymbolValue > uint32(FSE_MAX_SYMBOL_VALUE) {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	if tableLog > uint32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	/* Init, lay down lowprob symbols */
	(*(*FSE_DTableHeader)(unsafe.Pointer(bp))).FtableLog = uint16(tableLog)
	(*(*FSE_DTableHeader)(unsafe.Pointer(bp))).FfastMode = uint16(1)
	largeLimit = int16(libc.Int32FromInt32(1) << (tableLog - libc.Uint32FromInt32(1)))
	s = uint32(0)
	for {
		if !(s < maxSV1) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) == -int32(1) {
			v2 = highThreshold
			highThreshold = highThreshold - 1
			(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(v2)*4))).Fsymbol = uint8(s)
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = uint16(1)
		} else {
			if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) >= int32(largeLimit) {
				(*(*FSE_DTableHeader)(unsafe.Pointer(bp))).FfastMode = uint16(0)
			}
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = uint16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)))
		}
		goto _1
	_1:
		;
		s = s + 1
	}
	libc.Xmemcpy(tls, dt, bp, libc.Uint64FromInt64(4))
	/* Spread symbols */
	if highThreshold == tableSize-uint32(1) {
		tableMask = uint64(tableSize - uint32(1))
		step = uint64(tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3))
		/* First lay down the symbols in order.
		 * We use a uint64_t to lay down 8 bytes at a time. This reduces branch
		 * misses since small blocks generally have small table logs, so nearly
		 * all symbols have counts <= 8. We ensure we have 8 bytes at the end of
		 * our buffer to handle the over-write.
		 */
		add = uint64(0x0101010101010101)
		pos = uint64(0)
		sv = uint64(0)
		s1 = uint32(0)
		for {
			if !(s1 < maxSV1) {
				break
			}
			n = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s1)*2)))
			MEM_write64(tls, spread+uintptr(pos), sv)
			i = int32(8)
			for {
				if !(i < n) {
					break
				}
				MEM_write64(tls, spread+uintptr(pos)+uintptr(i), sv)
				goto _4
			_4:
				;
				i = i + int32(8)
			}
			pos = pos + uint64(n)
			goto _3
		_3:
			;
			s1 = s1 + 1
			sv = sv + add
		}
		/* Now we spread those positions across the table.
		 * The benefit of doing it in two stages is that we avoid the
		 * variable size inner loop, which caused lots of branch misses.
		 * Now we can run through all the positions without any branch misses.
		 * We unroll the loop twice, since that is what empirically worked best.
		 */
		position = uint64(0)
		unroll = uint64(2)
		/* FSE_MIN_TABLELOG is 5 */
		s2 = uint64(0)
		for {
			if !(s2 < uint64(tableSize)) {
				break
			}
			u = uint64(0)
			for {
				if !(u < unroll) {
					break
				}
				uPosition = (position + u*step) & tableMask
				(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(uPosition)*4))).Fsymbol = *(*BYTE)(unsafe.Pointer(spread + uintptr(s2+u)))
				goto _6
			_6:
				;
				u = u + 1
			}
			position = (position + unroll*step) & tableMask
			goto _5
		_5:
			;
			s2 = s2 + unroll
		}
	} else {
		tableMask1 = tableSize - uint32(1)
		step1 = tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3)
		position1 = uint32(0)
		s3 = uint32(0)
		for {
			if !(s3 < maxSV1) {
				break
			}
			i1 = 0
			for {
				if !(i1 < int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))) {
					break
				}
				(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(position1)*4))).Fsymbol = uint8(s3)
				position1 = (position1 + step1) & tableMask1
				for position1 > highThreshold {
					position1 = (position1 + step1) & tableMask1
				} /* lowprob area */
				goto _8
			_8:
				;
				i1 = i1 + 1
			}
			goto _7
		_7:
			;
			s3 = s3 + 1
		}
		if position1 != uint32(0) {
			return uint64(-int32(ZSTD_error_GENERIC))
		} /* position must reach all cells once, otherwise normalizedCounter is incorrect */
	}
	/* Build Decoding table */
	u1 = uint32(0)
	for {
		if !(u1 < tableSize) {
			break
		}
		symbol = (*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).Fsymbol
		v11 = symbolNext + uintptr(symbol)*2
		v10 = *(*U16)(unsafe.Pointer(v11))
		*(*U16)(unsafe.Pointer(v11)) = *(*U16)(unsafe.Pointer(v11)) + 1
		nextState = uint32(v10)
		(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).FnbBits = uint8(tableLog - ZSTD_highbit32(tls, nextState))
		(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).FnewState = uint16(nextState<<(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).FnbBits - tableSize)
		goto _9
	_9:
		;
		u1 = u1 + 1
	}
	return uint64(0)
}

func FSE_buildDTable_wksp(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	return FSE_buildDTable_internal(tls, dt, normalizedCounter, maxSymbolValue, tableLog, workSpace, wkspSize)
}

/*-*******************************************************
*  Decompression (Byte symbols)
*********************************************************/
func FSE_decompress_usingDTable_generic(tls *libc.TLS, dst uintptr, maxDstSize size_t, cSrc uintptr, cSrcSize size_t, dt uintptr, fast uint32) (r size_t) {
	bp := tls.Alloc(80)
	defer tls.Free(80)
	var _var_err__ size_t
	var olimit, omax, op, ostart, v6 uintptr
	var v2 int32
	var _ /* bitD at bp+0 */ BIT_DStream_t
	var _ /* state1 at bp+40 */ FSE_DState_t
	var _ /* state2 at bp+56 */ FSE_DState_t
	_, _, _, _, _, _, _ = _var_err__, olimit, omax, op, ostart, v2, v6
	ostart = dst
	op = ostart
	omax = op + uintptr(maxDstSize)
	olimit = omax - uintptr(3)
	/* Init */
	_var_err__ = BIT_initDStream(tls, bp, cSrc, cSrcSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	FSE_initDState(tls, bp+40, bp, dt)
	FSE_initDState(tls, bp+56, bp, dt)
	if BIT_reloadDStream(tls, bp) == int32(BIT_DStream_overflow) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* 4 symbols per loop */
	for {
		if !(libc.BoolInt32(BIT_reloadDStream(tls, bp) == int32(BIT_DStream_unfinished))&libc.BoolInt32(op < olimit) != 0) {
			break
		}
		if fast != 0 {
			v2 = int32(FSE_decodeSymbolFast(tls, bp+40, bp))
		} else {
			v2 = int32(FSE_decodeSymbol(tls, bp+40, bp))
		}
		*(*BYTE)(unsafe.Pointer(op)) = uint8(v2)
		if uint64((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(2)+libc.Int32FromInt32(7)) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) { /* This test must be static */
			BIT_reloadDStream(tls, bp)
		}
		if fast != 0 {
			v2 = int32(FSE_decodeSymbolFast(tls, bp+56, bp))
		} else {
			v2 = int32(FSE_decodeSymbol(tls, bp+56, bp))
		}
		*(*BYTE)(unsafe.Pointer(op + 1)) = uint8(v2)
		if uint64((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(4)+libc.Int32FromInt32(7)) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) { /* This test must be static */
			if BIT_reloadDStream(tls, bp) > int32(BIT_DStream_unfinished) {
				op = op + uintptr(2)
				break
			}
		}
		if fast != 0 {
			v2 = int32(FSE_decodeSymbolFast(tls, bp+40, bp))
		} else {
			v2 = int32(FSE_decodeSymbol(tls, bp+40, bp))
		}
		*(*BYTE)(unsafe.Pointer(op + 2)) = uint8(v2)
		if uint64((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(2)+libc.Int32FromInt32(7)) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) { /* This test must be static */
			BIT_reloadDStream(tls, bp)
		}
		if fast != 0 {
			v2 = int32(FSE_decodeSymbolFast(tls, bp+56, bp))
		} else {
			v2 = int32(FSE_decodeSymbol(tls, bp+56, bp))
		}
		*(*BYTE)(unsafe.Pointer(op + 3)) = uint8(v2)
		goto _1
	_1:
		;
		op = op + uintptr(4)
	}
	/* tail */
	/* note : BIT_reloadDStream(&bitD) >= FSE_DStream_partiallyFilled; Ends at exactly BIT_DStream_completed */
	for int32(1) != 0 {
		if op > omax-libc.UintptrFromInt32(2) {
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		v6 = op
		op = op + 1
		if fast != 0 {
			v2 = int32(FSE_decodeSymbolFast(tls, bp+40, bp))
		} else {
			v2 = int32(FSE_decodeSymbol(tls, bp+40, bp))
		}
		*(*BYTE)(unsafe.Pointer(v6)) = uint8(v2)
		if BIT_reloadDStream(tls, bp) == int32(BIT_DStream_overflow) {
			v6 = op
			op = op + 1
			if fast != 0 {
				v2 = int32(FSE_decodeSymbolFast(tls, bp+56, bp))
			} else {
				v2 = int32(FSE_decodeSymbol(tls, bp+56, bp))
			}
			*(*BYTE)(unsafe.Pointer(v6)) = uint8(v2)
			break
		}
		if op > omax-libc.UintptrFromInt32(2) {
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		v6 = op
		op = op + 1
		if fast != 0 {
			v2 = int32(FSE_decodeSymbolFast(tls, bp+56, bp))
		} else {
			v2 = int32(FSE_decodeSymbol(tls, bp+56, bp))
		}
		*(*BYTE)(unsafe.Pointer(v6)) = uint8(v2)
		if BIT_reloadDStream(tls, bp) == int32(BIT_DStream_overflow) {
			v6 = op
			op = op + 1
			if fast != 0 {
				v2 = int32(FSE_decodeSymbolFast(tls, bp+40, bp))
			} else {
				v2 = int32(FSE_decodeSymbol(tls, bp+40, bp))
			}
			*(*BYTE)(unsafe.Pointer(v6)) = uint8(v2)
			break
		}
	}
	return uint64(int64(op) - int64(ostart))
}

type FSE_DecompressWksp = struct {
	Fncount [256]int16
}

func FSE_decompress_wksp_body(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var DTableH, dtable, ip, istart, ptr, wksp uintptr
	var NCountLength, _var_err__, dtablePos size_t
	var fastMode U32
	var _ /* maxSymbolValue at bp+4 */ uint32
	var _ /* tableLog at bp+0 */ uint32
	_, _, _, _, _, _, _, _, _, _ = DTableH, NCountLength, _var_err__, dtable, dtablePos, fastMode, ip, istart, ptr, wksp
	istart = cSrc
	ip = istart
	*(*uint32)(unsafe.Pointer(bp + 4)) = uint32(FSE_MAX_SYMBOL_VALUE)
	wksp = workSpace
	dtablePos = libc.Uint64FromInt64(512) / libc.Uint64FromInt64(4)
	dtable = workSpace + uintptr(dtablePos)*4
	_ = libc.Uint64FromInt64(1)
	if wkspSize < uint64(512) {
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	/* correct offset to dtable depends on this property */
	_ = libc.Uint64FromInt64(1)
	/* normal FSE decoding mode */
	NCountLength = FSE_readNCount_bmi2(tls, wksp, bp+4, bp, istart, cSrcSize, bmi2)
	if ERR_isError(tls, NCountLength) != 0 {
		return NCountLength
	}
	if *(*uint32)(unsafe.Pointer(bp)) > maxLog {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	ip = ip + uintptr(NCountLength)
	cSrcSize = cSrcSize - NCountLength
	if (uint64(int32(1)+int32(1)<<*(*uint32)(unsafe.Pointer(bp))+int32(1))+(uint64(2)*uint64(*(*uint32)(unsafe.Pointer(bp + 4))+libc.Uint32FromInt32(1))+uint64(1)<<*(*uint32)(unsafe.Pointer(bp))+uint64(8)+uint64(4)-uint64(1))/uint64(4)+uint64((libc.Int32FromInt32(FSE_MAX_SYMBOL_VALUE)+libc.Int32FromInt32(1))/libc.Int32FromInt32(2))+uint64(1))*uint64(4) > wkspSize {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	workSpace = workSpace + uintptr(512) + uintptr(uint64(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)<<*(*uint32)(unsafe.Pointer(bp)))*libc.Uint64FromInt64(4))
	wkspSize = wkspSize - (uint64(512) + uint64(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)<<*(*uint32)(unsafe.Pointer(bp)))*uint64(4))
	_var_err__ = FSE_buildDTable_internal(tls, dtable, wksp, *(*uint32)(unsafe.Pointer(bp + 4)), *(*uint32)(unsafe.Pointer(bp)), workSpace, wkspSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	ptr = dtable
	DTableH = ptr
	fastMode = uint32((*FSE_DTableHeader)(unsafe.Pointer(DTableH)).FfastMode)
	/* select fast mode (static) */
	if fastMode != 0 {
		return FSE_decompress_usingDTable_generic(tls, dst, dstCapacity, ip, cSrcSize, dtable, uint32(1))
	}
	return FSE_decompress_usingDTable_generic(tls, dst, dstCapacity, ip, cSrcSize, dtable, uint32(0))
	return r
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func FSE_decompress_wksp_body_default(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	return FSE_decompress_wksp_body(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize, 0)
}

func FSE_decompress_wksp_body_bmi2(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	return FSE_decompress_wksp_body(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize, int32(1))
}

func FSE_decompress_wksp_bmi2(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	if bmi2 != 0 {
		return FSE_decompress_wksp_body_bmi2(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize)
	}
	_ = bmi2
	return FSE_decompress_wksp_body_default(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize)
}

type _EXCEPTION_POINTERS = struct {
	FExceptionRecord PEXCEPTION_RECORD
	FContextRecord   PCONTEXT
}

type _EXCEPTION_RECORD = struct {
	FExceptionCode        DWORD
	FExceptionFlags       DWORD
	FExceptionRecord      uintptr
	FExceptionAddress     PVOID
	FNumberParameters     DWORD
	FExceptionInformation [15]ULONG_PTR
}

type _CONTEXT = struct {
	FP1Home       DWORD64
	FP2Home       DWORD64
	FP3Home       DWORD64
	FP4Home       DWORD64
	FP5Home       DWORD64
	FP6Home       DWORD64
	FContextFlags DWORD
	FMxCsr        DWORD
	FSegCs        WORD
	FSegDs        WORD
	FSegEs        WORD
	FSegFs        WORD
	FSegGs        WORD
	FSegSs        WORD
	FEFlags       DWORD
	FDr0          DWORD64
	FDr1          DWORD64
	FDr2          DWORD64
	FDr3          DWORD64
	FDr6          DWORD64
	FDr7          DWORD64
	FRax          DWORD64
	FRcx          DWORD64
	FRdx          DWORD64
	FRbx          DWORD64
	FRsp          DWORD64
	FRbp          DWORD64
	FRsi          DWORD64
	FRdi          DWORD64
	FR8           DWORD64
	FR9           DWORD64
	FR10          DWORD64
	FR11          DWORD64
	FR12          DWORD64
	FR13          DWORD64
	FR14          DWORD64
	FR15          DWORD64
	FRip          DWORD64
	F__ccgo38_256 struct {
		FFloatSave [0]XMM_SAVE_AREA32
		F__ccgo2_0 [0]struct {
			FHeader [2]M128A
			FLegacy [8]M128A
			FXmm0   M128A
			FXmm1   M128A
			FXmm2   M128A
			FXmm3   M128A
			FXmm4   M128A
			FXmm5   M128A
			FXmm6   M128A
			FXmm7   M128A
			FXmm8   M128A
			FXmm9   M128A
			FXmm10  M128A
			FXmm11  M128A
			FXmm12  M128A
			FXmm13  M128A
			FXmm14  M128A
			FXmm15  M128A
		}
		FFltSave XMM_SAVE_AREA32
	}
	FVectorRegister       [26]M128A
	FVectorControl        DWORD64
	FDebugControl         DWORD64
	FLastBranchToRip      DWORD64
	FLastBranchFromRip    DWORD64
	FLastExceptionToRip   DWORD64
	FLastExceptionFromRip DWORD64
}

type _DISPATCHER_CONTEXT = struct {
	FControlPc        ULONG64
	FImageBase        ULONG64
	FFunctionEntry    PRUNTIME_FUNCTION
	FEstablisherFrame ULONG64
	FTargetIp         ULONG64
	FContextRecord    PCONTEXT
	FLanguageHandler  PEXCEPTION_ROUTINE
	FHandlerData      PVOID
	FHistoryTable     uintptr
	FScopeIndex       ULONG
	FFill0            ULONG
}

type _PHNDLR = uintptr

type _XCPT_ACTION = struct {
	FXcptNum    uint32
	FSigNum     int32
	FXcptAction _PHNDLR
}

type PEXCEPTION_HANDLER = uintptr

type ULONG = uint32

type PULONG = uintptr

type USHORT = uint16

type PUSHORT = uintptr

type UCHAR = uint8

type PUCHAR = uintptr

type PSZ = uintptr

type WINBOOL = int32

type BOOL = int32

type PBOOL = uintptr

type LPBOOL = uintptr

type WORD = uint16

type DWORD = uint32

type FLOAT = float32

type PFLOAT = uintptr

type PBYTE = uintptr

type LPBYTE = uintptr

type PINT = uintptr

type LPINT = uintptr

type PWORD = uintptr

type LPWORD = uintptr

type LPLONG = uintptr

type PDWORD = uintptr

type LPDWORD = uintptr

type LPVOID = uintptr

type LPCVOID = uintptr

type INT = int32

type UINT = uint32

type PUINT = uintptr

type POINTER_64_INT = uint64

type INT8 = int8

type PINT8 = uintptr

type INT16 = int16

type PINT16 = uintptr

type INT32 = int32

type PINT32 = uintptr

type INT64 = int64

type PINT64 = uintptr

type UINT8 = uint8

type PUINT8 = uintptr

type UINT16 = uint16

type PUINT16 = uintptr

type UINT32 = uint32

type PUINT32 = uintptr

type UINT64 = uint64

type PUINT64 = uintptr

type LONG32 = int32

type PLONG32 = uintptr

type ULONG32 = uint32

type PULONG32 = uintptr

type DWORD32 = uint32

type PDWORD32 = uintptr

type INT_PTR = int64

type PINT_PTR = uintptr

type UINT_PTR = uint64

type PUINT_PTR = uintptr

type LONG_PTR = int64

type PLONG_PTR = uintptr

type ULONG_PTR = uint64

type PULONG_PTR = uintptr

type SHANDLE_PTR = int64

type HANDLE_PTR = uint64

type UHALF_PTR = uint32

type PUHALF_PTR = uintptr

type HALF_PTR = int32

type PHALF_PTR = uintptr

type SIZE_T = uint64

type PSIZE_T = uintptr

type SSIZE_T = int64

type PSSIZE_T = uintptr

type DWORD_PTR = uint64

type PDWORD_PTR = uintptr

type LONG64 = int64

type PLONG64 = uintptr

type ULONG64 = uint64

type PULONG64 = uintptr

type DWORD64 = uint64

type PDWORD64 = uintptr

type KAFFINITY = uint64

type PKAFFINITY = uintptr

type PVOID = uintptr

type PVOID64 = uintptr

type CHAR = int8

type SHORT = int16

type LONG = int32

type WCHAR = uint16

type PWCHAR = uintptr

type LPWCH = uintptr

type PWCH = uintptr

type LPCWCH = uintptr

type PCWCH = uintptr

type NWPSTR = uintptr

type LPWSTR = uintptr

type PWSTR = uintptr

type PZPWSTR = uintptr

type PCZPWSTR = uintptr

type LPUWSTR = uintptr

type PUWSTR = uintptr

type LPCWSTR = uintptr

type PCWSTR = uintptr

type PZPCWSTR = uintptr

type LPCUWSTR = uintptr

type PCUWSTR = uintptr

type PZZWSTR = uintptr

type PCZZWSTR = uintptr

type PUZZWSTR = uintptr

type PCUZZWSTR = uintptr

type PNZWCH = uintptr

type PCNZWCH = uintptr

type PUNZWCH = uintptr

type PCUNZWCH = uintptr

type LPCWCHAR = uintptr

type PCWCHAR = uintptr

type LPCUWCHAR = uintptr

type PCUWCHAR = uintptr

type UCSCHAR = uint32

type PUCSCHAR = uintptr

type PCUCSCHAR = uintptr

type PUCSSTR = uintptr

type PUUCSSTR = uintptr

type PCUCSSTR = uintptr

type PCUUCSSTR = uintptr

type PUUCSCHAR = uintptr

type PCUUCSCHAR = uintptr

type PCHAR = uintptr

type LPCH = uintptr

type PCH = uintptr

type LPCCH = uintptr

type PCCH = uintptr

type NPSTR = uintptr

type LPSTR = uintptr

type PSTR = uintptr

type PZPSTR = uintptr

type PCZPSTR = uintptr

type LPCSTR = uintptr

type PCSTR = uintptr

type PZPCSTR = uintptr

type PZZSTR = uintptr

type PCZZSTR = uintptr

type PNZCH = uintptr

type PCNZCH = uintptr

type TCHAR = int8

type PTCHAR = uintptr

type TBYTE = uint8

type PTBYTE = uintptr

type LPTCH = uintptr

type PTCH = uintptr

type LPCTCH = uintptr

type PCTCH = uintptr

type PTSTR = uintptr

type LPTSTR = uintptr

type PUTSTR = uintptr

type LPUTSTR = uintptr

type PCTSTR = uintptr

type LPCTSTR = uintptr

type PCUTSTR = uintptr

type LPCUTSTR = uintptr

type PZZTSTR = uintptr

type PUZZTSTR = uintptr

type PCZZTSTR = uintptr

type PCUZZTSTR = uintptr

type PZPTSTR = uintptr

type PNZTCH = uintptr

type PUNZTCH = uintptr

type PCNZTCH = uintptr

type PCUNZTCH = uintptr

type PSHORT = uintptr

type PLONG = uintptr

type GROUP_AFFINITY = struct {
	FMask     KAFFINITY
	FGroup    WORD
	FReserved [3]WORD
}

type _GROUP_AFFINITY = GROUP_AFFINITY

type PGROUP_AFFINITY = uintptr

type GROUP_AFFINITY32 = struct {
	FMask     DWORD
	FGroup    WORD
	FReserved [3]WORD
}

type _GROUP_AFFINITY32 = GROUP_AFFINITY32

type PGROUP_AFFINITY32 = uintptr

type GROUP_AFFINITY64 = struct {
	FMask     uint64
	FGroup    WORD
	FReserved [3]WORD
}

type _GROUP_AFFINITY64 = GROUP_AFFINITY64

type PGROUP_AFFINITY64 = uintptr

type HANDLE = uintptr

type PHANDLE = uintptr

type FCHAR = uint8

type FSHORT = uint16

type FLONG = uint32

type HRESULT = int32

type CCHAR = int8

type LCID = uint32

type PLCID = uintptr

type LANGID = uint16

type COMPARTMENT_ID = int32

const UNSPECIFIED_COMPARTMENT_ID = 0
const DEFAULT_COMPARTMENT_ID = 1

type PCOMPARTMENT_ID = uintptr

type FLOAT128 = struct {
	FLowPart  int64
	FHighPart int64
}

type _FLOAT128 = FLOAT128

type PFLOAT128 = uintptr

type LONGLONG = int64

type ULONGLONG = uint64

type PLONGLONG = uintptr

type PULONGLONG = uintptr

type USN = int64

type LARGE_INTEGER = struct {
	Fu [0]struct {
		FLowPart  DWORD
		FHighPart LONG
	}
	FQuadPart  [0]LONGLONG
	F__ccgo0_0 struct {
		FLowPart  DWORD
		FHighPart LONG
	}
}

type _LARGE_INTEGER = LARGE_INTEGER

type PLARGE_INTEGER = uintptr

type ULARGE_INTEGER = struct {
	Fu [0]struct {
		FLowPart  DWORD
		FHighPart DWORD
	}
	FQuadPart  [0]ULONGLONG
	F__ccgo0_0 struct {
		FLowPart  DWORD
		FHighPart DWORD
	}
}

type _ULARGE_INTEGER = ULARGE_INTEGER

type PULARGE_INTEGER = uintptr

type RTL_REFERENCE_COUNT = int64

type PRTL_REFERENCE_COUNT = uintptr

type RTL_REFERENCE_COUNT32 = int32

type PRTL_REFERENCE_COUNT32 = uintptr

type LUID = struct {
	FLowPart  DWORD
	FHighPart LONG
}

type _LUID = LUID

type PLUID = uintptr

type DWORDLONG = uint64

type PDWORDLONG = uintptr

type BOOLEAN = uint8

type PBOOLEAN = uintptr

type LIST_ENTRY = struct {
	FFlink uintptr
	FBlink uintptr
}

type _LIST_ENTRY = LIST_ENTRY

type PLIST_ENTRY = uintptr

type PRLIST_ENTRY = uintptr

type SINGLE_LIST_ENTRY = struct {
	FNext uintptr
}

type _SINGLE_LIST_ENTRY = SINGLE_LIST_ENTRY

type PSINGLE_LIST_ENTRY = uintptr

type LIST_ENTRY32 = struct {
	FFlink DWORD
	FBlink DWORD
}

type PLIST_ENTRY32 = uintptr

type LIST_ENTRY64 = struct {
	FFlink ULONGLONG
	FBlink ULONGLONG
}

type PLIST_ENTRY64 = uintptr

type GUID = struct {
	FData1 uint32
	FData2 uint16
	FData3 uint16
	FData4 [8]uint8
}

type _GUID = GUID

type LPGUID = uintptr

type LPCGUID = uintptr

type IID = struct {
	FData1 uint32
	FData2 uint16
	FData3 uint16
	FData4 [8]uint8
}

type LPIID = uintptr

type CLSID = struct {
	FData1 uint32
	FData2 uint16
	FData3 uint16
	FData4 [8]uint8
}

type LPCLSID = uintptr

type FMTID = struct {
	FData1 uint32
	FData2 uint16
	FData3 uint16
	FData4 [8]uint8
}

type LPFMTID = uintptr

type OBJECTID = struct {
	FLineage    GUID
	FUniquifier DWORD
}

type _OBJECTID = OBJECTID

type PEXCEPTION_ROUTINE = uintptr

type KSPIN_LOCK = uint64

type PKSPIN_LOCK = uintptr

type M128A = struct {
	FLow  ULONGLONG
	FHigh LONGLONG
}

type _M128A = M128A

type PM128A = uintptr

type XSAVE_FORMAT = struct {
	FControlWord    WORD
	FStatusWord     WORD
	FTagWord        BYTE
	FReserved1      BYTE
	FErrorOpcode    WORD
	FErrorOffset    DWORD
	FErrorSelector  WORD
	FReserved2      WORD
	FDataOffset     DWORD
	FDataSelector   WORD
	FReserved3      WORD
	FMxCsr          DWORD
	FMxCsr_Mask     DWORD
	FFloatRegisters [8]M128A
	FXmmRegisters   [16]M128A
	FReserved4      [96]BYTE
}

type _XSAVE_FORMAT = XSAVE_FORMAT

type PXSAVE_FORMAT = uintptr

type XSAVE_CET_U_FORMAT = struct {
	FIa32CetUMsr   DWORD64
	FIa32Pl3SspMsr DWORD64
}

type _XSAVE_CET_U_FORMAT = XSAVE_CET_U_FORMAT

type PXSAVE_CET_U_FORMAT = uintptr

type XSAVE_ARM64_SVE_HEADER = struct {
	FVectorLength            DWORD
	FVectorRegisterOffset    DWORD
	FPredicateRegisterOffset DWORD
	FReserved                [5]DWORD
}

type _XSAVE_ARM64_SVE_HEADER = XSAVE_ARM64_SVE_HEADER

type PXSAVE_ARM64_SVE_HEADER = uintptr

type XSAVE_AREA_HEADER = struct {
	FMask     DWORD64
	FReserved [7]DWORD64
}

type _XSAVE_AREA_HEADER = XSAVE_AREA_HEADER

type PXSAVE_AREA_HEADER = uintptr

type XSAVE_AREA = struct {
	FLegacyState XSAVE_FORMAT
	FHeader      XSAVE_AREA_HEADER
}

type _XSAVE_AREA = XSAVE_AREA

type PXSAVE_AREA = uintptr

type XSTATE_CONTEXT = struct {
	FMask      DWORD64
	FLength    DWORD
	FFlags     BYTE
	FReserved0 [3]BYTE
	FArea      PXSAVE_AREA
	FBuffer    PVOID
}

type _XSTATE_CONTEXT = XSTATE_CONTEXT

type PXSTATE_CONTEXT = uintptr

type KERNEL_CET_CONTEXT = struct {
	FSsp        DWORD64
	FRip        DWORD64
	FSegCs      WORD
	F__ccgo3_18 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint16
		}
		FAllFlags WORD
	}
	FFill [2]WORD
}

type _KERNEL_CET_CONTEXT = KERNEL_CET_CONTEXT

type PKERNEL_CET_CONTEXT = uintptr

type SCOPE_TABLE_AMD64 = struct {
	FCount       DWORD
	FScopeRecord [1]struct {
		FBeginAddress   DWORD
		FEndAddress     DWORD
		FHandlerAddress DWORD
		FJumpTarget     DWORD
	}
}

type _SCOPE_TABLE_AMD64 = SCOPE_TABLE_AMD64

type PSCOPE_TABLE_AMD64 = uintptr

type _CMPCCX_ENUM = int32

const _CMPCCX_O = 0
const _CMPCCX_NO = 1
const _CMPCCX_B = 2
const _CMPCCX_NB = 3
const _CMPCCX_Z = 4
const _CMPCCX_NZ = 5
const _CMPCCX_BE = 6
const _CMPCCX_NBE = 7
const _CMPCCX_S = 8
const _CMPCCX_NS = 9
const _CMPCCX_P = 10
const _CMPCCX_NP = 11
const _CMPCCX_L = 12
const _CMPCCX_NL = 13
const _CMPCCX_LE = 14
const _CMPCCX_NLE = 15

type __uintr_frame = struct {
	Frip    uint64
	Frflags uint64
	Frsp    uint64
}

type _mm_hint = int32

const _MM_HINT_IT0 = 19
const _MM_HINT_IT1 = 18
const _MM_HINT_RST2 = 9
const _MM_HINT_ET0 = 7
const _MM_HINT_T0 = 3
const _MM_HINT_T1 = 2
const _MM_HINT_T2 = 1
const _MM_HINT_NTA = 0

type __x86_float_u = float32

type __x86_double_u = float64

type XMM_SAVE_AREA32 = struct {
	FControlWord    WORD
	FStatusWord     WORD
	FTagWord        BYTE
	FReserved1      BYTE
	FErrorOpcode    WORD
	FErrorOffset    DWORD
	FErrorSelector  WORD
	FReserved2      WORD
	FDataOffset     DWORD
	FDataSelector   WORD
	FReserved3      WORD
	FMxCsr          DWORD
	FMxCsr_Mask     DWORD
	FFloatRegisters [8]M128A
	FXmmRegisters   [16]M128A
	FReserved4      [96]BYTE
}

type _XMM_SAVE_AREA32 = XMM_SAVE_AREA32

type PXMM_SAVE_AREA32 = uintptr

type CONTEXT = struct {
	FP1Home       DWORD64
	FP2Home       DWORD64
	FP3Home       DWORD64
	FP4Home       DWORD64
	FP5Home       DWORD64
	FP6Home       DWORD64
	FContextFlags DWORD
	FMxCsr        DWORD
	FSegCs        WORD
	FSegDs        WORD
	FSegEs        WORD
	FSegFs        WORD
	FSegGs        WORD
	FSegSs        WORD
	FEFlags       DWORD
	FDr0          DWORD64
	FDr1          DWORD64
	FDr2          DWORD64
	FDr3          DWORD64
	FDr6          DWORD64
	FDr7          DWORD64
	FRax          DWORD64
	FRcx          DWORD64
	FRdx          DWORD64
	FRbx          DWORD64
	FRsp          DWORD64
	FRbp          DWORD64
	FRsi          DWORD64
	FRdi          DWORD64
	FR8           DWORD64
	FR9           DWORD64
	FR10          DWORD64
	FR11          DWORD64
	FR12          DWORD64
	FR13          DWORD64
	FR14          DWORD64
	FR15          DWORD64
	FRip          DWORD64
	F__ccgo38_256 struct {
		FFloatSave [0]XMM_SAVE_AREA32
		F__ccgo2_0 [0]struct {
			FHeader [2]M128A
			FLegacy [8]M128A
			FXmm0   M128A
			FXmm1   M128A
			FXmm2   M128A
			FXmm3   M128A
			FXmm4   M128A
			FXmm5   M128A
			FXmm6   M128A
			FXmm7   M128A
			FXmm8   M128A
			FXmm9   M128A
			FXmm10  M128A
			FXmm11  M128A
			FXmm12  M128A
			FXmm13  M128A
			FXmm14  M128A
			FXmm15  M128A
		}
		FFltSave XMM_SAVE_AREA32
	}
	FVectorRegister       [26]M128A
	FVectorControl        DWORD64
	FDebugControl         DWORD64
	FLastBranchToRip      DWORD64
	FLastBranchFromRip    DWORD64
	FLastExceptionToRip   DWORD64
	FLastExceptionFromRip DWORD64
}

type PCONTEXT = uintptr

type RUNTIME_FUNCTION = struct {
	FBeginAddress DWORD
	FEndAddress   DWORD
	FUnwindData   DWORD
}

type _RUNTIME_FUNCTION = RUNTIME_FUNCTION

type PRUNTIME_FUNCTION = uintptr

type PGET_RUNTIME_FUNCTION_CALLBACK = uintptr

type POUT_OF_PROCESS_FUNCTION_TABLE_CALLBACK = uintptr

type ARM64_NT_NEON128 = struct {
	FD         [0][2]float64
	FS         [0][4]float32
	FH         [0][8]WORD
	FB         [0][16]BYTE
	F__ccgo0_0 struct {
		FLow  ULONGLONG
		FHigh LONGLONG
	}
}

type _ARM64_NT_NEON128 = ARM64_NT_NEON128

type PARM64_NT_NEON128 = uintptr

type ARM64_NT_CONTEXT = struct {
	FContextFlags ULONG
	FCpsr         ULONG
	F__ccgo2_8    struct {
		FX         [0][31]DWORD64
		F__ccgo0_0 struct {
			FX0  DWORD64
			FX1  DWORD64
			FX2  DWORD64
			FX3  DWORD64
			FX4  DWORD64
			FX5  DWORD64
			FX6  DWORD64
			FX7  DWORD64
			FX8  DWORD64
			FX9  DWORD64
			FX10 DWORD64
			FX11 DWORD64
			FX12 DWORD64
			FX13 DWORD64
			FX14 DWORD64
			FX15 DWORD64
			FX16 DWORD64
			FX17 DWORD64
			FX18 DWORD64
			FX19 DWORD64
			FX20 DWORD64
			FX21 DWORD64
			FX22 DWORD64
			FX23 DWORD64
			FX24 DWORD64
			FX25 DWORD64
			FX26 DWORD64
			FX27 DWORD64
			FX28 DWORD64
			FFp  DWORD64
			FLr  DWORD64
		}
	}
	FSp   DWORD64
	FPc   DWORD64
	FV    [32]ARM64_NT_NEON128
	FFpcr DWORD
	FFpsr DWORD
	FBcr  [8]DWORD
	FBvr  [8]DWORD64
	FWcr  [2]DWORD
	FWvr  [2]DWORD64
}

type _ARM64_NT_CONTEXT = ARM64_NT_CONTEXT

type PARM64_NT_CONTEXT = uintptr

type ARM64EC_NT_CONTEXT = struct {
	F__ccgo0_0 struct {
		F__ccgo0_0 struct {
			FAMD64_P1Home     DWORD64
			FAMD64_P2Home     DWORD64
			FAMD64_P3Home     DWORD64
			FAMD64_P4Home     DWORD64
			FAMD64_P5Home     DWORD64
			FAMD64_P6Home     DWORD64
			FContextFlags     DWORD
			FAMD64_MxCsr_copy DWORD
			FAMD64_SegCs      WORD
			FAMD64_SegDs      WORD
			FAMD64_SegEs      WORD
			FAMD64_SegFs      WORD
			FAMD64_SegGs      WORD
			FAMD64_SegSs      WORD
			FAMD64_EFlags     DWORD
			FAMD64_Dr0        DWORD64
			FAMD64_Dr1        DWORD64
			FAMD64_Dr2        DWORD64
			FAMD64_Dr3        DWORD64
			FAMD64_Dr6        DWORD64
			FAMD64_Dr7        DWORD64
			FX8               DWORD64
			FX0               DWORD64
			FX1               DWORD64
			FX27              DWORD64
			FSp               DWORD64
			FFp               DWORD64
			FX25              DWORD64
			FX26              DWORD64
			FX2               DWORD64
			FX3               DWORD64
			FX4               DWORD64
			FX5               DWORD64
			FX19              DWORD64
			FX20              DWORD64
			FX21              DWORD64
			FX22              DWORD64
			FPc               DWORD64
			F__ccgo38_256     struct {
				FAMD64_ControlWord            WORD
				FAMD64_StatusWord             WORD
				FAMD64_TagWord                BYTE
				FAMD64_Reserved1              BYTE
				FAMD64_ErrorOpcode            WORD
				FAMD64_ErrorOffset            DWORD
				FAMD64_ErrorSelector          WORD
				FAMD64_Reserved2              WORD
				FAMD64_DataOffset             DWORD
				FAMD64_DataSelector           WORD
				FAMD64_Reserved3              WORD
				FAMD64_MxCsr                  DWORD
				FAMD64_MxCsr_Mask             DWORD
				FLr                           DWORD64
				FX16_0                        WORD
				FAMD64_St0_Reserved1          WORD
				FAMD64_St0_Reserved2          DWORD
				FX6                           DWORD64
				FX16_1                        WORD
				FAMD64_St1_Reserved1          WORD
				FAMD64_St1_Reserved2          DWORD
				FX7                           DWORD64
				FX16_2                        WORD
				FAMD64_St2_Reserved1          WORD
				FAMD64_St2_Reserved2          DWORD
				FX9                           DWORD64
				FX16_3                        WORD
				FAMD64_St3_Reserved1          WORD
				FAMD64_St3_Reserved2          DWORD
				FX10                          DWORD64
				FX17_0                        WORD
				FAMD64_St4_Reserved1          WORD
				FAMD64_St4_Reserved2          DWORD
				FX11                          DWORD64
				FX17_1                        WORD
				FAMD64_St5_Reserved1          WORD
				FAMD64_St5_Reserved2          DWORD
				FX12                          DWORD64
				FX17_2                        WORD
				FAMD64_St6_Reserved1          WORD
				FAMD64_St6_Reserved2          DWORD
				FX15                          DWORD64
				FX17_3                        WORD
				FAMD64_St7_Reserved1          WORD
				FAMD64_St7_Reserved2          DWORD
				FV                            [16]ARM64_NT_NEON128
				FAMD64_XSAVE_FORMAT_Reserved4 [96]BYTE
			}
			FAMD64_VectorRegister       [26]ARM64_NT_NEON128
			FAMD64_VectorControl        DWORD64
			FAMD64_DebugControl         DWORD64
			FAMD64_LastBranchToRip      DWORD64
			FAMD64_LastBranchFromRip    DWORD64
			FAMD64_LastExceptionToRip   DWORD64
			FAMD64_LastExceptionFromRip DWORD64
		}
	}
}

type _ARM64EC_NT_CONTEXT = ARM64EC_NT_CONTEXT

type PARM64EC_NT_CONTEXT = uintptr

type ARM64_RUNTIME_FUNCTION = struct {
	FBeginAddress DWORD
	F__ccgo1_4    struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FUnwindData DWORD
	}
}

type _IMAGE_ARM64_RUNTIME_FUNCTION_ENTRY = ARM64_RUNTIME_FUNCTION

type PARM64_RUNTIME_FUNCTION = uintptr

type DISPATCHER_CONTEXT_NONVOLREG_ARM64 = struct {
	F__ccgo1_0 [0]struct {
		FGpNvRegs [11]DWORD64
		FFpNvRegs [8]float64
	}
	FBuffer [152]BYTE
}

type _DISPATCHER_CONTEXT_NONVOLREG_ARM64 = DISPATCHER_CONTEXT_NONVOLREG_ARM64

type DISPATCHER_CONTEXT_ARM64 = struct {
	FControlPc            ULONG_PTR
	FImageBase            ULONG_PTR
	FFunctionEntry        PARM64_RUNTIME_FUNCTION
	FEstablisherFrame     ULONG_PTR
	FTargetPc             ULONG_PTR
	FContextRecord        PARM64_NT_CONTEXT
	FLanguageHandler      PEXCEPTION_ROUTINE
	FHandlerData          PVOID
	FHistoryTable         uintptr
	FScopeIndex           ULONG
	FControlPcIsUnwound   BOOLEAN
	FNonVolatileRegisters PBYTE
}

type _DISPATCHER_CONTEXT_ARM64 = DISPATCHER_CONTEXT_ARM64

type PDISPATCHER_CONTEXT_ARM64 = uintptr

type KNONVOLATILE_CONTEXT_POINTERS_ARM64 = struct {
	FX19 PDWORD64
	FX20 PDWORD64
	FX21 PDWORD64
	FX22 PDWORD64
	FX23 PDWORD64
	FX24 PDWORD64
	FX25 PDWORD64
	FX26 PDWORD64
	FX27 PDWORD64
	FX28 PDWORD64
	FFp  PDWORD64
	FLr  PDWORD64
	FD8  PDWORD64
	FD9  PDWORD64
	FD10 PDWORD64
	FD11 PDWORD64
	FD12 PDWORD64
	FD13 PDWORD64
	FD14 PDWORD64
	FD15 PDWORD64
}

type _KNONVOLATILE_CONTEXT_POINTERS_ARM64 = KNONVOLATILE_CONTEXT_POINTERS_ARM64

type PKNONVOLATILE_CONTEXT_POINTERS_ARM64 = uintptr

type LDT_ENTRY = struct {
	FLimitLow WORD
	FBaseLow  WORD
	FHighWord struct {
		FBits [0]struct {
			F__ccgo0 uint32
		}
		FBytes struct {
			FBaseMid BYTE
			FFlags1  BYTE
			FFlags2  BYTE
			FBaseHi  BYTE
		}
	}
}

type _LDT_ENTRY = LDT_ENTRY

type PLDT_ENTRY = uintptr

type EXCEPTION_RECORD = struct {
	FExceptionCode        DWORD
	FExceptionFlags       DWORD
	FExceptionRecord      uintptr
	FExceptionAddress     PVOID
	FNumberParameters     DWORD
	FExceptionInformation [15]ULONG_PTR
}

type PEXCEPTION_RECORD = uintptr

type EXCEPTION_RECORD32 = struct {
	FExceptionCode        DWORD
	FExceptionFlags       DWORD
	FExceptionRecord      DWORD
	FExceptionAddress     DWORD
	FNumberParameters     DWORD
	FExceptionInformation [15]DWORD
}

type _EXCEPTION_RECORD32 = EXCEPTION_RECORD32

type PEXCEPTION_RECORD32 = uintptr

type EXCEPTION_RECORD64 = struct {
	FExceptionCode        DWORD
	FExceptionFlags       DWORD
	FExceptionRecord      DWORD64
	FExceptionAddress     DWORD64
	FNumberParameters     DWORD
	F__unusedAlignment    DWORD
	FExceptionInformation [15]DWORD64
}

type _EXCEPTION_RECORD64 = EXCEPTION_RECORD64

type PEXCEPTION_RECORD64 = uintptr

type EXCEPTION_POINTERS = struct {
	FExceptionRecord PEXCEPTION_RECORD
	FContextRecord   PCONTEXT
}

type PEXCEPTION_POINTERS = uintptr

type DISPATCHER_CONTEXT = struct {
	FControlPc        ULONG64
	FImageBase        ULONG64
	FFunctionEntry    PRUNTIME_FUNCTION
	FEstablisherFrame ULONG64
	FTargetIp         ULONG64
	FContextRecord    PCONTEXT
	FLanguageHandler  PEXCEPTION_ROUTINE
	FHandlerData      PVOID
	FHistoryTable     uintptr
	FScopeIndex       ULONG
	FFill0            ULONG
}

type PDISPATCHER_CONTEXT = uintptr

type KNONVOLATILE_CONTEXT_POINTERS = struct {
	FFloatingContext [16]PM128A
	FIntegerContext  [16]PULONG64
}

type _KNONVOLATILE_CONTEXT_POINTERS = KNONVOLATILE_CONTEXT_POINTERS

type PKNONVOLATILE_CONTEXT_POINTERS = uintptr

type PACCESS_TOKEN = uintptr

type PSECURITY_DESCRIPTOR = uintptr

type PSID = uintptr

type PCLAIMS_BLOB = uintptr

type ACCESS_MASK = uint32

type PACCESS_MASK = uintptr

type GENERIC_MAPPING = struct {
	FGenericRead    ACCESS_MASK
	FGenericWrite   ACCESS_MASK
	FGenericExecute ACCESS_MASK
	FGenericAll     ACCESS_MASK
}

type _GENERIC_MAPPING = GENERIC_MAPPING

type PGENERIC_MAPPING = uintptr

type LUID_AND_ATTRIBUTES = struct {
	FLuid       LUID
	FAttributes DWORD
}

type _LUID_AND_ATTRIBUTES = LUID_AND_ATTRIBUTES

type PLUID_AND_ATTRIBUTES = uintptr

type LUID_AND_ATTRIBUTES_ARRAY = [1]LUID_AND_ATTRIBUTES

type PLUID_AND_ATTRIBUTES_ARRAY = uintptr

type SID_IDENTIFIER_AUTHORITY = struct {
	FValue [6]BYTE
}

type _SID_IDENTIFIER_AUTHORITY = SID_IDENTIFIER_AUTHORITY

type PSID_IDENTIFIER_AUTHORITY = uintptr

type SID = struct {
	FRevision            BYTE
	FSubAuthorityCount   BYTE
	FIdentifierAuthority SID_IDENTIFIER_AUTHORITY
	FSubAuthority        [1]DWORD
}

type _SID = SID

type PISID = uintptr

type SID_NAME_USE = int32

type _SID_NAME_USE = int32

const SidTypeUser = 1
const SidTypeGroup = 2
const SidTypeDomain = 3
const SidTypeAlias = 4
const SidTypeWellKnownGroup = 5
const SidTypeDeletedAccount = 6
const SidTypeInvalid = 7
const SidTypeUnknown = 8
const SidTypeComputer = 9
const SidTypeLabel = 10
const SidTypeLogonSession = 11

type PSID_NAME_USE = uintptr

type SID_AND_ATTRIBUTES = struct {
	FSid        PSID
	FAttributes DWORD
}

type _SID_AND_ATTRIBUTES = SID_AND_ATTRIBUTES

type PSID_AND_ATTRIBUTES = uintptr

type SID_AND_ATTRIBUTES_ARRAY = [1]SID_AND_ATTRIBUTES

type PSID_AND_ATTRIBUTES_ARRAY = uintptr

type SID_HASH_ENTRY = uint64

type PSID_HASH_ENTRY = uintptr

type SID_AND_ATTRIBUTES_HASH = struct {
	FSidCount DWORD
	FSidAttr  PSID_AND_ATTRIBUTES
	FHash     [32]SID_HASH_ENTRY
}

type _SID_AND_ATTRIBUTES_HASH = SID_AND_ATTRIBUTES_HASH

type PSID_AND_ATTRIBUTES_HASH = uintptr

type ATTRIBUTES_AND_SID = struct {
	FAttributes UINT32
	FSidStart   DWORD
}

type _ATTRIBUTES_AND_SID = ATTRIBUTES_AND_SID

type PATTRIBUTES_AND_SID = uintptr

type WELL_KNOWN_SID_TYPE = int32

const WinNullSid = 0
const WinWorldSid = 1
const WinLocalSid = 2
const WinCreatorOwnerSid = 3
const WinCreatorGroupSid = 4
const WinCreatorOwnerServerSid = 5
const WinCreatorGroupServerSid = 6
const WinNtAuthoritySid = 7
const WinDialupSid = 8
const WinNetworkSid = 9
const WinBatchSid = 10
const WinInteractiveSid = 11
const WinServiceSid = 12
const WinAnonymousSid = 13
const WinProxySid = 14
const WinEnterpriseControllersSid = 15
const WinSelfSid = 16
const WinAuthenticatedUserSid = 17
const WinRestrictedCodeSid = 18
const WinTerminalServerSid = 19
const WinRemoteLogonIdSid = 20
const WinLogonIdsSid = 21
const WinLocalSystemSid = 22
const WinLocalServiceSid = 23
const WinNetworkServiceSid = 24
const WinBuiltinDomainSid = 25
const WinBuiltinAdministratorsSid = 26
const WinBuiltinUsersSid = 27
const WinBuiltinGuestsSid = 28
const WinBuiltinPowerUsersSid = 29
const WinBuiltinAccountOperatorsSid = 30
const WinBuiltinSystemOperatorsSid = 31
const WinBuiltinPrintOperatorsSid = 32
const WinBuiltinBackupOperatorsSid = 33
const WinBuiltinReplicatorSid = 34
const WinBuiltinPreWindows2000CompatibleAccessSid = 35
const WinBuiltinRemoteDesktopUsersSid = 36
const WinBuiltinNetworkConfigurationOperatorsSid = 37
const WinAccountAdministratorSid = 38
const WinAccountGuestSid = 39
const WinAccountKrbtgtSid = 40
const WinAccountDomainAdminsSid = 41
const WinAccountDomainUsersSid = 42
const WinAccountDomainGuestsSid = 43
const WinAccountComputersSid = 44
const WinAccountControllersSid = 45
const WinAccountCertAdminsSid = 46
const WinAccountSchemaAdminsSid = 47
const WinAccountEnterpriseAdminsSid = 48
const WinAccountPolicyAdminsSid = 49
const WinAccountRasAndIasServersSid = 50
const WinNTLMAuthenticationSid = 51
const WinDigestAuthenticationSid = 52
const WinSChannelAuthenticationSid = 53
const WinThisOrganizationSid = 54
const WinOtherOrganizationSid = 55
const WinBuiltinIncomingForestTrustBuildersSid = 56
const WinBuiltinPerfMonitoringUsersSid = 57
const WinBuiltinPerfLoggingUsersSid = 58
const WinBuiltinAuthorizationAccessSid = 59
const WinBuiltinTerminalServerLicenseServersSid = 60
const WinBuiltinDCOMUsersSid = 61
const WinBuiltinIUsersSid = 62
const WinIUserSid = 63
const WinBuiltinCryptoOperatorsSid = 64
const WinUntrustedLabelSid = 65
const WinLowLabelSid = 66
const WinMediumLabelSid = 67
const WinHighLabelSid = 68
const WinSystemLabelSid = 69
const WinWriteRestrictedCodeSid = 70
const WinCreatorOwnerRightsSid = 71
const WinCacheablePrincipalsGroupSid = 72
const WinNonCacheablePrincipalsGroupSid = 73
const WinEnterpriseReadonlyControllersSid = 74
const WinAccountReadonlyControllersSid = 75
const WinBuiltinEventLogReadersGroup = 76
const WinNewEnterpriseReadonlyControllersSid = 77
const WinBuiltinCertSvcDComAccessGroup = 78
const WinMediumPlusLabelSid = 79
const WinLocalLogonSid = 80
const WinConsoleLogonSid = 81
const WinThisOrganizationCertificateSid = 82
const WinApplicationPackageAuthoritySid = 83
const WinBuiltinAnyPackageSid = 84
const WinCapabilityInternetClientSid = 85
const WinCapabilityInternetClientServerSid = 86
const WinCapabilityPrivateNetworkClientServerSid = 87
const WinCapabilityPicturesLibrarySid = 88
const WinCapabilityVideosLibrarySid = 89
const WinCapabilityMusicLibrarySid = 90
const WinCapabilityDocumentsLibrarySid = 91
const WinCapabilitySharedUserCertificatesSid = 92
const WinCapabilityEnterpriseAuthenticationSid = 93
const WinCapabilityRemovableStorageSid = 94
const WinBuiltinRDSRemoteAccessServersSid = 95
const WinBuiltinRDSEndpointServersSid = 96
const WinBuiltinRDSManagementServersSid = 97
const WinUserModeDriversSid = 98
const WinBuiltinHyperVAdminsSid = 99
const WinAccountCloneableControllersSid = 100
const WinBuiltinAccessControlAssistanceOperatorsSid = 101
const WinBuiltinRemoteManagementUsersSid = 102
const WinAuthenticationAuthorityAssertedSid = 103
const WinAuthenticationServiceAssertedSid = 104
const WinLocalAccountSid = 105
const WinLocalAccountAndAdministratorSid = 106
const WinAccountProtectedUsersSid = 107
const WinCapabilityAppointmentsSid = 108
const WinCapabilityContactsSid = 109
const WinAccountDefaultSystemManagedSid = 110
const WinBuiltinDefaultSystemManagedGroupSid = 111
const WinBuiltinStorageReplicaAdminsSid = 112
const WinAccountKeyAdminsSid = 113
const WinAccountEnterpriseKeyAdminsSid = 114
const WinAuthenticationKeyTrustSid = 115
const WinAuthenticationKeyPropertyMFASid = 116
const WinAuthenticationKeyPropertyAttestationSid = 117
const WinAuthenticationFreshKeyAuthSid = 118
const WinBuiltinDeviceOwnersSid = 119
const WinBuiltinUserModeHardwareOperatorsSid = 120
const WinBuiltinOpenSSHUsersSid = 121

type ACL = struct {
	FAclRevision BYTE
	FSbz1        BYTE
	FAclSize     WORD
	FAceCount    WORD
	FSbz2        WORD
}

type _ACL = ACL

type PACL = uintptr

type ACE_HEADER = struct {
	FAceType  BYTE
	FAceFlags BYTE
	FAceSize  WORD
}

type _ACE_HEADER = ACE_HEADER

type PACE_HEADER = uintptr

type ACCESS_ALLOWED_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _ACCESS_ALLOWED_ACE = ACCESS_ALLOWED_ACE

type PACCESS_ALLOWED_ACE = uintptr

type ACCESS_DENIED_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _ACCESS_DENIED_ACE = ACCESS_DENIED_ACE

type PACCESS_DENIED_ACE = uintptr

type SYSTEM_AUDIT_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_AUDIT_ACE = SYSTEM_AUDIT_ACE

type PSYSTEM_AUDIT_ACE = uintptr

type SYSTEM_ALARM_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_ALARM_ACE = SYSTEM_ALARM_ACE

type PSYSTEM_ALARM_ACE = uintptr

type SYSTEM_RESOURCE_ATTRIBUTE_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_RESOURCE_ATTRIBUTE_ACE = SYSTEM_RESOURCE_ATTRIBUTE_ACE

type PSYSTEM_RESOURCE_ATTRIBUTE_ACE = uintptr

type SYSTEM_SCOPED_POLICY_ID_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_SCOPED_POLICY_ID_ACE = SYSTEM_SCOPED_POLICY_ID_ACE

type PSYSTEM_SCOPED_POLICY_ID_ACE = uintptr

type SYSTEM_MANDATORY_LABEL_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_MANDATORY_LABEL_ACE = SYSTEM_MANDATORY_LABEL_ACE

type PSYSTEM_MANDATORY_LABEL_ACE = uintptr

type SYSTEM_PROCESS_TRUST_LABEL_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_PROCESS_TRUST_LABEL_ACE = SYSTEM_PROCESS_TRUST_LABEL_ACE

type PSYSTEM_PROCESS_TRUST_LABEL_ACE = uintptr

type SYSTEM_ACCESS_FILTER_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_ACCESS_FILTER_ACE = SYSTEM_ACCESS_FILTER_ACE

type PSYSTEM_ACCESS_FILTER_ACE = uintptr

type ACCESS_ALLOWED_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _ACCESS_ALLOWED_OBJECT_ACE = ACCESS_ALLOWED_OBJECT_ACE

type PACCESS_ALLOWED_OBJECT_ACE = uintptr

type ACCESS_DENIED_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _ACCESS_DENIED_OBJECT_ACE = ACCESS_DENIED_OBJECT_ACE

type PACCESS_DENIED_OBJECT_ACE = uintptr

type SYSTEM_AUDIT_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _SYSTEM_AUDIT_OBJECT_ACE = SYSTEM_AUDIT_OBJECT_ACE

type PSYSTEM_AUDIT_OBJECT_ACE = uintptr

type SYSTEM_ALARM_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _SYSTEM_ALARM_OBJECT_ACE = SYSTEM_ALARM_OBJECT_ACE

type PSYSTEM_ALARM_OBJECT_ACE = uintptr

type ACCESS_ALLOWED_CALLBACK_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _ACCESS_ALLOWED_CALLBACK_ACE = ACCESS_ALLOWED_CALLBACK_ACE

type PACCESS_ALLOWED_CALLBACK_ACE = uintptr

type ACCESS_DENIED_CALLBACK_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _ACCESS_DENIED_CALLBACK_ACE = ACCESS_DENIED_CALLBACK_ACE

type PACCESS_DENIED_CALLBACK_ACE = uintptr

type SYSTEM_AUDIT_CALLBACK_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_AUDIT_CALLBACK_ACE = SYSTEM_AUDIT_CALLBACK_ACE

type PSYSTEM_AUDIT_CALLBACK_ACE = uintptr

type SYSTEM_ALARM_CALLBACK_ACE = struct {
	FHeader   ACE_HEADER
	FMask     ACCESS_MASK
	FSidStart DWORD
}

type _SYSTEM_ALARM_CALLBACK_ACE = SYSTEM_ALARM_CALLBACK_ACE

type PSYSTEM_ALARM_CALLBACK_ACE = uintptr

type ACCESS_ALLOWED_CALLBACK_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _ACCESS_ALLOWED_CALLBACK_OBJECT_ACE = ACCESS_ALLOWED_CALLBACK_OBJECT_ACE

type PACCESS_ALLOWED_CALLBACK_OBJECT_ACE = uintptr

type ACCESS_DENIED_CALLBACK_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _ACCESS_DENIED_CALLBACK_OBJECT_ACE = ACCESS_DENIED_CALLBACK_OBJECT_ACE

type PACCESS_DENIED_CALLBACK_OBJECT_ACE = uintptr

type SYSTEM_AUDIT_CALLBACK_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _SYSTEM_AUDIT_CALLBACK_OBJECT_ACE = SYSTEM_AUDIT_CALLBACK_OBJECT_ACE

type PSYSTEM_AUDIT_CALLBACK_OBJECT_ACE = uintptr

type SYSTEM_ALARM_CALLBACK_OBJECT_ACE = struct {
	FHeader              ACE_HEADER
	FMask                ACCESS_MASK
	FFlags               DWORD
	FObjectType          GUID
	FInheritedObjectType GUID
	FSidStart            DWORD
}

type _SYSTEM_ALARM_CALLBACK_OBJECT_ACE = SYSTEM_ALARM_CALLBACK_OBJECT_ACE

type PSYSTEM_ALARM_CALLBACK_OBJECT_ACE = uintptr

type ACL_INFORMATION_CLASS = int32

type _ACL_INFORMATION_CLASS = int32

const AclRevisionInformation = 1
const AclSizeInformation = 2

type ACL_REVISION_INFORMATION = struct {
	FAclRevision DWORD
}

type _ACL_REVISION_INFORMATION = ACL_REVISION_INFORMATION

type PACL_REVISION_INFORMATION = uintptr

type ACL_SIZE_INFORMATION = struct {
	FAceCount      DWORD
	FAclBytesInUse DWORD
	FAclBytesFree  DWORD
}

type _ACL_SIZE_INFORMATION = ACL_SIZE_INFORMATION

type PACL_SIZE_INFORMATION = uintptr

type SECURITY_DESCRIPTOR_CONTROL = uint16

type PSECURITY_DESCRIPTOR_CONTROL = uintptr

type SECURITY_DESCRIPTOR_RELATIVE = struct {
	FRevision BYTE
	FSbz1     BYTE
	FControl  SECURITY_DESCRIPTOR_CONTROL
	FOwner    DWORD
	FGroup    DWORD
	FSacl     DWORD
	FDacl     DWORD
}

type _SECURITY_DESCRIPTOR_RELATIVE = SECURITY_DESCRIPTOR_RELATIVE

type PISECURITY_DESCRIPTOR_RELATIVE = uintptr

type SECURITY_DESCRIPTOR = struct {
	FRevision BYTE
	FSbz1     BYTE
	FControl  SECURITY_DESCRIPTOR_CONTROL
	FOwner    PSID
	FGroup    PSID
	FSacl     PACL
	FDacl     PACL
}

type _SECURITY_DESCRIPTOR = SECURITY_DESCRIPTOR

type PISECURITY_DESCRIPTOR = uintptr

type SECURITY_OBJECT_AI_PARAMS = struct {
	FSize           DWORD
	FConstraintMask DWORD
}

type _SECURITY_OBJECT_AI_PARAMS = SECURITY_OBJECT_AI_PARAMS

type PSECURITY_OBJECT_AI_PARAMS = uintptr

type OBJECT_TYPE_LIST = struct {
	FLevel      WORD
	FSbz        WORD
	FObjectType uintptr
}

type _OBJECT_TYPE_LIST = OBJECT_TYPE_LIST

type POBJECT_TYPE_LIST = uintptr

type AUDIT_EVENT_TYPE = int32

type _AUDIT_EVENT_TYPE = int32

const AuditEventObjectAccess = 0
const AuditEventDirectoryServiceAccess = 1

type PAUDIT_EVENT_TYPE = uintptr

type PRIVILEGE_SET = struct {
	FPrivilegeCount DWORD
	FControl        DWORD
	FPrivilege      [1]LUID_AND_ATTRIBUTES
}

type _PRIVILEGE_SET = PRIVILEGE_SET

type PPRIVILEGE_SET = uintptr

type ACCESS_REASON_TYPE = int32

type _ACCESS_REASON_TYPE = int32

const AccessReasonNone = 0
const AccessReasonAllowedAce = 65536
const AccessReasonDeniedAce = 131072
const AccessReasonAllowedParentAce = 196608
const AccessReasonDeniedParentAce = 262144
const AccessReasonNotGrantedByCape = 327680
const AccessReasonNotGrantedByParentCape = 393216
const AccessReasonNotGrantedToAppContainer = 458752
const AccessReasonMissingPrivilege = 1048576
const AccessReasonFromPrivilege = 2097152
const AccessReasonIntegrityLevel = 3145728
const AccessReasonOwnership = 4194304
const AccessReasonNullDacl = 5242880
const AccessReasonEmptyDacl = 6291456
const AccessReasonNoSD = 7340032
const AccessReasonNoGrant = 8388608
const AccessReasonTrustLabel = 9437184
const AccessReasonFilterAce = 10485760

type ACCESS_REASON = uint32

type ACCESS_REASONS = struct {
	FData [32]ACCESS_REASON
}

type _ACCESS_REASONS = ACCESS_REASONS

type PACCESS_REASONS = uintptr

type SE_SECURITY_DESCRIPTOR = struct {
	FSize               DWORD
	FFlags              DWORD
	FSecurityDescriptor PSECURITY_DESCRIPTOR
}

type _SE_SECURITY_DESCRIPTOR = SE_SECURITY_DESCRIPTOR

type PSE_SECURITY_DESCRIPTOR = uintptr

type SE_ACCESS_REQUEST = struct {
	FSize                    DWORD
	FSeSecurityDescriptor    PSE_SECURITY_DESCRIPTOR
	FDesiredAccess           ACCESS_MASK
	FPreviouslyGrantedAccess ACCESS_MASK
	FPrincipalSelfSid        PSID
	FGenericMapping          PGENERIC_MAPPING
	FObjectTypeListCount     DWORD
	FObjectTypeList          POBJECT_TYPE_LIST
}

type _SE_ACCESS_REQUEST = SE_ACCESS_REQUEST

type PSE_ACCESS_REQUEST = uintptr

type SE_ACCESS_REPLY = struct {
	FSize            DWORD
	FResultListCount DWORD
	FGrantedAccess   PACCESS_MASK
	FAccessStatus    PDWORD
	FAccessReason    PACCESS_REASONS
	FPrivileges      uintptr
}

type _SE_ACCESS_REPLY = SE_ACCESS_REPLY

type PSE_ACCESS_REPLY = uintptr

type SECURITY_IMPERSONATION_LEVEL = int32

type _SECURITY_IMPERSONATION_LEVEL = int32

const SecurityAnonymous = 0
const SecurityIdentification = 1
const SecurityImpersonation = 2
const SecurityDelegation = 3

type PSECURITY_IMPERSONATION_LEVEL = uintptr

type TOKEN_TYPE = int32

type _TOKEN_TYPE = int32

const TokenPrimary = 1
const TokenImpersonation = 2

type PTOKEN_TYPE = uintptr

type TOKEN_ELEVATION_TYPE = int32

type _TOKEN_ELEVATION_TYPE = int32

const TokenElevationTypeDefault = 1
const TokenElevationTypeFull = 2
const TokenElevationTypeLimited = 3

type PTOKEN_ELEVATION_TYPE = uintptr

type TOKEN_INFORMATION_CLASS = int32

type _TOKEN_INFORMATION_CLASS = int32

const TokenUser = 1
const TokenGroups = 2
const TokenPrivileges = 3
const TokenOwner = 4
const TokenPrimaryGroup = 5
const TokenDefaultDacl = 6
const TokenSource = 7
const TokenType = 8
const TokenImpersonationLevel = 9
const TokenStatistics = 10
const TokenRestrictedSids = 11
const TokenSessionId = 12
const TokenGroupsAndPrivileges = 13
const TokenSessionReference = 14
const TokenSandBoxInert = 15
const TokenAuditPolicy = 16
const TokenOrigin = 17
const TokenElevationType = 18
const TokenLinkedToken = 19
const TokenElevation = 20
const TokenHasRestrictions = 21
const TokenAccessInformation = 22
const TokenVirtualizationAllowed = 23
const TokenVirtualizationEnabled = 24
const TokenIntegrityLevel = 25
const TokenUIAccess = 26
const TokenMandatoryPolicy = 27
const TokenLogonSid = 28
const TokenIsAppContainer = 29
const TokenCapabilities = 30
const TokenAppContainerSid = 31
const TokenAppContainerNumber = 32
const TokenUserClaimAttributes = 33
const TokenDeviceClaimAttributes = 34
const TokenRestrictedUserClaimAttributes = 35
const TokenRestrictedDeviceClaimAttributes = 36
const TokenDeviceGroups = 37
const TokenRestrictedDeviceGroups = 38
const TokenSecurityAttributes = 39
const TokenIsRestricted = 40
const TokenProcessTrustLevel = 41
const TokenPrivateNameSpace = 42
const TokenSingletonAttributes = 43
const TokenBnoIsolation = 44
const TokenChildProcessFlags = 45
const TokenIsLessPrivilegedAppContainer = 46
const TokenIsSandboxed = 47
const TokenIsAppSilo = 48
const MaxTokenInfoClass = 49

type PTOKEN_INFORMATION_CLASS = uintptr

type TOKEN_USER = struct {
	FUser SID_AND_ATTRIBUTES
}

type _TOKEN_USER = TOKEN_USER

type PTOKEN_USER = uintptr

type SE_TOKEN_USER = struct {
	F__ccgo0_0 struct {
		FUser      [0]SID_AND_ATTRIBUTES
		FTokenUser TOKEN_USER
	}
	F__ccgo1_16 struct {
		FBuffer      [0][68]BYTE
		FSid         SID
		F__ccgo_pad2 [56]byte
	}
}

type _SE_TOKEN_USER = SE_TOKEN_USER

type PSE_TOKEN_USER = uintptr

type TOKEN_GROUPS = struct {
	FGroupCount DWORD
	FGroups     [1]SID_AND_ATTRIBUTES
}

type _TOKEN_GROUPS = TOKEN_GROUPS

type PTOKEN_GROUPS = uintptr

type TOKEN_PRIVILEGES = struct {
	FPrivilegeCount DWORD
	FPrivileges     [1]LUID_AND_ATTRIBUTES
}

type _TOKEN_PRIVILEGES = TOKEN_PRIVILEGES

type PTOKEN_PRIVILEGES = uintptr

type TOKEN_OWNER = struct {
	FOwner PSID
}

type _TOKEN_OWNER = TOKEN_OWNER

type PTOKEN_OWNER = uintptr

type TOKEN_PRIMARY_GROUP = struct {
	FPrimaryGroup PSID
}

type _TOKEN_PRIMARY_GROUP = TOKEN_PRIMARY_GROUP

type PTOKEN_PRIMARY_GROUP = uintptr

type TOKEN_DEFAULT_DACL = struct {
	FDefaultDacl PACL
}

type _TOKEN_DEFAULT_DACL = TOKEN_DEFAULT_DACL

type PTOKEN_DEFAULT_DACL = uintptr

type TOKEN_USER_CLAIMS = struct {
	FUserClaims PCLAIMS_BLOB
}

type _TOKEN_USER_CLAIMS = TOKEN_USER_CLAIMS

type PTOKEN_USER_CLAIMS = uintptr

type TOKEN_DEVICE_CLAIMS = struct {
	FDeviceClaims PCLAIMS_BLOB
}

type _TOKEN_DEVICE_CLAIMS = TOKEN_DEVICE_CLAIMS

type PTOKEN_DEVICE_CLAIMS = uintptr

type TOKEN_GROUPS_AND_PRIVILEGES = struct {
	FSidCount            DWORD
	FSidLength           DWORD
	FSids                PSID_AND_ATTRIBUTES
	FRestrictedSidCount  DWORD
	FRestrictedSidLength DWORD
	FRestrictedSids      PSID_AND_ATTRIBUTES
	FPrivilegeCount      DWORD
	FPrivilegeLength     DWORD
	FPrivileges          PLUID_AND_ATTRIBUTES
	FAuthenticationId    LUID
}

type _TOKEN_GROUPS_AND_PRIVILEGES = TOKEN_GROUPS_AND_PRIVILEGES

type PTOKEN_GROUPS_AND_PRIVILEGES = uintptr

type TOKEN_LINKED_TOKEN = struct {
	FLinkedToken HANDLE
}

type _TOKEN_LINKED_TOKEN = TOKEN_LINKED_TOKEN

type PTOKEN_LINKED_TOKEN = uintptr

type TOKEN_ELEVATION = struct {
	FTokenIsElevated DWORD
}

type _TOKEN_ELEVATION = TOKEN_ELEVATION

type PTOKEN_ELEVATION = uintptr

type TOKEN_MANDATORY_LABEL = struct {
	FLabel SID_AND_ATTRIBUTES
}

type _TOKEN_MANDATORY_LABEL = TOKEN_MANDATORY_LABEL

type PTOKEN_MANDATORY_LABEL = uintptr

type TOKEN_MANDATORY_POLICY = struct {
	FPolicy DWORD
}

type _TOKEN_MANDATORY_POLICY = TOKEN_MANDATORY_POLICY

type PTOKEN_MANDATORY_POLICY = uintptr

type PSECURITY_ATTRIBUTES_OPAQUE = uintptr

type TOKEN_ACCESS_INFORMATION = struct {
	FSidHash            PSID_AND_ATTRIBUTES_HASH
	FRestrictedSidHash  PSID_AND_ATTRIBUTES_HASH
	FPrivileges         PTOKEN_PRIVILEGES
	FAuthenticationId   LUID
	FTokenType          TOKEN_TYPE
	FImpersonationLevel SECURITY_IMPERSONATION_LEVEL
	FMandatoryPolicy    TOKEN_MANDATORY_POLICY
	FFlags              DWORD
	FAppContainerNumber DWORD
	FPackageSid         PSID
	FCapabilitiesHash   PSID_AND_ATTRIBUTES_HASH
}

type _TOKEN_ACCESS_INFORMATION = TOKEN_ACCESS_INFORMATION

type PTOKEN_ACCESS_INFORMATION = uintptr

type TOKEN_AUDIT_POLICY = struct {
	FPerUserPolicy [29]UCHAR
}

type _TOKEN_AUDIT_POLICY = TOKEN_AUDIT_POLICY

type PTOKEN_AUDIT_POLICY = uintptr

type TOKEN_SOURCE = struct {
	FSourceName       [8]CHAR
	FSourceIdentifier LUID
}

type _TOKEN_SOURCE = TOKEN_SOURCE

type PTOKEN_SOURCE = uintptr

type TOKEN_STATISTICS = struct {
	FTokenId            LUID
	FAuthenticationId   LUID
	FExpirationTime     LARGE_INTEGER
	FTokenType          TOKEN_TYPE
	FImpersonationLevel SECURITY_IMPERSONATION_LEVEL
	FDynamicCharged     DWORD
	FDynamicAvailable   DWORD
	FGroupCount         DWORD
	FPrivilegeCount     DWORD
	FModifiedId         LUID
}

type _TOKEN_STATISTICS = TOKEN_STATISTICS

type PTOKEN_STATISTICS = uintptr

type TOKEN_CONTROL = struct {
	FTokenId          LUID
	FAuthenticationId LUID
	FModifiedId       LUID
	FTokenSource      TOKEN_SOURCE
}

type _TOKEN_CONTROL = TOKEN_CONTROL

type PTOKEN_CONTROL = uintptr

type TOKEN_ORIGIN = struct {
	FOriginatingLogonSession LUID
}

type _TOKEN_ORIGIN = TOKEN_ORIGIN

type PTOKEN_ORIGIN = uintptr

type MANDATORY_LEVEL = int32

type _MANDATORY_LEVEL = int32

const MandatoryLevelUntrusted = 0
const MandatoryLevelLow = 1
const MandatoryLevelMedium = 2
const MandatoryLevelHigh = 3
const MandatoryLevelSystem = 4
const MandatoryLevelSecureProcess = 5
const MandatoryLevelCount = 6

type PMANDATORY_LEVEL = uintptr

type TOKEN_APPCONTAINER_INFORMATION = struct {
	FTokenAppContainer PSID
}

type _TOKEN_APPCONTAINER_INFORMATION = TOKEN_APPCONTAINER_INFORMATION

type PTOKEN_APPCONTAINER_INFORMATION = uintptr

type TOKEN_SID_INFORMATION = struct {
	FSid PSID
}

type _TOKEN_SID_INFORMATION = TOKEN_SID_INFORMATION

type PTOKEN_SID_INFORMATION = uintptr

type TOKEN_BNO_ISOLATION_INFORMATION = struct {
	FIsolationPrefix  PWSTR
	FIsolationEnabled BOOLEAN
}

type _TOKEN_BNO_ISOLATION_INFORMATION = TOKEN_BNO_ISOLATION_INFORMATION

type PTOKEN_BNO_ISOLATION_INFORMATION = uintptr

type CLAIM_SECURITY_ATTRIBUTE_FQBN_VALUE = struct {
	FVersion DWORD64
	FName    PWSTR
}

type _CLAIM_SECURITY_ATTRIBUTE_FQBN_VALUE = CLAIM_SECURITY_ATTRIBUTE_FQBN_VALUE

type PCLAIM_SECURITY_ATTRIBUTE_FQBN_VALUE = uintptr

type CLAIM_SECURITY_ATTRIBUTE_OCTET_STRING_VALUE = struct {
	FpValue      PVOID
	FValueLength DWORD
}

type _CLAIM_SECURITY_ATTRIBUTE_OCTET_STRING_VALUE = CLAIM_SECURITY_ATTRIBUTE_OCTET_STRING_VALUE

type PCLAIM_SECURITY_ATTRIBUTE_OCTET_STRING_VALUE = uintptr

type CLAIM_SECURITY_ATTRIBUTE_V1 = struct {
	FName       PWSTR
	FValueType  WORD
	FReserved   WORD
	FFlags      DWORD
	FValueCount DWORD
	FValues     struct {
		FpUint64      [0]PDWORD64
		FppString     [0]uintptr
		FpFqbn        [0]PCLAIM_SECURITY_ATTRIBUTE_FQBN_VALUE
		FpOctetString [0]PCLAIM_SECURITY_ATTRIBUTE_OCTET_STRING_VALUE
		FpInt64       PLONG64
	}
}

type _CLAIM_SECURITY_ATTRIBUTE_V1 = CLAIM_SECURITY_ATTRIBUTE_V1

type PCLAIM_SECURITY_ATTRIBUTE_V1 = uintptr

type CLAIM_SECURITY_ATTRIBUTE_RELATIVE_V1 = struct {
	FName       DWORD
	FValueType  WORD
	FReserved   WORD
	FFlags      DWORD
	FValueCount DWORD
	FValues     struct {
		FpUint64      [0][1]DWORD
		FppString     [0][1]DWORD
		FpFqbn        [0][1]DWORD
		FpOctetString [0][1]DWORD
		FpInt64       [1]DWORD
	}
}

type _CLAIM_SECURITY_ATTRIBUTE_RELATIVE_V1 = CLAIM_SECURITY_ATTRIBUTE_RELATIVE_V1

type PCLAIM_SECURITY_ATTRIBUTE_RELATIVE_V1 = uintptr

type CLAIM_SECURITY_ATTRIBUTES_INFORMATION = struct {
	FVersion        WORD
	FReserved       WORD
	FAttributeCount DWORD
	FAttribute      struct {
		FpAttributeV1 PCLAIM_SECURITY_ATTRIBUTE_V1
	}
}

type _CLAIM_SECURITY_ATTRIBUTES_INFORMATION = CLAIM_SECURITY_ATTRIBUTES_INFORMATION

type PCLAIM_SECURITY_ATTRIBUTES_INFORMATION = uintptr

type SECURITY_CONTEXT_TRACKING_MODE = uint8

type PSECURITY_CONTEXT_TRACKING_MODE = uintptr

type SECURITY_QUALITY_OF_SERVICE = struct {
	FLength              DWORD
	FImpersonationLevel  SECURITY_IMPERSONATION_LEVEL
	FContextTrackingMode SECURITY_CONTEXT_TRACKING_MODE
	FEffectiveOnly       BOOLEAN
}

type _SECURITY_QUALITY_OF_SERVICE = SECURITY_QUALITY_OF_SERVICE

type PSECURITY_QUALITY_OF_SERVICE = uintptr

type SE_IMPERSONATION_STATE = struct {
	FToken         PACCESS_TOKEN
	FCopyOnOpen    BOOLEAN
	FEffectiveOnly BOOLEAN
	FLevel         SECURITY_IMPERSONATION_LEVEL
}

type _SE_IMPERSONATION_STATE = SE_IMPERSONATION_STATE

type PSE_IMPERSONATION_STATE = uintptr

type SECURITY_INFORMATION = uint32

type PSECURITY_INFORMATION = uintptr

type SE_SIGNING_LEVEL = uint8

type PSE_SIGNING_LEVEL = uintptr

type SE_IMAGE_SIGNATURE_TYPE = int32

type _SE_IMAGE_SIGNATURE_TYPE = int32

const SeImageSignatureNone = 0
const SeImageSignatureEmbedded = 1
const SeImageSignatureCache = 2
const SeImageSignatureCatalogCached = 3
const SeImageSignatureCatalogNotCached = 4
const SeImageSignatureCatalogHint = 5
const SeImageSignaturePackageCatalog = 6
const SeImageSignaturePplMitigated = 7

type PSE_IMAGE_SIGNATURE_TYPE = uintptr

type SE_LEARNING_MODE_DATA_TYPE = int32

type _SE_LEARNING_MODE_DATA_TYPE = int32

const SeLearningModeInvalidType = 0
const SeLearningModeSettings = 1
const SeLearningModeMax = 2

type SECURITY_CAPABILITIES = struct {
	FAppContainerSid PSID
	FCapabilities    PSID_AND_ATTRIBUTES
	FCapabilityCount DWORD
	FReserved        DWORD
}

type _SECURITY_CAPABILITIES = SECURITY_CAPABILITIES

type PSECURITY_CAPABILITIES = uintptr

type LPSECURITY_CAPABILITIES = uintptr

type JOB_SET_ARRAY = struct {
	FJobHandle   HANDLE
	FMemberLevel DWORD
	FFlags       DWORD
}

type _JOB_SET_ARRAY = JOB_SET_ARRAY

type PJOB_SET_ARRAY = uintptr

type EXCEPTION_REGISTRATION_RECORD = struct {
	F__ccgo0_0 struct {
		Fprev [0]uintptr
		FNext uintptr
	}
	F__ccgo1_8 struct {
		Fhandler [0]PEXCEPTION_ROUTINE
		FHandler PEXCEPTION_ROUTINE
	}
}

type _EXCEPTION_REGISTRATION_RECORD = EXCEPTION_REGISTRATION_RECORD

type PEXCEPTION_REGISTRATION_RECORD = uintptr

type EXCEPTION_REGISTRATION = struct {
	F__ccgo0_0 struct {
		Fprev [0]uintptr
		FNext uintptr
	}
	F__ccgo1_8 struct {
		Fhandler [0]PEXCEPTION_ROUTINE
		FHandler PEXCEPTION_ROUTINE
	}
}

type PEXCEPTION_REGISTRATION = uintptr

type NT_TIB = struct {
	FExceptionList uintptr
	FStackBase     PVOID
	FStackLimit    PVOID
	FSubSystemTib  PVOID
	F__ccgo4_32    struct {
		FVersion   [0]DWORD
		FFiberData PVOID
	}
	FArbitraryUserPointer PVOID
	FSelf                 uintptr
}

type _NT_TIB = NT_TIB

type PNT_TIB = uintptr

type NT_TIB32 = struct {
	FExceptionList DWORD
	FStackBase     DWORD
	FStackLimit    DWORD
	FSubSystemTib  DWORD
	F__ccgo4_16    struct {
		FVersion   [0]DWORD
		FFiberData DWORD
	}
	FArbitraryUserPointer DWORD
	FSelf                 DWORD
}

type _NT_TIB32 = NT_TIB32

type PNT_TIB32 = uintptr

type NT_TIB64 = struct {
	FExceptionList DWORD64
	FStackBase     DWORD64
	FStackLimit    DWORD64
	FSubSystemTib  DWORD64
	F__ccgo4_32    struct {
		FVersion   [0]DWORD
		FFiberData DWORD64
	}
	FArbitraryUserPointer DWORD64
	FSelf                 DWORD64
}

type _NT_TIB64 = NT_TIB64

type PNT_TIB64 = uintptr

type UMS_CREATE_THREAD_ATTRIBUTES = struct {
	FUmsVersion        DWORD
	FUmsContext        PVOID
	FUmsCompletionList PVOID
}

type _UMS_CREATE_THREAD_ATTRIBUTES = UMS_CREATE_THREAD_ATTRIBUTES

type PUMS_CREATE_THREAD_ATTRIBUTES = uintptr

type COMPONENT_FILTER = struct {
	FComponentFlags DWORD
}

type _COMPONENT_FILTER = COMPONENT_FILTER

type PCOMPONENT_FILTER = uintptr

type PROCESS_DYNAMIC_EH_CONTINUATION_TARGET = struct {
	FTargetAddress ULONG_PTR
	FFlags         ULONG_PTR
}

type _PROCESS_DYNAMIC_EH_CONTINUATION_TARGET = PROCESS_DYNAMIC_EH_CONTINUATION_TARGET

type PPROCESS_DYNAMIC_EH_CONTINUATION_TARGET = uintptr

type PROCESS_DYNAMIC_EH_CONTINUATION_TARGETS_INFORMATION = struct {
	FNumberOfTargets WORD
	FReserved        WORD
	FReserved2       DWORD
	FTargets         PPROCESS_DYNAMIC_EH_CONTINUATION_TARGET
}

type _PROCESS_DYNAMIC_EH_CONTINUATION_TARGETS_INFORMATION = PROCESS_DYNAMIC_EH_CONTINUATION_TARGETS_INFORMATION

type PPROCESS_DYNAMIC_EH_CONTINUATION_TARGETS_INFORMATION = uintptr

type PROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGE = struct {
	FBaseAddress ULONG_PTR
	FSize        SIZE_T
	FFlags       DWORD
}

type _PROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGE = PROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGE

type PPROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGE = uintptr

type PROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGES_INFORMATION = struct {
	FNumberOfRanges WORD
	FReserved       WORD
	FReserved2      DWORD
	FRanges         PPROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGE
}

type _PROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGES_INFORMATION = PROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGES_INFORMATION

type PPROCESS_DYNAMIC_ENFORCED_ADDRESS_RANGES_INFORMATION = uintptr

type QUOTA_LIMITS = struct {
	FPagedPoolLimit        SIZE_T
	FNonPagedPoolLimit     SIZE_T
	FMinimumWorkingSetSize SIZE_T
	FMaximumWorkingSetSize SIZE_T
	FPagefileLimit         SIZE_T
	FTimeLimit             LARGE_INTEGER
}

type _QUOTA_LIMITS = QUOTA_LIMITS

type PQUOTA_LIMITS = uintptr

type RATE_QUOTA_LIMIT = struct {
	F__ccgo1_0 [0]struct {
		F__ccgo0 uint32
	}
	FRateData DWORD
}

type _RATE_QUOTA_LIMIT = RATE_QUOTA_LIMIT

type PRATE_QUOTA_LIMIT = uintptr

type QUOTA_LIMITS_EX = struct {
	FPagedPoolLimit        SIZE_T
	FNonPagedPoolLimit     SIZE_T
	FMinimumWorkingSetSize SIZE_T
	FMaximumWorkingSetSize SIZE_T
	FPagefileLimit         SIZE_T
	FTimeLimit             LARGE_INTEGER
	FWorkingSetLimit       SIZE_T
	FReserved2             SIZE_T
	FReserved3             SIZE_T
	FReserved4             SIZE_T
	FFlags                 DWORD
	FCpuRateLimit          RATE_QUOTA_LIMIT
}

type _QUOTA_LIMITS_EX = QUOTA_LIMITS_EX

type PQUOTA_LIMITS_EX = uintptr

type IO_COUNTERS = struct {
	FReadOperationCount  ULONGLONG
	FWriteOperationCount ULONGLONG
	FOtherOperationCount ULONGLONG
	FReadTransferCount   ULONGLONG
	FWriteTransferCount  ULONGLONG
	FOtherTransferCount  ULONGLONG
}

type _IO_COUNTERS = IO_COUNTERS

type PIO_COUNTERS = uintptr

type HARDWARE_COUNTER_TYPE = int32

type _HARDWARE_COUNTER_TYPE = int32

const PMCCounter = 0
const MaxHardwareCounterType = 1

type PHARDWARE_COUNTER_TYPE = uintptr

type PROCESS_MITIGATION_POLICY = int32

type _PROCESS_MITIGATION_POLICY = int32

const ProcessDEPPolicy = 0
const ProcessASLRPolicy = 1
const ProcessDynamicCodePolicy = 2
const ProcessStrictHandleCheckPolicy = 3
const ProcessSystemCallDisablePolicy = 4
const ProcessMitigationOptionsMask = 5
const ProcessExtensionPointDisablePolicy = 6
const ProcessControlFlowGuardPolicy = 7
const ProcessSignaturePolicy = 8
const ProcessFontDisablePolicy = 9
const ProcessImageLoadPolicy = 10
const ProcessSystemCallFilterPolicy = 11
const ProcessPayloadRestrictionPolicy = 12
const ProcessChildProcessPolicy = 13
const ProcessSideChannelIsolationPolicy = 14
const ProcessUserShadowStackPolicy = 15
const ProcessRedirectionTrustPolicy = 16
const ProcessUserPointerAuthPolicy = 17
const ProcessSEHOPPolicy = 18
const MaxProcessMitigationPolicy = 19

type PPROCESS_MITIGATION_POLICY = uintptr

type PROCESS_MITIGATION_ASLR_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_ASLR_POLICY = PROCESS_MITIGATION_ASLR_POLICY

type PPROCESS_MITIGATION_ASLR_POLICY = uintptr

type PROCESS_MITIGATION_DEP_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
	FPermanent BOOLEAN
}

type _PROCESS_MITIGATION_DEP_POLICY = PROCESS_MITIGATION_DEP_POLICY

type PPROCESS_MITIGATION_DEP_POLICY = uintptr

type PROCESS_MITIGATION_SEHOP_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_SEHOP_POLICY = PROCESS_MITIGATION_SEHOP_POLICY

type PPROCESS_MITIGATION_SEHOP_POLICY = uintptr

type PROCESS_MITIGATION_STRICT_HANDLE_CHECK_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_STRICT_HANDLE_CHECK_POLICY = PROCESS_MITIGATION_STRICT_HANDLE_CHECK_POLICY

type PPROCESS_MITIGATION_STRICT_HANDLE_CHECK_POLICY = uintptr

type PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY = PROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY

type PPROCESS_MITIGATION_SYSTEM_CALL_DISABLE_POLICY = uintptr

type PROCESS_MITIGATION_EXTENSION_POINT_DISABLE_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_EXTENSION_POINT_DISABLE_POLICY = PROCESS_MITIGATION_EXTENSION_POINT_DISABLE_POLICY

type PPROCESS_MITIGATION_EXTENSION_POINT_DISABLE_POLICY = uintptr

type PROCESS_MITIGATION_CONTROL_FLOW_GUARD_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_CONTROL_FLOW_GUARD_POLICY = PROCESS_MITIGATION_CONTROL_FLOW_GUARD_POLICY

type PPROCESS_MITIGATION_CONTROL_FLOW_GUARD_POLICY = uintptr

type PROCESS_MITIGATION_BINARY_SIGNATURE_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_BINARY_SIGNATURE_POLICY = PROCESS_MITIGATION_BINARY_SIGNATURE_POLICY

type PPROCESS_MITIGATION_BINARY_SIGNATURE_POLICY = uintptr

type PROCESS_MITIGATION_DYNAMIC_CODE_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_DYNAMIC_CODE_POLICY = PROCESS_MITIGATION_DYNAMIC_CODE_POLICY

type PPROCESS_MITIGATION_DYNAMIC_CODE_POLICY = uintptr

type PROCESS_MITIGATION_FONT_DISABLE_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_FONT_DISABLE_POLICY = PROCESS_MITIGATION_FONT_DISABLE_POLICY

type PPROCESS_MITIGATION_FONT_DISABLE_POLICY = uintptr

type PROCESS_MITIGATION_IMAGE_LOAD_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_IMAGE_LOAD_POLICY = PROCESS_MITIGATION_IMAGE_LOAD_POLICY

type PPROCESS_MITIGATION_IMAGE_LOAD_POLICY = uintptr

type PROCESS_MITIGATION_SYSTEM_CALL_FILTER_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_SYSTEM_CALL_FILTER_POLICY = PROCESS_MITIGATION_SYSTEM_CALL_FILTER_POLICY

type PPROCESS_MITIGATION_SYSTEM_CALL_FILTER_POLICY = uintptr

type PROCESS_MITIGATION_PAYLOAD_RESTRICTION_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_PAYLOAD_RESTRICTION_POLICY = PROCESS_MITIGATION_PAYLOAD_RESTRICTION_POLICY

type PPROCESS_MITIGATION_PAYLOAD_RESTRICTION_POLICY = uintptr

type PROCESS_MITIGATION_CHILD_PROCESS_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_CHILD_PROCESS_POLICY = PROCESS_MITIGATION_CHILD_PROCESS_POLICY

type PPROCESS_MITIGATION_CHILD_PROCESS_POLICY = uintptr

type PROCESS_MITIGATION_SIDE_CHANNEL_ISOLATION_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_SIDE_CHANNEL_ISOLATION_POLICY = PROCESS_MITIGATION_SIDE_CHANNEL_ISOLATION_POLICY

type PPROCESS_MITIGATION_SIDE_CHANNEL_ISOLATION_POLICY = uintptr

type PROCESS_MITIGATION_USER_SHADOW_STACK_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_USER_SHADOW_STACK_POLICY = PROCESS_MITIGATION_USER_SHADOW_STACK_POLICY

type PPROCESS_MITIGATION_USER_SHADOW_STACK_POLICY = uintptr

type PROCESS_MITIGATION_USER_POINTER_AUTH_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_USER_POINTER_AUTH_POLICY = PROCESS_MITIGATION_USER_POINTER_AUTH_POLICY

type PPROCESS_MITIGATION_USER_POINTER_AUTH_POLICY = uintptr

type PROCESS_MITIGATION_REDIRECTION_TRUST_POLICY = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _PROCESS_MITIGATION_REDIRECTION_TRUST_POLICY = PROCESS_MITIGATION_REDIRECTION_TRUST_POLICY

type PPROCESS_MITIGATION_REDIRECTION_TRUST_POLICY = uintptr

type JOBOBJECT_BASIC_ACCOUNTING_INFORMATION = struct {
	FTotalUserTime             LARGE_INTEGER
	FTotalKernelTime           LARGE_INTEGER
	FThisPeriodTotalUserTime   LARGE_INTEGER
	FThisPeriodTotalKernelTime LARGE_INTEGER
	FTotalPageFaultCount       DWORD
	FTotalProcesses            DWORD
	FActiveProcesses           DWORD
	FTotalTerminatedProcesses  DWORD
}

type _JOBOBJECT_BASIC_ACCOUNTING_INFORMATION = JOBOBJECT_BASIC_ACCOUNTING_INFORMATION

type PJOBOBJECT_BASIC_ACCOUNTING_INFORMATION = uintptr

type JOBOBJECT_BASIC_LIMIT_INFORMATION = struct {
	FPerProcessUserTimeLimit LARGE_INTEGER
	FPerJobUserTimeLimit     LARGE_INTEGER
	FLimitFlags              DWORD
	FMinimumWorkingSetSize   SIZE_T
	FMaximumWorkingSetSize   SIZE_T
	FActiveProcessLimit      DWORD
	FAffinity                ULONG_PTR
	FPriorityClass           DWORD
	FSchedulingClass         DWORD
}

type _JOBOBJECT_BASIC_LIMIT_INFORMATION = JOBOBJECT_BASIC_LIMIT_INFORMATION

type PJOBOBJECT_BASIC_LIMIT_INFORMATION = uintptr

type JOBOBJECT_EXTENDED_LIMIT_INFORMATION = struct {
	FBasicLimitInformation JOBOBJECT_BASIC_LIMIT_INFORMATION
	FIoInfo                IO_COUNTERS
	FProcessMemoryLimit    SIZE_T
	FJobMemoryLimit        SIZE_T
	FPeakProcessMemoryUsed SIZE_T
	FPeakJobMemoryUsed     SIZE_T
}

type _JOBOBJECT_EXTENDED_LIMIT_INFORMATION = JOBOBJECT_EXTENDED_LIMIT_INFORMATION

type PJOBOBJECT_EXTENDED_LIMIT_INFORMATION = uintptr

type JOBOBJECT_BASIC_PROCESS_ID_LIST = struct {
	FNumberOfAssignedProcesses DWORD
	FNumberOfProcessIdsInList  DWORD
	FProcessIdList             [1]ULONG_PTR
}

type _JOBOBJECT_BASIC_PROCESS_ID_LIST = JOBOBJECT_BASIC_PROCESS_ID_LIST

type PJOBOBJECT_BASIC_PROCESS_ID_LIST = uintptr

type JOBOBJECT_BASIC_UI_RESTRICTIONS = struct {
	FUIRestrictionsClass DWORD
}

type _JOBOBJECT_BASIC_UI_RESTRICTIONS = JOBOBJECT_BASIC_UI_RESTRICTIONS

type PJOBOBJECT_BASIC_UI_RESTRICTIONS = uintptr

type JOBOBJECT_SECURITY_LIMIT_INFORMATION = struct {
	FSecurityLimitFlags DWORD
	FJobToken           HANDLE
	FSidsToDisable      PTOKEN_GROUPS
	FPrivilegesToDelete PTOKEN_PRIVILEGES
	FRestrictedSids     PTOKEN_GROUPS
}

type _JOBOBJECT_SECURITY_LIMIT_INFORMATION = JOBOBJECT_SECURITY_LIMIT_INFORMATION

type PJOBOBJECT_SECURITY_LIMIT_INFORMATION = uintptr

type JOBOBJECT_END_OF_JOB_TIME_INFORMATION = struct {
	FEndOfJobTimeAction DWORD
}

type _JOBOBJECT_END_OF_JOB_TIME_INFORMATION = JOBOBJECT_END_OF_JOB_TIME_INFORMATION

type PJOBOBJECT_END_OF_JOB_TIME_INFORMATION = uintptr

type JOBOBJECT_ASSOCIATE_COMPLETION_PORT = struct {
	FCompletionKey  PVOID
	FCompletionPort HANDLE
}

type _JOBOBJECT_ASSOCIATE_COMPLETION_PORT = JOBOBJECT_ASSOCIATE_COMPLETION_PORT

type PJOBOBJECT_ASSOCIATE_COMPLETION_PORT = uintptr

type JOBOBJECT_BASIC_AND_IO_ACCOUNTING_INFORMATION = struct {
	FBasicInfo JOBOBJECT_BASIC_ACCOUNTING_INFORMATION
	FIoInfo    IO_COUNTERS
}

type _JOBOBJECT_BASIC_AND_IO_ACCOUNTING_INFORMATION = JOBOBJECT_BASIC_AND_IO_ACCOUNTING_INFORMATION

type PJOBOBJECT_BASIC_AND_IO_ACCOUNTING_INFORMATION = uintptr

type JOBOBJECT_JOBSET_INFORMATION = struct {
	FMemberLevel DWORD
}

type _JOBOBJECT_JOBSET_INFORMATION = JOBOBJECT_JOBSET_INFORMATION

type PJOBOBJECT_JOBSET_INFORMATION = uintptr

type JOBOBJECT_RATE_CONTROL_TOLERANCE = int32

type _JOBOBJECT_RATE_CONTROL_TOLERANCE = int32

const ToleranceLow = 1
const ToleranceMedium = 2
const ToleranceHigh = 3

type JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL = int32

type _JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL = int32

const ToleranceIntervalShort = 1
const ToleranceIntervalMedium = 2
const ToleranceIntervalLong = 3

type JOBOBJECT_NOTIFICATION_LIMIT_INFORMATION = struct {
	FIoReadBytesLimit             DWORD64
	FIoWriteBytesLimit            DWORD64
	FPerJobUserTimeLimit          LARGE_INTEGER
	FJobMemoryLimit               DWORD64
	FRateControlTolerance         JOBOBJECT_RATE_CONTROL_TOLERANCE
	FRateControlToleranceInterval JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL
	FLimitFlags                   DWORD
}

type _JOBOBJECT_NOTIFICATION_LIMIT_INFORMATION = JOBOBJECT_NOTIFICATION_LIMIT_INFORMATION

type PJOBOBJECT_NOTIFICATION_LIMIT_INFORMATION = uintptr

type JOBOBJECT_NOTIFICATION_LIMIT_INFORMATION_2 = struct {
	FIoReadBytesLimit    DWORD64
	FIoWriteBytesLimit   DWORD64
	FPerJobUserTimeLimit LARGE_INTEGER
	F__ccgo3_24          struct {
		FJobMemoryLimit     [0]DWORD64
		FJobHighMemoryLimit DWORD64
	}
	F__ccgo4_32 struct {
		FCpuRateControlTolerance [0]JOBOBJECT_RATE_CONTROL_TOLERANCE
		FRateControlTolerance    JOBOBJECT_RATE_CONTROL_TOLERANCE
	}
	F__ccgo5_36 struct {
		FCpuRateControlToleranceInterval [0]JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL
		FRateControlToleranceInterval    JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL
	}
	FLimitFlags                      DWORD
	FIoRateControlTolerance          JOBOBJECT_RATE_CONTROL_TOLERANCE
	FJobLowMemoryLimit               DWORD64
	FIoRateControlToleranceInterval  JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL
	FNetRateControlTolerance         JOBOBJECT_RATE_CONTROL_TOLERANCE
	FNetRateControlToleranceInterval JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL
}

type JOBOBJECT_LIMIT_VIOLATION_INFORMATION = struct {
	FLimitFlags                DWORD
	FViolationLimitFlags       DWORD
	FIoReadBytes               DWORD64
	FIoReadBytesLimit          DWORD64
	FIoWriteBytes              DWORD64
	FIoWriteBytesLimit         DWORD64
	FPerJobUserTime            LARGE_INTEGER
	FPerJobUserTimeLimit       LARGE_INTEGER
	FJobMemory                 DWORD64
	FJobMemoryLimit            DWORD64
	FRateControlTolerance      JOBOBJECT_RATE_CONTROL_TOLERANCE
	FRateControlToleranceLimit JOBOBJECT_RATE_CONTROL_TOLERANCE_INTERVAL
}

type _JOBOBJECT_LIMIT_VIOLATION_INFORMATION = JOBOBJECT_LIMIT_VIOLATION_INFORMATION

type PJOBOBJECT_LIMIT_VIOLATION_INFORMATION = uintptr

type JOBOBJECT_LIMIT_VIOLATION_INFORMATION_2 = struct {
	FLimitFlags          DWORD
	FViolationLimitFlags DWORD
	FIoReadBytes         DWORD64
	FIoReadBytesLimit    DWORD64
	FIoWriteBytes        DWORD64
	FIoWriteBytesLimit   DWORD64
	FPerJobUserTime      LARGE_INTEGER
	FPerJobUserTimeLimit LARGE_INTEGER
	FJobMemory           DWORD64
	F__ccgo9_64          struct {
		FJobMemoryLimit     [0]DWORD64
		FJobHighMemoryLimit DWORD64
	}
	F__ccgo10_72 struct {
		FCpuRateControlTolerance [0]JOBOBJECT_RATE_CONTROL_TOLERANCE
		FRateControlTolerance    JOBOBJECT_RATE_CONTROL_TOLERANCE
	}
	F__ccgo11_76 struct {
		FCpuRateControlToleranceLimit [0]JOBOBJECT_RATE_CONTROL_TOLERANCE
		FRateControlToleranceLimit    JOBOBJECT_RATE_CONTROL_TOLERANCE
	}
	FJobLowMemoryLimit            DWORD64
	FIoRateControlTolerance       JOBOBJECT_RATE_CONTROL_TOLERANCE
	FIoRateControlToleranceLimit  JOBOBJECT_RATE_CONTROL_TOLERANCE
	FNetRateControlTolerance      JOBOBJECT_RATE_CONTROL_TOLERANCE
	FNetRateControlToleranceLimit JOBOBJECT_RATE_CONTROL_TOLERANCE
}

type JOBOBJECT_CPU_RATE_CONTROL_INFORMATION = struct {
	FControlFlags DWORD
	F__ccgo1_4    struct {
		FWeight  [0]DWORD
		FCpuRate DWORD
	}
}

type _JOBOBJECT_CPU_RATE_CONTROL_INFORMATION = JOBOBJECT_CPU_RATE_CONTROL_INFORMATION

type PJOBOBJECT_CPU_RATE_CONTROL_INFORMATION = uintptr

type JOB_OBJECT_NET_RATE_CONTROL_FLAGS1 = int32

type JOB_OBJECT_NET_RATE_CONTROL_FLAGS = int32

const JOB_OBJECT_NET_RATE_CONTROL_ENABLE = 1
const JOB_OBJECT_NET_RATE_CONTROL_MAX_BANDWIDTH = 2
const JOB_OBJECT_NET_RATE_CONTROL_DSCP_TAG = 4
const JOB_OBJECT_NET_RATE_CONTROL_VALID_FLAGS = 7

type JOBOBJECT_NET_RATE_CONTROL_INFORMATION = struct {
	FMaxBandwidth DWORD64
	FControlFlags JOB_OBJECT_NET_RATE_CONTROL_FLAGS1
	FDscpTag      BYTE
}

type JOB_OBJECT_IO_RATE_CONTROL_FLAGS1 = int32

type JOB_OBJECT_IO_RATE_CONTROL_FLAGS = int32

const JOB_OBJECT_IO_RATE_CONTROL_ENABLE = 1
const JOB_OBJECT_IO_RATE_CONTROL_STANDALONE_VOLUME = 2
const JOB_OBJECT_IO_RATE_CONTROL_FORCE_UNIT_ACCESS_ALL = 4
const JOB_OBJECT_IO_RATE_CONTROL_FORCE_UNIT_ACCESS_ON_SOFT_CAP = 8
const JOB_OBJECT_IO_RATE_CONTROL_VALID_FLAGS = 15

type JOBOBJECT_IO_RATE_CONTROL_INFORMATION_NATIVE = struct {
	FMaxIops          LONG64
	FMaxBandwidth     LONG64
	FReservationIops  LONG64
	FVolumeName       PWSTR
	FBaseIoSize       DWORD
	FControlFlags     JOB_OBJECT_IO_RATE_CONTROL_FLAGS1
	FVolumeNameLength WORD
}

type JOBOBJECT_IO_RATE_CONTROL_INFORMATION_NATIVE_V1 = struct {
	FMaxIops          LONG64
	FMaxBandwidth     LONG64
	FReservationIops  LONG64
	FVolumeName       PWSTR
	FBaseIoSize       DWORD
	FControlFlags     JOB_OBJECT_IO_RATE_CONTROL_FLAGS1
	FVolumeNameLength WORD
}

type JOBOBJECT_IO_RATE_CONTROL_INFORMATION_NATIVE_V2 = struct {
	FMaxIops                        LONG64
	FMaxBandwidth                   LONG64
	FReservationIops                LONG64
	FVolumeName                     PWSTR
	FBaseIoSize                     DWORD
	FControlFlags                   JOB_OBJECT_IO_RATE_CONTROL_FLAGS1
	FVolumeNameLength               WORD
	FCriticalReservationIops        LONG64
	FReservationBandwidth           LONG64
	FCriticalReservationBandwidth   LONG64
	FMaxTimePercent                 LONG64
	FReservationTimePercent         LONG64
	FCriticalReservationTimePercent LONG64
}

type JOBOBJECT_IO_RATE_CONTROL_INFORMATION_NATIVE_V3 = struct {
	FMaxIops                        LONG64
	FMaxBandwidth                   LONG64
	FReservationIops                LONG64
	FVolumeName                     PWSTR
	FBaseIoSize                     DWORD
	FControlFlags                   JOB_OBJECT_IO_RATE_CONTROL_FLAGS1
	FVolumeNameLength               WORD
	FCriticalReservationIops        LONG64
	FReservationBandwidth           LONG64
	FCriticalReservationBandwidth   LONG64
	FMaxTimePercent                 LONG64
	FReservationTimePercent         LONG64
	FCriticalReservationTimePercent LONG64
	FSoftMaxIops                    LONG64
	FSoftMaxBandwidth               LONG64
	FSoftMaxTimePercent             LONG64
	FLimitExcessNotifyIops          LONG64
	FLimitExcessNotifyBandwidth     LONG64
	FLimitExcessNotifyTimePercent   LONG64
}

type JOBOBJECT_IO_ATTRIBUTION_CONTROL_FLAGS1 = int32

type JOBOBJECT_IO_ATTRIBUTION_CONTROL_FLAGS = int32

const JOBOBJECT_IO_ATTRIBUTION_CONTROL_ENABLE = 1
const JOBOBJECT_IO_ATTRIBUTION_CONTROL_DISABLE = 2
const JOBOBJECT_IO_ATTRIBUTION_CONTROL_VALID_FLAGS = 3

type JOBOBJECT_IO_ATTRIBUTION_STATS = struct {
	FIoCount                       ULONG_PTR
	FTotalNonOverlappedQueueTime   ULONGLONG
	FTotalNonOverlappedServiceTime ULONGLONG
	FTotalSize                     ULONGLONG
}

type _JOBOBJECT_IO_ATTRIBUTION_STATS = JOBOBJECT_IO_ATTRIBUTION_STATS

type PJOBOBJECT_IO_ATTRIBUTION_STATS = uintptr

type JOBOBJECT_IO_ATTRIBUTION_INFORMATION = struct {
	FControlFlags DWORD
	FReadStats    JOBOBJECT_IO_ATTRIBUTION_STATS
	FWriteStats   JOBOBJECT_IO_ATTRIBUTION_STATS
}

type _JOBOBJECT_IO_ATTRIBUTION_INFORMATION = JOBOBJECT_IO_ATTRIBUTION_INFORMATION

type PJOBOBJECT_IO_ATTRIBUTION_INFORMATION = uintptr

type JOBOBJECTINFOCLASS = int32

type _JOBOBJECTINFOCLASS = int32

const JobObjectBasicAccountingInformation = 1
const JobObjectBasicLimitInformation = 2
const JobObjectBasicProcessIdList = 3
const JobObjectBasicUIRestrictions = 4
const JobObjectSecurityLimitInformation = 5
const JobObjectEndOfJobTimeInformation = 6
const JobObjectAssociateCompletionPortInformation = 7
const JobObjectBasicAndIoAccountingInformation = 8
const JobObjectExtendedLimitInformation = 9
const JobObjectJobSetInformation = 10
const JobObjectGroupInformation = 11
const JobObjectNotificationLimitInformation = 12
const JobObjectLimitViolationInformation = 13
const JobObjectGroupInformationEx = 14
const JobObjectCpuRateControlInformation = 15
const JobObjectCompletionFilter = 16
const JobObjectCompletionCounter = 17
const JobObjectReserved1Information = 18
const JobObjectReserved2Information = 19
const JobObjectReserved3Information = 20
const JobObjectReserved4Information = 21
const JobObjectReserved5Information = 22
const JobObjectReserved6Information = 23
const JobObjectReserved7Information = 24
const JobObjectReserved8Information = 25
const JobObjectReserved9Information = 26
const JobObjectReserved10Information = 27
const JobObjectReserved11Information = 28
const JobObjectReserved12Information = 29
const JobObjectReserved13Information = 30
const JobObjectReserved14Information = 31
const JobObjectNetRateControlInformation = 32
const JobObjectNotificationLimitInformation2 = 33
const JobObjectLimitViolationInformation2 = 34
const JobObjectCreateSilo = 35
const JobObjectSiloBasicInformation = 36
const JobObjectReserved15Information = 37
const JobObjectReserved16Information = 38
const JobObjectReserved17Information = 39
const JobObjectReserved18Information = 40
const JobObjectReserved19Information = 41
const JobObjectReserved20Information = 42
const JobObjectReserved21Information = 43
const JobObjectReserved22Information = 44
const JobObjectReserved23Information = 45
const JobObjectReserved24Information = 46
const JobObjectReserved25Information = 47
const MaxJobObjectInfoClass = 48

type SILOOBJECT_BASIC_INFORMATION = struct {
	FSiloId            DWORD
	FSiloParentId      DWORD
	FNumberOfProcesses DWORD
	FIsInServerSilo    BOOLEAN
	FReserved          [3]BYTE
}

type _SILOOBJECT_BASIC_INFORMATION = SILOOBJECT_BASIC_INFORMATION

type PSILOOBJECT_BASIC_INFORMATION = uintptr

type SERVERSILO_STATE = int32

type _SERVERSILO_STATE = int32

const SERVERSILO_INITING = 0
const SERVERSILO_STARTED = 1
const SERVERSILO_SHUTTING_DOWN = 2
const SERVERSILO_TERMINATING = 3
const SERVERSILO_TERMINATED = 4

type PSERVERSILO_STATE = uintptr

type SERVERSILO_BASIC_INFORMATION = struct {
	FServiceSessionId     DWORD
	FState                SERVERSILO_STATE
	FExitStatus           DWORD
	FIsDownlevelContainer BOOLEAN
	FApiSetSchema         PVOID
	FHostApiSetSchema     PVOID
}

type _SERVERSILO_BASIC_INFORMATION = SERVERSILO_BASIC_INFORMATION

type PSERVERSILO_BASIC_INFORMATION = uintptr

type FIRMWARE_TYPE = int32

type _FIRMWARE_TYPE = int32

const FirmwareTypeUnknown = 0
const FirmwareTypeBios = 1
const FirmwareTypeUefi = 2
const FirmwareTypeMax = 3

type PFIRMWARE_TYPE = uintptr

type LOGICAL_PROCESSOR_RELATIONSHIP = int32

type _LOGICAL_PROCESSOR_RELATIONSHIP = int32

const RelationProcessorCore = 0
const RelationNumaNode = 1
const RelationCache = 2
const RelationProcessorPackage = 3
const RelationGroup = 4
const RelationProcessorDie = 5
const RelationNumaNodeEx = 6
const RelationProcessorModule = 7
const RelationAll = 65535

type PROCESSOR_CACHE_TYPE = int32

type _PROCESSOR_CACHE_TYPE = int32

const CacheUnified = 0
const CacheInstruction = 1
const CacheData = 2
const CacheTrace = 3
const CacheUnknown = 4

type CACHE_DESCRIPTOR = struct {
	FLevel         BYTE
	FAssociativity BYTE
	FLineSize      WORD
	FSize          DWORD
	FType          PROCESSOR_CACHE_TYPE
}

type _CACHE_DESCRIPTOR = CACHE_DESCRIPTOR

type PCACHE_DESCRIPTOR = uintptr

type SYSTEM_LOGICAL_PROCESSOR_INFORMATION = struct {
	FProcessorMask ULONG_PTR
	FRelationship  LOGICAL_PROCESSOR_RELATIONSHIP
	F__ccgo2_16    struct {
		FNumaNode [0]struct {
			FNodeNumber DWORD
		}
		FCache         [0]CACHE_DESCRIPTOR
		FReserved      [0][2]ULONGLONG
		FProcessorCore struct {
			FFlags BYTE
		}
		F__ccgo_pad4 [15]byte
	}
}

type _SYSTEM_LOGICAL_PROCESSOR_INFORMATION = SYSTEM_LOGICAL_PROCESSOR_INFORMATION

type PSYSTEM_LOGICAL_PROCESSOR_INFORMATION = uintptr

type PROCESSOR_RELATIONSHIP = struct {
	FFlags           BYTE
	FEfficiencyClass BYTE
	FReserved        [20]BYTE
	FGroupCount      WORD
	FGroupMask       [1]GROUP_AFFINITY
}

type _PROCESSOR_RELATIONSHIP = PROCESSOR_RELATIONSHIP

type PPROCESSOR_RELATIONSHIP = uintptr

type NUMA_NODE_RELATIONSHIP = struct {
	FNodeNumber DWORD
	FReserved   [18]BYTE
	FGroupCount WORD
	F__ccgo3_24 struct {
		FGroupMasks [0][1]GROUP_AFFINITY
		FGroupMask  GROUP_AFFINITY
	}
}

type _NUMA_NODE_RELATIONSHIP = NUMA_NODE_RELATIONSHIP

type PNUMA_NODE_RELATIONSHIP = uintptr

type CACHE_RELATIONSHIP = struct {
	FLevel         BYTE
	FAssociativity BYTE
	FLineSize      WORD
	FCacheSize     DWORD
	FType          PROCESSOR_CACHE_TYPE
	FReserved      [18]BYTE
	FGroupCount    WORD
	F__ccgo7_32    struct {
		FGroupMasks [0][1]GROUP_AFFINITY
		FGroupMask  GROUP_AFFINITY
	}
}

type _CACHE_RELATIONSHIP = CACHE_RELATIONSHIP

type PCACHE_RELATIONSHIP = uintptr

type PROCESSOR_GROUP_INFO = struct {
	FMaximumProcessorCount BYTE
	FActiveProcessorCount  BYTE
	FReserved              [38]BYTE
	FActiveProcessorMask   KAFFINITY
}

type _PROCESSOR_GROUP_INFO = PROCESSOR_GROUP_INFO

type PPROCESSOR_GROUP_INFO = uintptr

type GROUP_RELATIONSHIP = struct {
	FMaximumGroupCount WORD
	FActiveGroupCount  WORD
	FReserved          [20]BYTE
	FGroupInfo         [1]PROCESSOR_GROUP_INFO
}

type _GROUP_RELATIONSHIP = GROUP_RELATIONSHIP

type PGROUP_RELATIONSHIP = uintptr

type _SYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX = struct {
	FRelationship LOGICAL_PROCESSOR_RELATIONSHIP
	FSize         DWORD
	F__ccgo2_8    struct {
		FNumaNode    [0]NUMA_NODE_RELATIONSHIP
		FCache       [0]CACHE_RELATIONSHIP
		FGroup       [0]GROUP_RELATIONSHIP
		FProcessor   PROCESSOR_RELATIONSHIP
		F__ccgo_pad4 [32]byte
	}
}

type SYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX = struct {
	FRelationship LOGICAL_PROCESSOR_RELATIONSHIP
	FSize         DWORD
	F__ccgo2_8    struct {
		FNumaNode    [0]NUMA_NODE_RELATIONSHIP
		FCache       [0]CACHE_RELATIONSHIP
		FGroup       [0]GROUP_RELATIONSHIP
		FProcessor   PROCESSOR_RELATIONSHIP
		F__ccgo_pad4 [32]byte
	}
}

type PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX = uintptr

type CPU_SET_INFORMATION_TYPE = int32

type _CPU_SET_INFORMATION_TYPE = int32

const CpuSetInformation = 0

type PCPU_SET_INFORMATION_TYPE = uintptr

type _SYSTEM_CPU_SET_INFORMATION = struct {
	FSize      DWORD
	FType      CPU_SET_INFORMATION_TYPE
	F__ccgo2_8 struct {
		FCpuSet struct {
			FId                    DWORD
			FGroup                 WORD
			FLogicalProcessorIndex BYTE
			FCoreIndex             BYTE
			FLastLevelCacheIndex   BYTE
			FNumaNodeIndex         BYTE
			FEfficiencyClass       BYTE
			F__ccgo7_11            struct {
				F__ccgo1_0 [0]struct {
					F__ccgo0 uint8
				}
				FAllFlags BYTE
			}
			F__ccgo8_12 struct {
				FSchedulingClass [0]BYTE
				FReserved        DWORD
			}
			FAllocationTag DWORD64
		}
	}
}

type SYSTEM_CPU_SET_INFORMATION = struct {
	FSize      DWORD
	FType      CPU_SET_INFORMATION_TYPE
	F__ccgo2_8 struct {
		FCpuSet struct {
			FId                    DWORD
			FGroup                 WORD
			FLogicalProcessorIndex BYTE
			FCoreIndex             BYTE
			FLastLevelCacheIndex   BYTE
			FNumaNodeIndex         BYTE
			FEfficiencyClass       BYTE
			F__ccgo7_11            struct {
				F__ccgo1_0 [0]struct {
					F__ccgo0 uint8
				}
				FAllFlags BYTE
			}
			F__ccgo8_12 struct {
				FSchedulingClass [0]BYTE
				FReserved        DWORD
			}
			FAllocationTag DWORD64
		}
	}
}

type PSYSTEM_CPU_SET_INFORMATION = uintptr

type SYSTEM_POOL_ZEROING_INFORMATION = struct {
	FPoolZeroingSupportPresent BOOLEAN
}

type _SYSTEM_POOL_ZEROING_INFORMATION = SYSTEM_POOL_ZEROING_INFORMATION

type PSYSTEM_POOL_ZEROING_INFORMATION = uintptr

type SYSTEM_PROCESSOR_CYCLE_TIME_INFORMATION = struct {
	FCycleTime DWORD64
}

type _SYSTEM_PROCESSOR_CYCLE_TIME_INFORMATION = SYSTEM_PROCESSOR_CYCLE_TIME_INFORMATION

type PSYSTEM_PROCESSOR_CYCLE_TIME_INFORMATION = uintptr

type SYSTEM_SUPPORTED_PROCESSOR_ARCHITECTURES_INFORMATION = struct {
	F__ccgo0 uint32
}

type _SYSTEM_SUPPORTED_PROCESSOR_ARCHITECTURES_INFORMATION = SYSTEM_SUPPORTED_PROCESSOR_ARCHITECTURES_INFORMATION

type PSYSTEM_SUPPORTED_PROCESSOR_ARCHITECTURES_INFORMATION = uintptr

type XSTATE_FEATURE = struct {
	FOffset DWORD
	FSize   DWORD
}

type _XSTATE_FEATURE = XSTATE_FEATURE

type PXSTATE_FEATURE = uintptr

type XSTATE_CONFIGURATION = struct {
	FEnabledFeatures         DWORD64
	FEnabledVolatileFeatures DWORD64
	FSize                    DWORD
	F__ccgo3_20              struct {
		F__ccgo1_0 [0]struct {
			F__ccgo_align [0]uint32
			F__ccgo0      uint8
		}
		FControlFlags DWORD
	}
	FFeatures                             [64]XSTATE_FEATURE
	FEnabledSupervisorFeatures            DWORD64
	FAlignedFeatures                      DWORD64
	FAllFeatureSize                       DWORD
	FAllFeatures                          [64]DWORD
	FEnabledUserVisibleSupervisorFeatures DWORD64
	FExtendedFeatureDisableFeatures       DWORD64
	FAllNonLargeFeatureSize               DWORD
	FSpare                                DWORD
}

type _XSTATE_CONFIGURATION = XSTATE_CONFIGURATION

type PXSTATE_CONFIGURATION = uintptr

type MEMORY_BASIC_INFORMATION = struct {
	FBaseAddress       PVOID
	FAllocationBase    PVOID
	FAllocationProtect DWORD
	FPartitionId       WORD
	FRegionSize        SIZE_T
	FState             DWORD
	FProtect           DWORD
	FType              DWORD
}

type _MEMORY_BASIC_INFORMATION = MEMORY_BASIC_INFORMATION

type PMEMORY_BASIC_INFORMATION = uintptr

type MEMORY_BASIC_INFORMATION32 = struct {
	FBaseAddress       DWORD
	FAllocationBase    DWORD
	FAllocationProtect DWORD
	FRegionSize        DWORD
	FState             DWORD
	FProtect           DWORD
	FType              DWORD
}

type _MEMORY_BASIC_INFORMATION32 = MEMORY_BASIC_INFORMATION32

type PMEMORY_BASIC_INFORMATION32 = uintptr

type MEMORY_BASIC_INFORMATION64 = struct {
	FBaseAddress       ULONGLONG
	FAllocationBase    ULONGLONG
	FAllocationProtect DWORD
	F__alignment1      DWORD
	FRegionSize        ULONGLONG
	FState             DWORD
	FProtect           DWORD
	FType              DWORD
	F__alignment2      DWORD
}

type _MEMORY_BASIC_INFORMATION64 = MEMORY_BASIC_INFORMATION64

type PMEMORY_BASIC_INFORMATION64 = uintptr

type CFG_CALL_TARGET_INFO = struct {
	FOffset ULONG_PTR
	FFlags  ULONG_PTR
}

type _CFG_CALL_TARGET_INFO = CFG_CALL_TARGET_INFO

type PCFG_CALL_TARGET_INFO = uintptr

type MEM_ADDRESS_REQUIREMENTS = struct {
	FLowestStartingAddress PVOID
	FHighestEndingAddress  PVOID
	FAlignment             SIZE_T
}

type _MEM_ADDRESS_REQUIREMENTS = MEM_ADDRESS_REQUIREMENTS

type PMEM_ADDRESS_REQUIREMENTS = uintptr

type MEM_EXTENDED_PARAMETER_TYPE1 = int32

type MEM_EXTENDED_PARAMETER_TYPE = int32

const MemExtendedParameterInvalidType = 0
const MemExtendedParameterAddressRequirements = 1
const MemExtendedParameterNumaNode = 2
const MemExtendedParameterPartitionHandle = 3
const MemExtendedParameterUserPhysicalHandle = 4
const MemExtendedParameterAttributeFlags = 5
const MemExtendedParameterImageMachine = 6
const MemExtendedParameterMax = 7

type PMEM_EXTENDED_PARAMETER_TYPE = uintptr

type MEM_EXTENDED_PARAMETER = struct {
	F__ccgo0_0 struct {
		F__ccgo0 uint64
	}
	F__ccgo1_8 struct {
		FPointer [0]PVOID
		FSize    [0]SIZE_T
		FHandle  [0]HANDLE
		FULong   [0]DWORD
		FULong64 DWORD64
	}
}

type PMEM_EXTENDED_PARAMETER = uintptr

type MEM_DEDICATED_ATTRIBUTE_TYPE = int32

type _MEM_DEDICATED_ATTRIBUTE_TYPE = int32

const MemDedicatedAttributeReadBandwidth = 0
const MemDedicatedAttributeReadLatency = 1
const MemDedicatedAttributeWriteBandwidth = 2
const MemDedicatedAttributeWriteLatency = 3
const MemDedicatedAttributeMax = 4

type PMEM_DEDICATED_ATTRIBUTE_TYPE = uintptr

type MEM_SECTION_EXTENDED_PARAMETER_TYPE1 = int32

type MEM_SECTION_EXTENDED_PARAMETER_TYPE = int32

const MemSectionExtendedParameterInvalidType = 0
const MemSectionExtendedParameterUserPhysicalFlags = 1
const MemSectionExtendedParameterNumaNode = 2
const MemSectionExtendedParameterSigningLevel = 3
const MemSectionExtendedParameterMax = 4

type PMEM_SECTION_EXTENDED_PARAMETER_TYPE = uintptr

type ENCLAVE_CREATE_INFO_SGX = struct {
	FSecs [4096]BYTE
}

type _ENCLAVE_CREATE_INFO_SGX = ENCLAVE_CREATE_INFO_SGX

type PENCLAVE_CREATE_INFO_SGX = uintptr

type ENCLAVE_INIT_INFO_SGX = struct {
	FSigStruct  [1808]BYTE
	FReserved1  [240]BYTE
	FEInitToken [304]BYTE
	FReserved2  [1744]BYTE
}

type _ENCLAVE_INIT_INFO_SGX = ENCLAVE_INIT_INFO_SGX

type PENCLAVE_INIT_INFO_SGX = uintptr

type ENCLAVE_CREATE_INFO_VBS = struct {
	FFlags   DWORD
	FOwnerID [32]BYTE
}

type _ENCLAVE_CREATE_INFO_VBS = ENCLAVE_CREATE_INFO_VBS

type PENCLAVE_CREATE_INFO_VBS = uintptr

type ENCLAVE_CREATE_INFO_VBS_BASIC = struct {
	FFlags   DWORD
	FOwnerID [32]BYTE
}

type _ENCLAVE_CREATE_INFO_VBS_BASIC = ENCLAVE_CREATE_INFO_VBS_BASIC

type PENCLAVE_CREATE_INFO_VBS_BASIC = uintptr

type ENCLAVE_LOAD_DATA_VBS_BASIC = struct {
	FPageType DWORD
}

type _ENCLAVE_LOAD_DATA_VBS_BASIC = ENCLAVE_LOAD_DATA_VBS_BASIC

type PENCLAVE_LOAD_DATA_VBS_BASIC = uintptr

type ENCLAVE_INIT_INFO_VBS_BASIC = struct {
	FFamilyId    [16]BYTE
	FImageId     [16]BYTE
	FEnclaveSize ULONGLONG
	FEnclaveSvn  DWORD
	FReserved    DWORD
	F__ccgo5_48  struct {
		FUnused              [0]ULONGLONG
		FSignatureInfoHandle HANDLE
	}
}

type _ENCLAVE_INIT_INFO_VBS_BASIC = ENCLAVE_INIT_INFO_VBS_BASIC

type PENCLAVE_INIT_INFO_VBS_BASIC = uintptr

type ENCLAVE_INIT_INFO_VBS = struct {
	FLength      DWORD
	FThreadCount DWORD
}

type _ENCLAVE_INIT_INFO_VBS = ENCLAVE_INIT_INFO_VBS

type PENCLAVE_INIT_INFO_VBS = uintptr

type PENCLAVE_TARGET_FUNCTION = uintptr

type LPENCLAVE_TARGET_FUNCTION = uintptr

type MEMORY_PARTITION_DEDICATED_MEMORY_ATTRIBUTE = struct {
	FType     MEM_DEDICATED_ATTRIBUTE_TYPE
	FReserved DWORD
	FValue    DWORD64
}

type _MEMORY_PARTITION_DEDICATED_MEMORY_ATTRIBUTE = MEMORY_PARTITION_DEDICATED_MEMORY_ATTRIBUTE

type PMEMORY_PARTITION_DEDICATED_MEMORY_ATTRIBUTE = uintptr

type MEMORY_PARTITION_DEDICATED_MEMORY_INFORMATION = struct {
	FNextEntryOffset   DWORD
	FSizeOfInformation DWORD
	FFlags             DWORD
	FAttributesOffset  DWORD
	FAttributeCount    DWORD
	FReserved          DWORD
	FTypeId            DWORD64
}

type _MEMORY_PARTITION_DEDICATED_MEMORY_INFORMATION = MEMORY_PARTITION_DEDICATED_MEMORY_INFORMATION

type PMEMORY_PARTITION_DEDICATED_MEMORY_INFORMATION = uintptr

type FILE_ID_128 = struct {
	FIdentifier [16]BYTE
}

type _FILE_ID_128 = FILE_ID_128

type PFILE_ID_128 = uintptr

type FILE_NOTIFY_INFORMATION = struct {
	FNextEntryOffset DWORD
	FAction          DWORD
	FFileNameLength  DWORD
	FFileName        [1]WCHAR
}

type _FILE_NOTIFY_INFORMATION = FILE_NOTIFY_INFORMATION

type PFILE_NOTIFY_INFORMATION = uintptr

type FILE_NOTIFY_EXTENDED_INFORMATION = struct {
	FNextEntryOffset      DWORD
	FAction               DWORD
	FCreationTime         LARGE_INTEGER
	FLastModificationTime LARGE_INTEGER
	FLastChangeTime       LARGE_INTEGER
	FLastAccessTime       LARGE_INTEGER
	FAllocatedLength      LARGE_INTEGER
	FFileSize             LARGE_INTEGER
	FFileAttributes       DWORD
	F__ccgo9_60           struct {
		FEaSize          [0]DWORD
		FReparsePointTag DWORD
	}
	FFileId         LARGE_INTEGER
	FParentFileId   LARGE_INTEGER
	FFileNameLength DWORD
	FFileName       [1]WCHAR
}

type _FILE_NOTIFY_EXTENDED_INFORMATION = FILE_NOTIFY_EXTENDED_INFORMATION

type PFILE_NOTIFY_EXTENDED_INFORMATION = uintptr

type FILE_NOTIFY_FULL_INFORMATION = struct {
	FNextEntryOffset      DWORD
	FAction               DWORD
	FCreationTime         LARGE_INTEGER
	FLastModificationTime LARGE_INTEGER
	FLastChangeTime       LARGE_INTEGER
	FLastAccessTime       LARGE_INTEGER
	FAllocatedLength      LARGE_INTEGER
	FFileSize             LARGE_INTEGER
	FFileAttributes       DWORD
	F__ccgo9_60           struct {
		FEaSize          [0]DWORD
		FReparsePointTag DWORD
	}
	FFileId         LARGE_INTEGER
	FParentFileId   LARGE_INTEGER
	FFileNameLength WORD
	FFileNameFlags  BYTE
	FReserved       BYTE
	FFileName       [1]WCHAR
}

type _FILE_NOTIFY_FULL_INFORMATION = FILE_NOTIFY_FULL_INFORMATION

type PFILE_NOTIFY_FULL_INFORMATION = uintptr

type FILE_STAT_INFORMATION = struct {
	FFileId          LARGE_INTEGER
	FCreationTime    LARGE_INTEGER
	FLastAccessTime  LARGE_INTEGER
	FLastWriteTime   LARGE_INTEGER
	FChangeTime      LARGE_INTEGER
	FAllocationSize  LARGE_INTEGER
	FEndOfFile       LARGE_INTEGER
	FFileAttributes  DWORD
	FReparseTag      DWORD
	FNumberOfLinks   DWORD
	FEffectiveAccess ACCESS_MASK
}

type _FILE_STAT_INFORMATION = FILE_STAT_INFORMATION

type PFILE_STAT_INFORMATION = uintptr

type FILE_STAT_LX_INFORMATION = struct {
	FFileId          LARGE_INTEGER
	FCreationTime    LARGE_INTEGER
	FLastAccessTime  LARGE_INTEGER
	FLastWriteTime   LARGE_INTEGER
	FChangeTime      LARGE_INTEGER
	FAllocationSize  LARGE_INTEGER
	FEndOfFile       LARGE_INTEGER
	FFileAttributes  DWORD
	FReparseTag      DWORD
	FNumberOfLinks   DWORD
	FEffectiveAccess ACCESS_MASK
	FLxFlags         DWORD
	FLxUid           DWORD
	FLxGid           DWORD
	FLxMode          DWORD
	FLxDeviceIdMajor DWORD
	FLxDeviceIdMinor DWORD
}

type _FILE_STAT_LX_INFORMATION = FILE_STAT_LX_INFORMATION

type PFILE_STAT_LX_INFORMATION = uintptr

type FILE_CASE_SENSITIVE_INFORMATION = struct {
	FFlags DWORD
}

type _FILE_CASE_SENSITIVE_INFORMATION = FILE_CASE_SENSITIVE_INFORMATION

type PFILE_CASE_SENSITIVE_INFORMATION = uintptr

type FILE_SEGMENT_ELEMENT = struct {
	FAlignment [0]ULONGLONG
	FBuffer    PVOID64
}

type _FILE_SEGMENT_ELEMENT = FILE_SEGMENT_ELEMENT

type PFILE_SEGMENT_ELEMENT = uintptr

type REPARSE_GUID_DATA_BUFFER = struct {
	FReparseTag           DWORD
	FReparseDataLength    WORD
	FReserved             WORD
	FReparseGuid          GUID
	FGenericReparseBuffer struct {
		FDataBuffer [1]BYTE
	}
}

type _REPARSE_GUID_DATA_BUFFER = REPARSE_GUID_DATA_BUFFER

type PREPARSE_GUID_DATA_BUFFER = uintptr

type SYSTEM_POWER_STATE = int32

type _SYSTEM_POWER_STATE = int32

const PowerSystemUnspecified = 0
const PowerSystemWorking = 1
const PowerSystemSleeping1 = 2
const PowerSystemSleeping2 = 3
const PowerSystemSleeping3 = 4
const PowerSystemHibernate = 5
const PowerSystemShutdown = 6
const PowerSystemMaximum = 7

type PSYSTEM_POWER_STATE = uintptr

type POWER_ACTION = int32

const PowerActionNone = 0
const PowerActionReserved = 1
const PowerActionSleep = 2
const PowerActionHibernate = 3
const PowerActionShutdown = 4
const PowerActionShutdownReset = 5
const PowerActionShutdownOff = 6
const PowerActionWarmEject = 7

type PPOWER_ACTION = uintptr

type DEVICE_POWER_STATE = int32

type _DEVICE_POWER_STATE = int32

const PowerDeviceUnspecified = 0
const PowerDeviceD0 = 1
const PowerDeviceD1 = 2
const PowerDeviceD2 = 3
const PowerDeviceD3 = 4
const PowerDeviceMaximum = 5

type PDEVICE_POWER_STATE = uintptr

type MONITOR_DISPLAY_STATE = int32

type _MONITOR_DISPLAY_STATE = int32

const PowerMonitorOff = 0
const PowerMonitorOn = 1
const PowerMonitorDim = 2

type PMONITOR_DISPLAY_STATE = uintptr

type USER_ACTIVITY_PRESENCE = int32

type _USER_ACTIVITY_PRESENCE = int32

const PowerUserPresent = 0
const PowerUserNotPresent = 1
const PowerUserInactive = 2
const PowerUserMaximum = 3
const PowerUserInvalid = 3

type PUSER_ACTIVITY_PRESENCE = uintptr

type EXECUTION_STATE = uint32

type PEXECUTION_STATE = uintptr

type LATENCY_TIME = int32

const LT_DONT_CARE = 0
const LT_LOWEST_LATENCY = 1

type POWER_REQUEST_TYPE = int32

type _POWER_REQUEST_TYPE = int32

const PowerRequestDisplayRequired = 0
const PowerRequestSystemRequired = 1
const PowerRequestAwayModeRequired = 2
const PowerRequestExecutionRequired = 3

type PPOWER_REQUEST_TYPE = uintptr

type CM_POWER_DATA = struct {
	FPD_Size                 DWORD
	FPD_MostRecentPowerState DEVICE_POWER_STATE
	FPD_Capabilities         DWORD
	FPD_D1Latency            DWORD
	FPD_D2Latency            DWORD
	FPD_D3Latency            DWORD
	FPD_PowerStateMapping    [7]DEVICE_POWER_STATE
	FPD_DeepestSystemWake    SYSTEM_POWER_STATE
}

type CM_Power_Data_s = CM_POWER_DATA

type PCM_POWER_DATA = uintptr

type POWER_INFORMATION_LEVEL = int32

const SystemPowerPolicyAc = 0
const SystemPowerPolicyDc = 1
const VerifySystemPolicyAc = 2
const VerifySystemPolicyDc = 3
const SystemPowerCapabilities = 4
const SystemBatteryState = 5
const SystemPowerStateHandler = 6
const ProcessorStateHandler = 7
const SystemPowerPolicyCurrent = 8
const AdministratorPowerPolicy = 9
const SystemReserveHiberFile = 10
const ProcessorInformation = 11
const SystemPowerInformation = 12
const ProcessorStateHandler2 = 13
const LastWakeTime = 14
const LastSleepTime = 15
const SystemExecutionState = 16
const SystemPowerStateNotifyHandler = 17
const ProcessorPowerPolicyAc = 18
const ProcessorPowerPolicyDc = 19
const VerifyProcessorPowerPolicyAc = 20
const VerifyProcessorPowerPolicyDc = 21
const ProcessorPowerPolicyCurrent = 22
const SystemPowerStateLogging = 23
const SystemPowerLoggingEntry = 24
const SetPowerSettingValue = 25
const NotifyUserPowerSetting = 26
const PowerInformationLevelUnused0 = 27
const SystemMonitorHiberBootPowerOff = 28
const SystemVideoState = 29
const TraceApplicationPowerMessage = 30
const TraceApplicationPowerMessageEnd = 31
const ProcessorPerfStates = 32
const ProcessorIdleStates = 33
const ProcessorCap = 34
const SystemWakeSource = 35
const SystemHiberFileInformation = 36
const TraceServicePowerMessage = 37
const ProcessorLoad = 38
const PowerShutdownNotification = 39
const MonitorCapabilities = 40
const SessionPowerInit = 41
const SessionDisplayState = 42
const PowerRequestCreate = 43
const PowerRequestAction = 44
const GetPowerRequestList = 45
const ProcessorInformationEx = 46
const NotifyUserModeLegacyPowerEvent = 47
const GroupPark = 48
const ProcessorIdleDomains = 49
const WakeTimerList = 50
const SystemHiberFileSize = 51
const ProcessorIdleStatesHv = 52
const ProcessorPerfStatesHv = 53
const ProcessorPerfCapHv = 54
const ProcessorSetIdle = 55
const LogicalProcessorIdling = 56
const UserPresence = 57
const PowerSettingNotificationName = 58
const GetPowerSettingValue = 59
const IdleResiliency = 60
const SessionRITState = 61
const SessionConnectNotification = 62
const SessionPowerCleanup = 63
const SessionLockState = 64
const SystemHiberbootState = 65
const PlatformInformation = 66
const PdcInvocation = 67
const MonitorInvocation = 68
const FirmwareTableInformationRegistered = 69
const SetShutdownSelectedTime = 70
const SuspendResumeInvocation = 71
const PlmPowerRequestCreate = 72
const ScreenOff = 73
const CsDeviceNotification = 74
const PlatformRole = 75
const LastResumePerformance = 76
const DisplayBurst = 77
const ExitLatencySamplingPercentage = 78
const ApplyLowPowerScenarioSettings = 79
const PowerInformationLevelMaximum = 80

type POWER_USER_PRESENCE_TYPE = int32

const UserNotPresent = 0
const UserPresent = 1
const UserUnknown = 255

type PPOWER_USER_PRESENCE_TYPE = uintptr

type POWER_USER_PRESENCE = struct {
	FUserPresence POWER_USER_PRESENCE_TYPE
}

type _POWER_USER_PRESENCE = POWER_USER_PRESENCE

type PPOWER_USER_PRESENCE = uintptr

type POWER_SESSION_CONNECT = struct {
	FConnected BOOLEAN
	FConsole   BOOLEAN
}

type _POWER_SESSION_CONNECT = POWER_SESSION_CONNECT

type PPOWER_SESSION_CONNECT = uintptr

type POWER_SESSION_TIMEOUTS = struct {
	FInputTimeout   DWORD
	FDisplayTimeout DWORD
}

type _POWER_SESSION_TIMEOUTS = POWER_SESSION_TIMEOUTS

type PPOWER_SESSION_TIMEOUTS = uintptr

type POWER_SESSION_RIT_STATE = struct {
	FActive        BOOLEAN
	FLastInputTime DWORD
}

type _POWER_SESSION_RIT_STATE = POWER_SESSION_RIT_STATE

type PPOWER_SESSION_RIT_STATE = uintptr

type POWER_SESSION_WINLOGON = struct {
	FSessionId DWORD
	FConsole   BOOLEAN
	FLocked    BOOLEAN
}

type _POWER_SESSION_WINLOGON = POWER_SESSION_WINLOGON

type PPOWER_SESSION_WINLOGON = uintptr

type POWER_IDLE_RESILIENCY = struct {
	FCoalescingTimeout    DWORD
	FIdleResiliencyPeriod DWORD
}

type _POWER_IDLE_RESILIENCY = POWER_IDLE_RESILIENCY

type PPOWER_IDLE_RESILIENCY = uintptr

type POWER_MONITOR_REQUEST_REASON = int32

const MonitorRequestReasonUnknown = 0
const MonitorRequestReasonPowerButton = 1
const MonitorRequestReasonRemoteConnection = 2
const MonitorRequestReasonScMonitorpower = 3
const MonitorRequestReasonUserInput = 4
const MonitorRequestReasonAcDcDisplayBurst = 5
const MonitorRequestReasonUserDisplayBurst = 6
const MonitorRequestReasonPoSetSystemState = 7
const MonitorRequestReasonSetThreadExecutionState = 8
const MonitorRequestReasonFullWake = 9
const MonitorRequestReasonSessionUnlock = 10
const MonitorRequestReasonScreenOffRequest = 11
const MonitorRequestReasonIdleTimeout = 12
const MonitorRequestReasonPolicyChange = 13
const MonitorRequestReasonMax = 14

type POWER_MONITOR_INVOCATION = struct {
	FOn            BOOLEAN
	FConsole       BOOLEAN
	FRequestReason POWER_MONITOR_REQUEST_REASON
}

type _POWER_MONITOR_INVOCATION = POWER_MONITOR_INVOCATION

type PPOWER_MONITOR_INVOCATION = uintptr

type RESUME_PERFORMANCE = struct {
	FPostTimeMs              DWORD
	FTotalResumeTimeMs       ULONGLONG
	FResumeCompleteTimestamp ULONGLONG
}

type _RESUME_PERFORMANCE = RESUME_PERFORMANCE

type PRESUME_PERFORMANCE = uintptr

type SYSTEM_POWER_CONDITION = int32

const PoAc = 0
const PoDc = 1
const PoHot = 2
const PoConditionMaximum = 3

type SET_POWER_SETTING_VALUE = struct {
	FVersion        DWORD
	FGuid           GUID
	FPowerCondition SYSTEM_POWER_CONDITION
	FDataLength     DWORD
	FData           [1]BYTE
}

type PSET_POWER_SETTING_VALUE = uintptr

type NOTIFY_USER_POWER_SETTING = struct {
	FGuid GUID
}

type PNOTIFY_USER_POWER_SETTING = uintptr

type APPLICATIONLAUNCH_SETTING_VALUE = struct {
	FActivationTime   LARGE_INTEGER
	FFlags            DWORD
	FButtonInstanceID DWORD
}

type _APPLICATIONLAUNCH_SETTING_VALUE = APPLICATIONLAUNCH_SETTING_VALUE

type PAPPLICATIONLAUNCH_SETTING_VALUE = uintptr

type POWER_PLATFORM_ROLE = int32

type _POWER_PLATFORM_ROLE = int32

const PlatformRoleUnspecified = 0
const PlatformRoleDesktop = 1
const PlatformRoleMobile = 2
const PlatformRoleWorkstation = 3
const PlatformRoleEnterpriseServer = 4
const PlatformRoleSOHOServer = 5
const PlatformRoleAppliancePC = 6
const PlatformRolePerformanceServer = 7
const PlatformRoleSlate = 8
const PlatformRoleMaximum = 9

type PPOWER_PLATFORM_ROLE = uintptr

type POWER_PLATFORM_INFORMATION = struct {
	FAoAc BOOLEAN
}

type _POWER_PLATFORM_INFORMATION = POWER_PLATFORM_INFORMATION

type PPOWER_PLATFORM_INFORMATION = uintptr

type BATTERY_REPORTING_SCALE = struct {
	FGranularity DWORD
	FCapacity    DWORD
}

type PBATTERY_REPORTING_SCALE = uintptr

type PPM_WMI_LEGACY_PERFSTATE = struct {
	FFrequency        DWORD
	FFlags            DWORD
	FPercentFrequency DWORD
}

type PPPM_WMI_LEGACY_PERFSTATE = uintptr

type PPM_WMI_IDLE_STATE = struct {
	FLatency        DWORD
	FPower          DWORD
	FTimeCheck      DWORD
	FPromotePercent BYTE
	FDemotePercent  BYTE
	FStateType      BYTE
	FReserved       BYTE
	FStateFlags     DWORD
	FContext        DWORD
	FIdleHandler    DWORD
	FReserved1      DWORD
}

type PPPM_WMI_IDLE_STATE = uintptr

type PPM_WMI_IDLE_STATES = struct {
	FType             DWORD
	FCount            DWORD
	FTargetState      DWORD
	FOldState         DWORD
	FTargetProcessors DWORD64
	FState            [1]PPM_WMI_IDLE_STATE
}

type PPPM_WMI_IDLE_STATES = uintptr

type PPM_WMI_IDLE_STATES_EX = struct {
	FType             DWORD
	FCount            DWORD
	FTargetState      DWORD
	FOldState         DWORD
	FTargetProcessors PVOID
	FState            [1]PPM_WMI_IDLE_STATE
}

type PPPM_WMI_IDLE_STATES_EX = uintptr

type PPM_WMI_PERF_STATE = struct {
	FFrequency        DWORD
	FPower            DWORD
	FPercentFrequency BYTE
	FIncreaseLevel    BYTE
	FDecreaseLevel    BYTE
	FType             BYTE
	FIncreaseTime     DWORD
	FDecreaseTime     DWORD
	FControl          DWORD64
	FStatus           DWORD64
	FHitCount         DWORD
	FReserved1        DWORD
	FReserved2        DWORD64
	FReserved3        DWORD64
}

type PPPM_WMI_PERF_STATE = uintptr

type PPM_WMI_PERF_STATES = struct {
	FCount             DWORD
	FMaxFrequency      DWORD
	FCurrentState      DWORD
	FMaxPerfState      DWORD
	FMinPerfState      DWORD
	FLowestPerfState   DWORD
	FThermalConstraint DWORD
	FBusyAdjThreshold  BYTE
	FPolicyType        BYTE
	FType              BYTE
	FReserved          BYTE
	FTimerInterval     DWORD
	FTargetProcessors  DWORD64
	FPStateHandler     DWORD
	FPStateContext     DWORD
	FTStateHandler     DWORD
	FTStateContext     DWORD
	FFeedbackHandler   DWORD
	FReserved1         DWORD
	FReserved2         DWORD64
	FState             [1]PPM_WMI_PERF_STATE
}

type PPPM_WMI_PERF_STATES = uintptr

type PPM_WMI_PERF_STATES_EX = struct {
	FCount             DWORD
	FMaxFrequency      DWORD
	FCurrentState      DWORD
	FMaxPerfState      DWORD
	FMinPerfState      DWORD
	FLowestPerfState   DWORD
	FThermalConstraint DWORD
	FBusyAdjThreshold  BYTE
	FPolicyType        BYTE
	FType              BYTE
	FReserved          BYTE
	FTimerInterval     DWORD
	FTargetProcessors  PVOID
	FPStateHandler     DWORD
	FPStateContext     DWORD
	FTStateHandler     DWORD
	FTStateContext     DWORD
	FFeedbackHandler   DWORD
	FReserved1         DWORD
	FReserved2         DWORD64
	FState             [1]PPM_WMI_PERF_STATE
}

type PPPM_WMI_PERF_STATES_EX = uintptr

type PPM_IDLE_STATE_ACCOUNTING = struct {
	FIdleTransitions    DWORD
	FFailedTransitions  DWORD
	FInvalidBucketIndex DWORD
	FTotalTime          DWORD64
	FIdleTimeBuckets    [6]DWORD
}

type PPPM_IDLE_STATE_ACCOUNTING = uintptr

type PPM_IDLE_ACCOUNTING = struct {
	FStateCount       DWORD
	FTotalTransitions DWORD
	FResetCount       DWORD
	FStartTime        DWORD64
	FState            [1]PPM_IDLE_STATE_ACCOUNTING
}

type PPPM_IDLE_ACCOUNTING = uintptr

type PPM_IDLE_STATE_BUCKET_EX = struct {
	FTotalTimeUs DWORD64
	FMinTimeUs   DWORD
	FMaxTimeUs   DWORD
	FCount       DWORD
}

type PPPM_IDLE_STATE_BUCKET_EX = uintptr

type PPM_IDLE_STATE_ACCOUNTING_EX = struct {
	FTotalTime            DWORD64
	FIdleTransitions      DWORD
	FFailedTransitions    DWORD
	FInvalidBucketIndex   DWORD
	FMinTimeUs            DWORD
	FMaxTimeUs            DWORD
	FCancelledTransitions DWORD
	FIdleTimeBuckets      [16]PPM_IDLE_STATE_BUCKET_EX
}

type PPPM_IDLE_STATE_ACCOUNTING_EX = uintptr

type PPM_IDLE_ACCOUNTING_EX = struct {
	FStateCount       DWORD
	FTotalTransitions DWORD
	FResetCount       DWORD
	FAbortCount       DWORD
	FStartTime        DWORD64
	FState            [1]PPM_IDLE_STATE_ACCOUNTING_EX
}

type PPPM_IDLE_ACCOUNTING_EX = uintptr

type PPM_PERFSTATE_EVENT = struct {
	FState     DWORD
	FStatus    DWORD
	FLatency   DWORD
	FSpeed     DWORD
	FProcessor DWORD
}

type PPPM_PERFSTATE_EVENT = uintptr

type PPM_PERFSTATE_DOMAIN_EVENT = struct {
	FState      DWORD
	FLatency    DWORD
	FSpeed      DWORD
	FProcessors DWORD64
}

type PPPM_PERFSTATE_DOMAIN_EVENT = uintptr

type PPM_IDLESTATE_EVENT = struct {
	FNewState   DWORD
	FOldState   DWORD
	FProcessors DWORD64
}

type PPPM_IDLESTATE_EVENT = uintptr

type PPM_THERMALCHANGE_EVENT = struct {
	FThermalConstraint DWORD
	FProcessors        DWORD64
}

type PPPM_THERMALCHANGE_EVENT = uintptr

type PPM_THERMAL_POLICY_EVENT = struct {
	FMode       BYTE
	FProcessors DWORD64
}

type PPPM_THERMAL_POLICY_EVENT = uintptr

type POWER_ACTION_POLICY = struct {
	FAction    POWER_ACTION
	FFlags     DWORD
	FEventCode DWORD
}

type PPOWER_ACTION_POLICY = uintptr

type PROCESSOR_IDLESTATE_INFO = struct {
	FTimeCheck      DWORD
	FDemotePercent  BYTE
	FPromotePercent BYTE
	FSpare          [2]BYTE
}

type PPROCESSOR_IDLESTATE_INFO = uintptr

type SYSTEM_POWER_LEVEL = struct {
	FEnable         BOOLEAN
	FSpare          [3]BYTE
	FBatteryLevel   DWORD
	FPowerPolicy    POWER_ACTION_POLICY
	FMinSystemState SYSTEM_POWER_STATE
}

type PSYSTEM_POWER_LEVEL = uintptr

type SYSTEM_POWER_POLICY = struct {
	FRevision                    DWORD
	FPowerButton                 POWER_ACTION_POLICY
	FSleepButton                 POWER_ACTION_POLICY
	FLidClose                    POWER_ACTION_POLICY
	FLidOpenWake                 SYSTEM_POWER_STATE
	FReserved                    DWORD
	FIdle                        POWER_ACTION_POLICY
	FIdleTimeout                 DWORD
	FIdleSensitivity             BYTE
	FDynamicThrottle             BYTE
	FSpare2                      [2]BYTE
	FMinSleep                    SYSTEM_POWER_STATE
	FMaxSleep                    SYSTEM_POWER_STATE
	FReducedLatencySleep         SYSTEM_POWER_STATE
	FWinLogonFlags               DWORD
	FSpare3                      DWORD
	FDozeS4Timeout               DWORD
	FBroadcastCapacityResolution DWORD
	FDischargePolicy             [4]SYSTEM_POWER_LEVEL
	FVideoTimeout                DWORD
	FVideoDimDisplay             BOOLEAN
	FVideoReserved               [3]DWORD
	FSpindownTimeout             DWORD
	FOptimizeForPower            BOOLEAN
	FFanThrottleTolerance        BYTE
	FForcedThrottle              BYTE
	FMinThrottle                 BYTE
	FOverThrottled               POWER_ACTION_POLICY
}

type _SYSTEM_POWER_POLICY = SYSTEM_POWER_POLICY

type PSYSTEM_POWER_POLICY = uintptr

type PROCESSOR_IDLESTATE_POLICY = struct {
	FRevision WORD
	FFlags    struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint16
		}
		FAsWORD WORD
	}
	FPolicyCount DWORD
	FPolicy      [3]PROCESSOR_IDLESTATE_INFO
}

type PPROCESSOR_IDLESTATE_POLICY = uintptr

type PROCESSOR_POWER_POLICY_INFO = struct {
	FTimeCheck      DWORD
	FDemoteLimit    DWORD
	FPromoteLimit   DWORD
	FDemotePercent  BYTE
	FPromotePercent BYTE
	FSpare          [2]BYTE
	F__ccgo16       uint32
}

type _PROCESSOR_POWER_POLICY_INFO = PROCESSOR_POWER_POLICY_INFO

type PPROCESSOR_POWER_POLICY_INFO = uintptr

type PROCESSOR_POWER_POLICY = struct {
	FRevision        DWORD
	FDynamicThrottle BYTE
	FSpare           [3]BYTE
	F__ccgo8         uint32
	FPolicyCount     DWORD
	FPolicy          [3]PROCESSOR_POWER_POLICY_INFO
}

type _PROCESSOR_POWER_POLICY = PROCESSOR_POWER_POLICY

type PPROCESSOR_POWER_POLICY = uintptr

type PROCESSOR_PERFSTATE_POLICY = struct {
	FRevision         DWORD
	FMaxThrottle      BYTE
	FMinThrottle      BYTE
	FBusyAdjThreshold BYTE
	F__ccgo4_7        struct {
		FFlags [0]struct {
			F__ccgo1_0 [0]struct {
				F__ccgo0 uint8
			}
			FAsBYTE BYTE
		}
		FSpare BYTE
	}
	FTimeCheck       DWORD
	FIncreaseTime    DWORD
	FDecreaseTime    DWORD
	FIncreasePercent DWORD
	FDecreasePercent DWORD
}

type PPROCESSOR_PERFSTATE_POLICY = uintptr

type ADMINISTRATOR_POWER_POLICY = struct {
	FMinSleep           SYSTEM_POWER_STATE
	FMaxSleep           SYSTEM_POWER_STATE
	FMinVideoTimeout    DWORD
	FMaxVideoTimeout    DWORD
	FMinSpindownTimeout DWORD
	FMaxSpindownTimeout DWORD
}

type _ADMINISTRATOR_POWER_POLICY = ADMINISTRATOR_POWER_POLICY

type PADMINISTRATOR_POWER_POLICY = uintptr

type SYSTEM_POWER_CAPABILITIES = struct {
	FPowerButtonPresent     BOOLEAN
	FSleepButtonPresent     BOOLEAN
	FLidPresent             BOOLEAN
	FSystemS1               BOOLEAN
	FSystemS2               BOOLEAN
	FSystemS3               BOOLEAN
	FSystemS4               BOOLEAN
	FSystemS5               BOOLEAN
	FHiberFilePresent       BOOLEAN
	FFullWake               BOOLEAN
	FVideoDimPresent        BOOLEAN
	FApmPresent             BOOLEAN
	FUpsPresent             BOOLEAN
	FThermalControl         BOOLEAN
	FProcessorThrottle      BOOLEAN
	FProcessorMinThrottle   BYTE
	FProcessorMaxThrottle   BYTE
	FFastSystemS4           BOOLEAN
	Fspare2                 [3]BYTE
	FDiskSpinDown           BOOLEAN
	Fspare3                 [8]BYTE
	FSystemBatteriesPresent BOOLEAN
	FBatteriesAreShortTerm  BOOLEAN
	FBatteryScale           [3]BATTERY_REPORTING_SCALE
	FAcOnLineWake           SYSTEM_POWER_STATE
	FSoftLidWake            SYSTEM_POWER_STATE
	FRtcWake                SYSTEM_POWER_STATE
	FMinDeviceWakeState     SYSTEM_POWER_STATE
	FDefaultLowLatencyWake  SYSTEM_POWER_STATE
}

type PSYSTEM_POWER_CAPABILITIES = uintptr

type SYSTEM_BATTERY_STATE = struct {
	FAcOnLine          BOOLEAN
	FBatteryPresent    BOOLEAN
	FCharging          BOOLEAN
	FDischarging       BOOLEAN
	FSpare1            [4]BOOLEAN
	FMaxCapacity       DWORD
	FRemainingCapacity DWORD
	FRate              DWORD
	FEstimatedTime     DWORD
	FDefaultAlert1     DWORD
	FDefaultAlert2     DWORD
}

type PSYSTEM_BATTERY_STATE = uintptr

type IMAGE_DOS_HEADER = struct {
	Fe_magic    WORD
	Fe_cblp     WORD
	Fe_cp       WORD
	Fe_crlc     WORD
	Fe_cparhdr  WORD
	Fe_minalloc WORD
	Fe_maxalloc WORD
	Fe_ss       WORD
	Fe_sp       WORD
	Fe_csum     WORD
	Fe_ip       WORD
	Fe_cs       WORD
	Fe_lfarlc   WORD
	Fe_ovno     WORD
	Fe_res      [4]WORD
	Fe_oemid    WORD
	Fe_oeminfo  WORD
	Fe_res2     [10]WORD
	Fe_lfanew   LONG
}

type _IMAGE_DOS_HEADER = IMAGE_DOS_HEADER

type PIMAGE_DOS_HEADER = uintptr

type IMAGE_OS2_HEADER = struct {
	Fne_magic        WORD
	Fne_ver          CHAR
	Fne_rev          CHAR
	Fne_enttab       WORD
	Fne_cbenttab     WORD
	Fne_crc          LONG
	Fne_flags        WORD
	Fne_autodata     WORD
	Fne_heap         WORD
	Fne_stack        WORD
	Fne_csip         LONG
	Fne_sssp         LONG
	Fne_cseg         WORD
	Fne_cmod         WORD
	Fne_cbnrestab    WORD
	Fne_segtab       WORD
	Fne_rsrctab      WORD
	Fne_restab       WORD
	Fne_modtab       WORD
	Fne_imptab       WORD
	Fne_nrestab      LONG
	Fne_cmovent      WORD
	Fne_align        WORD
	Fne_cres         WORD
	Fne_exetyp       BYTE
	Fne_flagsothers  BYTE
	Fne_pretthunks   WORD
	Fne_psegrefbytes WORD
	Fne_swaparea     WORD
	Fne_expver       WORD
}

type _IMAGE_OS2_HEADER = IMAGE_OS2_HEADER

type PIMAGE_OS2_HEADER = uintptr

type IMAGE_VXD_HEADER = struct {
	Fe32_magic        WORD
	Fe32_border       BYTE
	Fe32_worder       BYTE
	Fe32_level        DWORD
	Fe32_cpu          WORD
	Fe32_os           WORD
	Fe32_ver          DWORD
	Fe32_mflags       DWORD
	Fe32_mpages       DWORD
	Fe32_startobj     DWORD
	Fe32_eip          DWORD
	Fe32_stackobj     DWORD
	Fe32_esp          DWORD
	Fe32_pagesize     DWORD
	Fe32_lastpagesize DWORD
	Fe32_fixupsize    DWORD
	Fe32_fixupsum     DWORD
	Fe32_ldrsize      DWORD
	Fe32_ldrsum       DWORD
	Fe32_objtab       DWORD
	Fe32_objcnt       DWORD
	Fe32_objmap       DWORD
	Fe32_itermap      DWORD
	Fe32_rsrctab      DWORD
	Fe32_rsrccnt      DWORD
	Fe32_restab       DWORD
	Fe32_enttab       DWORD
	Fe32_dirtab       DWORD
	Fe32_dircnt       DWORD
	Fe32_fpagetab     DWORD
	Fe32_frectab      DWORD
	Fe32_impmod       DWORD
	Fe32_impmodcnt    DWORD
	Fe32_impproc      DWORD
	Fe32_pagesum      DWORD
	Fe32_datapage     DWORD
	Fe32_preload      DWORD
	Fe32_nrestab      DWORD
	Fe32_cbnrestab    DWORD
	Fe32_nressum      DWORD
	Fe32_autodata     DWORD
	Fe32_debuginfo    DWORD
	Fe32_debuglen     DWORD
	Fe32_instpreload  DWORD
	Fe32_instdemand   DWORD
	Fe32_heapsize     DWORD
	Fe32_res3         [12]BYTE
	Fe32_winresoff    DWORD
	Fe32_winreslen    DWORD
	Fe32_devid        WORD
	Fe32_ddkver       WORD
}

type _IMAGE_VXD_HEADER = IMAGE_VXD_HEADER

type PIMAGE_VXD_HEADER = uintptr

type IMAGE_FILE_HEADER = struct {
	FMachine              WORD
	FNumberOfSections     WORD
	FTimeDateStamp        DWORD
	FPointerToSymbolTable DWORD
	FNumberOfSymbols      DWORD
	FSizeOfOptionalHeader WORD
	FCharacteristics      WORD
}

type _IMAGE_FILE_HEADER = IMAGE_FILE_HEADER

type PIMAGE_FILE_HEADER = uintptr

type IMAGE_DATA_DIRECTORY = struct {
	FVirtualAddress DWORD
	FSize           DWORD
}

type _IMAGE_DATA_DIRECTORY = IMAGE_DATA_DIRECTORY

type PIMAGE_DATA_DIRECTORY = uintptr

type IMAGE_OPTIONAL_HEADER32 = struct {
	FMagic                       WORD
	FMajorLinkerVersion          BYTE
	FMinorLinkerVersion          BYTE
	FSizeOfCode                  DWORD
	FSizeOfInitializedData       DWORD
	FSizeOfUninitializedData     DWORD
	FAddressOfEntryPoint         DWORD
	FBaseOfCode                  DWORD
	FBaseOfData                  DWORD
	FImageBase                   DWORD
	FSectionAlignment            DWORD
	FFileAlignment               DWORD
	FMajorOperatingSystemVersion WORD
	FMinorOperatingSystemVersion WORD
	FMajorImageVersion           WORD
	FMinorImageVersion           WORD
	FMajorSubsystemVersion       WORD
	FMinorSubsystemVersion       WORD
	FWin32VersionValue           DWORD
	FSizeOfImage                 DWORD
	FSizeOfHeaders               DWORD
	FCheckSum                    DWORD
	FSubsystem                   WORD
	FDllCharacteristics          WORD
	FSizeOfStackReserve          DWORD
	FSizeOfStackCommit           DWORD
	FSizeOfHeapReserve           DWORD
	FSizeOfHeapCommit            DWORD
	FLoaderFlags                 DWORD
	FNumberOfRvaAndSizes         DWORD
	FDataDirectory               [16]IMAGE_DATA_DIRECTORY
}

type _IMAGE_OPTIONAL_HEADER = IMAGE_OPTIONAL_HEADER32

type PIMAGE_OPTIONAL_HEADER32 = uintptr

type IMAGE_ROM_OPTIONAL_HEADER = struct {
	FMagic                   WORD
	FMajorLinkerVersion      BYTE
	FMinorLinkerVersion      BYTE
	FSizeOfCode              DWORD
	FSizeOfInitializedData   DWORD
	FSizeOfUninitializedData DWORD
	FAddressOfEntryPoint     DWORD
	FBaseOfCode              DWORD
	FBaseOfData              DWORD
	FBaseOfBss               DWORD
	FGprMask                 DWORD
	FCprMask                 [4]DWORD
	FGpValue                 DWORD
}

type _IMAGE_ROM_OPTIONAL_HEADER = IMAGE_ROM_OPTIONAL_HEADER

type PIMAGE_ROM_OPTIONAL_HEADER = uintptr

type IMAGE_OPTIONAL_HEADER64 = struct {
	FMagic                       WORD
	FMajorLinkerVersion          BYTE
	FMinorLinkerVersion          BYTE
	FSizeOfCode                  DWORD
	FSizeOfInitializedData       DWORD
	FSizeOfUninitializedData     DWORD
	FAddressOfEntryPoint         DWORD
	FBaseOfCode                  DWORD
	FImageBase                   ULONGLONG
	FSectionAlignment            DWORD
	FFileAlignment               DWORD
	FMajorOperatingSystemVersion WORD
	FMinorOperatingSystemVersion WORD
	FMajorImageVersion           WORD
	FMinorImageVersion           WORD
	FMajorSubsystemVersion       WORD
	FMinorSubsystemVersion       WORD
	FWin32VersionValue           DWORD
	FSizeOfImage                 DWORD
	FSizeOfHeaders               DWORD
	FCheckSum                    DWORD
	FSubsystem                   WORD
	FDllCharacteristics          WORD
	FSizeOfStackReserve          ULONGLONG
	FSizeOfStackCommit           ULONGLONG
	FSizeOfHeapReserve           ULONGLONG
	FSizeOfHeapCommit            ULONGLONG
	FLoaderFlags                 DWORD
	FNumberOfRvaAndSizes         DWORD
	FDataDirectory               [16]IMAGE_DATA_DIRECTORY
}

type _IMAGE_OPTIONAL_HEADER64 = IMAGE_OPTIONAL_HEADER64

type PIMAGE_OPTIONAL_HEADER64 = uintptr

type IMAGE_OPTIONAL_HEADER = struct {
	FMagic                       WORD
	FMajorLinkerVersion          BYTE
	FMinorLinkerVersion          BYTE
	FSizeOfCode                  DWORD
	FSizeOfInitializedData       DWORD
	FSizeOfUninitializedData     DWORD
	FAddressOfEntryPoint         DWORD
	FBaseOfCode                  DWORD
	FImageBase                   ULONGLONG
	FSectionAlignment            DWORD
	FFileAlignment               DWORD
	FMajorOperatingSystemVersion WORD
	FMinorOperatingSystemVersion WORD
	FMajorImageVersion           WORD
	FMinorImageVersion           WORD
	FMajorSubsystemVersion       WORD
	FMinorSubsystemVersion       WORD
	FWin32VersionValue           DWORD
	FSizeOfImage                 DWORD
	FSizeOfHeaders               DWORD
	FCheckSum                    DWORD
	FSubsystem                   WORD
	FDllCharacteristics          WORD
	FSizeOfStackReserve          ULONGLONG
	FSizeOfStackCommit           ULONGLONG
	FSizeOfHeapReserve           ULONGLONG
	FSizeOfHeapCommit            ULONGLONG
	FLoaderFlags                 DWORD
	FNumberOfRvaAndSizes         DWORD
	FDataDirectory               [16]IMAGE_DATA_DIRECTORY
}

type PIMAGE_OPTIONAL_HEADER = uintptr

type IMAGE_NT_HEADERS64 = struct {
	FSignature      DWORD
	FFileHeader     IMAGE_FILE_HEADER
	FOptionalHeader IMAGE_OPTIONAL_HEADER64
}

type _IMAGE_NT_HEADERS64 = IMAGE_NT_HEADERS64

type PIMAGE_NT_HEADERS64 = uintptr

type IMAGE_NT_HEADERS32 = struct {
	FSignature      DWORD
	FFileHeader     IMAGE_FILE_HEADER
	FOptionalHeader IMAGE_OPTIONAL_HEADER32
}

type _IMAGE_NT_HEADERS = IMAGE_NT_HEADERS32

type PIMAGE_NT_HEADERS32 = uintptr

type IMAGE_ROM_HEADERS = struct {
	FFileHeader     IMAGE_FILE_HEADER
	FOptionalHeader IMAGE_ROM_OPTIONAL_HEADER
}

type _IMAGE_ROM_HEADERS = IMAGE_ROM_HEADERS

type PIMAGE_ROM_HEADERS = uintptr

type IMAGE_NT_HEADERS = struct {
	FSignature      DWORD
	FFileHeader     IMAGE_FILE_HEADER
	FOptionalHeader IMAGE_OPTIONAL_HEADER64
}

type PIMAGE_NT_HEADERS = uintptr

type ANON_OBJECT_HEADER = struct {
	FSig1          WORD
	FSig2          WORD
	FVersion       WORD
	FMachine       WORD
	FTimeDateStamp DWORD
	FClassID       CLSID
	FSizeOfData    DWORD
}

type ANON_OBJECT_HEADER_V2 = struct {
	FSig1           WORD
	FSig2           WORD
	FVersion        WORD
	FMachine        WORD
	FTimeDateStamp  DWORD
	FClassID        CLSID
	FSizeOfData     DWORD
	FFlags          DWORD
	FMetaDataSize   DWORD
	FMetaDataOffset DWORD
}

type ANON_OBJECT_HEADER_BIGOBJ = struct {
	FSig1                 WORD
	FSig2                 WORD
	FVersion              WORD
	FMachine              WORD
	FTimeDateStamp        DWORD
	FClassID              CLSID
	FSizeOfData           DWORD
	FFlags                DWORD
	FMetaDataSize         DWORD
	FMetaDataOffset       DWORD
	FNumberOfSections     DWORD
	FPointerToSymbolTable DWORD
	FNumberOfSymbols      DWORD
}

type IMAGE_SECTION_HEADER = struct {
	FName [8]BYTE
	FMisc struct {
		FVirtualSize     [0]DWORD
		FPhysicalAddress DWORD
	}
	FVirtualAddress       DWORD
	FSizeOfRawData        DWORD
	FPointerToRawData     DWORD
	FPointerToRelocations DWORD
	FPointerToLinenumbers DWORD
	FNumberOfRelocations  WORD
	FNumberOfLinenumbers  WORD
	FCharacteristics      DWORD
}

type _IMAGE_SECTION_HEADER = IMAGE_SECTION_HEADER

type PIMAGE_SECTION_HEADER = uintptr

type IMAGE_SYMBOL = struct {
	FN struct {
		FName [0]struct {
			FShort DWORD
			FLong  DWORD
		}
		FLongName  [0][2]DWORD
		FShortName [8]BYTE
	}
	FValue              DWORD
	FSectionNumber      SHORT
	FType               WORD
	FStorageClass       BYTE
	FNumberOfAuxSymbols BYTE
}

type _IMAGE_SYMBOL = IMAGE_SYMBOL

type PIMAGE_SYMBOL = uintptr

type IMAGE_SYMBOL_EX = struct {
	FN struct {
		FName [0]struct {
			FShort DWORD
			FLong  DWORD
		}
		FLongName  [0][2]DWORD
		FShortName [8]BYTE
	}
	FValue              DWORD
	FSectionNumber      LONG
	FType               WORD
	FStorageClass       BYTE
	FNumberOfAuxSymbols BYTE
}

type _IMAGE_SYMBOL_EX = IMAGE_SYMBOL_EX

type PIMAGE_SYMBOL_EX = uintptr

type IMAGE_AUX_SYMBOL_TOKEN_DEF = struct {
	FbAuxType         BYTE
	FbReserved        BYTE
	FSymbolTableIndex DWORD
	FrgbReserved      [12]BYTE
}

type PIMAGE_AUX_SYMBOL_TOKEN_DEF = uintptr

type IMAGE_AUX_SYMBOL = struct {
	FFile [0]struct {
		FName [18]BYTE
	}
	FSection [0]struct {
		FLength              DWORD
		FNumberOfRelocations WORD
		FNumberOfLinenumbers WORD
		FCheckSum            DWORD
		FNumber              SHORT
		FSelection           BYTE
	}
	FTokenDef [0]IMAGE_AUX_SYMBOL_TOKEN_DEF
	FCRC      [0]struct {
		Fcrc         DWORD
		FrgbReserved [14]BYTE
	}
	FSym struct {
		FTagIndex DWORD
		FMisc     struct {
			FTotalSize [0]DWORD
			FLnSz      struct {
				FLinenumber WORD
				FSize       WORD
			}
		}
		FFcnAry struct {
			FArray [0]struct {
				FDimension [4]WORD
			}
			FFunction struct {
				FPointerToLinenumber   DWORD
				FPointerToNextFunction DWORD
			}
		}
		FTvIndex WORD
	}
}

type _IMAGE_AUX_SYMBOL = IMAGE_AUX_SYMBOL

type PIMAGE_AUX_SYMBOL = uintptr

type IMAGE_AUX_SYMBOL_EX = struct {
	FFile [0]struct {
		FName [20]BYTE
	}
	FSection [0]struct {
		FLength              DWORD
		FNumberOfRelocations WORD
		FNumberOfLinenumbers WORD
		FCheckSum            DWORD
		FNumber              SHORT
		FSelection           BYTE
		FbReserved           BYTE
		FHighNumber          SHORT
		FrgbReserved         [2]BYTE
	}
	F__ccgo3_0 [0]struct {
		FTokenDef    IMAGE_AUX_SYMBOL_TOKEN_DEF
		FrgbReserved [2]BYTE
	}
	FCRC [0]struct {
		Fcrc         DWORD
		FrgbReserved [16]BYTE
	}
	FSym struct {
		FWeakDefaultSymIndex DWORD
		FWeakSearchType      DWORD
		FrgbReserved         [12]BYTE
	}
	F__ccgo_pad5 [4]byte
}

type _IMAGE_AUX_SYMBOL_EX = IMAGE_AUX_SYMBOL_EX

type PIMAGE_AUX_SYMBOL_EX = uintptr

type IMAGE_AUX_SYMBOL_TYPE1 = int32

type IMAGE_AUX_SYMBOL_TYPE = int32

const IMAGE_AUX_SYMBOL_TYPE_TOKEN_DEF = 1

type IMAGE_RELOCATION = struct {
	F__ccgo0_0 struct {
		FRelocCount     [0]DWORD
		FVirtualAddress DWORD
	}
	FSymbolTableIndex DWORD
	FType             WORD
}

type _IMAGE_RELOCATION = IMAGE_RELOCATION

type PIMAGE_RELOCATION = uintptr

type IMAGE_LINENUMBER = struct {
	FType struct {
		FVirtualAddress   [0]DWORD
		FSymbolTableIndex DWORD
	}
	FLinenumber WORD
}

type _IMAGE_LINENUMBER = IMAGE_LINENUMBER

type PIMAGE_LINENUMBER = uintptr

type IMAGE_BASE_RELOCATION = struct {
	FVirtualAddress DWORD
	FSizeOfBlock    DWORD
}

type _IMAGE_BASE_RELOCATION = IMAGE_BASE_RELOCATION

type PIMAGE_BASE_RELOCATION = uintptr

type IMAGE_ARCHIVE_MEMBER_HEADER = struct {
	FName      [16]BYTE
	FDate      [12]BYTE
	FUserID    [6]BYTE
	FGroupID   [6]BYTE
	FMode      [8]BYTE
	FSize      [10]BYTE
	FEndHeader [2]BYTE
}

type _IMAGE_ARCHIVE_MEMBER_HEADER = IMAGE_ARCHIVE_MEMBER_HEADER

type PIMAGE_ARCHIVE_MEMBER_HEADER = uintptr

type IMAGE_EXPORT_DIRECTORY = struct {
	FCharacteristics       DWORD
	FTimeDateStamp         DWORD
	FMajorVersion          WORD
	FMinorVersion          WORD
	FName                  DWORD
	FBase                  DWORD
	FNumberOfFunctions     DWORD
	FNumberOfNames         DWORD
	FAddressOfFunctions    DWORD
	FAddressOfNames        DWORD
	FAddressOfNameOrdinals DWORD
}

type _IMAGE_EXPORT_DIRECTORY = IMAGE_EXPORT_DIRECTORY

type PIMAGE_EXPORT_DIRECTORY = uintptr

type IMAGE_IMPORT_BY_NAME = struct {
	FHint WORD
	FName [1]CHAR
}

type _IMAGE_IMPORT_BY_NAME = IMAGE_IMPORT_BY_NAME

type PIMAGE_IMPORT_BY_NAME = uintptr

type IMAGE_THUNK_DATA64 = struct {
	Fu1 struct {
		FFunction        [0]ULONGLONG
		FOrdinal         [0]ULONGLONG
		FAddressOfData   [0]ULONGLONG
		FForwarderString ULONGLONG
	}
}

type _IMAGE_THUNK_DATA64 = IMAGE_THUNK_DATA64

type PIMAGE_THUNK_DATA64 = uintptr

type IMAGE_THUNK_DATA32 = struct {
	Fu1 struct {
		FFunction        [0]DWORD
		FOrdinal         [0]DWORD
		FAddressOfData   [0]DWORD
		FForwarderString DWORD
	}
}

type _IMAGE_THUNK_DATA32 = IMAGE_THUNK_DATA32

type PIMAGE_THUNK_DATA32 = uintptr

type PIMAGE_TLS_CALLBACK = uintptr

type IMAGE_TLS_DIRECTORY64 = struct {
	FStartAddressOfRawData ULONGLONG
	FEndAddressOfRawData   ULONGLONG
	FAddressOfIndex        ULONGLONG
	FAddressOfCallBacks    ULONGLONG
	FSizeOfZeroFill        DWORD
	FCharacteristics       DWORD
}

type _IMAGE_TLS_DIRECTORY64 = IMAGE_TLS_DIRECTORY64

type PIMAGE_TLS_DIRECTORY64 = uintptr

type IMAGE_TLS_DIRECTORY32 = struct {
	FStartAddressOfRawData DWORD
	FEndAddressOfRawData   DWORD
	FAddressOfIndex        DWORD
	FAddressOfCallBacks    DWORD
	FSizeOfZeroFill        DWORD
	FCharacteristics       DWORD
}

type _IMAGE_TLS_DIRECTORY32 = IMAGE_TLS_DIRECTORY32

type PIMAGE_TLS_DIRECTORY32 = uintptr

type IMAGE_THUNK_DATA = struct {
	Fu1 struct {
		FFunction        [0]ULONGLONG
		FOrdinal         [0]ULONGLONG
		FAddressOfData   [0]ULONGLONG
		FForwarderString ULONGLONG
	}
}

type PIMAGE_THUNK_DATA = uintptr

type IMAGE_TLS_DIRECTORY = struct {
	FStartAddressOfRawData ULONGLONG
	FEndAddressOfRawData   ULONGLONG
	FAddressOfIndex        ULONGLONG
	FAddressOfCallBacks    ULONGLONG
	FSizeOfZeroFill        DWORD
	FCharacteristics       DWORD
}

type PIMAGE_TLS_DIRECTORY = uintptr

type IMAGE_IMPORT_DESCRIPTOR = struct {
	F__ccgo0_0 struct {
		FOriginalFirstThunk [0]DWORD
		FCharacteristics    DWORD
	}
	FTimeDateStamp  DWORD
	FForwarderChain DWORD
	FName           DWORD
	FFirstThunk     DWORD
}

type _IMAGE_IMPORT_DESCRIPTOR = IMAGE_IMPORT_DESCRIPTOR

type PIMAGE_IMPORT_DESCRIPTOR = uintptr

type IMAGE_BOUND_IMPORT_DESCRIPTOR = struct {
	FTimeDateStamp               DWORD
	FOffsetModuleName            WORD
	FNumberOfModuleForwarderRefs WORD
}

type _IMAGE_BOUND_IMPORT_DESCRIPTOR = IMAGE_BOUND_IMPORT_DESCRIPTOR

type PIMAGE_BOUND_IMPORT_DESCRIPTOR = uintptr

type IMAGE_BOUND_FORWARDER_REF = struct {
	FTimeDateStamp    DWORD
	FOffsetModuleName WORD
	FReserved         WORD
}

type _IMAGE_BOUND_FORWARDER_REF = IMAGE_BOUND_FORWARDER_REF

type PIMAGE_BOUND_FORWARDER_REF = uintptr

type IMAGE_DELAYLOAD_DESCRIPTOR = struct {
	FAttributes struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FAllAttributes DWORD
	}
	FDllNameRVA                 DWORD
	FModuleHandleRVA            DWORD
	FImportAddressTableRVA      DWORD
	FImportNameTableRVA         DWORD
	FBoundImportAddressTableRVA DWORD
	FUnloadInformationTableRVA  DWORD
	FTimeDateStamp              DWORD
}

type _IMAGE_DELAYLOAD_DESCRIPTOR = IMAGE_DELAYLOAD_DESCRIPTOR

type PIMAGE_DELAYLOAD_DESCRIPTOR = uintptr

type PCIMAGE_DELAYLOAD_DESCRIPTOR = uintptr

type IMAGE_RESOURCE_DIRECTORY = struct {
	FCharacteristics      DWORD
	FTimeDateStamp        DWORD
	FMajorVersion         WORD
	FMinorVersion         WORD
	FNumberOfNamedEntries WORD
	FNumberOfIdEntries    WORD
}

type _IMAGE_RESOURCE_DIRECTORY = IMAGE_RESOURCE_DIRECTORY

type PIMAGE_RESOURCE_DIRECTORY = uintptr

type IMAGE_RESOURCE_DIRECTORY_ENTRY = struct {
	F__ccgo0_0 struct {
		FName      [0]DWORD
		FId        [0]WORD
		F__ccgo0_0 struct {
			F__ccgo0 uint32
		}
	}
	F__ccgo1_4 struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FOffsetToData DWORD
	}
}

type _IMAGE_RESOURCE_DIRECTORY_ENTRY = IMAGE_RESOURCE_DIRECTORY_ENTRY

type PIMAGE_RESOURCE_DIRECTORY_ENTRY = uintptr

type IMAGE_RESOURCE_DIRECTORY_STRING = struct {
	FLength     WORD
	FNameString [1]CHAR
}

type _IMAGE_RESOURCE_DIRECTORY_STRING = IMAGE_RESOURCE_DIRECTORY_STRING

type PIMAGE_RESOURCE_DIRECTORY_STRING = uintptr

type IMAGE_RESOURCE_DIR_STRING_U = struct {
	FLength     WORD
	FNameString [1]WCHAR
}

type _IMAGE_RESOURCE_DIR_STRING_U = IMAGE_RESOURCE_DIR_STRING_U

type PIMAGE_RESOURCE_DIR_STRING_U = uintptr

type IMAGE_RESOURCE_DATA_ENTRY = struct {
	FOffsetToData DWORD
	FSize         DWORD
	FCodePage     DWORD
	FReserved     DWORD
}

type _IMAGE_RESOURCE_DATA_ENTRY = IMAGE_RESOURCE_DATA_ENTRY

type PIMAGE_RESOURCE_DATA_ENTRY = uintptr

type IMAGE_LOAD_CONFIG_CODE_INTEGRITY = struct {
	FFlags         WORD
	FCatalog       WORD
	FCatalogOffset DWORD
	FReserved      DWORD
}

type _IMAGE_LOAD_CONFIG_CODE_INTEGRITY = IMAGE_LOAD_CONFIG_CODE_INTEGRITY

type PIMAGE_LOAD_CONFIG_CODE_INTEGRITY = uintptr

type IMAGE_LOAD_CONFIG_DIRECTORY32 = struct {
	FSize                                     DWORD
	FTimeDateStamp                            DWORD
	FMajorVersion                             WORD
	FMinorVersion                             WORD
	FGlobalFlagsClear                         DWORD
	FGlobalFlagsSet                           DWORD
	FCriticalSectionDefaultTimeout            DWORD
	FDeCommitFreeBlockThreshold               DWORD
	FDeCommitTotalFreeThreshold               DWORD
	FLockPrefixTable                          DWORD
	FMaximumAllocationSize                    DWORD
	FVirtualMemoryThreshold                   DWORD
	FProcessHeapFlags                         DWORD
	FProcessAffinityMask                      DWORD
	FCSDVersion                               WORD
	FDependentLoadFlags                       WORD
	FEditList                                 DWORD
	FSecurityCookie                           DWORD
	FSEHandlerTable                           DWORD
	FSEHandlerCount                           DWORD
	FGuardCFCheckFunctionPointer              DWORD
	FGuardCFDispatchFunctionPointer           DWORD
	FGuardCFFunctionTable                     DWORD
	FGuardCFFunctionCount                     DWORD
	FGuardFlags                               DWORD
	FCodeIntegrity                            IMAGE_LOAD_CONFIG_CODE_INTEGRITY
	FGuardAddressTakenIatEntryTable           DWORD
	FGuardAddressTakenIatEntryCount           DWORD
	FGuardLongJumpTargetTable                 DWORD
	FGuardLongJumpTargetCount                 DWORD
	FDynamicValueRelocTable                   DWORD
	FCHPEMetadataPointer                      DWORD
	FGuardRFFailureRoutine                    DWORD
	FGuardRFFailureRoutineFunctionPointer     DWORD
	FDynamicValueRelocTableOffset             DWORD
	FDynamicValueRelocTableSection            WORD
	FReserved2                                WORD
	FGuardRFVerifyStackPointerFunctionPointer DWORD
	FHotPatchTableOffset                      DWORD
	FReserved3                                DWORD
	FEnclaveConfigurationPointer              DWORD
	FVolatileMetadataPointer                  DWORD
	FGuardEHContinuationTable                 DWORD
	FGuardEHContinuationCount                 DWORD
	FGuardXFGCheckFunctionPointer             DWORD
	FGuardXFGDispatchFunctionPointer          DWORD
	FGuardXFGTableDispatchFunctionPointer     DWORD
	FCastGuardOsDeterminedFailureMode         DWORD
	FGuardMemcpyFunctionPointer               DWORD
}

type PIMAGE_LOAD_CONFIG_DIRECTORY32 = uintptr

type IMAGE_LOAD_CONFIG_DIRECTORY64 = struct {
	FSize                                     DWORD
	FTimeDateStamp                            DWORD
	FMajorVersion                             WORD
	FMinorVersion                             WORD
	FGlobalFlagsClear                         DWORD
	FGlobalFlagsSet                           DWORD
	FCriticalSectionDefaultTimeout            DWORD
	FDeCommitFreeBlockThreshold               ULONGLONG
	FDeCommitTotalFreeThreshold               ULONGLONG
	FLockPrefixTable                          ULONGLONG
	FMaximumAllocationSize                    ULONGLONG
	FVirtualMemoryThreshold                   ULONGLONG
	FProcessAffinityMask                      ULONGLONG
	FProcessHeapFlags                         DWORD
	FCSDVersion                               WORD
	FDependentLoadFlags                       WORD
	FEditList                                 ULONGLONG
	FSecurityCookie                           ULONGLONG
	FSEHandlerTable                           ULONGLONG
	FSEHandlerCount                           ULONGLONG
	FGuardCFCheckFunctionPointer              ULONGLONG
	FGuardCFDispatchFunctionPointer           ULONGLONG
	FGuardCFFunctionTable                     ULONGLONG
	FGuardCFFunctionCount                     ULONGLONG
	FGuardFlags                               DWORD
	FCodeIntegrity                            IMAGE_LOAD_CONFIG_CODE_INTEGRITY
	FGuardAddressTakenIatEntryTable           ULONGLONG
	FGuardAddressTakenIatEntryCount           ULONGLONG
	FGuardLongJumpTargetTable                 ULONGLONG
	FGuardLongJumpTargetCount                 ULONGLONG
	FDynamicValueRelocTable                   ULONGLONG
	FCHPEMetadataPointer                      ULONGLONG
	FGuardRFFailureRoutine                    ULONGLONG
	FGuardRFFailureRoutineFunctionPointer     ULONGLONG
	FDynamicValueRelocTableOffset             DWORD
	FDynamicValueRelocTableSection            WORD
	FReserved2                                WORD
	FGuardRFVerifyStackPointerFunctionPointer ULONGLONG
	FHotPatchTableOffset                      DWORD
	FReserved3                                DWORD
	FEnclaveConfigurationPointer              ULONGLONG
	FVolatileMetadataPointer                  ULONGLONG
	FGuardEHContinuationTable                 ULONGLONG
	FGuardEHContinuationCount                 ULONGLONG
	FGuardXFGCheckFunctionPointer             ULONGLONG
	FGuardXFGDispatchFunctionPointer          ULONGLONG
	FGuardXFGTableDispatchFunctionPointer     ULONGLONG
	FCastGuardOsDeterminedFailureMode         ULONGLONG
	FGuardMemcpyFunctionPointer               ULONGLONG
}

type PIMAGE_LOAD_CONFIG_DIRECTORY64 = uintptr

type IMAGE_LOAD_CONFIG_DIRECTORY = struct {
	FSize                                     DWORD
	FTimeDateStamp                            DWORD
	FMajorVersion                             WORD
	FMinorVersion                             WORD
	FGlobalFlagsClear                         DWORD
	FGlobalFlagsSet                           DWORD
	FCriticalSectionDefaultTimeout            DWORD
	FDeCommitFreeBlockThreshold               ULONGLONG
	FDeCommitTotalFreeThreshold               ULONGLONG
	FLockPrefixTable                          ULONGLONG
	FMaximumAllocationSize                    ULONGLONG
	FVirtualMemoryThreshold                   ULONGLONG
	FProcessAffinityMask                      ULONGLONG
	FProcessHeapFlags                         DWORD
	FCSDVersion                               WORD
	FDependentLoadFlags                       WORD
	FEditList                                 ULONGLONG
	FSecurityCookie                           ULONGLONG
	FSEHandlerTable                           ULONGLONG
	FSEHandlerCount                           ULONGLONG
	FGuardCFCheckFunctionPointer              ULONGLONG
	FGuardCFDispatchFunctionPointer           ULONGLONG
	FGuardCFFunctionTable                     ULONGLONG
	FGuardCFFunctionCount                     ULONGLONG
	FGuardFlags                               DWORD
	FCodeIntegrity                            IMAGE_LOAD_CONFIG_CODE_INTEGRITY
	FGuardAddressTakenIatEntryTable           ULONGLONG
	FGuardAddressTakenIatEntryCount           ULONGLONG
	FGuardLongJumpTargetTable                 ULONGLONG
	FGuardLongJumpTargetCount                 ULONGLONG
	FDynamicValueRelocTable                   ULONGLONG
	FCHPEMetadataPointer                      ULONGLONG
	FGuardRFFailureRoutine                    ULONGLONG
	FGuardRFFailureRoutineFunctionPointer     ULONGLONG
	FDynamicValueRelocTableOffset             DWORD
	FDynamicValueRelocTableSection            WORD
	FReserved2                                WORD
	FGuardRFVerifyStackPointerFunctionPointer ULONGLONG
	FHotPatchTableOffset                      DWORD
	FReserved3                                DWORD
	FEnclaveConfigurationPointer              ULONGLONG
	FVolatileMetadataPointer                  ULONGLONG
	FGuardEHContinuationTable                 ULONGLONG
	FGuardEHContinuationCount                 ULONGLONG
	FGuardXFGCheckFunctionPointer             ULONGLONG
	FGuardXFGDispatchFunctionPointer          ULONGLONG
	FGuardXFGTableDispatchFunctionPointer     ULONGLONG
	FCastGuardOsDeterminedFailureMode         ULONGLONG
	FGuardMemcpyFunctionPointer               ULONGLONG
}

type PIMAGE_LOAD_CONFIG_DIRECTORY = uintptr

type IMAGE_CE_RUNTIME_FUNCTION_ENTRY = struct {
	FFuncStart DWORD
	F__ccgo4   uint32
}

type _IMAGE_CE_RUNTIME_FUNCTION_ENTRY = IMAGE_CE_RUNTIME_FUNCTION_ENTRY

type PIMAGE_CE_RUNTIME_FUNCTION_ENTRY = uintptr

type IMAGE_ALPHA64_RUNTIME_FUNCTION_ENTRY = struct {
	FBeginAddress     ULONGLONG
	FEndAddress       ULONGLONG
	FExceptionHandler ULONGLONG
	FHandlerData      ULONGLONG
	FPrologEndAddress ULONGLONG
}

type _IMAGE_ALPHA64_RUNTIME_FUNCTION_ENTRY = IMAGE_ALPHA64_RUNTIME_FUNCTION_ENTRY

type PIMAGE_ALPHA64_RUNTIME_FUNCTION_ENTRY = uintptr

type IMAGE_ALPHA_RUNTIME_FUNCTION_ENTRY = struct {
	FBeginAddress     DWORD
	FEndAddress       DWORD
	FExceptionHandler DWORD
	FHandlerData      DWORD
	FPrologEndAddress DWORD
}

type _IMAGE_ALPHA_RUNTIME_FUNCTION_ENTRY = IMAGE_ALPHA_RUNTIME_FUNCTION_ENTRY

type PIMAGE_ALPHA_RUNTIME_FUNCTION_ENTRY = uintptr

type IMAGE_ARM_RUNTIME_FUNCTION_ENTRY = struct {
	FBeginAddress DWORD
	F__ccgo1_4    struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FUnwindData DWORD
	}
}

type _IMAGE_ARM_RUNTIME_FUNCTION_ENTRY = IMAGE_ARM_RUNTIME_FUNCTION_ENTRY

type PIMAGE_ARM_RUNTIME_FUNCTION_ENTRY = uintptr

type ARM64_FNPDATA_FLAGS1 = int32

type ARM64_FNPDATA_FLAGS = int32

const PdataRefToFullXdata = 0
const PdataPackedUnwindFunction = 1
const PdataPackedUnwindFragment = 2

type ARM64_FNPDATA_CR1 = int32

type ARM64_FNPDATA_CR = int32

const PdataCrUnchained = 0
const PdataCrUnchainedSavedLr = 1
const PdataCrChainedWithPac = 2
const PdataCrChained = 3

type IMAGE_ARM64_RUNTIME_FUNCTION_ENTRY = struct {
	FBeginAddress DWORD
	F__ccgo1_4    struct {
		F__ccgo1_0 [0]struct {
			F__ccgo0 uint32
		}
		FUnwindData DWORD
	}
}

type PIMAGE_ARM64_RUNTIME_FUNCTION_ENTRY = uintptr

type IMAGE_ARM64_RUNTIME_FUNCTION_ENTRY_XDATA = struct {
	F__ccgo1_0 [0]struct {
		F__ccgo0 uint32
	}
	FHeaderData DWORD
}

type _IMAGE_RUNTIME_FUNCTION_ENTRY = struct {
	FBeginAddress DWORD
	FEndAddress   DWORD
	F__ccgo2_8    struct {
		FUnwindData        [0]DWORD
		FUnwindInfoAddress DWORD
	}
}

type _PIMAGE_RUNTIME_FUNCTION_ENTRY = uintptr

type IMAGE_IA64_RUNTIME_FUNCTION_ENTRY = struct {
	FBeginAddress DWORD
	FEndAddress   DWORD
	F__ccgo2_8    struct {
		FUnwindData        [0]DWORD
		FUnwindInfoAddress DWORD
	}
}

type PIMAGE_IA64_RUNTIME_FUNCTION_ENTRY = uintptr

type IMAGE_RUNTIME_FUNCTION_ENTRY = struct {
	FBeginAddress DWORD
	FEndAddress   DWORD
	F__ccgo2_8    struct {
		FUnwindData        [0]DWORD
		FUnwindInfoAddress DWORD
	}
}

type PIMAGE_RUNTIME_FUNCTION_ENTRY = uintptr

type IMAGE_DEBUG_DIRECTORY = struct {
	FCharacteristics  DWORD
	FTimeDateStamp    DWORD
	FMajorVersion     WORD
	FMinorVersion     WORD
	FType             DWORD
	FSizeOfData       DWORD
	FAddressOfRawData DWORD
	FPointerToRawData DWORD
}

type _IMAGE_DEBUG_DIRECTORY = IMAGE_DEBUG_DIRECTORY

type PIMAGE_DEBUG_DIRECTORY = uintptr

type IMAGE_COFF_SYMBOLS_HEADER = struct {
	FNumberOfSymbols      DWORD
	FLvaToFirstSymbol     DWORD
	FNumberOfLinenumbers  DWORD
	FLvaToFirstLinenumber DWORD
	FRvaToFirstByteOfCode DWORD
	FRvaToLastByteOfCode  DWORD
	FRvaToFirstByteOfData DWORD
	FRvaToLastByteOfData  DWORD
}

type _IMAGE_COFF_SYMBOLS_HEADER = IMAGE_COFF_SYMBOLS_HEADER

type PIMAGE_COFF_SYMBOLS_HEADER = uintptr

type FPO_DATA = struct {
	FulOffStart DWORD
	FcbProcSize DWORD
	FcdwLocals  DWORD
	FcdwParams  WORD
	F__ccgo14   uint16
}

type _FPO_DATA = FPO_DATA

type PFPO_DATA = uintptr

type IMAGE_DEBUG_MISC = struct {
	FDataType DWORD
	FLength   DWORD
	FUnicode  BOOLEAN
	FReserved [3]BYTE
	FData     [1]BYTE
}

type _IMAGE_DEBUG_MISC = IMAGE_DEBUG_MISC

type PIMAGE_DEBUG_MISC = uintptr

type IMAGE_FUNCTION_ENTRY = struct {
	FStartingAddress DWORD
	FEndingAddress   DWORD
	FEndOfPrologue   DWORD
}

type _IMAGE_FUNCTION_ENTRY = IMAGE_FUNCTION_ENTRY

type PIMAGE_FUNCTION_ENTRY = uintptr

type IMAGE_FUNCTION_ENTRY64 = struct {
	FStartingAddress ULONGLONG
	FEndingAddress   ULONGLONG
	F__ccgo2_16      struct {
		FUnwindInfoAddress [0]ULONGLONG
		FEndOfPrologue     ULONGLONG
	}
}

type _IMAGE_FUNCTION_ENTRY64 = IMAGE_FUNCTION_ENTRY64

type PIMAGE_FUNCTION_ENTRY64 = uintptr

type IMAGE_SEPARATE_DEBUG_HEADER = struct {
	FSignature          WORD
	FFlags              WORD
	FMachine            WORD
	FCharacteristics    WORD
	FTimeDateStamp      DWORD
	FCheckSum           DWORD
	FImageBase          DWORD
	FSizeOfImage        DWORD
	FNumberOfSections   DWORD
	FExportedNamesSize  DWORD
	FDebugDirectorySize DWORD
	FSectionAlignment   DWORD
	FReserved           [2]DWORD
}

type _IMAGE_SEPARATE_DEBUG_HEADER = IMAGE_SEPARATE_DEBUG_HEADER

type PIMAGE_SEPARATE_DEBUG_HEADER = uintptr

type NON_PAGED_DEBUG_INFO = struct {
	FSignature       WORD
	FFlags           WORD
	FSize            DWORD
	FMachine         WORD
	FCharacteristics WORD
	FTimeDateStamp   DWORD
	FCheckSum        DWORD
	FSizeOfImage     DWORD
	FImageBase       ULONGLONG
}

type _NON_PAGED_DEBUG_INFO = NON_PAGED_DEBUG_INFO

type PNON_PAGED_DEBUG_INFO = uintptr

type IMAGE_ARCHITECTURE_HEADER = struct {
	F__ccgo0       uint32
	FFirstEntryRVA DWORD
}

type _ImageArchitectureHeader = IMAGE_ARCHITECTURE_HEADER

type PIMAGE_ARCHITECTURE_HEADER = uintptr

type IMAGE_ARCHITECTURE_ENTRY = struct {
	FFixupInstRVA DWORD
	FNewInst      DWORD
}

type _ImageArchitectureEntry = IMAGE_ARCHITECTURE_ENTRY

type PIMAGE_ARCHITECTURE_ENTRY = uintptr

type IMPORT_OBJECT_HEADER = struct {
	FSig1          WORD
	FSig2          WORD
	FVersion       WORD
	FMachine       WORD
	FTimeDateStamp DWORD
	FSizeOfData    DWORD
	F__ccgo6_16    struct {
		FHint    [0]WORD
		FOrdinal WORD
	}
	F__ccgo18 uint16
}

type IMPORT_OBJECT_TYPE1 = int32

type IMPORT_OBJECT_TYPE = int32

const IMPORT_OBJECT_CODE = 0
const IMPORT_OBJECT_DATA = 1
const IMPORT_OBJECT_CONST = 2

type IMPORT_OBJECT_NAME_TYPE1 = int32

type IMPORT_OBJECT_NAME_TYPE = int32

const IMPORT_OBJECT_ORDINAL = 0
const IMPORT_OBJECT_NAME = 1
const IMPORT_OBJECT_NAME_NO_PREFIX = 2
const IMPORT_OBJECT_NAME_UNDECORATE = 3

type ReplacesCorHdrNumericDefines1 = int32

type ReplacesCorHdrNumericDefines = int32

const COMIMAGE_FLAGS_ILONLY = 1
const COMIMAGE_FLAGS_32BITREQUIRED = 2
const COMIMAGE_FLAGS_IL_LIBRARY = 4
const COMIMAGE_FLAGS_STRONGNAMESIGNED = 8
const COMIMAGE_FLAGS_TRACKDEBUGDATA = 65536
const COR_VERSION_MAJOR_V2 = 2
const COR_VERSION_MAJOR = 2
const COR_VERSION_MINOR = 0
const COR_DELETED_NAME_LENGTH = 8
const COR_VTABLEGAP_NAME_LENGTH = 8
const NATIVE_TYPE_MAX_CB = 1
const COR_ILMETHOD_SECT_SMALL_MAX_DATASIZE = 255
const IMAGE_COR_MIH_METHODRVA = 1
const IMAGE_COR_MIH_EHRVA = 2
const IMAGE_COR_MIH_BASICBLOCK = 8
const COR_VTABLE_32BIT = 1
const COR_VTABLE_64BIT = 2
const COR_VTABLE_FROM_UNMANAGED = 4
const COR_VTABLE_CALL_MOST_DERIVED = 16
const IMAGE_COR_EATJ_THUNK_SIZE = 32
const MAX_CLASS_NAME = 1024
const MAX_PACKAGE_NAME = 1024

type IMAGE_COR20_HEADER = struct {
	Fcb                  DWORD
	FMajorRuntimeVersion WORD
	FMinorRuntimeVersion WORD
	FMetaData            IMAGE_DATA_DIRECTORY
	FFlags               DWORD
	F__ccgo5_20          struct {
		FEntryPointRVA   [0]DWORD
		FEntryPointToken DWORD
	}
	FResources               IMAGE_DATA_DIRECTORY
	FStrongNameSignature     IMAGE_DATA_DIRECTORY
	FCodeManagerTable        IMAGE_DATA_DIRECTORY
	FVTableFixups            IMAGE_DATA_DIRECTORY
	FExportAddressTableJumps IMAGE_DATA_DIRECTORY
	FManagedNativeHeader     IMAGE_DATA_DIRECTORY
}

type PIMAGE_COR20_HEADER = uintptr

type UNWIND_HISTORY_TABLE_ENTRY = struct {
	FImageBase     ULONG_PTR
	FFunctionEntry PRUNTIME_FUNCTION
}

type _UNWIND_HISTORY_TABLE_ENTRY = UNWIND_HISTORY_TABLE_ENTRY

type PUNWIND_HISTORY_TABLE_ENTRY = uintptr

type UNWIND_HISTORY_TABLE = struct {
	FCount       DWORD
	FLocalHint   BYTE
	FGlobalHint  BYTE
	FSearch      BYTE
	FOnce        BYTE
	FLowAddress  ULONG_PTR
	FHighAddress ULONG_PTR
	FEntry       [12]UNWIND_HISTORY_TABLE_ENTRY
}

type _UNWIND_HISTORY_TABLE = UNWIND_HISTORY_TABLE

type PUNWIND_HISTORY_TABLE = uintptr

type SLIST_ENTRY = struct {
	FNext uintptr
}

type _SLIST_ENTRY = SLIST_ENTRY

type PSLIST_ENTRY = uintptr

type SLIST_HEADER = struct {
	FHeader8 [0]struct {
		F__ccgo0 uint64
		F__ccgo8 uint64
	}
	FHeaderX64 [0]struct {
		F__ccgo0 uint64
		F__ccgo8 uint64
	}
	F__ccgo0_0 struct {
		FAlignment ULONGLONG
		FRegion    ULONGLONG
	}
}

type _SLIST_HEADER = SLIST_HEADER

type PSLIST_HEADER = uintptr

type RTL_RUN_ONCE = struct {
	FPtr PVOID
}

type _RTL_RUN_ONCE = RTL_RUN_ONCE

type PRTL_RUN_ONCE = uintptr

type PRTL_RUN_ONCE_INIT_FN = uintptr

type RTL_BARRIER = struct {
	FReserved1 DWORD
	FReserved2 DWORD
	FReserved3 [2]ULONG_PTR
	FReserved4 DWORD
	FReserved5 DWORD
}

type _RTL_BARRIER = RTL_BARRIER

type PRTL_BARRIER = uintptr

type MESSAGE_RESOURCE_ENTRY = struct {
	FLength WORD
	FFlags  WORD
	FText   [1]BYTE
}

type _MESSAGE_RESOURCE_ENTRY = MESSAGE_RESOURCE_ENTRY

type PMESSAGE_RESOURCE_ENTRY = uintptr

type MESSAGE_RESOURCE_BLOCK = struct {
	FLowId           DWORD
	FHighId          DWORD
	FOffsetToEntries DWORD
}

type _MESSAGE_RESOURCE_BLOCK = MESSAGE_RESOURCE_BLOCK

type PMESSAGE_RESOURCE_BLOCK = uintptr

type MESSAGE_RESOURCE_DATA = struct {
	FNumberOfBlocks DWORD
	FBlocks         [1]MESSAGE_RESOURCE_BLOCK
}

type _MESSAGE_RESOURCE_DATA = MESSAGE_RESOURCE_DATA

type PMESSAGE_RESOURCE_DATA = uintptr

type OSVERSIONINFOA = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]CHAR
}

type _OSVERSIONINFOA = OSVERSIONINFOA

type POSVERSIONINFOA = uintptr

type LPOSVERSIONINFOA = uintptr

type OSVERSIONINFOW = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]WCHAR
}

type _OSVERSIONINFOW = OSVERSIONINFOW

type POSVERSIONINFOW = uintptr

type LPOSVERSIONINFOW = uintptr

type RTL_OSVERSIONINFOW = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]WCHAR
}

type PRTL_OSVERSIONINFOW = uintptr

type OSVERSIONINFO = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]CHAR
}

type POSVERSIONINFO = uintptr

type LPOSVERSIONINFO = uintptr

type OSVERSIONINFOEXA = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]CHAR
	FwServicePackMajor   WORD
	FwServicePackMinor   WORD
	FwSuiteMask          WORD
	FwProductType        BYTE
	FwReserved           BYTE
}

type _OSVERSIONINFOEXA = OSVERSIONINFOEXA

type POSVERSIONINFOEXA = uintptr

type LPOSVERSIONINFOEXA = uintptr

type OSVERSIONINFOEXW = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]WCHAR
	FwServicePackMajor   WORD
	FwServicePackMinor   WORD
	FwSuiteMask          WORD
	FwProductType        BYTE
	FwReserved           BYTE
}

type _OSVERSIONINFOEXW = OSVERSIONINFOEXW

type POSVERSIONINFOEXW = uintptr

type LPOSVERSIONINFOEXW = uintptr

type RTL_OSVERSIONINFOEXW = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]WCHAR
	FwServicePackMajor   WORD
	FwServicePackMinor   WORD
	FwSuiteMask          WORD
	FwProductType        BYTE
	FwReserved           BYTE
}

type PRTL_OSVERSIONINFOEXW = uintptr

type OSVERSIONINFOEX = struct {
	FdwOSVersionInfoSize DWORD
	FdwMajorVersion      DWORD
	FdwMinorVersion      DWORD
	FdwBuildNumber       DWORD
	FdwPlatformId        DWORD
	FszCSDVersion        [128]CHAR
	FwServicePackMajor   WORD
	FwServicePackMinor   WORD
	FwSuiteMask          WORD
	FwProductType        BYTE
	FwReserved           BYTE
}

type POSVERSIONINFOEX = uintptr

type LPOSVERSIONINFOEX = uintptr

type RTL_UMS_THREAD_INFO_CLASS = int32

type _RTL_UMS_THREAD_INFO_CLASS = int32

const UmsThreadInvalidInfoClass = 0
const UmsThreadUserContext = 1
const UmsThreadPriority = 2
const UmsThreadAffinity = 3
const UmsThreadTeb = 4
const UmsThreadIsSuspended = 5
const UmsThreadIsTerminated = 6
const UmsThreadMaxInfoClass = 7

type PRTL_UMS_THREAD_INFO_CLASS = uintptr

type RTL_UMS_SCHEDULER_REASON = int32

type _RTL_UMS_SCHEDULER_REASON = int32

const UmsSchedulerStartup = 0
const UmsSchedulerThreadBlocked = 1
const UmsSchedulerThreadYield = 2

type PRTL_UMS_SCHEDULER_REASON = uintptr

type PRTL_UMS_SCHEDULER_ENTRY_POINT = uintptr

type RTL_CRITICAL_SECTION_DEBUG = struct {
	FType                      WORD
	FCreatorBackTraceIndex     WORD
	FCriticalSection           uintptr
	FProcessLocksList          LIST_ENTRY
	FEntryCount                DWORD
	FContentionCount           DWORD
	FFlags                     DWORD
	FCreatorBackTraceIndexHigh WORD
	FSpareWORD                 WORD
}

type _RTL_CRITICAL_SECTION_DEBUG = RTL_CRITICAL_SECTION_DEBUG

type PRTL_CRITICAL_SECTION_DEBUG = uintptr

type RTL_RESOURCE_DEBUG = struct {
	FType                      WORD
	FCreatorBackTraceIndex     WORD
	FCriticalSection           uintptr
	FProcessLocksList          LIST_ENTRY
	FEntryCount                DWORD
	FContentionCount           DWORD
	FFlags                     DWORD
	FCreatorBackTraceIndexHigh WORD
	FSpareWORD                 WORD
}

type PRTL_RESOURCE_DEBUG = uintptr

type RTL_CRITICAL_SECTION = struct {
	FDebugInfo      PRTL_CRITICAL_SECTION_DEBUG
	FLockCount      LONG
	FRecursionCount LONG
	FOwningThread   HANDLE
	FLockSemaphore  HANDLE
	FSpinCount      ULONG_PTR
}

type _RTL_CRITICAL_SECTION = RTL_CRITICAL_SECTION

type PRTL_CRITICAL_SECTION = uintptr

type RTL_SRWLOCK = struct {
	FPtr PVOID
}

type _RTL_SRWLOCK = RTL_SRWLOCK

type PRTL_SRWLOCK = uintptr

type RTL_CONDITION_VARIABLE = struct {
	FPtr PVOID
}

type _RTL_CONDITION_VARIABLE = RTL_CONDITION_VARIABLE

type PRTL_CONDITION_VARIABLE = uintptr

type PAPCFUNC = uintptr

type PVECTORED_EXCEPTION_HANDLER = uintptr

type HEAP_INFORMATION_CLASS = int32

type _HEAP_INFORMATION_CLASS = int32

const HeapCompatibilityInformation = 0
const HeapEnableTerminationOnCorruption = 1
const HeapTag = 7

type WORKERCALLBACKFUNC = uintptr

type APC_CALLBACK_FUNCTION = uintptr

type WAITORTIMERCALLBACKFUNC = uintptr

type WAITORTIMERCALLBACK = uintptr

type PFLS_CALLBACK_FUNCTION = uintptr

type PSECURE_MEMORY_CACHE_CALLBACK = uintptr

type ACTIVATION_CONTEXT_INFO_CLASS = int32

type _ACTIVATION_CONTEXT_INFO_CLASS = int32

const ActivationContextBasicInformation = 1
const ActivationContextDetailedInformation = 2
const AssemblyDetailedInformationInActivationContext = 3
const FileInformationInAssemblyOfAssemblyInActivationContext = 4
const RunlevelInformationInActivationContext = 5
const CompatibilityInformationInActivationContext = 6
const ActivationContextManifestResourceName = 7
const MaxActivationContextInfoClass = 8
const AssemblyDetailedInformationInActivationContxt = 3
const FileInformationInAssemblyOfAssemblyInActivationContxt = 4

type ACTCTX_REQUESTED_RUN_LEVEL = int32

const ACTCTX_RUN_LEVEL_UNSPECIFIED = 0
const ACTCTX_RUN_LEVEL_AS_INVOKER = 1
const ACTCTX_RUN_LEVEL_HIGHEST_AVAILABLE = 2
const ACTCTX_RUN_LEVEL_REQUIRE_ADMIN = 3
const ACTCTX_RUN_LEVEL_NUMBERS = 4

type ACTCTX_COMPATIBILITY_ELEMENT_TYPE = int32

const ACTCTX_COMPATIBILITY_ELEMENT_TYPE_UNKNOWN = 0
const ACTCTX_COMPATIBILITY_ELEMENT_TYPE_OS = 1
const ACTCTX_COMPATIBILITY_ELEMENT_TYPE_MITIGATION = 2

type ACTIVATION_CONTEXT_QUERY_INDEX = struct {
	FulAssemblyIndex       DWORD
	FulFileIndexInAssembly DWORD
}

type _ACTIVATION_CONTEXT_QUERY_INDEX = ACTIVATION_CONTEXT_QUERY_INDEX

type PACTIVATION_CONTEXT_QUERY_INDEX = uintptr

type ASSEMBLY_FILE_DETAILED_INFORMATION = struct {
	FulFlags          DWORD
	FulFilenameLength DWORD
	FulPathLength     DWORD
	FlpFileName       PCWSTR
	FlpFilePath       PCWSTR
}

type _ASSEMBLY_FILE_DETAILED_INFORMATION = ASSEMBLY_FILE_DETAILED_INFORMATION

type PASSEMBLY_FILE_DETAILED_INFORMATION = uintptr

type ACTIVATION_CONTEXT_ASSEMBLY_DETAILED_INFORMATION = struct {
	FulFlags                           DWORD
	FulEncodedAssemblyIdentityLength   DWORD
	FulManifestPathType                DWORD
	FulManifestPathLength              DWORD
	FliManifestLastWriteTime           LARGE_INTEGER
	FulPolicyPathType                  DWORD
	FulPolicyPathLength                DWORD
	FliPolicyLastWriteTime             LARGE_INTEGER
	FulMetadataSatelliteRosterIndex    DWORD
	FulManifestVersionMajor            DWORD
	FulManifestVersionMinor            DWORD
	FulPolicyVersionMajor              DWORD
	FulPolicyVersionMinor              DWORD
	FulAssemblyDirectoryNameLength     DWORD
	FlpAssemblyEncodedAssemblyIdentity PCWSTR
	FlpAssemblyManifestPath            PCWSTR
	FlpAssemblyPolicyPath              PCWSTR
	FlpAssemblyDirectoryName           PCWSTR
	FulFileCount                       DWORD
}

type _ACTIVATION_CONTEXT_ASSEMBLY_DETAILED_INFORMATION = ACTIVATION_CONTEXT_ASSEMBLY_DETAILED_INFORMATION

type PACTIVATION_CONTEXT_ASSEMBLY_DETAILED_INFORMATION = uintptr

type ACTIVATION_CONTEXT_RUN_LEVEL_INFORMATION = struct {
	FulFlags  DWORD
	FRunLevel ACTCTX_REQUESTED_RUN_LEVEL
	FUiAccess DWORD
}

type _ACTIVATION_CONTEXT_RUN_LEVEL_INFORMATION = ACTIVATION_CONTEXT_RUN_LEVEL_INFORMATION

type PACTIVATION_CONTEXT_RUN_LEVEL_INFORMATION = uintptr

type COMPATIBILITY_CONTEXT_ELEMENT = struct {
	FId   GUID
	FType ACTCTX_COMPATIBILITY_ELEMENT_TYPE
}

type _COMPATIBILITY_CONTEXT_ELEMENT = COMPATIBILITY_CONTEXT_ELEMENT

type PCOMPATIBILITY_CONTEXT_ELEMENT = uintptr

type ACTIVATION_CONTEXT_COMPATIBILITY_INFORMATION = struct {
	FElementCount DWORD
}

type _ACTIVATION_CONTEXT_COMPATIBILITY_INFORMATION = ACTIVATION_CONTEXT_COMPATIBILITY_INFORMATION

type PACTIVATION_CONTEXT_COMPATIBILITY_INFORMATION = uintptr

type SUPPORTED_OS_INFO = struct {
	FOsCount         WORD
	FMitigationExist WORD
	FOsList          [4]WORD
}

type _SUPPORTED_OS_INFO = SUPPORTED_OS_INFO

type PSUPPORTED_OS_INFO = uintptr

type ACTIVATION_CONTEXT_DETAILED_INFORMATION = struct {
	FdwFlags                      DWORD
	FulFormatVersion              DWORD
	FulAssemblyCount              DWORD
	FulRootManifestPathType       DWORD
	FulRootManifestPathChars      DWORD
	FulRootConfigurationPathType  DWORD
	FulRootConfigurationPathChars DWORD
	FulAppDirPathType             DWORD
	FulAppDirPathChars            DWORD
	FlpRootManifestPath           PCWSTR
	FlpRootConfigurationPath      PCWSTR
	FlpAppDirPath                 PCWSTR
}

type _ACTIVATION_CONTEXT_DETAILED_INFORMATION = ACTIVATION_CONTEXT_DETAILED_INFORMATION

type PACTIVATION_CONTEXT_DETAILED_INFORMATION = uintptr

type PCACTIVATION_CONTEXT_QUERY_INDEX = uintptr

type PCASSEMBLY_FILE_DETAILED_INFORMATION = uintptr

type PCACTIVATION_CONTEXT_ASSEMBLY_DETAILED_INFORMATION = uintptr

type PCACTIVATION_CONTEXT_RUN_LEVEL_INFORMATION = uintptr

type PCCOMPATIBILITY_CONTEXT_ELEMENT = uintptr

type PCACTIVATION_CONTEXT_COMPATIBILITY_INFORMATION = uintptr

type PCACTIVATION_CONTEXT_DETAILED_INFORMATION = uintptr

type RTL_VERIFIER_DLL_LOAD_CALLBACK = uintptr

type RTL_VERIFIER_DLL_UNLOAD_CALLBACK = uintptr

type RTL_VERIFIER_NTDLLHEAPFREE_CALLBACK = uintptr

type RTL_VERIFIER_THUNK_DESCRIPTOR = struct {
	FThunkName       PCHAR
	FThunkOldAddress PVOID
	FThunkNewAddress PVOID
}

type _RTL_VERIFIER_THUNK_DESCRIPTOR = RTL_VERIFIER_THUNK_DESCRIPTOR

type PRTL_VERIFIER_THUNK_DESCRIPTOR = uintptr

type RTL_VERIFIER_DLL_DESCRIPTOR = struct {
	FDllName    PWCHAR
	FDllFlags   DWORD
	FDllAddress PVOID
	FDllThunks  PRTL_VERIFIER_THUNK_DESCRIPTOR
}

type _RTL_VERIFIER_DLL_DESCRIPTOR = RTL_VERIFIER_DLL_DESCRIPTOR

type PRTL_VERIFIER_DLL_DESCRIPTOR = uintptr

type RTL_VERIFIER_PROVIDER_DESCRIPTOR = struct {
	FLength                        DWORD
	FProviderDlls                  PRTL_VERIFIER_DLL_DESCRIPTOR
	FProviderDllLoadCallback       RTL_VERIFIER_DLL_LOAD_CALLBACK
	FProviderDllUnloadCallback     RTL_VERIFIER_DLL_UNLOAD_CALLBACK
	FVerifierImage                 PWSTR
	FVerifierFlags                 DWORD
	FVerifierDebug                 DWORD
	FRtlpGetStackTraceAddress      PVOID
	FRtlpDebugPageHeapCreate       PVOID
	FRtlpDebugPageHeapDestroy      PVOID
	FProviderNtdllHeapFreeCallback RTL_VERIFIER_NTDLLHEAPFREE_CALLBACK
}

type _RTL_VERIFIER_PROVIDER_DESCRIPTOR = RTL_VERIFIER_PROVIDER_DESCRIPTOR

type PRTL_VERIFIER_PROVIDER_DESCRIPTOR = uintptr

type HARDWARE_COUNTER_DATA = struct {
	FType     HARDWARE_COUNTER_TYPE
	FReserved DWORD
	FValue    DWORD64
}

type _HARDWARE_COUNTER_DATA = HARDWARE_COUNTER_DATA

type PHARDWARE_COUNTER_DATA = uintptr

type PERFORMANCE_DATA = struct {
	FSize               WORD
	FVersion            BYTE
	FHwCountersCount    BYTE
	FContextSwitchCount DWORD
	FWaitReasonBitMap   DWORD64
	FCycleTime          DWORD64
	FRetryCount         DWORD
	FReserved           DWORD
	FHwCounters         [16]HARDWARE_COUNTER_DATA
}

type _PERFORMANCE_DATA = PERFORMANCE_DATA

type PPERFORMANCE_DATA = uintptr

type EVENTLOGRECORD = struct {
	FLength              DWORD
	FReserved            DWORD
	FRecordNumber        DWORD
	FTimeGenerated       DWORD
	FTimeWritten         DWORD
	FEventID             DWORD
	FEventType           WORD
	FNumStrings          WORD
	FEventCategory       WORD
	FReservedFlags       WORD
	FClosingRecordNumber DWORD
	FStringOffset        DWORD
	FUserSidLength       DWORD
	FUserSidOffset       DWORD
	FDataLength          DWORD
	FDataOffset          DWORD
}

type _EVENTLOGRECORD = EVENTLOGRECORD

type PEVENTLOGRECORD = uintptr

type EVENTSFORLOGFILE = struct {
	FulSize           DWORD
	FszLogicalLogFile [256]WCHAR
	FulNumRecords     DWORD
}

type _EVENTSFORLOGFILE = EVENTSFORLOGFILE

type PEVENTSFORLOGFILE = uintptr

type PACKEDEVENTINFO = struct {
	FulSize                DWORD
	FulNumEventsForLogFile DWORD
}

type _PACKEDEVENTINFO = PACKEDEVENTINFO

type PPACKEDEVENTINFO = uintptr

type SERVICE_NODE_TYPE = int32

type _CM_SERVICE_NODE_TYPE = int32

const DriverType = 1
const FileSystemType = 2
const Win32ServiceOwnProcess = 16
const Win32ServiceShareProcess = 32
const AdapterType = 4
const RecognizerType = 8

type SERVICE_LOAD_TYPE = int32

type _CM_SERVICE_LOAD_TYPE = int32

const BootLoad = 0
const SystemLoad = 1
const AutoLoad = 2
const DemandLoad = 3
const DisableLoad = 4

type SERVICE_ERROR_TYPE = int32

type _CM_ERROR_CONTROL_TYPE = int32

const IgnoreError = 0
const NormalError = 1
const SevereError = 2
const CriticalError = 3

type TAPE_ERASE = struct {
	FType      DWORD
	FImmediate BOOLEAN
}

type _TAPE_ERASE = TAPE_ERASE

type PTAPE_ERASE = uintptr

type TAPE_PREPARE = struct {
	FOperation DWORD
	FImmediate BOOLEAN
}

type _TAPE_PREPARE = TAPE_PREPARE

type PTAPE_PREPARE = uintptr

type TAPE_WRITE_MARKS = struct {
	FType      DWORD
	FCount     DWORD
	FImmediate BOOLEAN
}

type _TAPE_WRITE_MARKS = TAPE_WRITE_MARKS

type PTAPE_WRITE_MARKS = uintptr

type TAPE_GET_POSITION = struct {
	FType      DWORD
	FPartition DWORD
	FOffset    LARGE_INTEGER
}

type _TAPE_GET_POSITION = TAPE_GET_POSITION

type PTAPE_GET_POSITION = uintptr

type TAPE_SET_POSITION = struct {
	FMethod    DWORD
	FPartition DWORD
	FOffset    LARGE_INTEGER
	FImmediate BOOLEAN
}

type _TAPE_SET_POSITION = TAPE_SET_POSITION

type PTAPE_SET_POSITION = uintptr

type TAPE_GET_DRIVE_PARAMETERS = struct {
	FECC                   BOOLEAN
	FCompression           BOOLEAN
	FDataPadding           BOOLEAN
	FReportSetmarks        BOOLEAN
	FDefaultBlockSize      DWORD
	FMaximumBlockSize      DWORD
	FMinimumBlockSize      DWORD
	FMaximumPartitionCount DWORD
	FFeaturesLow           DWORD
	FFeaturesHigh          DWORD
	FEOTWarningZoneSize    DWORD
}

type _TAPE_GET_DRIVE_PARAMETERS = TAPE_GET_DRIVE_PARAMETERS

type PTAPE_GET_DRIVE_PARAMETERS = uintptr

type TAPE_SET_DRIVE_PARAMETERS = struct {
	FECC                BOOLEAN
	FCompression        BOOLEAN
	FDataPadding        BOOLEAN
	FReportSetmarks     BOOLEAN
	FEOTWarningZoneSize DWORD
}

type _TAPE_SET_DRIVE_PARAMETERS = TAPE_SET_DRIVE_PARAMETERS

type PTAPE_SET_DRIVE_PARAMETERS = uintptr

type TAPE_GET_MEDIA_PARAMETERS = struct {
	FCapacity       LARGE_INTEGER
	FRemaining      LARGE_INTEGER
	FBlockSize      DWORD
	FPartitionCount DWORD
	FWriteProtected BOOLEAN
}

type _TAPE_GET_MEDIA_PARAMETERS = TAPE_GET_MEDIA_PARAMETERS

type PTAPE_GET_MEDIA_PARAMETERS = uintptr

type TAPE_SET_MEDIA_PARAMETERS = struct {
	FBlockSize DWORD
}

type _TAPE_SET_MEDIA_PARAMETERS = TAPE_SET_MEDIA_PARAMETERS

type PTAPE_SET_MEDIA_PARAMETERS = uintptr

type TAPE_CREATE_PARTITION = struct {
	FMethod DWORD
	FCount  DWORD
	FSize   DWORD
}

type _TAPE_CREATE_PARTITION = TAPE_CREATE_PARTITION

type PTAPE_CREATE_PARTITION = uintptr

type TAPE_WMI_OPERATIONS = struct {
	FMethod         DWORD
	FDataBufferSize DWORD
	FDataBuffer     PVOID
}

type _TAPE_WMI_OPERATIONS = TAPE_WMI_OPERATIONS

type PTAPE_WMI_OPERATIONS = uintptr

type TAPE_DRIVE_PROBLEM_TYPE = int32

type _TAPE_DRIVE_PROBLEM_TYPE = int32

const TapeDriveProblemNone = 0
const TapeDriveReadWriteWarning = 1
const TapeDriveReadWriteError = 2
const TapeDriveReadWarning = 3
const TapeDriveWriteWarning = 4
const TapeDriveReadError = 5
const TapeDriveWriteError = 6
const TapeDriveHardwareError = 7
const TapeDriveUnsupportedMedia = 8
const TapeDriveScsiConnectionError = 9
const TapeDriveTimetoClean = 10
const TapeDriveCleanDriveNow = 11
const TapeDriveMediaLifeExpired = 12
const TapeDriveSnappedTape = 13

type TP_VERSION = uint32

type PTP_VERSION = uintptr

type PTP_CALLBACK_INSTANCE = uintptr

type PTP_SIMPLE_CALLBACK = uintptr

type PTP_POOL = uintptr

type TP_CALLBACK_PRIORITY = int32

type _TP_CALLBACK_PRIORITY = int32

const TP_CALLBACK_PRIORITY_HIGH = 0
const TP_CALLBACK_PRIORITY_NORMAL = 1
const TP_CALLBACK_PRIORITY_LOW = 2
const TP_CALLBACK_PRIORITY_INVALID = 3
const TP_CALLBACK_PRIORITY_COUNT = 3

type TP_POOL_STACK_INFORMATION = struct {
	FStackReserve SIZE_T
	FStackCommit  SIZE_T
}

type _TP_POOL_STACK_INFORMATION = TP_POOL_STACK_INFORMATION

type PTP_POOL_STACK_INFORMATION = uintptr

type PTP_CLEANUP_GROUP = uintptr

type PTP_CLEANUP_GROUP_CANCEL_CALLBACK = uintptr

type TP_CALLBACK_ENVIRON_V1 = struct {
	FVersion                    TP_VERSION
	FPool                       PTP_POOL
	FCleanupGroup               PTP_CLEANUP_GROUP
	FCleanupGroupCancelCallback PTP_CLEANUP_GROUP_CANCEL_CALLBACK
	FRaceDll                    PVOID
	FActivationContext          uintptr
	FFinalizationCallback       PTP_SIMPLE_CALLBACK
	Fu                          struct {
		Fs [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type _TP_CALLBACK_ENVIRON_V1 = TP_CALLBACK_ENVIRON_V1

type TP_CALLBACK_ENVIRON = struct {
	FVersion                    TP_VERSION
	FPool                       PTP_POOL
	FCleanupGroup               PTP_CLEANUP_GROUP
	FCleanupGroupCancelCallback PTP_CLEANUP_GROUP_CANCEL_CALLBACK
	FRaceDll                    PVOID
	FActivationContext          uintptr
	FFinalizationCallback       PTP_SIMPLE_CALLBACK
	Fu                          struct {
		Fs [0]struct {
			F__ccgo0 uint32
		}
		FFlags DWORD
	}
}

type PTP_CALLBACK_ENVIRON = uintptr

type PTP_WORK = uintptr

type PTP_WORK_CALLBACK = uintptr

type PTP_TIMER = uintptr

type PTP_TIMER_CALLBACK = uintptr

type TP_WAIT_RESULT = uint32

type PTP_WAIT = uintptr

type PTP_WAIT_CALLBACK = uintptr

type PTP_IO = uintptr

type CRM_PROTOCOL_ID = struct {
	FData1 uint32
	FData2 uint16
	FData3 uint16
	FData4 [8]uint8
}

type PCRM_PROTOCOL_ID = uintptr

type NOTIFICATION_MASK = uint32

type TRANSACTION_NOTIFICATION = struct {
	FTransactionKey          PVOID
	FTransactionNotification ULONG
	FTmVirtualClock          LARGE_INTEGER
	FArgumentLength          ULONG
}

type _TRANSACTION_NOTIFICATION = TRANSACTION_NOTIFICATION

type PTRANSACTION_NOTIFICATION = uintptr

type TRANSACTION_NOTIFICATION_RECOVERY_ARGUMENT = struct {
	FEnlistmentId GUID
	FUOW          GUID
}

type _TRANSACTION_NOTIFICATION_RECOVERY_ARGUMENT = TRANSACTION_NOTIFICATION_RECOVERY_ARGUMENT

type PTRANSACTION_NOTIFICATION_RECOVERY_ARGUMENT = uintptr

type TRANSACTION_NOTIFICATION_TM_ONLINE_ARGUMENT = struct {
	FTmIdentity GUID
	FFlags      ULONG
}

type _TRANSACTION_NOTIFICATION_TM_ONLINE_ARGUMENT = TRANSACTION_NOTIFICATION_TM_ONLINE_ARGUMENT

type PTRANSACTION_NOTIFICATION_TM_ONLINE_ARGUMENT = uintptr

type SAVEPOINT_ID = uint32

type PSAVEPOINT_ID = uintptr

type TRANSACTION_NOTIFICATION_SAVEPOINT_ARGUMENT = struct {
	FSavepointId SAVEPOINT_ID
}

type _TRANSACTION_NOTIFICATION_SAVEPOINT_ARGUMENT = TRANSACTION_NOTIFICATION_SAVEPOINT_ARGUMENT

type PTRANSACTION_NOTIFICATION_SAVEPOINT_ARGUMENT = uintptr

type TRANSACTION_NOTIFICATION_PROPAGATE_ARGUMENT = struct {
	FPropagationCookie ULONG
	FUOW               GUID
	FTmIdentity        GUID
	FBufferLength      ULONG
}

type _TRANSACTION_NOTIFICATION_PROPAGATE_ARGUMENT = TRANSACTION_NOTIFICATION_PROPAGATE_ARGUMENT

type PTRANSACTION_NOTIFICATION_PROPAGATE_ARGUMENT = uintptr

type TRANSACTION_NOTIFICATION_MARSHAL_ARGUMENT = struct {
	FMarshalCookie ULONG
	FUOW           GUID
}

type _TRANSACTION_NOTIFICATION_MARSHAL_ARGUMENT = TRANSACTION_NOTIFICATION_MARSHAL_ARGUMENT

type PTRANSACTION_NOTIFICATION_MARSHAL_ARGUMENT = uintptr

type TRANSACTION_NOTIFICATION_PROMOTE_ARGUMENT = struct {
	FPropagationCookie ULONG
	FUOW               GUID
	FTmIdentity        GUID
	FBufferLength      ULONG
}

type PTRANSACTION_NOTIFICATION_PROMOTE_ARGUMENT = uintptr

type KCRM_MARSHAL_HEADER = struct {
	FVersionMajor ULONG
	FVersionMinor ULONG
	FNumProtocols ULONG
	FUnused       ULONG
}

type _KCRM_MARSHAL_HEADER = KCRM_MARSHAL_HEADER

type PKCRM_MARSHAL_HEADER = uintptr

type PRKCRM_MARSHAL_HEADER = uintptr

type KCRM_TRANSACTION_BLOB = struct {
	FUOW            GUID
	FTmIdentity     GUID
	FIsolationLevel ULONG
	FIsolationFlags ULONG
	FTimeout        ULONG
	FDescription    [64]WCHAR
}

type _KCRM_TRANSACTION_BLOB = KCRM_TRANSACTION_BLOB

type PKCRM_TRANSACTION_BLOB = uintptr

type PRKCRM_TRANSACTION_BLOB = uintptr

type KCRM_PROTOCOL_BLOB = struct {
	FProtocolId              CRM_PROTOCOL_ID
	FStaticInfoLength        ULONG
	FTransactionIdInfoLength ULONG
	FUnused1                 ULONG
	FUnused2                 ULONG
}

type _KCRM_PROTOCOL_BLOB = KCRM_PROTOCOL_BLOB

type PKCRM_PROTOCOL_BLOB = uintptr

type PRKCRM_PROTOCOL_BLOB = uintptr

type TRANSACTION_OUTCOME = int32

type _TRANSACTION_OUTCOME = int32

const TransactionOutcomeUndetermined = 1
const TransactionOutcomeCommitted = 2
const TransactionOutcomeAborted = 3

type TRANSACTION_STATE = int32

type _TRANSACTION_STATE = int32

const TransactionStateNormal = 1
const TransactionStateIndoubt = 2
const TransactionStateCommittedNotify = 3

type TRANSACTION_BASIC_INFORMATION = struct {
	FTransactionId GUID
	FState         DWORD
	FOutcome       DWORD
}

type _TRANSACTION_BASIC_INFORMATION = TRANSACTION_BASIC_INFORMATION

type PTRANSACTION_BASIC_INFORMATION = uintptr

type TRANSACTIONMANAGER_BASIC_INFORMATION = struct {
	FTmIdentity   GUID
	FVirtualClock LARGE_INTEGER
}

type _TRANSACTIONMANAGER_BASIC_INFORMATION = TRANSACTIONMANAGER_BASIC_INFORMATION

type PTRANSACTIONMANAGER_BASIC_INFORMATION = uintptr

type TRANSACTIONMANAGER_LOG_INFORMATION = struct {
	FLogIdentity GUID
}

type _TRANSACTIONMANAGER_LOG_INFORMATION = TRANSACTIONMANAGER_LOG_INFORMATION

type PTRANSACTIONMANAGER_LOG_INFORMATION = uintptr

type TRANSACTIONMANAGER_LOGPATH_INFORMATION = struct {
	FLogPathLength DWORD
	FLogPath       [1]WCHAR
}

type _TRANSACTIONMANAGER_LOGPATH_INFORMATION = TRANSACTIONMANAGER_LOGPATH_INFORMATION

type PTRANSACTIONMANAGER_LOGPATH_INFORMATION = uintptr

type TRANSACTIONMANAGER_RECOVERY_INFORMATION = struct {
	FLastRecoveredLsn ULONGLONG
}

type _TRANSACTIONMANAGER_RECOVERY_INFORMATION = TRANSACTIONMANAGER_RECOVERY_INFORMATION

type PTRANSACTIONMANAGER_RECOVERY_INFORMATION = uintptr

type TRANSACTIONMANAGER_OLDEST_INFORMATION = struct {
	FOldestTransactionGuid GUID
}

type _TRANSACTIONMANAGER_OLDEST_INFORMATION = TRANSACTIONMANAGER_OLDEST_INFORMATION

type PTRANSACTIONMANAGER_OLDEST_INFORMATION = uintptr

type TRANSACTION_PROPERTIES_INFORMATION = struct {
	FIsolationLevel    DWORD
	FIsolationFlags    DWORD
	FTimeout           LARGE_INTEGER
	FOutcome           DWORD
	FDescriptionLength DWORD
	FDescription       [1]WCHAR
}

type _TRANSACTION_PROPERTIES_INFORMATION = TRANSACTION_PROPERTIES_INFORMATION

type PTRANSACTION_PROPERTIES_INFORMATION = uintptr

type TRANSACTION_BIND_INFORMATION = struct {
	FTmHandle HANDLE
}

type _TRANSACTION_BIND_INFORMATION = TRANSACTION_BIND_INFORMATION

type PTRANSACTION_BIND_INFORMATION = uintptr

type TRANSACTION_ENLISTMENT_PAIR = struct {
	FEnlistmentId      GUID
	FResourceManagerId GUID
}

type _TRANSACTION_ENLISTMENT_PAIR = TRANSACTION_ENLISTMENT_PAIR

type PTRANSACTION_ENLISTMENT_PAIR = uintptr

type TRANSACTION_ENLISTMENTS_INFORMATION = struct {
	FNumberOfEnlistments DWORD
	FEnlistmentPair      [1]TRANSACTION_ENLISTMENT_PAIR
}

type _TRANSACTION_ENLISTMENTS_INFORMATION = TRANSACTION_ENLISTMENTS_INFORMATION

type PTRANSACTION_ENLISTMENTS_INFORMATION = uintptr

type TRANSACTION_SUPERIOR_ENLISTMENT_INFORMATION = struct {
	FSuperiorEnlistmentPair TRANSACTION_ENLISTMENT_PAIR
}

type _TRANSACTION_SUPERIOR_ENLISTMENT_INFORMATION = TRANSACTION_SUPERIOR_ENLISTMENT_INFORMATION

type PTRANSACTION_SUPERIOR_ENLISTMENT_INFORMATION = uintptr

type RESOURCEMANAGER_BASIC_INFORMATION = struct {
	FResourceManagerId GUID
	FDescriptionLength DWORD
	FDescription       [1]WCHAR
}

type _RESOURCEMANAGER_BASIC_INFORMATION = RESOURCEMANAGER_BASIC_INFORMATION

type PRESOURCEMANAGER_BASIC_INFORMATION = uintptr

type RESOURCEMANAGER_COMPLETION_INFORMATION = struct {
	FIoCompletionPortHandle HANDLE
	FCompletionKey          ULONG_PTR
}

type _RESOURCEMANAGER_COMPLETION_INFORMATION = RESOURCEMANAGER_COMPLETION_INFORMATION

type PRESOURCEMANAGER_COMPLETION_INFORMATION = uintptr

type TRANSACTION_INFORMATION_CLASS = int32

type _TRANSACTION_INFORMATION_CLASS = int32

const TransactionBasicInformation = 0
const TransactionPropertiesInformation = 1
const TransactionEnlistmentInformation = 2
const TransactionSuperiorEnlistmentInformation = 3
const TransactionBindInformation = 4
const TransactionDTCPrivateInformation = 5

type TRANSACTIONMANAGER_INFORMATION_CLASS = int32

type _TRANSACTIONMANAGER_INFORMATION_CLASS = int32

const TransactionManagerBasicInformation = 0
const TransactionManagerLogInformation = 1
const TransactionManagerLogPathInformation = 2
const TransactionManagerOnlineProbeInformation = 3
const TransactionManagerRecoveryInformation = 4
const TransactionManagerOldestTransactionInformation = 5

type RESOURCEMANAGER_INFORMATION_CLASS = int32

type _RESOURCEMANAGER_INFORMATION_CLASS = int32

const ResourceManagerBasicInformation = 0
const ResourceManagerCompletionInformation = 1

type ENLISTMENT_BASIC_INFORMATION = struct {
	FEnlistmentId      GUID
	FTransactionId     GUID
	FResourceManagerId GUID
}

type _ENLISTMENT_BASIC_INFORMATION = ENLISTMENT_BASIC_INFORMATION

type PENLISTMENT_BASIC_INFORMATION = uintptr

type ENLISTMENT_CRM_INFORMATION = struct {
	FCrmTransactionManagerId GUID
	FCrmResourceManagerId    GUID
	FCrmEnlistmentId         GUID
}

type _ENLISTMENT_CRM_INFORMATION = ENLISTMENT_CRM_INFORMATION

type PENLISTMENT_CRM_INFORMATION = uintptr

type ENLISTMENT_INFORMATION_CLASS = int32

type _ENLISTMENT_INFORMATION_CLASS = int32

const EnlistmentBasicInformation = 0
const EnlistmentRecoveryInformation = 1
const EnlistmentCrmInformation = 2

type TRANSACTION_LIST_ENTRY = struct {
	FUOW GUID
}

type _TRANSACTION_LIST_ENTRY = TRANSACTION_LIST_ENTRY

type PTRANSACTION_LIST_ENTRY = uintptr

type TRANSACTION_LIST_INFORMATION = struct {
	FNumberOfTransactions   DWORD
	FTransactionInformation [1]TRANSACTION_LIST_ENTRY
}

type _TRANSACTION_LIST_INFORMATION = TRANSACTION_LIST_INFORMATION

type PTRANSACTION_LIST_INFORMATION = uintptr

type KTMOBJECT_TYPE = int32

type _KTMOBJECT_TYPE = int32

const KTMOBJECT_TRANSACTION = 0
const KTMOBJECT_TRANSACTION_MANAGER = 1
const KTMOBJECT_RESOURCE_MANAGER = 2
const KTMOBJECT_ENLISTMENT = 3
const KTMOBJECT_INVALID = 4

type PKTMOBJECT_TYPE = uintptr

type KTMOBJECT_CURSOR = struct {
	FLastQuery     GUID
	FObjectIdCount DWORD
	FObjectIds     [1]GUID
}

type _KTMOBJECT_CURSOR = KTMOBJECT_CURSOR

type PKTMOBJECT_CURSOR = uintptr

type WOW64_FLOATING_SAVE_AREA = struct {
	FControlWord   DWORD
	FStatusWord    DWORD
	FTagWord       DWORD
	FErrorOffset   DWORD
	FErrorSelector DWORD
	FDataOffset    DWORD
	FDataSelector  DWORD
	FRegisterArea  [80]BYTE
	FCr0NpxState   DWORD
}

type _WOW64_FLOATING_SAVE_AREA = WOW64_FLOATING_SAVE_AREA

type PWOW64_FLOATING_SAVE_AREA = uintptr

type WOW64_CONTEXT = struct {
	FContextFlags      DWORD
	FDr0               DWORD
	FDr1               DWORD
	FDr2               DWORD
	FDr3               DWORD
	FDr6               DWORD
	FDr7               DWORD
	FFloatSave         WOW64_FLOATING_SAVE_AREA
	FSegGs             DWORD
	FSegFs             DWORD
	FSegEs             DWORD
	FSegDs             DWORD
	FEdi               DWORD
	FEsi               DWORD
	FEbx               DWORD
	FEdx               DWORD
	FEcx               DWORD
	FEax               DWORD
	FEbp               DWORD
	FEip               DWORD
	FSegCs             DWORD
	FEFlags            DWORD
	FEsp               DWORD
	FSegSs             DWORD
	FExtendedRegisters [512]BYTE
}

type _WOW64_CONTEXT = WOW64_CONTEXT

type PWOW64_CONTEXT = uintptr

type WOW64_LDT_ENTRY = struct {
	FLimitLow WORD
	FBaseLow  WORD
	FHighWord struct {
		FBits [0]struct {
			F__ccgo0 uint32
		}
		FBytes struct {
			FBaseMid BYTE
			FFlags1  BYTE
			FFlags2  BYTE
			FBaseHi  BYTE
		}
	}
}

type _WOW64_LDT_ENTRY = WOW64_LDT_ENTRY

type PWOW64_LDT_ENTRY = uintptr

type WOW64_DESCRIPTOR_TABLE_ENTRY = struct {
	FSelector   DWORD
	FDescriptor WOW64_LDT_ENTRY
}

type _WOW64_DESCRIPTOR_TABLE_ENTRY = WOW64_DESCRIPTOR_TABLE_ENTRY

type PWOW64_DESCRIPTOR_TABLE_ENTRY = uintptr

type WPARAM = uint64

type LPARAM = int64

type LRESULT = int64

type SPHANDLE = uintptr

type LPHANDLE = uintptr

type HGLOBAL = uintptr

type HLOCAL = uintptr

type GLOBALHANDLE = uintptr

type LOCALHANDLE = uintptr

type FARPROC = uintptr

type NEARPROC = uintptr

type PROC = uintptr

type ATOM = uint16

type HFILE = int32

type HINSTANCE__ = struct {
	Funused int32
}

type HINSTANCE = uintptr

type HKEY__ = struct {
	Funused int32
}

type HKEY = uintptr

type PHKEY = uintptr

type HKL__ = struct {
	Funused int32
}

type HKL = uintptr

type HLSURF__ = struct {
	Funused int32
}

type HLSURF = uintptr

type HMETAFILE__ = struct {
	Funused int32
}

type HMETAFILE = uintptr

type HMODULE = uintptr

type HRGN__ = struct {
	Funused int32
}

type HRGN = uintptr

type HRSRC__ = struct {
	Funused int32
}

type HRSRC = uintptr

type HSPRITE__ = struct {
	Funused int32
}

type HSPRITE = uintptr

type HSTR__ = struct {
	Funused int32
}

type HSTR = uintptr

type HTASK__ = struct {
	Funused int32
}

type HTASK = uintptr

type HWINSTA__ = struct {
	Funused int32
}

type HWINSTA = uintptr

type FILETIME = struct {
	FdwLowDateTime  DWORD
	FdwHighDateTime DWORD
}

type _FILETIME = FILETIME

type PFILETIME = uintptr

type LPFILETIME = uintptr

type HWND__ = struct {
	Funused int32
}

type HWND = uintptr

type HHOOK__ = struct {
	Funused int32
}

type HHOOK = uintptr

type HGDIOBJ = uintptr

type HACCEL__ = struct {
	Funused int32
}

type HACCEL = uintptr

type HBITMAP__ = struct {
	Funused int32
}

type HBITMAP = uintptr

type HBRUSH__ = struct {
	Funused int32
}

type HBRUSH = uintptr

type HCOLORSPACE__ = struct {
	Funused int32
}

type HCOLORSPACE = uintptr

type HDC__ = struct {
	Funused int32
}

type HDC = uintptr

type HGLRC__ = struct {
	Funused int32
}

type HGLRC = uintptr

type HDESK__ = struct {
	Funused int32
}

type HDESK = uintptr

type HENHMETAFILE__ = struct {
	Funused int32
}

type HENHMETAFILE = uintptr

type HFONT__ = struct {
	Funused int32
}

type HFONT = uintptr

type HICON__ = struct {
	Funused int32
}

type HICON = uintptr

type HMENU__ = struct {
	Funused int32
}

type HMENU = uintptr

type HPALETTE__ = struct {
	Funused int32
}

type HPALETTE = uintptr

type HPEN__ = struct {
	Funused int32
}

type HPEN = uintptr

type HMONITOR__ = struct {
	Funused int32
}

type HMONITOR = uintptr

type HWINEVENTHOOK__ = struct {
	Funused int32
}

type HWINEVENTHOOK = uintptr

type HCURSOR = uintptr

type COLORREF = uint32

type HUMPD__ = struct {
	Funused int32
}

type HUMPD = uintptr

type LPCOLORREF = uintptr

type RECT = struct {
	Fleft   LONG
	Ftop    LONG
	Fright  LONG
	Fbottom LONG
}

type tagRECT = RECT

type PRECT = uintptr

type NPRECT = uintptr

type LPRECT = uintptr

type LPCRECT = uintptr

type RECTL = struct {
	Fleft   LONG
	Ftop    LONG
	Fright  LONG
	Fbottom LONG
}

type _RECTL = RECTL

type PRECTL = uintptr

type LPRECTL = uintptr

type LPCRECTL = uintptr

type POINT = struct {
	Fx LONG
	Fy LONG
}

type tagPOINT = POINT

type PPOINT = uintptr

type NPPOINT = uintptr

type LPPOINT = uintptr

type POINTL = struct {
	Fx LONG
	Fy LONG
}

type _POINTL = POINTL

type PPOINTL = uintptr

type SIZE = struct {
	Fcx LONG
	Fcy LONG
}

type tagSIZE = SIZE

type PSIZE = uintptr

type LPSIZE = uintptr

type SIZEL = struct {
	Fcx LONG
	Fcy LONG
}

type PSIZEL = uintptr

type LPSIZEL = uintptr

type POINTS = struct {
	Fx SHORT
	Fy SHORT
}

type tagPOINTS = POINTS

type PPOINTS = uintptr

type LPPOINTS = uintptr

type APP_LOCAL_DEVICE_ID = struct {
	Fvalue [32]BYTE
}

type DPI_AWARENESS_CONTEXT__ = struct {
	Funused int32
}

type DPI_AWARENESS_CONTEXT = uintptr

type DPI_AWARENESS1 = int32

type DPI_AWARENESS = int32

const DPI_AWARENESS_INVALID = -1
const DPI_AWARENESS_UNAWARE = 0
const DPI_AWARENESS_SYSTEM_AWARE = 1
const DPI_AWARENESS_PER_MONITOR_AWARE = 2

type DPI_HOSTING_BEHAVIOR1 = int32

type DPI_HOSTING_BEHAVIOR = int32

const DPI_HOSTING_BEHAVIOR_INVALID = -1
const DPI_HOSTING_BEHAVIOR_DEFAULT = 0
const DPI_HOSTING_BEHAVIOR_MIXED = 1

type SECURITY_ATTRIBUTES = struct {
	FnLength              DWORD
	FlpSecurityDescriptor LPVOID
	FbInheritHandle       WINBOOL
}

type _SECURITY_ATTRIBUTES = SECURITY_ATTRIBUTES

type PSECURITY_ATTRIBUTES = uintptr

type LPSECURITY_ATTRIBUTES = uintptr

type OVERLAPPED = struct {
	FInternal     ULONG_PTR
	FInternalHigh ULONG_PTR
	F__ccgo2_16   struct {
		FPointer   [0]PVOID
		F__ccgo0_0 struct {
			FOffset     DWORD
			FOffsetHigh DWORD
		}
	}
	FhEvent HANDLE
}

type _OVERLAPPED = OVERLAPPED

type LPOVERLAPPED = uintptr

type OVERLAPPED_ENTRY = struct {
	FlpCompletionKey            ULONG_PTR
	FlpOverlapped               LPOVERLAPPED
	FInternal                   ULONG_PTR
	FdwNumberOfBytesTransferred DWORD
}

type _OVERLAPPED_ENTRY = OVERLAPPED_ENTRY

type LPOVERLAPPED_ENTRY = uintptr

type SYSTEMTIME = struct {
	FwYear         WORD
	FwMonth        WORD
	FwDayOfWeek    WORD
	FwDay          WORD
	FwHour         WORD
	FwMinute       WORD
	FwSecond       WORD
	FwMilliseconds WORD
}

type _SYSTEMTIME = SYSTEMTIME

type PSYSTEMTIME = uintptr

type LPSYSTEMTIME = uintptr

type WIN32_FIND_DATAA = struct {
	FdwFileAttributes   DWORD
	FftCreationTime     FILETIME
	FftLastAccessTime   FILETIME
	FftLastWriteTime    FILETIME
	FnFileSizeHigh      DWORD
	FnFileSizeLow       DWORD
	FdwReserved0        DWORD
	FdwReserved1        DWORD
	FcFileName          [260]CHAR
	FcAlternateFileName [14]CHAR
}

type _WIN32_FIND_DATAA = WIN32_FIND_DATAA

type PWIN32_FIND_DATAA = uintptr

type LPWIN32_FIND_DATAA = uintptr

type WIN32_FIND_DATAW = struct {
	FdwFileAttributes   DWORD
	FftCreationTime     FILETIME
	FftLastAccessTime   FILETIME
	FftLastWriteTime    FILETIME
	FnFileSizeHigh      DWORD
	FnFileSizeLow       DWORD
	FdwReserved0        DWORD
	FdwReserved1        DWORD
	FcFileName          [260]WCHAR
	FcAlternateFileName [14]WCHAR
}

type _WIN32_FIND_DATAW = WIN32_FIND_DATAW

type PWIN32_FIND_DATAW = uintptr

type LPWIN32_FIND_DATAW = uintptr

type WIN32_FIND_DATA = struct {
	FdwFileAttributes   DWORD
	FftCreationTime     FILETIME
	FftLastAccessTime   FILETIME
	FftLastWriteTime    FILETIME
	FnFileSizeHigh      DWORD
	FnFileSizeLow       DWORD
	FdwReserved0        DWORD
	FdwReserved1        DWORD
	FcFileName          [260]CHAR
	FcAlternateFileName [14]CHAR
}

type PWIN32_FIND_DATA = uintptr

type LPWIN32_FIND_DATA = uintptr

type FINDEX_INFO_LEVELS = int32

type _FINDEX_INFO_LEVELS = int32

const FindExInfoStandard = 0
const FindExInfoBasic = 1
const FindExInfoMaxInfoLevel = 2

type FINDEX_SEARCH_OPS = int32

type _FINDEX_SEARCH_OPS = int32

const FindExSearchNameMatch = 0
const FindExSearchLimitToDirectories = 1
const FindExSearchLimitToDevices = 2
const FindExSearchMaxSearchOp = 3

type GET_FILEEX_INFO_LEVELS = int32

type _GET_FILEEX_INFO_LEVELS = int32

const GetFileExInfoStandard = 0
const GetFileExMaxInfoLevel = 1

type FILE_INFO_BY_HANDLE_CLASS = int32

type _FILE_INFO_BY_HANDLE_CLASS = int32

const FileBasicInfo = 0
const FileStandardInfo = 1
const FileNameInfo = 2
const FileRenameInfo = 3
const FileDispositionInfo = 4
const FileAllocationInfo = 5
const FileEndOfFileInfo = 6
const FileStreamInfo = 7
const FileCompressionInfo = 8
const FileAttributeTagInfo = 9
const FileIdBothDirectoryInfo = 10
const FileIdBothDirectoryRestartInfo = 11
const FileIoPriorityHintInfo = 12
const FileRemoteProtocolInfo = 13
const FileFullDirectoryInfo = 14
const FileFullDirectoryRestartInfo = 15
const MaximumFileInfoByHandleClass = 16

type PFILE_INFO_BY_HANDLE_CLASS = uintptr

type CRITICAL_SECTION = struct {
	FDebugInfo      PRTL_CRITICAL_SECTION_DEBUG
	FLockCount      LONG
	FRecursionCount LONG
	FOwningThread   HANDLE
	FLockSemaphore  HANDLE
	FSpinCount      ULONG_PTR
}

type PCRITICAL_SECTION = uintptr

type LPCRITICAL_SECTION = uintptr

type CRITICAL_SECTION_DEBUG = struct {
	FType                      WORD
	FCreatorBackTraceIndex     WORD
	FCriticalSection           uintptr
	FProcessLocksList          LIST_ENTRY
	FEntryCount                DWORD
	FContentionCount           DWORD
	FFlags                     DWORD
	FCreatorBackTraceIndexHigh WORD
	FSpareWORD                 WORD
}

type PCRITICAL_SECTION_DEBUG = uintptr

type LPCRITICAL_SECTION_DEBUG = uintptr

type LPOVERLAPPED_COMPLETION_ROUTINE = uintptr

type PROCESS_HEAP_ENTRY = struct {
	FlpData       PVOID
	FcbData       DWORD
	FcbOverhead   BYTE
	FiRegionIndex BYTE
	FwFlags       WORD
	F__ccgo5_16   struct {
		FRegion [0]struct {
			FdwCommittedSize   DWORD
			FdwUnCommittedSize DWORD
			FlpFirstBlock      LPVOID
			FlpLastBlock       LPVOID
		}
		FBlock struct {
			FhMem       HANDLE
			FdwReserved [3]DWORD
		}
	}
}

type _PROCESS_HEAP_ENTRY = PROCESS_HEAP_ENTRY

type LPPROCESS_HEAP_ENTRY = uintptr

type PPROCESS_HEAP_ENTRY = uintptr

type REASON_CONTEXT = struct {
	FVersion ULONG
	FFlags   DWORD
	FReason  struct {
		FSimpleReasonString [0]LPWSTR
		FDetailed           struct {
			FLocalizedReasonModule HMODULE
			FLocalizedReasonId     ULONG
			FReasonStringCount     ULONG
			FReasonStrings         uintptr
		}
	}
}

type _REASON_CONTEXT = REASON_CONTEXT

type PREASON_CONTEXT = uintptr

type PTHREAD_START_ROUTINE = uintptr

type LPTHREAD_START_ROUTINE = uintptr

type PENCLAVE_ROUTINE = uintptr

type LPENCLAVE_ROUTINE = uintptr

type EXCEPTION_DEBUG_INFO = struct {
	FExceptionRecord EXCEPTION_RECORD
	FdwFirstChance   DWORD
}

type _EXCEPTION_DEBUG_INFO = EXCEPTION_DEBUG_INFO

type LPEXCEPTION_DEBUG_INFO = uintptr

type CREATE_THREAD_DEBUG_INFO = struct {
	FhThread           HANDLE
	FlpThreadLocalBase LPVOID
	FlpStartAddress    LPTHREAD_START_ROUTINE
}

type _CREATE_THREAD_DEBUG_INFO = CREATE_THREAD_DEBUG_INFO

type LPCREATE_THREAD_DEBUG_INFO = uintptr

type CREATE_PROCESS_DEBUG_INFO = struct {
	FhFile                 HANDLE
	FhProcess              HANDLE
	FhThread               HANDLE
	FlpBaseOfImage         LPVOID
	FdwDebugInfoFileOffset DWORD
	FnDebugInfoSize        DWORD
	FlpThreadLocalBase     LPVOID
	FlpStartAddress        LPTHREAD_START_ROUTINE
	FlpImageName           LPVOID
	FfUnicode              WORD
}

type _CREATE_PROCESS_DEBUG_INFO = CREATE_PROCESS_DEBUG_INFO

type LPCREATE_PROCESS_DEBUG_INFO = uintptr

type EXIT_THREAD_DEBUG_INFO = struct {
	FdwExitCode DWORD
}

type _EXIT_THREAD_DEBUG_INFO = EXIT_THREAD_DEBUG_INFO

type LPEXIT_THREAD_DEBUG_INFO = uintptr

type EXIT_PROCESS_DEBUG_INFO = struct {
	FdwExitCode DWORD
}

type _EXIT_PROCESS_DEBUG_INFO = EXIT_PROCESS_DEBUG_INFO

type LPEXIT_PROCESS_DEBUG_INFO = uintptr

type LOAD_DLL_DEBUG_INFO = struct {
	FhFile                 HANDLE
	FlpBaseOfDll           LPVOID
	FdwDebugInfoFileOffset DWORD
	FnDebugInfoSize        DWORD
	FlpImageName           LPVOID
	FfUnicode              WORD
}

type _LOAD_DLL_DEBUG_INFO = LOAD_DLL_DEBUG_INFO

type LPLOAD_DLL_DEBUG_INFO = uintptr

type UNLOAD_DLL_DEBUG_INFO = struct {
	FlpBaseOfDll LPVOID
}

type _UNLOAD_DLL_DEBUG_INFO = UNLOAD_DLL_DEBUG_INFO

type LPUNLOAD_DLL_DEBUG_INFO = uintptr

type OUTPUT_DEBUG_STRING_INFO = struct {
	FlpDebugStringData  LPSTR
	FfUnicode           WORD
	FnDebugStringLength WORD
}

type _OUTPUT_DEBUG_STRING_INFO = OUTPUT_DEBUG_STRING_INFO

type LPOUTPUT_DEBUG_STRING_INFO = uintptr

type RIP_INFO = struct {
	FdwError DWORD
	FdwType  DWORD
}

type _RIP_INFO = RIP_INFO

type LPRIP_INFO = uintptr

type DEBUG_EVENT = struct {
	FdwDebugEventCode DWORD
	FdwProcessId      DWORD
	FdwThreadId       DWORD
	Fu                struct {
		FCreateThread      [0]CREATE_THREAD_DEBUG_INFO
		FCreateProcessInfo [0]CREATE_PROCESS_DEBUG_INFO
		FExitThread        [0]EXIT_THREAD_DEBUG_INFO
		FExitProcess       [0]EXIT_PROCESS_DEBUG_INFO
		FLoadDll           [0]LOAD_DLL_DEBUG_INFO
		FUnloadDll         [0]UNLOAD_DLL_DEBUG_INFO
		FDebugString       [0]OUTPUT_DEBUG_STRING_INFO
		FRipInfo           [0]RIP_INFO
		FException         EXCEPTION_DEBUG_INFO
	}
}

type _DEBUG_EVENT = DEBUG_EVENT

type LPDEBUG_EVENT = uintptr

type LPCONTEXT = uintptr

type BEM_FREE_INTERFACE_CALLBACK = uintptr

type PTOP_LEVEL_EXCEPTION_FILTER = uintptr

type LPTOP_LEVEL_EXCEPTION_FILTER = uintptr

type BY_HANDLE_FILE_INFORMATION = struct {
	FdwFileAttributes     DWORD
	FftCreationTime       FILETIME
	FftLastAccessTime     FILETIME
	FftLastWriteTime      FILETIME
	FdwVolumeSerialNumber DWORD
	FnFileSizeHigh        DWORD
	FnFileSizeLow         DWORD
	FnNumberOfLinks       DWORD
	FnFileIndexHigh       DWORD
	FnFileIndexLow        DWORD
}

type _BY_HANDLE_FILE_INFORMATION = BY_HANDLE_FILE_INFORMATION

type PBY_HANDLE_FILE_INFORMATION = uintptr

type LPBY_HANDLE_FILE_INFORMATION = uintptr

type WIN32_FILE_ATTRIBUTE_DATA = struct {
	FdwFileAttributes DWORD
	FftCreationTime   FILETIME
	FftLastAccessTime FILETIME
	FftLastWriteTime  FILETIME
	FnFileSizeHigh    DWORD
	FnFileSizeLow     DWORD
}

type _WIN32_FILE_ATTRIBUTE_DATA = WIN32_FILE_ATTRIBUTE_DATA

type LPWIN32_FILE_ATTRIBUTE_DATA = uintptr

type DISK_SPACE_INFORMATION = struct {
	FActualTotalAllocationUnits           ULONGLONG
	FActualAvailableAllocationUnits       ULONGLONG
	FActualPoolUnavailableAllocationUnits ULONGLONG
	FCallerTotalAllocationUnits           ULONGLONG
	FCallerAvailableAllocationUnits       ULONGLONG
	FCallerPoolUnavailableAllocationUnits ULONGLONG
	FUsedAllocationUnits                  ULONGLONG
	FTotalReservedAllocationUnits         ULONGLONG
	FVolumeStorageReserveAllocationUnits  ULONGLONG
	FAvailableCommittedAllocationUnits    ULONGLONG
	FPoolAvailableAllocationUnits         ULONGLONG
	FSectorsPerAllocationUnit             DWORD
	FBytesPerSector                       DWORD
}

type HEAP_SUMMARY = struct {
	Fcb           DWORD
	FcbAllocated  SIZE_T
	FcbCommitted  SIZE_T
	FcbReserved   SIZE_T
	FcbMaxReserve SIZE_T
}

type _HEAP_SUMMARY = HEAP_SUMMARY

type PHEAP_SUMMARY = uintptr

type LPHEAP_SUMMARY = uintptr

type ENUMUILANG = struct {
	FNumOfEnumUILang    ULONG
	FSizeOfEnumUIBuffer ULONG
	FpEnumUIBuffer      uintptr
}

type tagENUMUILANG = ENUMUILANG

type PENUMUILANG = uintptr

type ENUMRESLANGPROCA = uintptr

type ENUMRESLANGPROCW = uintptr

type ENUMRESNAMEPROCA = uintptr

type ENUMRESNAMEPROCW = uintptr

type ENUMRESTYPEPROCA = uintptr

type ENUMRESTYPEPROCW = uintptr

type DLL_DIRECTORY_COOKIE = uintptr

type PDLL_DIRECTORY_COOKIE = uintptr

type REDIRECTION_FUNCTION_DESCRIPTOR = struct {
	FDllName           PCSTR
	FFunctionName      PCSTR
	FRedirectionTarget PVOID
}

type _REDIRECTION_FUNCTION_DESCRIPTOR = REDIRECTION_FUNCTION_DESCRIPTOR

type PREDIRECTION_FUNCTION_DESCRIPTOR = uintptr

type PCREDIRECTION_FUNCTION_DESCRIPTOR = uintptr

type REDIRECTION_DESCRIPTOR = struct {
	FVersion       ULONG
	FFunctionCount ULONG
	FRedirections  PCREDIRECTION_FUNCTION_DESCRIPTOR
}

type _REDIRECTION_DESCRIPTOR = REDIRECTION_DESCRIPTOR

type PREDIRECTION_DESCRIPTOR = uintptr

type PCREDIRECTION_DESCRIPTOR = uintptr

type PGET_MODULE_HANDLE_EXA = uintptr

type PGET_MODULE_HANDLE_EXW = uintptr

type MEMORY_RESOURCE_NOTIFICATION_TYPE = int32

type _MEMORY_RESOURCE_NOTIFICATION_TYPE = int32

const LowMemoryResourceNotification = 0
const HighMemoryResourceNotification = 1

type STARTUPINFOA = struct {
	Fcb              DWORD
	FlpReserved      LPSTR
	FlpDesktop       LPSTR
	FlpTitle         LPSTR
	FdwX             DWORD
	FdwY             DWORD
	FdwXSize         DWORD
	FdwYSize         DWORD
	FdwXCountChars   DWORD
	FdwYCountChars   DWORD
	FdwFillAttribute DWORD
	FdwFlags         DWORD
	FwShowWindow     WORD
	FcbReserved2     WORD
	FlpReserved2     LPBYTE
	FhStdInput       HANDLE
	FhStdOutput      HANDLE
	FhStdError       HANDLE
}

type _STARTUPINFOA = STARTUPINFOA

type LPSTARTUPINFOA = uintptr

type STARTUPINFOW = struct {
	Fcb              DWORD
	FlpReserved      LPWSTR
	FlpDesktop       LPWSTR
	FlpTitle         LPWSTR
	FdwX             DWORD
	FdwY             DWORD
	FdwXSize         DWORD
	FdwYSize         DWORD
	FdwXCountChars   DWORD
	FdwYCountChars   DWORD
	FdwFillAttribute DWORD
	FdwFlags         DWORD
	FwShowWindow     WORD
	FcbReserved2     WORD
	FlpReserved2     LPBYTE
	FhStdInput       HANDLE
	FhStdOutput      HANDLE
	FhStdError       HANDLE
}

type _STARTUPINFOW = STARTUPINFOW

type LPSTARTUPINFOW = uintptr

type STARTUPINFO = struct {
	Fcb              DWORD
	FlpReserved      LPSTR
	FlpDesktop       LPSTR
	FlpTitle         LPSTR
	FdwX             DWORD
	FdwY             DWORD
	FdwXSize         DWORD
	FdwYSize         DWORD
	FdwXCountChars   DWORD
	FdwYCountChars   DWORD
	FdwFillAttribute DWORD
	FdwFlags         DWORD
	FwShowWindow     WORD
	FcbReserved2     WORD
	FlpReserved2     LPBYTE
	FhStdInput       HANDLE
	FhStdOutput      HANDLE
	FhStdError       HANDLE
}

type LPSTARTUPINFO = uintptr

type PROCESS_INFORMATION = struct {
	FhProcess    HANDLE
	FhThread     HANDLE
	FdwProcessId DWORD
	FdwThreadId  DWORD
}

type _PROCESS_INFORMATION = PROCESS_INFORMATION

type PPROCESS_INFORMATION = uintptr

type LPPROCESS_INFORMATION = uintptr

type PROCESS_INFORMATION_CLASS = int32

type _PROCESS_INFORMATION_CLASS = int32

const ProcessMemoryPriority = 0
const ProcessMemoryExhaustionInfo = 1
const ProcessAppMemoryInfo = 2
const ProcessInPrivateInfo = 3
const ProcessPowerThrottling = 4
const ProcessReservedValue1 = 5
const ProcessTelemetryCoverageInfo = 6
const ProcessProtectionLevelInfo = 7
const ProcessLeapSecondInfo = 8
const ProcessMachineTypeInfo = 9
const ProcessOverrideSubsequentPrefetchParameter = 10
const ProcessMaxOverridePrefetchParameter = 11
const ProcessInformationClassMax = 12

type APP_MEMORY_INFORMATION = struct {
	FAvailableCommit        ULONG64
	FPrivateCommitUsage     ULONG64
	FPeakPrivateCommitUsage ULONG64
	FTotalCommitUsage       ULONG64
}

type _APP_MEMORY_INFORMATION = APP_MEMORY_INFORMATION

type PAPP_MEMORY_INFORMATION = uintptr

type MACHINE_ATTRIBUTES = int32

type _MACHINE_ATTRIBUTES = int32

const UserEnabled = 1
const KernelEnabled = 2
const Wow64Container = 4

type PROCESS_MACHINE_INFORMATION = struct {
	FProcessMachine    USHORT
	FRes0              USHORT
	FMachineAttributes MACHINE_ATTRIBUTES
}

type _PROCESS_MACHINE_INFORMATION = PROCESS_MACHINE_INFORMATION

type OVERRIDE_PREFETCH_PARAMETER = struct {
	FValue UINT32
}

type PROCESS_MEMORY_EXHAUSTION_TYPE = int32

type _PROCESS_MEMORY_EXHAUSTION_TYPE = int32

const PMETypeFailFastOnCommitFailure = 0
const PMETypeMax = 1

type PPROCESS_MEMORY_EXHAUSTION_TYPE = uintptr

type PROCESS_MEMORY_EXHAUSTION_INFO = struct {
	FVersion  USHORT
	FReserved USHORT
	FType     PROCESS_MEMORY_EXHAUSTION_TYPE
	FValue    ULONG_PTR
}

type _PROCESS_MEMORY_EXHAUSTION_INFO = PROCESS_MEMORY_EXHAUSTION_INFO

type PPROCESS_MEMORY_EXHAUSTION_INFO = uintptr

type PROCESS_POWER_THROTTLING_STATE = struct {
	FVersion     ULONG
	FControlMask ULONG
	FStateMask   ULONG
}

type _PROCESS_POWER_THROTTLING_STATE = PROCESS_POWER_THROTTLING_STATE

type PPROCESS_POWER_THROTTLING_STATE = uintptr

type PROCESS_PROTECTION_LEVEL_INFORMATION = struct {
	FProtectionLevel DWORD
}

type PROCESS_LEAP_SECOND_INFO = struct {
	FFlags    ULONG
	FReserved ULONG
}

type _PROCESS_LEAP_SECOND_INFO = PROCESS_LEAP_SECOND_INFO

type PPROCESS_LEAP_SECOND_INFO = uintptr

type PPROC_THREAD_ATTRIBUTE_LIST = uintptr

type LPPROC_THREAD_ATTRIBUTE_LIST = uintptr

type THREAD_POWER_THROTTLING_STATE = struct {
	FVersion     ULONG
	FControlMask ULONG
	FStateMask   ULONG
}

type _THREAD_POWER_THROTTLING_STATE = THREAD_POWER_THROTTLING_STATE

type SRWLOCK = struct {
	FPtr PVOID
}

type PSRWLOCK = uintptr

type INIT_ONCE = struct {
	FPtr PVOID
}

type PINIT_ONCE = uintptr

type LPINIT_ONCE = uintptr

type PINIT_ONCE_FN = uintptr

type CONDITION_VARIABLE = struct {
	FPtr PVOID
}

type PCONDITION_VARIABLE = uintptr

type PTIMERAPCROUTINE = uintptr

type SYNCHRONIZATION_BARRIER = struct {
	FReserved1 DWORD
	FReserved2 DWORD
	FReserved3 [2]ULONG_PTR
	FReserved4 DWORD
	FReserved5 DWORD
}

type PSYNCHRONIZATION_BARRIER = uintptr

type LPSYNCHRONIZATION_BARRIER = uintptr

type SYSTEM_INFO = struct {
	F__ccgo0_0 struct {
		F__ccgo1_0 [0]struct {
			FwProcessorArchitecture WORD
			FwReserved              WORD
		}
		FdwOemId DWORD
	}
	FdwPageSize                  DWORD
	FlpMinimumApplicationAddress LPVOID
	FlpMaximumApplicationAddress LPVOID
	FdwActiveProcessorMask       DWORD_PTR
	FdwNumberOfProcessors        DWORD
	FdwProcessorType             DWORD
	FdwAllocationGranularity     DWORD
	FwProcessorLevel             WORD
	FwProcessorRevision          WORD
}

type _SYSTEM_INFO = SYSTEM_INFO

type LPSYSTEM_INFO = uintptr

type MEMORYSTATUSEX = struct {
	FdwLength                DWORD
	FdwMemoryLoad            DWORD
	FullTotalPhys            DWORDLONG
	FullAvailPhys            DWORDLONG
	FullTotalPageFile        DWORDLONG
	FullAvailPageFile        DWORDLONG
	FullTotalVirtual         DWORDLONG
	FullAvailVirtual         DWORDLONG
	FullAvailExtendedVirtual DWORDLONG
}

type _MEMORYSTATUSEX = MEMORYSTATUSEX

type LPMEMORYSTATUSEX = uintptr

type COMPUTER_NAME_FORMAT = int32

type _COMPUTER_NAME_FORMAT = int32

const ComputerNameNetBIOS = 0
const ComputerNameDnsHostname = 1
const ComputerNameDnsDomain = 2
const ComputerNameDnsFullyQualified = 3
const ComputerNamePhysicalNetBIOS = 4
const ComputerNamePhysicalDnsHostname = 5
const ComputerNamePhysicalDnsDomain = 6
const ComputerNamePhysicalDnsFullyQualified = 7
const ComputerNameMax = 8

type PTP_WIN32_IO_CALLBACK = uintptr

type PFIBER_START_ROUTINE = uintptr

type LPFIBER_START_ROUTINE = uintptr

type PFIBER_CALLOUT_ROUTINE = uintptr

type LPLDT_ENTRY = uintptr

type COMMPROP = struct {
	FwPacketLength       WORD
	FwPacketVersion      WORD
	FdwServiceMask       DWORD
	FdwReserved1         DWORD
	FdwMaxTxQueue        DWORD
	FdwMaxRxQueue        DWORD
	FdwMaxBaud           DWORD
	FdwProvSubType       DWORD
	FdwProvCapabilities  DWORD
	FdwSettableParams    DWORD
	FdwSettableBaud      DWORD
	FwSettableData       WORD
	FwSettableStopParity WORD
	FdwCurrentTxQueue    DWORD
	FdwCurrentRxQueue    DWORD
	FdwProvSpec1         DWORD
	FdwProvSpec2         DWORD
	FwcProvChar          [1]WCHAR
}

type _COMMPROP = COMMPROP

type LPCOMMPROP = uintptr

type COMSTAT = struct {
	F__ccgo0  uint32
	FcbInQue  DWORD
	FcbOutQue DWORD
}

type _COMSTAT = COMSTAT

type LPCOMSTAT = uintptr

type DCB = struct {
	FDCBlength  DWORD
	FBaudRate   DWORD
	F__ccgo8    uint32
	FwReserved  WORD
	FXonLim     WORD
	FXoffLim    WORD
	FByteSize   BYTE
	FParity     BYTE
	FStopBits   BYTE
	FXonChar    int8
	FXoffChar   int8
	FErrorChar  int8
	FEofChar    int8
	FEvtChar    int8
	FwReserved1 WORD
}

type _DCB = DCB

type LPDCB = uintptr

type COMMTIMEOUTS = struct {
	FReadIntervalTimeout         DWORD
	FReadTotalTimeoutMultiplier  DWORD
	FReadTotalTimeoutConstant    DWORD
	FWriteTotalTimeoutMultiplier DWORD
	FWriteTotalTimeoutConstant   DWORD
}

type _COMMTIMEOUTS = COMMTIMEOUTS

type LPCOMMTIMEOUTS = uintptr

type COMMCONFIG = struct {
	FdwSize            DWORD
	FwVersion          WORD
	FwReserved         WORD
	Fdcb               DCB
	FdwProviderSubType DWORD
	FdwProviderOffset  DWORD
	FdwProviderSize    DWORD
	FwcProviderData    [1]WCHAR
}

type _COMMCONFIG = COMMCONFIG

type LPCOMMCONFIG = uintptr

type MEMORYSTATUS = struct {
	FdwLength        DWORD
	FdwMemoryLoad    DWORD
	FdwTotalPhys     SIZE_T
	FdwAvailPhys     SIZE_T
	FdwTotalPageFile SIZE_T
	FdwAvailPageFile SIZE_T
	FdwTotalVirtual  SIZE_T
	FdwAvailVirtual  SIZE_T
}

type _MEMORYSTATUS = MEMORYSTATUS

type LPMEMORYSTATUS = uintptr

type JIT_DEBUG_INFO = struct {
	FdwSize                  DWORD
	FdwProcessorArchitecture DWORD
	FdwThreadID              DWORD
	FdwReserved0             DWORD
	FlpExceptionAddress      ULONG64
	FlpExceptionRecord       ULONG64
	FlpContextRecord         ULONG64
}

type _JIT_DEBUG_INFO = JIT_DEBUG_INFO

type LPJIT_DEBUG_INFO = uintptr

type JIT_DEBUG_INFO32 = struct {
	FdwSize                  DWORD
	FdwProcessorArchitecture DWORD
	FdwThreadID              DWORD
	FdwReserved0             DWORD
	FlpExceptionAddress      ULONG64
	FlpExceptionRecord       ULONG64
	FlpContextRecord         ULONG64
}

type LPJIT_DEBUG_INFO32 = uintptr

type JIT_DEBUG_INFO64 = struct {
	FdwSize                  DWORD
	FdwProcessorArchitecture DWORD
	FdwThreadID              DWORD
	FdwReserved0             DWORD
	FlpExceptionAddress      ULONG64
	FlpExceptionRecord       ULONG64
	FlpContextRecord         ULONG64
}

type LPJIT_DEBUG_INFO64 = uintptr

type LPEXCEPTION_RECORD = uintptr

type LPEXCEPTION_POINTERS = uintptr

type OFSTRUCT = struct {
	FcBytes     BYTE
	FfFixedDisk BYTE
	FnErrCode   WORD
	FReserved1  WORD
	FReserved2  WORD
	FszPathName [128]CHAR
}

type _OFSTRUCT = OFSTRUCT

type LPOFSTRUCT = uintptr

type POFSTRUCT = uintptr

type THREAD_INFORMATION_CLASS = int32

type _THREAD_INFORMATION_CLASS = int32

const ThreadMemoryPriority = 0
const ThreadAbsoluteCpuPriority = 1
const ThreadDynamicCodePolicy = 2
const ThreadPowerThrottling = 3
const ThreadInformationClassMax = 4

type PUMS_CONTEXT = uintptr

type PUMS_COMPLETION_LIST = uintptr

type UMS_THREAD_INFO_CLASS = int32

type PUMS_THREAD_INFO_CLASS = uintptr

type UMS_SCHEDULER_REASON = int32

type PUMS_SCHEDULER_ENTRY_POINT = uintptr

type UMS_SCHEDULER_STARTUP_INFO = struct {
	FUmsVersion     ULONG
	FCompletionList PUMS_COMPLETION_LIST
	FSchedulerProc  PUMS_SCHEDULER_ENTRY_POINT
	FSchedulerParam PVOID
}

type _UMS_SCHEDULER_STARTUP_INFO = UMS_SCHEDULER_STARTUP_INFO

type PUMS_SCHEDULER_STARTUP_INFO = uintptr

type UMS_SYSTEM_THREAD_INFORMATION = struct {
	FUmsVersion ULONG
	F__ccgo1_4  struct {
		FThreadUmsFlags [0]ULONG
		F__ccgo0_0      struct {
			F__ccgo_align [0]uint32
			F__ccgo0      uint8
		}
	}
}

type _UMS_SYSTEM_THREAD_INFORMATION = UMS_SYSTEM_THREAD_INFORMATION

type PUMS_SYSTEM_THREAD_INFORMATION = uintptr

type DEP_SYSTEM_POLICY_TYPE = int32

type _DEP_SYSTEM_POLICY_TYPE = int32

const DEPPolicyAlwaysOff = 0
const DEPPolicyAlwaysOn = 1
const DEPPolicyOptIn = 2
const DEPPolicyOptOut = 3
const DEPTotalPolicyCount = 4

type PFE_EXPORT_FUNC = uintptr

type PFE_IMPORT_FUNC = uintptr

type WIN32_STREAM_ID = struct {
	FdwStreamId         DWORD
	FdwStreamAttributes DWORD
	FSize               LARGE_INTEGER
	FdwStreamNameSize   DWORD
	FcStreamName        [1]WCHAR
}

type _WIN32_STREAM_ID = WIN32_STREAM_ID

type LPWIN32_STREAM_ID = uintptr

type STARTUPINFOEXA = struct {
	FStartupInfo     STARTUPINFOA
	FlpAttributeList LPPROC_THREAD_ATTRIBUTE_LIST
}

type _STARTUPINFOEXA = STARTUPINFOEXA

type LPSTARTUPINFOEXA = uintptr

type STARTUPINFOEXW = struct {
	FStartupInfo     STARTUPINFOW
	FlpAttributeList LPPROC_THREAD_ATTRIBUTE_LIST
}

type _STARTUPINFOEXW = STARTUPINFOEXW

type LPSTARTUPINFOEXW = uintptr

type STARTUPINFOEX = struct {
	FStartupInfo     STARTUPINFOA
	FlpAttributeList LPPROC_THREAD_ATTRIBUTE_LIST
}

type LPSTARTUPINFOEX = uintptr

type PROC_THREAD_ATTRIBUTE_NUM = int32

type _PROC_THREAD_ATTRIBUTE_NUM = int32

const ProcThreadAttributeParentProcess = 0
const ProcThreadAttributeHandleList = 2
const ProcThreadAttributeProtectionLevel = 11

type PGET_SYSTEM_WOW64_DIRECTORY_A = uintptr

type PGET_SYSTEM_WOW64_DIRECTORY_W = uintptr

type LPPROGRESS_ROUTINE = uintptr

type STREAM_INFO_LEVELS = int32

type _STREAM_INFO_LEVELS = int32

const FindStreamInfoStandard = 0
const FindStreamInfoMaxInfoLevel = 1

type WIN32_FIND_STREAM_DATA = struct {
	FStreamSize  LARGE_INTEGER
	FcStreamName [296]WCHAR
}

type _WIN32_FIND_STREAM_DATA = WIN32_FIND_STREAM_DATA

type PWIN32_FIND_STREAM_DATA = uintptr

type EVENTLOG_FULL_INFORMATION = struct {
	FdwFull DWORD
}

type _EVENTLOG_FULL_INFORMATION = EVENTLOG_FULL_INFORMATION

type LPEVENTLOG_FULL_INFORMATION = uintptr

type HW_PROFILE_INFOA = struct {
	FdwDockInfo      DWORD
	FszHwProfileGuid [39]CHAR
	FszHwProfileName [80]CHAR
}

type tagHW_PROFILE_INFOA = HW_PROFILE_INFOA

type LPHW_PROFILE_INFOA = uintptr

type HW_PROFILE_INFOW = struct {
	FdwDockInfo      DWORD
	FszHwProfileGuid [39]WCHAR
	FszHwProfileName [80]WCHAR
}

type tagHW_PROFILE_INFOW = HW_PROFILE_INFOW

type LPHW_PROFILE_INFOW = uintptr

type HW_PROFILE_INFO = struct {
	FdwDockInfo      DWORD
	FszHwProfileGuid [39]CHAR
	FszHwProfileName [80]CHAR
}

type LPHW_PROFILE_INFO = uintptr

type TIME_ZONE_INFORMATION = struct {
	FBias         LONG
	FStandardName [32]WCHAR
	FStandardDate SYSTEMTIME
	FStandardBias LONG
	FDaylightName [32]WCHAR
	FDaylightDate SYSTEMTIME
	FDaylightBias LONG
}

type _TIME_ZONE_INFORMATION = TIME_ZONE_INFORMATION

type PTIME_ZONE_INFORMATION = uintptr

type LPTIME_ZONE_INFORMATION = uintptr

type DYNAMIC_TIME_ZONE_INFORMATION = struct {
	FBias                        LONG
	FStandardName                [32]WCHAR
	FStandardDate                SYSTEMTIME
	FStandardBias                LONG
	FDaylightName                [32]WCHAR
	FDaylightDate                SYSTEMTIME
	FDaylightBias                LONG
	FTimeZoneKeyName             [128]WCHAR
	FDynamicDaylightTimeDisabled BOOLEAN
}

type _TIME_DYNAMIC_ZONE_INFORMATION = DYNAMIC_TIME_ZONE_INFORMATION

type PDYNAMIC_TIME_ZONE_INFORMATION = uintptr

type SYSTEM_POWER_STATUS = struct {
	FACLineStatus        BYTE
	FBatteryFlag         BYTE
	FBatteryLifePercent  BYTE
	FSystemStatusFlag    BYTE
	FBatteryLifeTime     DWORD
	FBatteryFullLifeTime DWORD
}

type _SYSTEM_POWER_STATUS = SYSTEM_POWER_STATUS

type LPSYSTEM_POWER_STATUS = uintptr

type ACTCTXA = struct {
	FcbSize                 ULONG
	FdwFlags                DWORD
	FlpSource               LPCSTR
	FwProcessorArchitecture USHORT
	FwLangId                LANGID
	FlpAssemblyDirectory    LPCSTR
	FlpResourceName         LPCSTR
	FlpApplicationName      LPCSTR
	FhModule                HMODULE
}

type tagACTCTXA = ACTCTXA

type PACTCTXA = uintptr

type ACTCTXW = struct {
	FcbSize                 ULONG
	FdwFlags                DWORD
	FlpSource               LPCWSTR
	FwProcessorArchitecture USHORT
	FwLangId                LANGID
	FlpAssemblyDirectory    LPCWSTR
	FlpResourceName         LPCWSTR
	FlpApplicationName      LPCWSTR
	FhModule                HMODULE
}

type tagACTCTXW = ACTCTXW

type PACTCTXW = uintptr

type ACTCTX = struct {
	FcbSize                 ULONG
	FdwFlags                DWORD
	FlpSource               LPCSTR
	FwProcessorArchitecture USHORT
	FwLangId                LANGID
	FlpAssemblyDirectory    LPCSTR
	FlpResourceName         LPCSTR
	FlpApplicationName      LPCSTR
	FhModule                HMODULE
}

type PACTCTX = uintptr

type PCACTCTXA = uintptr

type PCACTCTXW = uintptr

type PCACTCTX = uintptr

type ACTCTX_SECTION_KEYED_DATA_2600 = struct {
	FcbSize                    ULONG
	FulDataFormatVersion       ULONG
	FlpData                    PVOID
	FulLength                  ULONG
	FlpSectionGlobalData       PVOID
	FulSectionGlobalDataLength ULONG
	FlpSectionBase             PVOID
	FulSectionTotalLength      ULONG
	FhActCtx                   HANDLE
	FulAssemblyRosterIndex     ULONG
}

type tagACTCTX_SECTION_KEYED_DATA_2600 = ACTCTX_SECTION_KEYED_DATA_2600

type PACTCTX_SECTION_KEYED_DATA_2600 = uintptr

type PCACTCTX_SECTION_KEYED_DATA_2600 = uintptr

type ACTCTX_SECTION_KEYED_DATA_ASSEMBLY_METADATA = struct {
	FlpInformation             PVOID
	FlpSectionBase             PVOID
	FulSectionLength           ULONG
	FlpSectionGlobalDataBase   PVOID
	FulSectionGlobalDataLength ULONG
}

type tagACTCTX_SECTION_KEYED_DATA_ASSEMBLY_METADATA = ACTCTX_SECTION_KEYED_DATA_ASSEMBLY_METADATA

type PACTCTX_SECTION_KEYED_DATA_ASSEMBLY_METADATA = uintptr

type PCACTCTX_SECTION_KEYED_DATA_ASSEMBLY_METADATA = uintptr

type ACTCTX_SECTION_KEYED_DATA = struct {
	FcbSize                    ULONG
	FulDataFormatVersion       ULONG
	FlpData                    PVOID
	FulLength                  ULONG
	FlpSectionGlobalData       PVOID
	FulSectionGlobalDataLength ULONG
	FlpSectionBase             PVOID
	FulSectionTotalLength      ULONG
	FhActCtx                   HANDLE
	FulAssemblyRosterIndex     ULONG
	FulFlags                   ULONG
	FAssemblyMetadata          ACTCTX_SECTION_KEYED_DATA_ASSEMBLY_METADATA
}

type tagACTCTX_SECTION_KEYED_DATA = ACTCTX_SECTION_KEYED_DATA

type PACTCTX_SECTION_KEYED_DATA = uintptr

type PCACTCTX_SECTION_KEYED_DATA = uintptr

type ACTIVATION_CONTEXT_BASIC_INFORMATION = struct {
	FhActCtx HANDLE
	FdwFlags DWORD
}

type _ACTIVATION_CONTEXT_BASIC_INFORMATION = ACTIVATION_CONTEXT_BASIC_INFORMATION

type PACTIVATION_CONTEXT_BASIC_INFORMATION = uintptr

type PCACTIVATION_CONTEXT_BASIC_INFORMATION = uintptr

type PQUERYACTCTXW_FUNC = uintptr

type APPLICATION_RECOVERY_CALLBACK = uintptr

type FILE_BASIC_INFO = struct {
	FCreationTime   LARGE_INTEGER
	FLastAccessTime LARGE_INTEGER
	FLastWriteTime  LARGE_INTEGER
	FChangeTime     LARGE_INTEGER
	FFileAttributes DWORD
}

type _FILE_BASIC_INFO = FILE_BASIC_INFO

type PFILE_BASIC_INFO = uintptr

type FILE_STANDARD_INFO = struct {
	FAllocationSize LARGE_INTEGER
	FEndOfFile      LARGE_INTEGER
	FNumberOfLinks  DWORD
	FDeletePending  BOOLEAN
	FDirectory      BOOLEAN
}

type _FILE_STANDARD_INFO = FILE_STANDARD_INFO

type PFILE_STANDARD_INFO = uintptr

type FILE_NAME_INFO = struct {
	FFileNameLength DWORD
	FFileName       [1]WCHAR
}

type _FILE_NAME_INFO = FILE_NAME_INFO

type PFILE_NAME_INFO = uintptr

type FILE_CASE_SENSITIVE_INFO = struct {
	FFlags ULONG
}

type _FILE_CASE_SENSITIVE_INFO = FILE_CASE_SENSITIVE_INFO

type PFILE_CASE_SENSITIVE_INFO = uintptr

type FILE_RENAME_INFO = struct {
	F__ccgo0_0 struct {
		FFlags           [0]DWORD
		FReplaceIfExists BOOLEAN
		F__ccgo_pad2     [3]byte
	}
	FRootDirectory  HANDLE
	FFileNameLength DWORD
	FFileName       [1]WCHAR
}

type _FILE_RENAME_INFO = FILE_RENAME_INFO

type PFILE_RENAME_INFO = uintptr

type FILE_ALLOCATION_INFO = struct {
	FAllocationSize LARGE_INTEGER
}

type _FILE_ALLOCATION_INFO = FILE_ALLOCATION_INFO

type PFILE_ALLOCATION_INFO = uintptr

type FILE_END_OF_FILE_INFO = struct {
	FEndOfFile LARGE_INTEGER
}

type _FILE_END_OF_FILE_INFO = FILE_END_OF_FILE_INFO

type PFILE_END_OF_FILE_INFO = uintptr

type FILE_STREAM_INFO = struct {
	FNextEntryOffset      DWORD
	FStreamNameLength     DWORD
	FStreamSize           LARGE_INTEGER
	FStreamAllocationSize LARGE_INTEGER
	FStreamName           [1]WCHAR
}

type _FILE_STREAM_INFO = FILE_STREAM_INFO

type PFILE_STREAM_INFO = uintptr

type FILE_COMPRESSION_INFO = struct {
	FCompressedFileSize   LARGE_INTEGER
	FCompressionFormat    WORD
	FCompressionUnitShift UCHAR
	FChunkShift           UCHAR
	FClusterShift         UCHAR
	FReserved             [3]UCHAR
}

type _FILE_COMPRESSION_INFO = FILE_COMPRESSION_INFO

type PFILE_COMPRESSION_INFO = uintptr

type FILE_ATTRIBUTE_TAG_INFO = struct {
	FFileAttributes DWORD
	FReparseTag     DWORD
}

type _FILE_ATTRIBUTE_TAG_INFO = FILE_ATTRIBUTE_TAG_INFO

type PFILE_ATTRIBUTE_TAG_INFO = uintptr

type FILE_DISPOSITION_INFO = struct {
	FDeleteFileA BOOLEAN
}

type _FILE_DISPOSITION_INFO = FILE_DISPOSITION_INFO

type PFILE_DISPOSITION_INFO = uintptr

type FILE_DISPOSITION_INFO_EX = struct {
	FFlags DWORD
}

type _FILE_DISPOSITION_INFO_EX = FILE_DISPOSITION_INFO_EX

type PFILE_DISPOSITION_INFO_EX = uintptr

type FILE_ID_BOTH_DIR_INFO = struct {
	FNextEntryOffset DWORD
	FFileIndex       DWORD
	FCreationTime    LARGE_INTEGER
	FLastAccessTime  LARGE_INTEGER
	FLastWriteTime   LARGE_INTEGER
	FChangeTime      LARGE_INTEGER
	FEndOfFile       LARGE_INTEGER
	FAllocationSize  LARGE_INTEGER
	FFileAttributes  DWORD
	FFileNameLength  DWORD
	FEaSize          DWORD
	FShortNameLength CCHAR
	FShortName       [12]WCHAR
	FFileId          LARGE_INTEGER
	FFileName        [1]WCHAR
}

type _FILE_ID_BOTH_DIR_INFO = FILE_ID_BOTH_DIR_INFO

type PFILE_ID_BOTH_DIR_INFO = uintptr

type FILE_FULL_DIR_INFO = struct {
	FNextEntryOffset ULONG
	FFileIndex       ULONG
	FCreationTime    LARGE_INTEGER
	FLastAccessTime  LARGE_INTEGER
	FLastWriteTime   LARGE_INTEGER
	FChangeTime      LARGE_INTEGER
	FEndOfFile       LARGE_INTEGER
	FAllocationSize  LARGE_INTEGER
	FFileAttributes  ULONG
	FFileNameLength  ULONG
	FEaSize          ULONG
	FFileName        [1]WCHAR
}

type _FILE_FULL_DIR_INFO = FILE_FULL_DIR_INFO

type PFILE_FULL_DIR_INFO = uintptr

type PRIORITY_HINT = int32

type _PRIORITY_HINT = int32

const IoPriorityHintVeryLow = 0
const IoPriorityHintLow = 1
const IoPriorityHintNormal = 2
const MaximumIoPriorityHintType = 3

type FILE_IO_PRIORITY_HINT_INFO = struct {
	FPriorityHint PRIORITY_HINT
}

type _FILE_IO_PRIORITY_HINT_INFO = FILE_IO_PRIORITY_HINT_INFO

type PFILE_IO_PRIORITY_HINT_INFO = uintptr

type FILE_REMOTE_PROTOCOL_INFO = struct {
	FStructureVersion     USHORT
	FStructureSize        USHORT
	FProtocol             ULONG
	FProtocolMajorVersion USHORT
	FProtocolMinorVersion USHORT
	FProtocolRevision     USHORT
	FReserved             USHORT
	FFlags                ULONG
	FGenericReserved      struct {
		FReserved [8]ULONG
	}
	FProtocolSpecificReserved struct {
		FReserved [16]ULONG
	}
}

type _FILE_REMOTE_PROTOCOL_INFO = FILE_REMOTE_PROTOCOL_INFO

type PFILE_REMOTE_PROTOCOL_INFO = uintptr

type FILE_ID_TYPE = int32

type _FILE_ID_TYPE = int32

const FileIdType = 0
const ObjectIdType = 1
const ExtendedFileIdType = 2
const MaximumFileIdType = 3

type PFILE_ID_TYPE = uintptr

type FILE_ID_DESCRIPTOR = struct {
	FdwSize    DWORD
	FType      FILE_ID_TYPE
	F__ccgo2_8 struct {
		FObjectId    [0]GUID
		FFileId      LARGE_INTEGER
		F__ccgo_pad2 [8]byte
	}
}

type LPFILE_ID_DESCRIPTOR = uintptr

type DRAWPATRECT = struct {
	FptPosition POINT
	FptSize     POINT
	FwStyle     WORD
	FwPattern   WORD
}

type _DRAWPATRECT = DRAWPATRECT

type PDRAWPATRECT = uintptr

type PSINJECTDATA = struct {
	FDataBytes      DWORD
	FInjectionPoint WORD
	FPageNumber     WORD
}

type _PSINJECTDATA = PSINJECTDATA

type PPSINJECTDATA = uintptr

type PSFEATURE_OUTPUT = struct {
	FbPageIndependent WINBOOL
	FbSetPageDevice   WINBOOL
}

type _PSFEATURE_OUTPUT = PSFEATURE_OUTPUT

type PPSFEATURE_OUTPUT = uintptr

type PSFEATURE_CUSTPAPER = struct {
	FlOrientation  LONG
	FlWidth        LONG
	FlHeight       LONG
	FlWidthOffset  LONG
	FlHeightOffset LONG
}

type _PSFEATURE_CUSTPAPER = PSFEATURE_CUSTPAPER

type PPSFEATURE_CUSTPAPER = uintptr

type XFORM = struct {
	FeM11 FLOAT
	FeM12 FLOAT
	FeM21 FLOAT
	FeM22 FLOAT
	FeDx  FLOAT
	FeDy  FLOAT
}

type tagXFORM = XFORM

type PXFORM = uintptr

type LPXFORM = uintptr

type BITMAP = struct {
	FbmType       LONG
	FbmWidth      LONG
	FbmHeight     LONG
	FbmWidthBytes LONG
	FbmPlanes     WORD
	FbmBitsPixel  WORD
	FbmBits       LPVOID
}

type tagBITMAP = BITMAP

type PBITMAP = uintptr

type NPBITMAP = uintptr

type LPBITMAP = uintptr

type RGBTRIPLE = struct {
	FrgbtBlue  BYTE
	FrgbtGreen BYTE
	FrgbtRed   BYTE
}

type tagRGBTRIPLE = RGBTRIPLE

type PRGBTRIPLE = uintptr

type NPRGBTRIPLE = uintptr

type LPRGBTRIPLE = uintptr

type RGBQUAD = struct {
	FrgbBlue     BYTE
	FrgbGreen    BYTE
	FrgbRed      BYTE
	FrgbReserved BYTE
}

type tagRGBQUAD = RGBQUAD

type LPRGBQUAD = uintptr

type LCSCSTYPE = int32

type LCSGAMUTMATCH = int32

type FXPT16DOT16 = int32

type LPFXPT16DOT16 = uintptr

type FXPT2DOT30 = int32

type LPFXPT2DOT30 = uintptr

type CIEXYZ = struct {
	FciexyzX FXPT2DOT30
	FciexyzY FXPT2DOT30
	FciexyzZ FXPT2DOT30
}

type tagCIEXYZ = CIEXYZ

type LPCIEXYZ = uintptr

type CIEXYZTRIPLE = struct {
	FciexyzRed   CIEXYZ
	FciexyzGreen CIEXYZ
	FciexyzBlue  CIEXYZ
}

type tagICEXYZTRIPLE = CIEXYZTRIPLE

type LPCIEXYZTRIPLE = uintptr

type LOGCOLORSPACEA = struct {
	FlcsSignature  DWORD
	FlcsVersion    DWORD
	FlcsSize       DWORD
	FlcsCSType     LCSCSTYPE
	FlcsIntent     LCSGAMUTMATCH
	FlcsEndpoints  CIEXYZTRIPLE
	FlcsGammaRed   DWORD
	FlcsGammaGreen DWORD
	FlcsGammaBlue  DWORD
	FlcsFilename   [260]CHAR
}

type tagLOGCOLORSPACEA = LOGCOLORSPACEA

type LPLOGCOLORSPACEA = uintptr

type LOGCOLORSPACEW = struct {
	FlcsSignature  DWORD
	FlcsVersion    DWORD
	FlcsSize       DWORD
	FlcsCSType     LCSCSTYPE
	FlcsIntent     LCSGAMUTMATCH
	FlcsEndpoints  CIEXYZTRIPLE
	FlcsGammaRed   DWORD
	FlcsGammaGreen DWORD
	FlcsGammaBlue  DWORD
	FlcsFilename   [260]WCHAR
}

type tagLOGCOLORSPACEW = LOGCOLORSPACEW

type LPLOGCOLORSPACEW = uintptr

type LOGCOLORSPACE = struct {
	FlcsSignature  DWORD
	FlcsVersion    DWORD
	FlcsSize       DWORD
	FlcsCSType     LCSCSTYPE
	FlcsIntent     LCSGAMUTMATCH
	FlcsEndpoints  CIEXYZTRIPLE
	FlcsGammaRed   DWORD
	FlcsGammaGreen DWORD
	FlcsGammaBlue  DWORD
	FlcsFilename   [260]CHAR
}

type LPLOGCOLORSPACE = uintptr

type BITMAPCOREHEADER = struct {
	FbcSize     DWORD
	FbcWidth    WORD
	FbcHeight   WORD
	FbcPlanes   WORD
	FbcBitCount WORD
}

type tagBITMAPCOREHEADER = BITMAPCOREHEADER

type LPBITMAPCOREHEADER = uintptr

type PBITMAPCOREHEADER = uintptr

type BITMAPINFOHEADER = struct {
	FbiSize          DWORD
	FbiWidth         LONG
	FbiHeight        LONG
	FbiPlanes        WORD
	FbiBitCount      WORD
	FbiCompression   DWORD
	FbiSizeImage     DWORD
	FbiXPelsPerMeter LONG
	FbiYPelsPerMeter LONG
	FbiClrUsed       DWORD
	FbiClrImportant  DWORD
}

type tagBITMAPINFOHEADER = BITMAPINFOHEADER

type LPBITMAPINFOHEADER = uintptr

type PBITMAPINFOHEADER = uintptr

type BITMAPV4HEADER = struct {
	FbV4Size          DWORD
	FbV4Width         LONG
	FbV4Height        LONG
	FbV4Planes        WORD
	FbV4BitCount      WORD
	FbV4V4Compression DWORD
	FbV4SizeImage     DWORD
	FbV4XPelsPerMeter LONG
	FbV4YPelsPerMeter LONG
	FbV4ClrUsed       DWORD
	FbV4ClrImportant  DWORD
	FbV4RedMask       DWORD
	FbV4GreenMask     DWORD
	FbV4BlueMask      DWORD
	FbV4AlphaMask     DWORD
	FbV4CSType        DWORD
	FbV4Endpoints     CIEXYZTRIPLE
	FbV4GammaRed      DWORD
	FbV4GammaGreen    DWORD
	FbV4GammaBlue     DWORD
}

type LPBITMAPV4HEADER = uintptr

type PBITMAPV4HEADER = uintptr

type BITMAPV5HEADER = struct {
	FbV5Size          DWORD
	FbV5Width         LONG
	FbV5Height        LONG
	FbV5Planes        WORD
	FbV5BitCount      WORD
	FbV5Compression   DWORD
	FbV5SizeImage     DWORD
	FbV5XPelsPerMeter LONG
	FbV5YPelsPerMeter LONG
	FbV5ClrUsed       DWORD
	FbV5ClrImportant  DWORD
	FbV5RedMask       DWORD
	FbV5GreenMask     DWORD
	FbV5BlueMask      DWORD
	FbV5AlphaMask     DWORD
	FbV5CSType        DWORD
	FbV5Endpoints     CIEXYZTRIPLE
	FbV5GammaRed      DWORD
	FbV5GammaGreen    DWORD
	FbV5GammaBlue     DWORD
	FbV5Intent        DWORD
	FbV5ProfileData   DWORD
	FbV5ProfileSize   DWORD
	FbV5Reserved      DWORD
}

type LPBITMAPV5HEADER = uintptr

type PBITMAPV5HEADER = uintptr

type BITMAPINFO = struct {
	FbmiHeader BITMAPINFOHEADER
	FbmiColors [1]RGBQUAD
}

type tagBITMAPINFO = BITMAPINFO

type LPBITMAPINFO = uintptr

type PBITMAPINFO = uintptr

type BITMAPCOREINFO = struct {
	FbmciHeader BITMAPCOREHEADER
	FbmciColors [1]RGBTRIPLE
}

type tagBITMAPCOREINFO = BITMAPCOREINFO

type LPBITMAPCOREINFO = uintptr

type PBITMAPCOREINFO = uintptr

type BITMAPFILEHEADER = struct {
	FbfType      WORD
	FbfSize      DWORD
	FbfReserved1 WORD
	FbfReserved2 WORD
	FbfOffBits   DWORD
}

type tagBITMAPFILEHEADER = BITMAPFILEHEADER

type LPBITMAPFILEHEADER = uintptr

type PBITMAPFILEHEADER = uintptr

type FONTSIGNATURE = struct {
	FfsUsb [4]DWORD
	FfsCsb [2]DWORD
}

type tagFONTSIGNATURE = FONTSIGNATURE

type PFONTSIGNATURE = uintptr

type LPFONTSIGNATURE = uintptr

type CHARSETINFO = struct {
	FciCharset UINT
	FciACP     UINT
	Ffs        FONTSIGNATURE
}

type tagCHARSETINFO = CHARSETINFO

type PCHARSETINFO = uintptr

type NPCHARSETINFO = uintptr

type LPCHARSETINFO = uintptr

type LOCALESIGNATURE = struct {
	FlsUsb          [4]DWORD
	FlsCsbDefault   [2]DWORD
	FlsCsbSupported [2]DWORD
}

type tagLOCALESIGNATURE = LOCALESIGNATURE

type PLOCALESIGNATURE = uintptr

type LPLOCALESIGNATURE = uintptr

type HANDLETABLE = struct {
	FobjectHandle [1]HGDIOBJ
}

type tagHANDLETABLE = HANDLETABLE

type PHANDLETABLE = uintptr

type LPHANDLETABLE = uintptr

type METARECORD = struct {
	FrdSize     DWORD
	FrdFunction WORD
	FrdParm     [1]WORD
}

type tagMETARECORD = METARECORD

type PMETARECORD = uintptr

type LPMETARECORD = uintptr

type METAFILEPICT = struct {
	Fmm   LONG
	FxExt LONG
	FyExt LONG
	FhMF  HMETAFILE
}

type tagMETAFILEPICT = METAFILEPICT

type LPMETAFILEPICT = uintptr

type METAHEADER = struct {
	FmtType         WORD
	FmtHeaderSize   WORD
	FmtVersion      WORD
	FmtSize         DWORD
	FmtNoObjects    WORD
	FmtMaxRecord    DWORD
	FmtNoParameters WORD
}

type tagMETAHEADER = METAHEADER

type PMETAHEADER = uintptr

type LPMETAHEADER = uintptr

type ENHMETARECORD = struct {
	FiType DWORD
	FnSize DWORD
	FdParm [1]DWORD
}

type tagENHMETARECORD = ENHMETARECORD

type PENHMETARECORD = uintptr

type LPENHMETARECORD = uintptr

type ENHMETAHEADER = struct {
	FiType          DWORD
	FnSize          DWORD
	FrclBounds      RECTL
	FrclFrame       RECTL
	FdSignature     DWORD
	FnVersion       DWORD
	FnBytes         DWORD
	FnRecords       DWORD
	FnHandles       WORD
	FsReserved      WORD
	FnDescription   DWORD
	FoffDescription DWORD
	FnPalEntries    DWORD
	FszlDevice      SIZEL
	FszlMillimeters SIZEL
	FcbPixelFormat  DWORD
	FoffPixelFormat DWORD
	FbOpenGL        DWORD
	FszlMicrometers SIZEL
}

type tagENHMETAHEADER = ENHMETAHEADER

type PENHMETAHEADER = uintptr

type LPENHMETAHEADER = uintptr

type BCHAR = uint8

type TEXTMETRICA = struct {
	FtmHeight           LONG
	FtmAscent           LONG
	FtmDescent          LONG
	FtmInternalLeading  LONG
	FtmExternalLeading  LONG
	FtmAveCharWidth     LONG
	FtmMaxCharWidth     LONG
	FtmWeight           LONG
	FtmOverhang         LONG
	FtmDigitizedAspectX LONG
	FtmDigitizedAspectY LONG
	FtmFirstChar        BYTE
	FtmLastChar         BYTE
	FtmDefaultChar      BYTE
	FtmBreakChar        BYTE
	FtmItalic           BYTE
	FtmUnderlined       BYTE
	FtmStruckOut        BYTE
	FtmPitchAndFamily   BYTE
	FtmCharSet          BYTE
}

type tagTEXTMETRICA = TEXTMETRICA

type PTEXTMETRICA = uintptr

type NPTEXTMETRICA = uintptr

type LPTEXTMETRICA = uintptr

type TEXTMETRICW = struct {
	FtmHeight           LONG
	FtmAscent           LONG
	FtmDescent          LONG
	FtmInternalLeading  LONG
	FtmExternalLeading  LONG
	FtmAveCharWidth     LONG
	FtmMaxCharWidth     LONG
	FtmWeight           LONG
	FtmOverhang         LONG
	FtmDigitizedAspectX LONG
	FtmDigitizedAspectY LONG
	FtmFirstChar        WCHAR
	FtmLastChar         WCHAR
	FtmDefaultChar      WCHAR
	FtmBreakChar        WCHAR
	FtmItalic           BYTE
	FtmUnderlined       BYTE
	FtmStruckOut        BYTE
	FtmPitchAndFamily   BYTE
	FtmCharSet          BYTE
}

type tagTEXTMETRICW = TEXTMETRICW

type PTEXTMETRICW = uintptr

type NPTEXTMETRICW = uintptr

type LPTEXTMETRICW = uintptr

type TEXTMETRIC = struct {
	FtmHeight           LONG
	FtmAscent           LONG
	FtmDescent          LONG
	FtmInternalLeading  LONG
	FtmExternalLeading  LONG
	FtmAveCharWidth     LONG
	FtmMaxCharWidth     LONG
	FtmWeight           LONG
	FtmOverhang         LONG
	FtmDigitizedAspectX LONG
	FtmDigitizedAspectY LONG
	FtmFirstChar        BYTE
	FtmLastChar         BYTE
	FtmDefaultChar      BYTE
	FtmBreakChar        BYTE
	FtmItalic           BYTE
	FtmUnderlined       BYTE
	FtmStruckOut        BYTE
	FtmPitchAndFamily   BYTE
	FtmCharSet          BYTE
}

type PTEXTMETRIC = uintptr

type NPTEXTMETRIC = uintptr

type LPTEXTMETRIC = uintptr

type NEWTEXTMETRICA = struct {
	FtmHeight           LONG
	FtmAscent           LONG
	FtmDescent          LONG
	FtmInternalLeading  LONG
	FtmExternalLeading  LONG
	FtmAveCharWidth     LONG
	FtmMaxCharWidth     LONG
	FtmWeight           LONG
	FtmOverhang         LONG
	FtmDigitizedAspectX LONG
	FtmDigitizedAspectY LONG
	FtmFirstChar        BYTE
	FtmLastChar         BYTE
	FtmDefaultChar      BYTE
	FtmBreakChar        BYTE
	FtmItalic           BYTE
	FtmUnderlined       BYTE
	FtmStruckOut        BYTE
	FtmPitchAndFamily   BYTE
	FtmCharSet          BYTE
	FntmFlags           DWORD
	FntmSizeEM          UINT
	FntmCellHeight      UINT
	FntmAvgWidth        UINT
}

type tagNEWTEXTMETRICA = NEWTEXTMETRICA

type PNEWTEXTMETRICA = uintptr

type NPNEWTEXTMETRICA = uintptr

type LPNEWTEXTMETRICA = uintptr

type NEWTEXTMETRICW = struct {
	FtmHeight           LONG
	FtmAscent           LONG
	FtmDescent          LONG
	FtmInternalLeading  LONG
	FtmExternalLeading  LONG
	FtmAveCharWidth     LONG
	FtmMaxCharWidth     LONG
	FtmWeight           LONG
	FtmOverhang         LONG
	FtmDigitizedAspectX LONG
	FtmDigitizedAspectY LONG
	FtmFirstChar        WCHAR
	FtmLastChar         WCHAR
	FtmDefaultChar      WCHAR
	FtmBreakChar        WCHAR
	FtmItalic           BYTE
	FtmUnderlined       BYTE
	FtmStruckOut        BYTE
	FtmPitchAndFamily   BYTE
	FtmCharSet          BYTE
	FntmFlags           DWORD
	FntmSizeEM          UINT
	FntmCellHeight      UINT
	FntmAvgWidth        UINT
}

type tagNEWTEXTMETRICW = NEWTEXTMETRICW

type PNEWTEXTMETRICW = uintptr

type NPNEWTEXTMETRICW = uintptr

type LPNEWTEXTMETRICW = uintptr

type NEWTEXTMETRIC = struct {
	FtmHeight           LONG
	FtmAscent           LONG
	FtmDescent          LONG
	FtmInternalLeading  LONG
	FtmExternalLeading  LONG
	FtmAveCharWidth     LONG
	FtmMaxCharWidth     LONG
	FtmWeight           LONG
	FtmOverhang         LONG
	FtmDigitizedAspectX LONG
	FtmDigitizedAspectY LONG
	FtmFirstChar        BYTE
	FtmLastChar         BYTE
	FtmDefaultChar      BYTE
	FtmBreakChar        BYTE
	FtmItalic           BYTE
	FtmUnderlined       BYTE
	FtmStruckOut        BYTE
	FtmPitchAndFamily   BYTE
	FtmCharSet          BYTE
	FntmFlags           DWORD
	FntmSizeEM          UINT
	FntmCellHeight      UINT
	FntmAvgWidth        UINT
}

type PNEWTEXTMETRIC = uintptr

type NPNEWTEXTMETRIC = uintptr

type LPNEWTEXTMETRIC = uintptr

type NEWTEXTMETRICEXA = struct {
	FntmTm      NEWTEXTMETRICA
	FntmFontSig FONTSIGNATURE
}

type tagNEWTEXTMETRICEXA = NEWTEXTMETRICEXA

type NEWTEXTMETRICEXW = struct {
	FntmTm      NEWTEXTMETRICW
	FntmFontSig FONTSIGNATURE
}

type tagNEWTEXTMETRICEXW = NEWTEXTMETRICEXW

type NEWTEXTMETRICEX = struct {
	FntmTm      NEWTEXTMETRICA
	FntmFontSig FONTSIGNATURE
}

type PELARRAY = struct {
	FpaXCount LONG
	FpaYCount LONG
	FpaXExt   LONG
	FpaYExt   LONG
	FpaRGBs   BYTE
}

type tagPELARRAY = PELARRAY

type PPELARRAY = uintptr

type NPPELARRAY = uintptr

type LPPELARRAY = uintptr

type LOGBRUSH = struct {
	FlbStyle UINT
	FlbColor COLORREF
	FlbHatch ULONG_PTR
}

type tagLOGBRUSH = LOGBRUSH

type PLOGBRUSH = uintptr

type NPLOGBRUSH = uintptr

type LPLOGBRUSH = uintptr

type LOGBRUSH32 = struct {
	FlbStyle UINT
	FlbColor COLORREF
	FlbHatch ULONG
}

type tagLOGBRUSH32 = LOGBRUSH32

type PLOGBRUSH32 = uintptr

type NPLOGBRUSH32 = uintptr

type LPLOGBRUSH32 = uintptr

type PATTERN = struct {
	FlbStyle UINT
	FlbColor COLORREF
	FlbHatch ULONG_PTR
}

type PPATTERN = uintptr

type NPPATTERN = uintptr

type LPPATTERN = uintptr

type LOGPEN = struct {
	FlopnStyle UINT
	FlopnWidth POINT
	FlopnColor COLORREF
}

type tagLOGPEN = LOGPEN

type PLOGPEN = uintptr

type NPLOGPEN = uintptr

type LPLOGPEN = uintptr

type EXTLOGPEN = struct {
	FelpPenStyle   DWORD
	FelpWidth      DWORD
	FelpBrushStyle UINT
	FelpColor      COLORREF
	FelpHatch      ULONG_PTR
	FelpNumEntries DWORD
	FelpStyleEntry [1]DWORD
}

type tagEXTLOGPEN = EXTLOGPEN

type PEXTLOGPEN = uintptr

type NPEXTLOGPEN = uintptr

type LPEXTLOGPEN = uintptr

type EXTLOGPEN32 = struct {
	FelpPenStyle   DWORD
	FelpWidth      DWORD
	FelpBrushStyle UINT
	FelpColor      COLORREF
	FelpHatch      ULONG
	FelpNumEntries DWORD
	FelpStyleEntry [1]DWORD
}

type tagEXTLOGPEN32 = EXTLOGPEN32

type PEXTLOGPEN32 = uintptr

type NPEXTLOGPEN32 = uintptr

type LPEXTLOGPEN32 = uintptr

type PALETTEENTRY = struct {
	FpeRed   BYTE
	FpeGreen BYTE
	FpeBlue  BYTE
	FpeFlags BYTE
}

type tagPALETTEENTRY = PALETTEENTRY

type PPALETTEENTRY = uintptr

type LPPALETTEENTRY = uintptr

type LOGPALETTE = struct {
	FpalVersion    WORD
	FpalNumEntries WORD
	FpalPalEntry   [1]PALETTEENTRY
}

type tagLOGPALETTE = LOGPALETTE

type PLOGPALETTE = uintptr

type NPLOGPALETTE = uintptr

type LPLOGPALETTE = uintptr

type LOGFONTA = struct {
	FlfHeight         LONG
	FlfWidth          LONG
	FlfEscapement     LONG
	FlfOrientation    LONG
	FlfWeight         LONG
	FlfItalic         BYTE
	FlfUnderline      BYTE
	FlfStrikeOut      BYTE
	FlfCharSet        BYTE
	FlfOutPrecision   BYTE
	FlfClipPrecision  BYTE
	FlfQuality        BYTE
	FlfPitchAndFamily BYTE
	FlfFaceName       [32]CHAR
}

type tagLOGFONTA = LOGFONTA

type PLOGFONTA = uintptr

type NPLOGFONTA = uintptr

type LPLOGFONTA = uintptr

type LOGFONTW = struct {
	FlfHeight         LONG
	FlfWidth          LONG
	FlfEscapement     LONG
	FlfOrientation    LONG
	FlfWeight         LONG
	FlfItalic         BYTE
	FlfUnderline      BYTE
	FlfStrikeOut      BYTE
	FlfCharSet        BYTE
	FlfOutPrecision   BYTE
	FlfClipPrecision  BYTE
	FlfQuality        BYTE
	FlfPitchAndFamily BYTE
	FlfFaceName       [32]WCHAR
}

type tagLOGFONTW = LOGFONTW

type PLOGFONTW = uintptr

type NPLOGFONTW = uintptr

type LPLOGFONTW = uintptr

type LOGFONT = struct {
	FlfHeight         LONG
	FlfWidth          LONG
	FlfEscapement     LONG
	FlfOrientation    LONG
	FlfWeight         LONG
	FlfItalic         BYTE
	FlfUnderline      BYTE
	FlfStrikeOut      BYTE
	FlfCharSet        BYTE
	FlfOutPrecision   BYTE
	FlfClipPrecision  BYTE
	FlfQuality        BYTE
	FlfPitchAndFamily BYTE
	FlfFaceName       [32]CHAR
}

type PLOGFONT = uintptr

type NPLOGFONT = uintptr

type LPLOGFONT = uintptr

type ENUMLOGFONTA = struct {
	FelfLogFont  LOGFONTA
	FelfFullName [64]BYTE
	FelfStyle    [32]BYTE
}

type tagENUMLOGFONTA = ENUMLOGFONTA

type LPENUMLOGFONTA = uintptr

type ENUMLOGFONTW = struct {
	FelfLogFont  LOGFONTW
	FelfFullName [64]WCHAR
	FelfStyle    [32]WCHAR
}

type tagENUMLOGFONTW = ENUMLOGFONTW

type LPENUMLOGFONTW = uintptr

type ENUMLOGFONT = struct {
	FelfLogFont  LOGFONTA
	FelfFullName [64]BYTE
	FelfStyle    [32]BYTE
}

type LPENUMLOGFONT = uintptr

type ENUMLOGFONTEXA = struct {
	FelfLogFont  LOGFONTA
	FelfFullName [64]BYTE
	FelfStyle    [32]BYTE
	FelfScript   [32]BYTE
}

type tagENUMLOGFONTEXA = ENUMLOGFONTEXA

type LPENUMLOGFONTEXA = uintptr

type ENUMLOGFONTEXW = struct {
	FelfLogFont  LOGFONTW
	FelfFullName [64]WCHAR
	FelfStyle    [32]WCHAR
	FelfScript   [32]WCHAR
}

type tagENUMLOGFONTEXW = ENUMLOGFONTEXW

type LPENUMLOGFONTEXW = uintptr

type ENUMLOGFONTEX = struct {
	FelfLogFont  LOGFONTA
	FelfFullName [64]BYTE
	FelfStyle    [32]BYTE
	FelfScript   [32]BYTE
}

type LPENUMLOGFONTEX = uintptr

type PANOSE = struct {
	FbFamilyType      BYTE
	FbSerifStyle      BYTE
	FbWeight          BYTE
	FbProportion      BYTE
	FbContrast        BYTE
	FbStrokeVariation BYTE
	FbArmStyle        BYTE
	FbLetterform      BYTE
	FbMidline         BYTE
	FbXHeight         BYTE
}

type tagPANOSE = PANOSE

type LPPANOSE = uintptr

type EXTLOGFONTA = struct {
	FelfLogFont   LOGFONTA
	FelfFullName  [64]BYTE
	FelfStyle     [32]BYTE
	FelfVersion   DWORD
	FelfStyleSize DWORD
	FelfMatch     DWORD
	FelfReserved  DWORD
	FelfVendorId  [4]BYTE
	FelfCulture   DWORD
	FelfPanose    PANOSE
}

type tagEXTLOGFONTA = EXTLOGFONTA

type PEXTLOGFONTA = uintptr

type NPEXTLOGFONTA = uintptr

type LPEXTLOGFONTA = uintptr

type EXTLOGFONTW = struct {
	FelfLogFont   LOGFONTW
	FelfFullName  [64]WCHAR
	FelfStyle     [32]WCHAR
	FelfVersion   DWORD
	FelfStyleSize DWORD
	FelfMatch     DWORD
	FelfReserved  DWORD
	FelfVendorId  [4]BYTE
	FelfCulture   DWORD
	FelfPanose    PANOSE
}

type tagEXTLOGFONTW = EXTLOGFONTW

type PEXTLOGFONTW = uintptr

type NPEXTLOGFONTW = uintptr

type LPEXTLOGFONTW = uintptr

type EXTLOGFONT = struct {
	FelfLogFont   LOGFONTA
	FelfFullName  [64]BYTE
	FelfStyle     [32]BYTE
	FelfVersion   DWORD
	FelfStyleSize DWORD
	FelfMatch     DWORD
	FelfReserved  DWORD
	FelfVendorId  [4]BYTE
	FelfCulture   DWORD
	FelfPanose    PANOSE
}

type PEXTLOGFONT = uintptr

type NPEXTLOGFONT = uintptr

type LPEXTLOGFONT = uintptr

type DEVMODEA = struct {
	FdmDeviceName    [32]BYTE
	FdmSpecVersion   WORD
	FdmDriverVersion WORD
	FdmSize          WORD
	FdmDriverExtra   WORD
	FdmFields        DWORD
	F__ccgo6_44      struct {
		F__ccgo1_0 [0]struct {
			FdmPosition           POINTL
			FdmDisplayOrientation DWORD
			FdmDisplayFixedOutput DWORD
		}
		F__ccgo0_0 struct {
			FdmOrientation   int16
			FdmPaperSize     int16
			FdmPaperLength   int16
			FdmPaperWidth    int16
			FdmScale         int16
			FdmCopies        int16
			FdmDefaultSource int16
			FdmPrintQuality  int16
		}
	}
	FdmColor       int16
	FdmDuplex      int16
	FdmYResolution int16
	FdmTTOption    int16
	FdmCollate     int16
	FdmFormName    [32]BYTE
	FdmLogPixels   WORD
	FdmBitsPerPel  DWORD
	FdmPelsWidth   DWORD
	FdmPelsHeight  DWORD
	F__ccgo17_116  struct {
		FdmNup          [0]DWORD
		FdmDisplayFlags DWORD
	}
	FdmDisplayFrequency DWORD
	FdmICMMethod        DWORD
	FdmICMIntent        DWORD
	FdmMediaType        DWORD
	FdmDitherType       DWORD
	FdmReserved1        DWORD
	FdmReserved2        DWORD
	FdmPanningWidth     DWORD
	FdmPanningHeight    DWORD
}

type _devicemodeA = DEVMODEA

type PDEVMODEA = uintptr

type NPDEVMODEA = uintptr

type LPDEVMODEA = uintptr

type DEVMODEW = struct {
	FdmDeviceName    [32]WCHAR
	FdmSpecVersion   WORD
	FdmDriverVersion WORD
	FdmSize          WORD
	FdmDriverExtra   WORD
	FdmFields        DWORD
	F__ccgo6_76      struct {
		F__ccgo1_0 [0]struct {
			FdmPosition           POINTL
			FdmDisplayOrientation DWORD
			FdmDisplayFixedOutput DWORD
		}
		F__ccgo0_0 struct {
			FdmOrientation   int16
			FdmPaperSize     int16
			FdmPaperLength   int16
			FdmPaperWidth    int16
			FdmScale         int16
			FdmCopies        int16
			FdmDefaultSource int16
			FdmPrintQuality  int16
		}
	}
	FdmColor       int16
	FdmDuplex      int16
	FdmYResolution int16
	FdmTTOption    int16
	FdmCollate     int16
	FdmFormName    [32]WCHAR
	FdmLogPixels   WORD
	FdmBitsPerPel  DWORD
	FdmPelsWidth   DWORD
	FdmPelsHeight  DWORD
	F__ccgo17_180  struct {
		FdmNup          [0]DWORD
		FdmDisplayFlags DWORD
	}
	FdmDisplayFrequency DWORD
	FdmICMMethod        DWORD
	FdmICMIntent        DWORD
	FdmMediaType        DWORD
	FdmDitherType       DWORD
	FdmReserved1        DWORD
	FdmReserved2        DWORD
	FdmPanningWidth     DWORD
	FdmPanningHeight    DWORD
}

type _devicemodeW = DEVMODEW

type PDEVMODEW = uintptr

type NPDEVMODEW = uintptr

type LPDEVMODEW = uintptr

type DEVMODE = struct {
	FdmDeviceName    [32]BYTE
	FdmSpecVersion   WORD
	FdmDriverVersion WORD
	FdmSize          WORD
	FdmDriverExtra   WORD
	FdmFields        DWORD
	F__ccgo6_44      struct {
		F__ccgo1_0 [0]struct {
			FdmPosition           POINTL
			FdmDisplayOrientation DWORD
			FdmDisplayFixedOutput DWORD
		}
		F__ccgo0_0 struct {
			FdmOrientation   int16
			FdmPaperSize     int16
			FdmPaperLength   int16
			FdmPaperWidth    int16
			FdmScale         int16
			FdmCopies        int16
			FdmDefaultSource int16
			FdmPrintQuality  int16
		}
	}
	FdmColor       int16
	FdmDuplex      int16
	FdmYResolution int16
	FdmTTOption    int16
	FdmCollate     int16
	FdmFormName    [32]BYTE
	FdmLogPixels   WORD
	FdmBitsPerPel  DWORD
	FdmPelsWidth   DWORD
	FdmPelsHeight  DWORD
	F__ccgo17_116  struct {
		FdmNup          [0]DWORD
		FdmDisplayFlags DWORD
	}
	FdmDisplayFrequency DWORD
	FdmICMMethod        DWORD
	FdmICMIntent        DWORD
	FdmMediaType        DWORD
	FdmDitherType       DWORD
	FdmReserved1        DWORD
	FdmReserved2        DWORD
	FdmPanningWidth     DWORD
	FdmPanningHeight    DWORD
}

type PDEVMODE = uintptr

type NPDEVMODE = uintptr

type LPDEVMODE = uintptr

type DISPLAY_DEVICEA = struct {
	Fcb           DWORD
	FDeviceName   [32]CHAR
	FDeviceString [128]CHAR
	FStateFlags   DWORD
	FDeviceID     [128]CHAR
	FDeviceKey    [128]CHAR
}

type _DISPLAY_DEVICEA = DISPLAY_DEVICEA

type PDISPLAY_DEVICEA = uintptr

type LPDISPLAY_DEVICEA = uintptr

type DISPLAY_DEVICEW = struct {
	Fcb           DWORD
	FDeviceName   [32]WCHAR
	FDeviceString [128]WCHAR
	FStateFlags   DWORD
	FDeviceID     [128]WCHAR
	FDeviceKey    [128]WCHAR
}

type _DISPLAY_DEVICEW = DISPLAY_DEVICEW

type PDISPLAY_DEVICEW = uintptr

type LPDISPLAY_DEVICEW = uintptr

type DISPLAY_DEVICE = struct {
	Fcb           DWORD
	FDeviceName   [32]CHAR
	FDeviceString [128]CHAR
	FStateFlags   DWORD
	FDeviceID     [128]CHAR
	FDeviceKey    [128]CHAR
}

type PDISPLAY_DEVICE = uintptr

type LPDISPLAY_DEVICE = uintptr

type RGNDATAHEADER = struct {
	FdwSize   DWORD
	FiType    DWORD
	FnCount   DWORD
	FnRgnSize DWORD
	FrcBound  RECT
}

type _RGNDATAHEADER = RGNDATAHEADER

type PRGNDATAHEADER = uintptr

type RGNDATA = struct {
	Frdh    RGNDATAHEADER
	FBuffer [1]int8
}

type _RGNDATA = RGNDATA

type PRGNDATA = uintptr

type NPRGNDATA = uintptr

type LPRGNDATA = uintptr

type ABC = struct {
	FabcA int32
	FabcB UINT
	FabcC int32
}

type _ABC = ABC

type PABC = uintptr

type NPABC = uintptr

type LPABC = uintptr

type ABCFLOAT = struct {
	FabcfA FLOAT
	FabcfB FLOAT
	FabcfC FLOAT
}

type _ABCFLOAT = ABCFLOAT

type PABCFLOAT = uintptr

type NPABCFLOAT = uintptr

type LPABCFLOAT = uintptr

type OUTLINETEXTMETRICA = struct {
	FotmSize                UINT
	FotmTextMetrics         TEXTMETRICA
	FotmFiller              BYTE
	FotmPanoseNumber        PANOSE
	FotmfsSelection         UINT
	FotmfsType              UINT
	FotmsCharSlopeRise      int32
	FotmsCharSlopeRun       int32
	FotmItalicAngle         int32
	FotmEMSquare            UINT
	FotmAscent              int32
	FotmDescent             int32
	FotmLineGap             UINT
	FotmsCapEmHeight        UINT
	FotmsXHeight            UINT
	FotmrcFontBox           RECT
	FotmMacAscent           int32
	FotmMacDescent          int32
	FotmMacLineGap          UINT
	FotmusMinimumPPEM       UINT
	FotmptSubscriptSize     POINT
	FotmptSubscriptOffset   POINT
	FotmptSuperscriptSize   POINT
	FotmptSuperscriptOffset POINT
	FotmsStrikeoutSize      UINT
	FotmsStrikeoutPosition  int32
	FotmsUnderscoreSize     int32
	FotmsUnderscorePosition int32
	FotmpFamilyName         PSTR
	FotmpFaceName           PSTR
	FotmpStyleName          PSTR
	FotmpFullName           PSTR
}

type _OUTLINETEXTMETRICA = OUTLINETEXTMETRICA

type POUTLINETEXTMETRICA = uintptr

type NPOUTLINETEXTMETRICA = uintptr

type LPOUTLINETEXTMETRICA = uintptr

type OUTLINETEXTMETRICW = struct {
	FotmSize                UINT
	FotmTextMetrics         TEXTMETRICW
	FotmFiller              BYTE
	FotmPanoseNumber        PANOSE
	FotmfsSelection         UINT
	FotmfsType              UINT
	FotmsCharSlopeRise      int32
	FotmsCharSlopeRun       int32
	FotmItalicAngle         int32
	FotmEMSquare            UINT
	FotmAscent              int32
	FotmDescent             int32
	FotmLineGap             UINT
	FotmsCapEmHeight        UINT
	FotmsXHeight            UINT
	FotmrcFontBox           RECT
	FotmMacAscent           int32
	FotmMacDescent          int32
	FotmMacLineGap          UINT
	FotmusMinimumPPEM       UINT
	FotmptSubscriptSize     POINT
	FotmptSubscriptOffset   POINT
	FotmptSuperscriptSize   POINT
	FotmptSuperscriptOffset POINT
	FotmsStrikeoutSize      UINT
	FotmsStrikeoutPosition  int32
	FotmsUnderscoreSize     int32
	FotmsUnderscorePosition int32
	FotmpFamilyName         PSTR
	FotmpFaceName           PSTR
	FotmpStyleName          PSTR
	FotmpFullName           PSTR
}

type _OUTLINETEXTMETRICW = OUTLINETEXTMETRICW

type POUTLINETEXTMETRICW = uintptr

type NPOUTLINETEXTMETRICW = uintptr

type LPOUTLINETEXTMETRICW = uintptr

type OUTLINETEXTMETRIC = struct {
	FotmSize                UINT
	FotmTextMetrics         TEXTMETRICA
	FotmFiller              BYTE
	FotmPanoseNumber        PANOSE
	FotmfsSelection         UINT
	FotmfsType              UINT
	FotmsCharSlopeRise      int32
	FotmsCharSlopeRun       int32
	FotmItalicAngle         int32
	FotmEMSquare            UINT
	FotmAscent              int32
	FotmDescent             int32
	FotmLineGap             UINT
	FotmsCapEmHeight        UINT
	FotmsXHeight            UINT
	FotmrcFontBox           RECT
	FotmMacAscent           int32
	FotmMacDescent          int32
	FotmMacLineGap          UINT
	FotmusMinimumPPEM       UINT
	FotmptSubscriptSize     POINT
	FotmptSubscriptOffset   POINT
	FotmptSuperscriptSize   POINT
	FotmptSuperscriptOffset POINT
	FotmsStrikeoutSize      UINT
	FotmsStrikeoutPosition  int32
	FotmsUnderscoreSize     int32
	FotmsUnderscorePosition int32
	FotmpFamilyName         PSTR
	FotmpFaceName           PSTR
	FotmpStyleName          PSTR
	FotmpFullName           PSTR
}

type POUTLINETEXTMETRIC = uintptr

type NPOUTLINETEXTMETRIC = uintptr

type LPOUTLINETEXTMETRIC = uintptr

type POLYTEXTA = struct {
	Fx       int32
	Fy       int32
	Fn       UINT
	Flpstr   LPCSTR
	FuiFlags UINT
	Frcl     RECT
	Fpdx     uintptr
}

type tagPOLYTEXTA = POLYTEXTA

type PPOLYTEXTA = uintptr

type NPPOLYTEXTA = uintptr

type LPPOLYTEXTA = uintptr

type POLYTEXTW = struct {
	Fx       int32
	Fy       int32
	Fn       UINT
	Flpstr   LPCWSTR
	FuiFlags UINT
	Frcl     RECT
	Fpdx     uintptr
}

type tagPOLYTEXTW = POLYTEXTW

type PPOLYTEXTW = uintptr

type NPPOLYTEXTW = uintptr

type LPPOLYTEXTW = uintptr

type POLYTEXT = struct {
	Fx       int32
	Fy       int32
	Fn       UINT
	Flpstr   LPCSTR
	FuiFlags UINT
	Frcl     RECT
	Fpdx     uintptr
}

type PPOLYTEXT = uintptr

type NPPOLYTEXT = uintptr

type LPPOLYTEXT = uintptr

type FIXED = struct {
	Ffract WORD
	Fvalue int16
}

type _FIXED = FIXED

type MAT2 = struct {
	FeM11 FIXED
	FeM12 FIXED
	FeM21 FIXED
	FeM22 FIXED
}

type _MAT2 = MAT2

type LPMAT2 = uintptr

type GLYPHMETRICS = struct {
	FgmBlackBoxX     UINT
	FgmBlackBoxY     UINT
	FgmptGlyphOrigin POINT
	FgmCellIncX      int16
	FgmCellIncY      int16
}

type _GLYPHMETRICS = GLYPHMETRICS

type LPGLYPHMETRICS = uintptr

type POINTFX = struct {
	Fx FIXED
	Fy FIXED
}

type tagPOINTFX = POINTFX

type LPPOINTFX = uintptr

type TTPOLYCURVE = struct {
	FwType WORD
	Fcpfx  WORD
	Fapfx  [1]POINTFX
}

type tagTTPOLYCURVE = TTPOLYCURVE

type LPTTPOLYCURVE = uintptr

type TTPOLYGONHEADER = struct {
	Fcb       DWORD
	FdwType   DWORD
	FpfxStart POINTFX
}

type tagTTPOLYGONHEADER = TTPOLYGONHEADER

type LPTTPOLYGONHEADER = uintptr

type GCP_RESULTSA = struct {
	FlStructSize DWORD
	FlpOutString LPSTR
	FlpOrder     uintptr
	FlpDx        uintptr
	FlpCaretPos  uintptr
	FlpClass     LPSTR
	FlpGlyphs    LPWSTR
	FnGlyphs     UINT
	FnMaxFit     int32
}

type tagGCP_RESULTSA = GCP_RESULTSA

type LPGCP_RESULTSA = uintptr

type GCP_RESULTSW = struct {
	FlStructSize DWORD
	FlpOutString LPWSTR
	FlpOrder     uintptr
	FlpDx        uintptr
	FlpCaretPos  uintptr
	FlpClass     LPSTR
	FlpGlyphs    LPWSTR
	FnGlyphs     UINT
	FnMaxFit     int32
}

type tagGCP_RESULTSW = GCP_RESULTSW

type LPGCP_RESULTSW = uintptr

type GCP_RESULTS = struct {
	FlStructSize DWORD
	FlpOutString LPSTR
	FlpOrder     uintptr
	FlpDx        uintptr
	FlpCaretPos  uintptr
	FlpClass     LPSTR
	FlpGlyphs    LPWSTR
	FnGlyphs     UINT
	FnMaxFit     int32
}

type LPGCP_RESULTS = uintptr

type RASTERIZER_STATUS = struct {
	FnSize       int16
	FwFlags      int16
	FnLanguageID int16
}

type _RASTERIZER_STATUS = RASTERIZER_STATUS

type LPRASTERIZER_STATUS = uintptr

type PIXELFORMATDESCRIPTOR = struct {
	FnSize           WORD
	FnVersion        WORD
	FdwFlags         DWORD
	FiPixelType      BYTE
	FcColorBits      BYTE
	FcRedBits        BYTE
	FcRedShift       BYTE
	FcGreenBits      BYTE
	FcGreenShift     BYTE
	FcBlueBits       BYTE
	FcBlueShift      BYTE
	FcAlphaBits      BYTE
	FcAlphaShift     BYTE
	FcAccumBits      BYTE
	FcAccumRedBits   BYTE
	FcAccumGreenBits BYTE
	FcAccumBlueBits  BYTE
	FcAccumAlphaBits BYTE
	FcDepthBits      BYTE
	FcStencilBits    BYTE
	FcAuxBuffers     BYTE
	FiLayerType      BYTE
	FbReserved       BYTE
	FdwLayerMask     DWORD
	FdwVisibleMask   DWORD
	FdwDamageMask    DWORD
}

type tagPIXELFORMATDESCRIPTOR = PIXELFORMATDESCRIPTOR

type PPIXELFORMATDESCRIPTOR = uintptr

type LPPIXELFORMATDESCRIPTOR = uintptr

type OLDFONTENUMPROCA = uintptr

type OLDFONTENUMPROCW = uintptr

type FONTENUMPROCA = uintptr

type FONTENUMPROCW = uintptr

type FONTENUMPROC = uintptr

type GOBJENUMPROC = uintptr

type LINEDDAPROC = uintptr

type LPFNDEVMODE = uintptr

type LPFNDEVCAPS = uintptr

type WCRANGE = struct {
	FwcLow   WCHAR
	FcGlyphs USHORT
}

type tagWCRANGE = WCRANGE

type PWCRANGE = uintptr

type LPWCRANGE = uintptr

type GLYPHSET = struct {
	FcbThis           DWORD
	FflAccel          DWORD
	FcGlyphsSupported DWORD
	FcRanges          DWORD
	Franges           [1]WCRANGE
}

type tagGLYPHSET = GLYPHSET

type PGLYPHSET = uintptr

type LPGLYPHSET = uintptr

type DESIGNVECTOR = struct {
	FdvReserved DWORD
	FdvNumAxes  DWORD
	FdvValues   [16]LONG
}

type tagDESIGNVECTOR = DESIGNVECTOR

type PDESIGNVECTOR = uintptr

type LPDESIGNVECTOR = uintptr

type AXISINFOA = struct {
	FaxMinValue LONG
	FaxMaxValue LONG
	FaxAxisName [16]BYTE
}

type tagAXISINFOA = AXISINFOA

type PAXISINFOA = uintptr

type LPAXISINFOA = uintptr

type AXISINFOW = struct {
	FaxMinValue LONG
	FaxMaxValue LONG
	FaxAxisName [16]WCHAR
}

type tagAXISINFOW = AXISINFOW

type PAXISINFOW = uintptr

type LPAXISINFOW = uintptr

type AXISINFO = struct {
	FaxMinValue LONG
	FaxMaxValue LONG
	FaxAxisName [16]BYTE
}

type PAXISINFO = uintptr

type LPAXISINFO = uintptr

type AXESLISTA = struct {
	FaxlReserved DWORD
	FaxlNumAxes  DWORD
	FaxlAxisInfo [16]AXISINFOA
}

type tagAXESLISTA = AXESLISTA

type PAXESLISTA = uintptr

type LPAXESLISTA = uintptr

type AXESLISTW = struct {
	FaxlReserved DWORD
	FaxlNumAxes  DWORD
	FaxlAxisInfo [16]AXISINFOW
}

type tagAXESLISTW = AXESLISTW

type PAXESLISTW = uintptr

type LPAXESLISTW = uintptr

type AXESLIST = struct {
	FaxlReserved DWORD
	FaxlNumAxes  DWORD
	FaxlAxisInfo [16]AXISINFOA
}

type PAXESLIST = uintptr

type LPAXESLIST = uintptr

type ENUMLOGFONTEXDVA = struct {
	FelfEnumLogfontEx ENUMLOGFONTEXA
	FelfDesignVector  DESIGNVECTOR
}

type tagENUMLOGFONTEXDVA = ENUMLOGFONTEXDVA

type PENUMLOGFONTEXDVA = uintptr

type LPENUMLOGFONTEXDVA = uintptr

type ENUMLOGFONTEXDVW = struct {
	FelfEnumLogfontEx ENUMLOGFONTEXW
	FelfDesignVector  DESIGNVECTOR
}

type tagENUMLOGFONTEXDVW = ENUMLOGFONTEXDVW

type PENUMLOGFONTEXDVW = uintptr

type LPENUMLOGFONTEXDVW = uintptr

type ENUMLOGFONTEXDV = struct {
	FelfEnumLogfontEx ENUMLOGFONTEXA
	FelfDesignVector  DESIGNVECTOR
}

type PENUMLOGFONTEXDV = uintptr

type LPENUMLOGFONTEXDV = uintptr

type ENUMTEXTMETRICA = struct {
	FetmNewTextMetricEx NEWTEXTMETRICEXA
	FetmAxesList        AXESLISTA
}

type tagENUMTEXTMETRICA = ENUMTEXTMETRICA

type PENUMTEXTMETRICA = uintptr

type LPENUMTEXTMETRICA = uintptr

type ENUMTEXTMETRICW = struct {
	FetmNewTextMetricEx NEWTEXTMETRICEXW
	FetmAxesList        AXESLISTW
}

type tagENUMTEXTMETRICW = ENUMTEXTMETRICW

type PENUMTEXTMETRICW = uintptr

type LPENUMTEXTMETRICW = uintptr

type ENUMTEXTMETRIC = struct {
	FetmNewTextMetricEx NEWTEXTMETRICEXA
	FetmAxesList        AXESLISTA
}

type PENUMTEXTMETRIC = uintptr

type LPENUMTEXTMETRIC = uintptr

type COLOR16 = uint16

type TRIVERTEX = struct {
	Fx     LONG
	Fy     LONG
	FRed   COLOR16
	FGreen COLOR16
	FBlue  COLOR16
	FAlpha COLOR16
}

type _TRIVERTEX = TRIVERTEX

type PTRIVERTEX = uintptr

type LPTRIVERTEX = uintptr

type GRADIENT_TRIANGLE = struct {
	FVertex1 ULONG
	FVertex2 ULONG
	FVertex3 ULONG
}

type _GRADIENT_TRIANGLE = GRADIENT_TRIANGLE

type PGRADIENT_TRIANGLE = uintptr

type LPGRADIENT_TRIANGLE = uintptr

type GRADIENT_RECT = struct {
	FUpperLeft  ULONG
	FLowerRight ULONG
}

type _GRADIENT_RECT = GRADIENT_RECT

type PGRADIENT_RECT = uintptr

type LPGRADIENT_RECT = uintptr

type BLENDFUNCTION = struct {
	FBlendOp             BYTE
	FBlendFlags          BYTE
	FSourceConstantAlpha BYTE
	FAlphaFormat         BYTE
}

type _BLENDFUNCTION = BLENDFUNCTION

type PBLENDFUNCTION = uintptr

type MFENUMPROC = uintptr

type ENHMFENUMPROC = uintptr

type DIBSECTION = struct {
	FdsBm        BITMAP
	FdsBmih      BITMAPINFOHEADER
	FdsBitfields [3]DWORD
	FdshSection  HANDLE
	FdsOffset    DWORD
}

type tagDIBSECTION = DIBSECTION

type LPDIBSECTION = uintptr

type PDIBSECTION = uintptr

type COLORADJUSTMENT = struct {
	FcaSize            WORD
	FcaFlags           WORD
	FcaIlluminantIndex WORD
	FcaRedGamma        WORD
	FcaGreenGamma      WORD
	FcaBlueGamma       WORD
	FcaReferenceBlack  WORD
	FcaReferenceWhite  WORD
	FcaContrast        SHORT
	FcaBrightness      SHORT
	FcaColorfulness    SHORT
	FcaRedGreenTint    SHORT
}

type tagCOLORADJUSTMENT = COLORADJUSTMENT

type PCOLORADJUSTMENT = uintptr

type LPCOLORADJUSTMENT = uintptr

type ABORTPROC = uintptr

type DOCINFOA = struct {
	FcbSize       int32
	FlpszDocName  LPCSTR
	FlpszOutput   LPCSTR
	FlpszDatatype LPCSTR
	FfwType       DWORD
}

type _DOCINFOA = DOCINFOA

type LPDOCINFOA = uintptr

type DOCINFOW = struct {
	FcbSize       int32
	FlpszDocName  LPCWSTR
	FlpszOutput   LPCWSTR
	FlpszDatatype LPCWSTR
	FfwType       DWORD
}

type _DOCINFOW = DOCINFOW

type LPDOCINFOW = uintptr

type DOCINFO = struct {
	FcbSize       int32
	FlpszDocName  LPCSTR
	FlpszOutput   LPCSTR
	FlpszDatatype LPCSTR
	FfwType       DWORD
}

type LPDOCINFO = uintptr

type KERNINGPAIR = struct {
	FwFirst      WORD
	FwSecond     WORD
	FiKernAmount int32
}

type tagKERNINGPAIR = KERNINGPAIR

type LPKERNINGPAIR = uintptr

type ICMENUMPROCA = uintptr

type ICMENUMPROCW = uintptr

type EMR = struct {
	FiType DWORD
	FnSize DWORD
}

type tagEMR = EMR

type PEMR = uintptr

type EMRTEXT = struct {
	FptlReference POINTL
	FnChars       DWORD
	FoffString    DWORD
	FfOptions     DWORD
	Frcl          RECTL
	FoffDx        DWORD
}

type tagEMRTEXT = EMRTEXT

type PEMRTEXT = uintptr

type EMRABORTPATH = struct {
	Femr EMR
}

type tagABORTPATH = EMRABORTPATH

type PEMRABORTPATH = uintptr

type EMRBEGINPATH = struct {
	Femr EMR
}

type PEMRBEGINPATH = uintptr

type EMRENDPATH = struct {
	Femr EMR
}

type PEMRENDPATH = uintptr

type EMRCLOSEFIGURE = struct {
	Femr EMR
}

type PEMRCLOSEFIGURE = uintptr

type EMRFLATTENPATH = struct {
	Femr EMR
}

type PEMRFLATTENPATH = uintptr

type EMRWIDENPATH = struct {
	Femr EMR
}

type PEMRWIDENPATH = uintptr

type EMRSETMETARGN = struct {
	Femr EMR
}

type PEMRSETMETARGN = uintptr

type EMRSAVEDC = struct {
	Femr EMR
}

type PEMRSAVEDC = uintptr

type EMRREALIZEPALETTE = struct {
	Femr EMR
}

type PEMRREALIZEPALETTE = uintptr

type EMRSELECTCLIPPATH = struct {
	Femr   EMR
	FiMode DWORD
}

type tagEMRSELECTCLIPPATH = EMRSELECTCLIPPATH

type PEMRSELECTCLIPPATH = uintptr

type EMRSETBKMODE = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETBKMODE = uintptr

type EMRSETMAPMODE = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETMAPMODE = uintptr

type EMRSETLAYOUT = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETLAYOUT = uintptr

type EMRSETPOLYFILLMODE = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETPOLYFILLMODE = uintptr

type EMRSETROP2 = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETROP2 = uintptr

type EMRSETSTRETCHBLTMODE = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETSTRETCHBLTMODE = uintptr

type EMRSETICMMODE = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETICMMODE = uintptr

type EMRSETTEXTALIGN = struct {
	Femr   EMR
	FiMode DWORD
}

type PEMRSETTEXTALIGN = uintptr

type EMRSETMITERLIMIT = struct {
	Femr         EMR
	FeMiterLimit FLOAT
}

type tagEMRSETMITERLIMIT = EMRSETMITERLIMIT

type PEMRSETMITERLIMIT = uintptr

type EMRRESTOREDC = struct {
	Femr       EMR
	FiRelative LONG
}

type tagEMRRESTOREDC = EMRRESTOREDC

type PEMRRESTOREDC = uintptr

type EMRSETARCDIRECTION = struct {
	Femr           EMR
	FiArcDirection DWORD
}

type tagEMRSETARCDIRECTION = EMRSETARCDIRECTION

type PEMRSETARCDIRECTION = uintptr

type EMRSETMAPPERFLAGS = struct {
	Femr     EMR
	FdwFlags DWORD
}

type tagEMRSETMAPPERFLAGS = EMRSETMAPPERFLAGS

type PEMRSETMAPPERFLAGS = uintptr

type EMRSETBKCOLOR = struct {
	Femr     EMR
	FcrColor COLORREF
}

type tagEMRSETTEXTCOLOR = EMRSETBKCOLOR

type PEMRSETBKCOLOR = uintptr

type EMRSETTEXTCOLOR = struct {
	Femr     EMR
	FcrColor COLORREF
}

type PEMRSETTEXTCOLOR = uintptr

type EMRSELECTOBJECT = struct {
	Femr      EMR
	FihObject DWORD
}

type tagEMRSELECTOBJECT = EMRSELECTOBJECT

type PEMRSELECTOBJECT = uintptr

type EMRDELETEOBJECT = struct {
	Femr      EMR
	FihObject DWORD
}

type PEMRDELETEOBJECT = uintptr

type EMRSELECTPALETTE = struct {
	Femr   EMR
	FihPal DWORD
}

type tagEMRSELECTPALETTE = EMRSELECTPALETTE

type PEMRSELECTPALETTE = uintptr

type EMRRESIZEPALETTE = struct {
	Femr      EMR
	FihPal    DWORD
	FcEntries DWORD
}

type tagEMRRESIZEPALETTE = EMRRESIZEPALETTE

type PEMRRESIZEPALETTE = uintptr

type EMRSETPALETTEENTRIES = struct {
	Femr         EMR
	FihPal       DWORD
	FiStart      DWORD
	FcEntries    DWORD
	FaPalEntries [1]PALETTEENTRY
}

type tagEMRSETPALETTEENTRIES = EMRSETPALETTEENTRIES

type PEMRSETPALETTEENTRIES = uintptr

type EMRSETCOLORADJUSTMENT = struct {
	Femr             EMR
	FColorAdjustment COLORADJUSTMENT
}

type tagEMRSETCOLORADJUSTMENT = EMRSETCOLORADJUSTMENT

type PEMRSETCOLORADJUSTMENT = uintptr

type EMRGDICOMMENT = struct {
	Femr    EMR
	FcbData DWORD
	FData   [1]BYTE
}

type tagEMRGDICOMMENT = EMRGDICOMMENT

type PEMRGDICOMMENT = uintptr

type EMREOF = struct {
	Femr           EMR
	FnPalEntries   DWORD
	FoffPalEntries DWORD
	FnSizeLast     DWORD
}

type tagEMREOF = EMREOF

type PEMREOF = uintptr

type EMRLINETO = struct {
	Femr EMR
	Fptl POINTL
}

type tagEMRLINETO = EMRLINETO

type PEMRLINETO = uintptr

type EMRMOVETOEX = struct {
	Femr EMR
	Fptl POINTL
}

type PEMRMOVETOEX = uintptr

type EMROFFSETCLIPRGN = struct {
	Femr       EMR
	FptlOffset POINTL
}

type tagEMROFFSETCLIPRGN = EMROFFSETCLIPRGN

type PEMROFFSETCLIPRGN = uintptr

type EMRFILLPATH = struct {
	Femr       EMR
	FrclBounds RECTL
}

type tagEMRFILLPATH = EMRFILLPATH

type PEMRFILLPATH = uintptr

type EMRSTROKEANDFILLPATH = struct {
	Femr       EMR
	FrclBounds RECTL
}

type PEMRSTROKEANDFILLPATH = uintptr

type EMRSTROKEPATH = struct {
	Femr       EMR
	FrclBounds RECTL
}

type PEMRSTROKEPATH = uintptr

type EMREXCLUDECLIPRECT = struct {
	Femr     EMR
	FrclClip RECTL
}

type tagEMREXCLUDECLIPRECT = EMREXCLUDECLIPRECT

type PEMREXCLUDECLIPRECT = uintptr

type EMRINTERSECTCLIPRECT = struct {
	Femr     EMR
	FrclClip RECTL
}

type PEMRINTERSECTCLIPRECT = uintptr

type EMRSETVIEWPORTORGEX = struct {
	Femr       EMR
	FptlOrigin POINTL
}

type tagEMRSETVIEWPORTORGEX = EMRSETVIEWPORTORGEX

type PEMRSETVIEWPORTORGEX = uintptr

type EMRSETWINDOWORGEX = struct {
	Femr       EMR
	FptlOrigin POINTL
}

type PEMRSETWINDOWORGEX = uintptr

type EMRSETBRUSHORGEX = struct {
	Femr       EMR
	FptlOrigin POINTL
}

type PEMRSETBRUSHORGEX = uintptr

type EMRSETVIEWPORTEXTEX = struct {
	Femr       EMR
	FszlExtent SIZEL
}

type tagEMRSETVIEWPORTEXTEX = EMRSETVIEWPORTEXTEX

type PEMRSETVIEWPORTEXTEX = uintptr

type EMRSETWINDOWEXTEX = struct {
	Femr       EMR
	FszlExtent SIZEL
}

type PEMRSETWINDOWEXTEX = uintptr

type EMRSCALEVIEWPORTEXTEX = struct {
	Femr    EMR
	FxNum   LONG
	FxDenom LONG
	FyNum   LONG
	FyDenom LONG
}

type tagEMRSCALEVIEWPORTEXTEX = EMRSCALEVIEWPORTEXTEX

type PEMRSCALEVIEWPORTEXTEX = uintptr

type EMRSCALEWINDOWEXTEX = struct {
	Femr    EMR
	FxNum   LONG
	FxDenom LONG
	FyNum   LONG
	FyDenom LONG
}

type PEMRSCALEWINDOWEXTEX = uintptr

type EMRSETWORLDTRANSFORM = struct {
	Femr   EMR
	Fxform XFORM
}

type tagEMRSETWORLDTRANSFORM = EMRSETWORLDTRANSFORM

type PEMRSETWORLDTRANSFORM = uintptr

type EMRMODIFYWORLDTRANSFORM = struct {
	Femr   EMR
	Fxform XFORM
	FiMode DWORD
}

type tagEMRMODIFYWORLDTRANSFORM = EMRMODIFYWORLDTRANSFORM

type PEMRMODIFYWORLDTRANSFORM = uintptr

type EMRSETPIXELV = struct {
	Femr      EMR
	FptlPixel POINTL
	FcrColor  COLORREF
}

type tagEMRSETPIXELV = EMRSETPIXELV

type PEMRSETPIXELV = uintptr

type EMREXTFLOODFILL = struct {
	Femr      EMR
	FptlStart POINTL
	FcrColor  COLORREF
	FiMode    DWORD
}

type tagEMREXTFLOODFILL = EMREXTFLOODFILL

type PEMREXTFLOODFILL = uintptr

type EMRELLIPSE = struct {
	Femr    EMR
	FrclBox RECTL
}

type tagEMRELLIPSE = EMRELLIPSE

type PEMRELLIPSE = uintptr

type EMRRECTANGLE = struct {
	Femr    EMR
	FrclBox RECTL
}

type PEMRRECTANGLE = uintptr

type EMRROUNDRECT = struct {
	Femr       EMR
	FrclBox    RECTL
	FszlCorner SIZEL
}

type tagEMRROUNDRECT = EMRROUNDRECT

type PEMRROUNDRECT = uintptr

type EMRARC = struct {
	Femr      EMR
	FrclBox   RECTL
	FptlStart POINTL
	FptlEnd   POINTL
}

type tagEMRARC = EMRARC

type PEMRARC = uintptr

type EMRARCTO = struct {
	Femr      EMR
	FrclBox   RECTL
	FptlStart POINTL
	FptlEnd   POINTL
}

type PEMRARCTO = uintptr

type EMRCHORD = struct {
	Femr      EMR
	FrclBox   RECTL
	FptlStart POINTL
	FptlEnd   POINTL
}

type PEMRCHORD = uintptr

type EMRPIE = struct {
	Femr      EMR
	FrclBox   RECTL
	FptlStart POINTL
	FptlEnd   POINTL
}

type PEMRPIE = uintptr

type EMRANGLEARC = struct {
	Femr         EMR
	FptlCenter   POINTL
	FnRadius     DWORD
	FeStartAngle FLOAT
	FeSweepAngle FLOAT
}

type tagEMRANGLEARC = EMRANGLEARC

type PEMRANGLEARC = uintptr

type EMRPOLYLINE = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcptl      DWORD
	Faptl      [1]POINTL
}

type tagEMRPOLYLINE = EMRPOLYLINE

type PEMRPOLYLINE = uintptr

type EMRPOLYBEZIER = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcptl      DWORD
	Faptl      [1]POINTL
}

type PEMRPOLYBEZIER = uintptr

type EMRPOLYGON = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcptl      DWORD
	Faptl      [1]POINTL
}

type PEMRPOLYGON = uintptr

type EMRPOLYBEZIERTO = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcptl      DWORD
	Faptl      [1]POINTL
}

type PEMRPOLYBEZIERTO = uintptr

type EMRPOLYLINETO = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcptl      DWORD
	Faptl      [1]POINTL
}

type PEMRPOLYLINETO = uintptr

type EMRPOLYLINE16 = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcpts      DWORD
	Fapts      [1]POINTS
}

type tagEMRPOLYLINE16 = EMRPOLYLINE16

type PEMRPOLYLINE16 = uintptr

type EMRPOLYBEZIER16 = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcpts      DWORD
	Fapts      [1]POINTS
}

type PEMRPOLYBEZIER16 = uintptr

type EMRPOLYGON16 = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcpts      DWORD
	Fapts      [1]POINTS
}

type PEMRPOLYGON16 = uintptr

type EMRPOLYBEZIERTO16 = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcpts      DWORD
	Fapts      [1]POINTS
}

type PEMRPOLYBEZIERTO16 = uintptr

type EMRPOLYLINETO16 = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcpts      DWORD
	Fapts      [1]POINTS
}

type PEMRPOLYLINETO16 = uintptr

type EMRPOLYDRAW = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcptl      DWORD
	Faptl      [1]POINTL
	FabTypes   [1]BYTE
}

type tagEMRPOLYDRAW = EMRPOLYDRAW

type PEMRPOLYDRAW = uintptr

type EMRPOLYDRAW16 = struct {
	Femr       EMR
	FrclBounds RECTL
	Fcpts      DWORD
	Fapts      [1]POINTS
	FabTypes   [1]BYTE
}

type tagEMRPOLYDRAW16 = EMRPOLYDRAW16

type PEMRPOLYDRAW16 = uintptr

type EMRPOLYPOLYLINE = struct {
	Femr         EMR
	FrclBounds   RECTL
	FnPolys      DWORD
	Fcptl        DWORD
	FaPolyCounts [1]DWORD
	Faptl        [1]POINTL
}

type tagEMRPOLYPOLYLINE = EMRPOLYPOLYLINE

type PEMRPOLYPOLYLINE = uintptr

type EMRPOLYPOLYGON = struct {
	Femr         EMR
	FrclBounds   RECTL
	FnPolys      DWORD
	Fcptl        DWORD
	FaPolyCounts [1]DWORD
	Faptl        [1]POINTL
}

type PEMRPOLYPOLYGON = uintptr

type EMRPOLYPOLYLINE16 = struct {
	Femr         EMR
	FrclBounds   RECTL
	FnPolys      DWORD
	Fcpts        DWORD
	FaPolyCounts [1]DWORD
	Fapts        [1]POINTS
}

type tagEMRPOLYPOLYLINE16 = EMRPOLYPOLYLINE16

type PEMRPOLYPOLYLINE16 = uintptr

type EMRPOLYPOLYGON16 = struct {
	Femr         EMR
	FrclBounds   RECTL
	FnPolys      DWORD
	Fcpts        DWORD
	FaPolyCounts [1]DWORD
	Fapts        [1]POINTS
}

type PEMRPOLYPOLYGON16 = uintptr

type EMRINVERTRGN = struct {
	Femr       EMR
	FrclBounds RECTL
	FcbRgnData DWORD
	FRgnData   [1]BYTE
}

type tagEMRINVERTRGN = EMRINVERTRGN

type PEMRINVERTRGN = uintptr

type EMRPAINTRGN = struct {
	Femr       EMR
	FrclBounds RECTL
	FcbRgnData DWORD
	FRgnData   [1]BYTE
}

type PEMRPAINTRGN = uintptr

type EMRFILLRGN = struct {
	Femr       EMR
	FrclBounds RECTL
	FcbRgnData DWORD
	FihBrush   DWORD
	FRgnData   [1]BYTE
}

type tagEMRFILLRGN = EMRFILLRGN

type PEMRFILLRGN = uintptr

type EMRFRAMERGN = struct {
	Femr       EMR
	FrclBounds RECTL
	FcbRgnData DWORD
	FihBrush   DWORD
	FszlStroke SIZEL
	FRgnData   [1]BYTE
}

type tagEMRFRAMERGN = EMRFRAMERGN

type PEMRFRAMERGN = uintptr

type EMREXTSELECTCLIPRGN = struct {
	Femr       EMR
	FcbRgnData DWORD
	FiMode     DWORD
	FRgnData   [1]BYTE
}

type tagEMREXTSELECTCLIPRGN = EMREXTSELECTCLIPRGN

type PEMREXTSELECTCLIPRGN = uintptr

type EMREXTTEXTOUTA = struct {
	Femr           EMR
	FrclBounds     RECTL
	FiGraphicsMode DWORD
	FexScale       FLOAT
	FeyScale       FLOAT
	Femrtext       EMRTEXT
}

type tagEMREXTTEXTOUTA = EMREXTTEXTOUTA

type PEMREXTTEXTOUTA = uintptr

type EMREXTTEXTOUTW = struct {
	Femr           EMR
	FrclBounds     RECTL
	FiGraphicsMode DWORD
	FexScale       FLOAT
	FeyScale       FLOAT
	Femrtext       EMRTEXT
}

type PEMREXTTEXTOUTW = uintptr

type EMRPOLYTEXTOUTA = struct {
	Femr           EMR
	FrclBounds     RECTL
	FiGraphicsMode DWORD
	FexScale       FLOAT
	FeyScale       FLOAT
	FcStrings      LONG
	Faemrtext      [1]EMRTEXT
}

type tagEMRPOLYTEXTOUTA = EMRPOLYTEXTOUTA

type PEMRPOLYTEXTOUTA = uintptr

type EMRPOLYTEXTOUTW = struct {
	Femr           EMR
	FrclBounds     RECTL
	FiGraphicsMode DWORD
	FexScale       FLOAT
	FeyScale       FLOAT
	FcStrings      LONG
	Faemrtext      [1]EMRTEXT
}

type PEMRPOLYTEXTOUTW = uintptr

type EMRBITBLT = struct {
	Femr          EMR
	FrclBounds    RECTL
	FxDest        LONG
	FyDest        LONG
	FcxDest       LONG
	FcyDest       LONG
	FdwRop        DWORD
	FxSrc         LONG
	FySrc         LONG
	FxformSrc     XFORM
	FcrBkColorSrc COLORREF
	FiUsageSrc    DWORD
	FoffBmiSrc    DWORD
	FcbBmiSrc     DWORD
	FoffBitsSrc   DWORD
	FcbBitsSrc    DWORD
}

type tagEMRBITBLT = EMRBITBLT

type PEMRBITBLT = uintptr

type EMRSTRETCHBLT = struct {
	Femr          EMR
	FrclBounds    RECTL
	FxDest        LONG
	FyDest        LONG
	FcxDest       LONG
	FcyDest       LONG
	FdwRop        DWORD
	FxSrc         LONG
	FySrc         LONG
	FxformSrc     XFORM
	FcrBkColorSrc COLORREF
	FiUsageSrc    DWORD
	FoffBmiSrc    DWORD
	FcbBmiSrc     DWORD
	FoffBitsSrc   DWORD
	FcbBitsSrc    DWORD
	FcxSrc        LONG
	FcySrc        LONG
}

type tagEMRSTRETCHBLT = EMRSTRETCHBLT

type PEMRSTRETCHBLT = uintptr

type EMRMASKBLT = struct {
	Femr          EMR
	FrclBounds    RECTL
	FxDest        LONG
	FyDest        LONG
	FcxDest       LONG
	FcyDest       LONG
	FdwRop        DWORD
	FxSrc         LONG
	FySrc         LONG
	FxformSrc     XFORM
	FcrBkColorSrc COLORREF
	FiUsageSrc    DWORD
	FoffBmiSrc    DWORD
	FcbBmiSrc     DWORD
	FoffBitsSrc   DWORD
	FcbBitsSrc    DWORD
	FxMask        LONG
	FyMask        LONG
	FiUsageMask   DWORD
	FoffBmiMask   DWORD
	FcbBmiMask    DWORD
	FoffBitsMask  DWORD
	FcbBitsMask   DWORD
}

type tagEMRMASKBLT = EMRMASKBLT

type PEMRMASKBLT = uintptr

type EMRPLGBLT = struct {
	Femr          EMR
	FrclBounds    RECTL
	FaptlDest     [3]POINTL
	FxSrc         LONG
	FySrc         LONG
	FcxSrc        LONG
	FcySrc        LONG
	FxformSrc     XFORM
	FcrBkColorSrc COLORREF
	FiUsageSrc    DWORD
	FoffBmiSrc    DWORD
	FcbBmiSrc     DWORD
	FoffBitsSrc   DWORD
	FcbBitsSrc    DWORD
	FxMask        LONG
	FyMask        LONG
	FiUsageMask   DWORD
	FoffBmiMask   DWORD
	FcbBmiMask    DWORD
	FoffBitsMask  DWORD
	FcbBitsMask   DWORD
}

type tagEMRPLGBLT = EMRPLGBLT

type PEMRPLGBLT = uintptr

type EMRSETDIBITSTODEVICE = struct {
	Femr        EMR
	FrclBounds  RECTL
	FxDest      LONG
	FyDest      LONG
	FxSrc       LONG
	FySrc       LONG
	FcxSrc      LONG
	FcySrc      LONG
	FoffBmiSrc  DWORD
	FcbBmiSrc   DWORD
	FoffBitsSrc DWORD
	FcbBitsSrc  DWORD
	FiUsageSrc  DWORD
	FiStartScan DWORD
	FcScans     DWORD
}

type tagEMRSETDIBITSTODEVICE = EMRSETDIBITSTODEVICE

type PEMRSETDIBITSTODEVICE = uintptr

type EMRSTRETCHDIBITS = struct {
	Femr        EMR
	FrclBounds  RECTL
	FxDest      LONG
	FyDest      LONG
	FxSrc       LONG
	FySrc       LONG
	FcxSrc      LONG
	FcySrc      LONG
	FoffBmiSrc  DWORD
	FcbBmiSrc   DWORD
	FoffBitsSrc DWORD
	FcbBitsSrc  DWORD
	FiUsageSrc  DWORD
	FdwRop      DWORD
	FcxDest     LONG
	FcyDest     LONG
}

type tagEMRSTRETCHDIBITS = EMRSTRETCHDIBITS

type PEMRSTRETCHDIBITS = uintptr

type EMREXTCREATEFONTINDIRECTW = struct {
	Femr    EMR
	FihFont DWORD
	Felfw   EXTLOGFONTW
}

type tagEMREXTCREATEFONTINDIRECTW = EMREXTCREATEFONTINDIRECTW

type PEMREXTCREATEFONTINDIRECTW = uintptr

type EMRCREATEPALETTE = struct {
	Femr   EMR
	FihPal DWORD
	Flgpl  LOGPALETTE
}

type tagEMRCREATEPALETTE = EMRCREATEPALETTE

type PEMRCREATEPALETTE = uintptr

type EMRCREATEPEN = struct {
	Femr   EMR
	FihPen DWORD
	Flopn  LOGPEN
}

type tagEMRCREATEPEN = EMRCREATEPEN

type PEMRCREATEPEN = uintptr

type EMREXTCREATEPEN = struct {
	Femr     EMR
	FihPen   DWORD
	FoffBmi  DWORD
	FcbBmi   DWORD
	FoffBits DWORD
	FcbBits  DWORD
	Felp     EXTLOGPEN
}

type tagEMREXTCREATEPEN = EMREXTCREATEPEN

type PEMREXTCREATEPEN = uintptr

type EMRCREATEBRUSHINDIRECT = struct {
	Femr     EMR
	FihBrush DWORD
	Flb      LOGBRUSH32
}

type tagEMRCREATEBRUSHINDIRECT = EMRCREATEBRUSHINDIRECT

type PEMRCREATEBRUSHINDIRECT = uintptr

type EMRCREATEMONOBRUSH = struct {
	Femr     EMR
	FihBrush DWORD
	FiUsage  DWORD
	FoffBmi  DWORD
	FcbBmi   DWORD
	FoffBits DWORD
	FcbBits  DWORD
}

type tagEMRCREATEMONOBRUSH = EMRCREATEMONOBRUSH

type PEMRCREATEMONOBRUSH = uintptr

type EMRCREATEDIBPATTERNBRUSHPT = struct {
	Femr     EMR
	FihBrush DWORD
	FiUsage  DWORD
	FoffBmi  DWORD
	FcbBmi   DWORD
	FoffBits DWORD
	FcbBits  DWORD
}

type tagEMRCREATEDIBPATTERNBRUSHPT = EMRCREATEDIBPATTERNBRUSHPT

type PEMRCREATEDIBPATTERNBRUSHPT = uintptr

type EMRFORMAT = struct {
	FdSignature DWORD
	FnVersion   DWORD
	FcbData     DWORD
	FoffData    DWORD
}

type tagEMRFORMAT = EMRFORMAT

type PEMRFORMAT = uintptr

type EMRGLSRECORD = struct {
	Femr    EMR
	FcbData DWORD
	FData   [1]BYTE
}

type tagEMRGLSRECORD = EMRGLSRECORD

type PEMRGLSRECORD = uintptr

type EMRGLSBOUNDEDRECORD = struct {
	Femr       EMR
	FrclBounds RECTL
	FcbData    DWORD
	FData      [1]BYTE
}

type tagEMRGLSBOUNDEDRECORD = EMRGLSBOUNDEDRECORD

type PEMRGLSBOUNDEDRECORD = uintptr

type EMRPIXELFORMAT = struct {
	Femr EMR
	Fpfd PIXELFORMATDESCRIPTOR
}

type tagEMRPIXELFORMAT = EMRPIXELFORMAT

type PEMRPIXELFORMAT = uintptr

type EMRCREATECOLORSPACE = struct {
	Femr  EMR
	FihCS DWORD
	Flcs  LOGCOLORSPACEA
}

type tagEMRCREATECOLORSPACE = EMRCREATECOLORSPACE

type PEMRCREATECOLORSPACE = uintptr

type EMRSETCOLORSPACE = struct {
	Femr  EMR
	FihCS DWORD
}

type tagEMRSETCOLORSPACE = EMRSETCOLORSPACE

type PEMRSETCOLORSPACE = uintptr

type EMRSELECTCOLORSPACE = struct {
	Femr  EMR
	FihCS DWORD
}

type PEMRSELECTCOLORSPACE = uintptr

type EMRDELETECOLORSPACE = struct {
	Femr  EMR
	FihCS DWORD
}

type PEMRDELETECOLORSPACE = uintptr

type EMREXTESCAPE = struct {
	Femr       EMR
	FiEscape   INT
	FcbEscData INT
	FEscData   [1]BYTE
}

type tagEMREXTESCAPE = EMREXTESCAPE

type PEMREXTESCAPE = uintptr

type EMRDRAWESCAPE = struct {
	Femr       EMR
	FiEscape   INT
	FcbEscData INT
	FEscData   [1]BYTE
}

type PEMRDRAWESCAPE = uintptr

type EMRNAMEDESCAPE = struct {
	Femr       EMR
	FiEscape   INT
	FcbDriver  INT
	FcbEscData INT
	FEscData   [1]BYTE
}

type tagEMRNAMEDESCAPE = EMRNAMEDESCAPE

type PEMRNAMEDESCAPE = uintptr

type EMRSETICMPROFILE = struct {
	Femr     EMR
	FdwFlags DWORD
	FcbName  DWORD
	FcbData  DWORD
	FData    [1]BYTE
}

type tagEMRSETICMPROFILE = EMRSETICMPROFILE

type PEMRSETICMPROFILE = uintptr

type EMRSETICMPROFILEA = struct {
	Femr     EMR
	FdwFlags DWORD
	FcbName  DWORD
	FcbData  DWORD
	FData    [1]BYTE
}

type PEMRSETICMPROFILEA = uintptr

type EMRSETICMPROFILEW = struct {
	Femr     EMR
	FdwFlags DWORD
	FcbName  DWORD
	FcbData  DWORD
	FData    [1]BYTE
}

type PEMRSETICMPROFILEW = uintptr

type EMRCREATECOLORSPACEW = struct {
	Femr     EMR
	FihCS    DWORD
	Flcs     LOGCOLORSPACEW
	FdwFlags DWORD
	FcbData  DWORD
	FData    [1]BYTE
}

type tagEMRCREATECOLORSPACEW = EMRCREATECOLORSPACEW

type PEMRCREATECOLORSPACEW = uintptr

type EMRCOLORMATCHTOTARGET = struct {
	Femr      EMR
	FdwAction DWORD
	FdwFlags  DWORD
	FcbName   DWORD
	FcbData   DWORD
	FData     [1]BYTE
}

type tagCOLORMATCHTOTARGET = EMRCOLORMATCHTOTARGET

type PEMRCOLORMATCHTOTARGET = uintptr

type EMRCOLORCORRECTPALETTE = struct {
	Femr         EMR
	FihPalette   DWORD
	FnFirstEntry DWORD
	FnPalEntries DWORD
	FnReserved   DWORD
}

type tagCOLORCORRECTPALETTE = EMRCOLORCORRECTPALETTE

type PEMRCOLORCORRECTPALETTE = uintptr

type EMRALPHABLEND = struct {
	Femr          EMR
	FrclBounds    RECTL
	FxDest        LONG
	FyDest        LONG
	FcxDest       LONG
	FcyDest       LONG
	FdwRop        DWORD
	FxSrc         LONG
	FySrc         LONG
	FxformSrc     XFORM
	FcrBkColorSrc COLORREF
	FiUsageSrc    DWORD
	FoffBmiSrc    DWORD
	FcbBmiSrc     DWORD
	FoffBitsSrc   DWORD
	FcbBitsSrc    DWORD
	FcxSrc        LONG
	FcySrc        LONG
}

type tagEMRALPHABLEND = EMRALPHABLEND

type PEMRALPHABLEND = uintptr

type EMRGRADIENTFILL = struct {
	Femr       EMR
	FrclBounds RECTL
	FnVer      DWORD
	FnTri      DWORD
	FulMode    ULONG
	FVer       [1]TRIVERTEX
}

type tagEMRGRADIENTFILL = EMRGRADIENTFILL

type PEMRGRADIENTFILL = uintptr

type EMRTRANSPARENTBLT = struct {
	Femr          EMR
	FrclBounds    RECTL
	FxDest        LONG
	FyDest        LONG
	FcxDest       LONG
	FcyDest       LONG
	FdwRop        DWORD
	FxSrc         LONG
	FySrc         LONG
	FxformSrc     XFORM
	FcrBkColorSrc COLORREF
	FiUsageSrc    DWORD
	FoffBmiSrc    DWORD
	FcbBmiSrc     DWORD
	FoffBitsSrc   DWORD
	FcbBitsSrc    DWORD
	FcxSrc        LONG
	FcySrc        LONG
}

type tagEMRTRANSPARENTBLT = EMRTRANSPARENTBLT

type PEMRTRANSPARENTBLT = uintptr

type POINTFLOAT = struct {
	Fx FLOAT
	Fy FLOAT
}

type _POINTFLOAT = POINTFLOAT

type PPOINTFLOAT = uintptr

type GLYPHMETRICSFLOAT = struct {
	FgmfBlackBoxX     FLOAT
	FgmfBlackBoxY     FLOAT
	FgmfptGlyphOrigin POINTFLOAT
	FgmfCellIncX      FLOAT
	FgmfCellIncY      FLOAT
}

type _GLYPHMETRICSFLOAT = GLYPHMETRICSFLOAT

type PGLYPHMETRICSFLOAT = uintptr

type LPGLYPHMETRICSFLOAT = uintptr

type LAYERPLANEDESCRIPTOR = struct {
	FnSize           WORD
	FnVersion        WORD
	FdwFlags         DWORD
	FiPixelType      BYTE
	FcColorBits      BYTE
	FcRedBits        BYTE
	FcRedShift       BYTE
	FcGreenBits      BYTE
	FcGreenShift     BYTE
	FcBlueBits       BYTE
	FcBlueShift      BYTE
	FcAlphaBits      BYTE
	FcAlphaShift     BYTE
	FcAccumBits      BYTE
	FcAccumRedBits   BYTE
	FcAccumGreenBits BYTE
	FcAccumBlueBits  BYTE
	FcAccumAlphaBits BYTE
	FcDepthBits      BYTE
	FcStencilBits    BYTE
	FcAuxBuffers     BYTE
	FiLayerPlane     BYTE
	FbReserved       BYTE
	FcrTransparent   COLORREF
}

type tagLAYERPLANEDESCRIPTOR = LAYERPLANEDESCRIPTOR

type PLAYERPLANEDESCRIPTOR = uintptr

type LPLAYERPLANEDESCRIPTOR = uintptr

type WGLSWAP = struct {
	Fhdc     HDC
	FuiFlags UINT
}

type _WGLSWAP = WGLSWAP

type PWGLSWAP = uintptr

type LPWGLSWAP = uintptr

type HDWP = uintptr

type MENUTEMPLATEA = struct{}

type MENUTEMPLATEW = struct{}

type LPMENUTEMPLATEA = uintptr

type LPMENUTEMPLATEW = uintptr

type MENUTEMPLATE = struct{}

type LPMENUTEMPLATE = uintptr

type WNDPROC = uintptr

type DLGPROC = uintptr

type TIMERPROC = uintptr

type GRAYSTRINGPROC = uintptr

type WNDENUMPROC = uintptr

type HOOKPROC = uintptr

type SENDASYNCPROC = uintptr

type PROPENUMPROCA = uintptr

type PROPENUMPROCW = uintptr

type PROPENUMPROCEXA = uintptr

type PROPENUMPROCEXW = uintptr

type EDITWORDBREAKPROCA = uintptr

type EDITWORDBREAKPROCW = uintptr

type DRAWSTATEPROC = uintptr

type PROPENUMPROC = uintptr

type PROPENUMPROCEX = uintptr

type EDITWORDBREAKPROC = uintptr

type NAMEENUMPROCA = uintptr

type NAMEENUMPROCW = uintptr

type WINSTAENUMPROCA = uintptr

type WINSTAENUMPROCW = uintptr

type DESKTOPENUMPROCA = uintptr

type DESKTOPENUMPROCW = uintptr

type WINSTAENUMPROC = uintptr

type DESKTOPENUMPROC = uintptr

type CBT_CREATEWNDA = struct {
	Flpcs            uintptr
	FhwndInsertAfter HWND
}

type tagCBT_CREATEWNDA = CBT_CREATEWNDA

type LPCBT_CREATEWNDA = uintptr

type CBT_CREATEWNDW = struct {
	Flpcs            uintptr
	FhwndInsertAfter HWND
}

type tagCBT_CREATEWNDW = CBT_CREATEWNDW

type LPCBT_CREATEWNDW = uintptr

type CBT_CREATEWND = struct {
	Flpcs            uintptr
	FhwndInsertAfter HWND
}

type LPCBT_CREATEWND = uintptr

type CBTACTIVATESTRUCT = struct {
	FfMouse     WINBOOL
	FhWndActive HWND
}

type tagCBTACTIVATESTRUCT = CBTACTIVATESTRUCT

type LPCBTACTIVATESTRUCT = uintptr

type WTSSESSION_NOTIFICATION = struct {
	FcbSize      DWORD
	FdwSessionId DWORD
}

type tagWTSSESSION_NOTIFICATION = WTSSESSION_NOTIFICATION

type PWTSSESSION_NOTIFICATION = uintptr

type SHELLHOOKINFO = struct {
	Fhwnd HWND
	Frc   RECT
}

type LPSHELLHOOKINFO = uintptr

type EVENTMSG = struct {
	Fmessage UINT
	FparamL  UINT
	FparamH  UINT
	Ftime    DWORD
	Fhwnd    HWND
}

type tagEVENTMSG = EVENTMSG

type PEVENTMSGMSG = uintptr

type NPEVENTMSGMSG = uintptr

type LPEVENTMSGMSG = uintptr

type PEVENTMSG = uintptr

type NPEVENTMSG = uintptr

type LPEVENTMSG = uintptr

type CWPSTRUCT = struct {
	FlParam  LPARAM
	FwParam  WPARAM
	Fmessage UINT
	Fhwnd    HWND
}

type tagCWPSTRUCT = CWPSTRUCT

type PCWPSTRUCT = uintptr

type NPCWPSTRUCT = uintptr

type LPCWPSTRUCT = uintptr

type CWPRETSTRUCT = struct {
	FlResult LRESULT
	FlParam  LPARAM
	FwParam  WPARAM
	Fmessage UINT
	Fhwnd    HWND
}

type tagCWPRETSTRUCT = CWPRETSTRUCT

type PCWPRETSTRUCT = uintptr

type NPCWPRETSTRUCT = uintptr

type LPCWPRETSTRUCT = uintptr

type KBDLLHOOKSTRUCT = struct {
	FvkCode      DWORD
	FscanCode    DWORD
	Fflags       DWORD
	Ftime        DWORD
	FdwExtraInfo ULONG_PTR
}

type tagKBDLLHOOKSTRUCT = KBDLLHOOKSTRUCT

type LPKBDLLHOOKSTRUCT = uintptr

type PKBDLLHOOKSTRUCT = uintptr

type MSLLHOOKSTRUCT = struct {
	Fpt          POINT
	FmouseData   DWORD
	Fflags       DWORD
	Ftime        DWORD
	FdwExtraInfo ULONG_PTR
}

type tagMSLLHOOKSTRUCT = MSLLHOOKSTRUCT

type LPMSLLHOOKSTRUCT = uintptr

type PMSLLHOOKSTRUCT = uintptr

type DEBUGHOOKINFO = struct {
	FidThread          DWORD
	FidThreadInstaller DWORD
	FlParam            LPARAM
	FwParam            WPARAM
	Fcode              int32
}

type tagDEBUGHOOKINFO = DEBUGHOOKINFO

type PDEBUGHOOKINFO = uintptr

type NPDEBUGHOOKINFO = uintptr

type LPDEBUGHOOKINFO = uintptr

type MOUSEHOOKSTRUCT = struct {
	Fpt           POINT
	Fhwnd         HWND
	FwHitTestCode UINT
	FdwExtraInfo  ULONG_PTR
}

type tagMOUSEHOOKSTRUCT = MOUSEHOOKSTRUCT

type LPMOUSEHOOKSTRUCT = uintptr

type PMOUSEHOOKSTRUCT = uintptr

type MOUSEHOOKSTRUCTEX = struct {
	F__unnamed MOUSEHOOKSTRUCT
	FmouseData DWORD
}

type tagMOUSEHOOKSTRUCTEX = MOUSEHOOKSTRUCTEX

type LPMOUSEHOOKSTRUCTEX = uintptr

type PMOUSEHOOKSTRUCTEX = uintptr

type HARDWAREHOOKSTRUCT = struct {
	Fhwnd    HWND
	Fmessage UINT
	FwParam  WPARAM
	FlParam  LPARAM
}

type tagHARDWAREHOOKSTRUCT = HARDWAREHOOKSTRUCT

type LPHARDWAREHOOKSTRUCT = uintptr

type PHARDWAREHOOKSTRUCT = uintptr

type MOUSEMOVEPOINT = struct {
	Fx           int32
	Fy           int32
	Ftime        DWORD
	FdwExtraInfo ULONG_PTR
}

type tagMOUSEMOVEPOINT = MOUSEMOVEPOINT

type PMOUSEMOVEPOINT = uintptr

type LPMOUSEMOVEPOINT = uintptr

type USEROBJECTFLAGS = struct {
	FfInherit  WINBOOL
	FfReserved WINBOOL
	FdwFlags   DWORD
}

type tagUSEROBJECTFLAGS = USEROBJECTFLAGS

type PUSEROBJECTFLAGS = uintptr

type WNDCLASSEXA = struct {
	FcbSize        UINT
	Fstyle         UINT
	FlpfnWndProc   WNDPROC
	FcbClsExtra    int32
	FcbWndExtra    int32
	FhInstance     HINSTANCE
	FhIcon         HICON
	FhCursor       HCURSOR
	FhbrBackground HBRUSH
	FlpszMenuName  LPCSTR
	FlpszClassName LPCSTR
	FhIconSm       HICON
}

type tagWNDCLASSEXA = WNDCLASSEXA

type PWNDCLASSEXA = uintptr

type NPWNDCLASSEXA = uintptr

type LPWNDCLASSEXA = uintptr

type WNDCLASSEXW = struct {
	FcbSize        UINT
	Fstyle         UINT
	FlpfnWndProc   WNDPROC
	FcbClsExtra    int32
	FcbWndExtra    int32
	FhInstance     HINSTANCE
	FhIcon         HICON
	FhCursor       HCURSOR
	FhbrBackground HBRUSH
	FlpszMenuName  LPCWSTR
	FlpszClassName LPCWSTR
	FhIconSm       HICON
}

type tagWNDCLASSEXW = WNDCLASSEXW

type PWNDCLASSEXW = uintptr

type NPWNDCLASSEXW = uintptr

type LPWNDCLASSEXW = uintptr

type WNDCLASSEX = struct {
	FcbSize        UINT
	Fstyle         UINT
	FlpfnWndProc   WNDPROC
	FcbClsExtra    int32
	FcbWndExtra    int32
	FhInstance     HINSTANCE
	FhIcon         HICON
	FhCursor       HCURSOR
	FhbrBackground HBRUSH
	FlpszMenuName  LPCSTR
	FlpszClassName LPCSTR
	FhIconSm       HICON
}

type PWNDCLASSEX = uintptr

type NPWNDCLASSEX = uintptr

type LPWNDCLASSEX = uintptr

type WNDCLASSA = struct {
	Fstyle         UINT
	FlpfnWndProc   WNDPROC
	FcbClsExtra    int32
	FcbWndExtra    int32
	FhInstance     HINSTANCE
	FhIcon         HICON
	FhCursor       HCURSOR
	FhbrBackground HBRUSH
	FlpszMenuName  LPCSTR
	FlpszClassName LPCSTR
}

type tagWNDCLASSA = WNDCLASSA

type PWNDCLASSA = uintptr

type NPWNDCLASSA = uintptr

type LPWNDCLASSA = uintptr

type WNDCLASSW = struct {
	Fstyle         UINT
	FlpfnWndProc   WNDPROC
	FcbClsExtra    int32
	FcbWndExtra    int32
	FhInstance     HINSTANCE
	FhIcon         HICON
	FhCursor       HCURSOR
	FhbrBackground HBRUSH
	FlpszMenuName  LPCWSTR
	FlpszClassName LPCWSTR
}

type tagWNDCLASSW = WNDCLASSW

type PWNDCLASSW = uintptr

type NPWNDCLASSW = uintptr

type LPWNDCLASSW = uintptr

type WNDCLASS = struct {
	Fstyle         UINT
	FlpfnWndProc   WNDPROC
	FcbClsExtra    int32
	FcbWndExtra    int32
	FhInstance     HINSTANCE
	FhIcon         HICON
	FhCursor       HCURSOR
	FhbrBackground HBRUSH
	FlpszMenuName  LPCSTR
	FlpszClassName LPCSTR
}

type PWNDCLASS = uintptr

type NPWNDCLASS = uintptr

type LPWNDCLASS = uintptr

type MSG = struct {
	Fhwnd    HWND
	Fmessage UINT
	FwParam  WPARAM
	FlParam  LPARAM
	Ftime    DWORD
	Fpt      POINT
}

type tagMSG = MSG

type PMSG = uintptr

type NPMSG = uintptr

type LPMSG = uintptr

type MINMAXINFO = struct {
	FptReserved     POINT
	FptMaxSize      POINT
	FptMaxPosition  POINT
	FptMinTrackSize POINT
	FptMaxTrackSize POINT
}

type tagMINMAXINFO = MINMAXINFO

type PMINMAXINFO = uintptr

type LPMINMAXINFO = uintptr

type COPYDATASTRUCT = struct {
	FdwData ULONG_PTR
	FcbData DWORD
	FlpData PVOID
}

type tagCOPYDATASTRUCT = COPYDATASTRUCT

type PCOPYDATASTRUCT = uintptr

type MDINEXTMENU = struct {
	FhmenuIn   HMENU
	FhmenuNext HMENU
	FhwndNext  HWND
}

type tagMDINEXTMENU = MDINEXTMENU

type PMDINEXTMENU = uintptr

type LPMDINEXTMENU = uintptr

type POWERBROADCAST_SETTING = struct {
	FPowerSetting GUID
	FDataLength   DWORD
	FData         [1]UCHAR
}

type PPOWERBROADCAST_SETTING = uintptr

type WINDOWPOS = struct {
	Fhwnd            HWND
	FhwndInsertAfter HWND
	Fx               int32
	Fy               int32
	Fcx              int32
	Fcy              int32
	Fflags           UINT
}

type tagWINDOWPOS = WINDOWPOS

type LPWINDOWPOS = uintptr

type PWINDOWPOS = uintptr

type NCCALCSIZE_PARAMS = struct {
	Frgrc  [3]RECT
	Flppos PWINDOWPOS
}

type tagNCCALCSIZE_PARAMS = NCCALCSIZE_PARAMS

type LPNCCALCSIZE_PARAMS = uintptr

type TRACKMOUSEEVENT = struct {
	FcbSize      DWORD
	FdwFlags     DWORD
	FhwndTrack   HWND
	FdwHoverTime DWORD
}

type tagTRACKMOUSEEVENT = TRACKMOUSEEVENT

type LPTRACKMOUSEEVENT = uintptr

type ACCEL = struct {
	FfVirt BYTE
	Fkey   WORD
	Fcmd   WORD
}

type tagACCEL = ACCEL

type LPACCEL = uintptr

type PAINTSTRUCT = struct {
	Fhdc         HDC
	FfErase      WINBOOL
	FrcPaint     RECT
	FfRestore    WINBOOL
	FfIncUpdate  WINBOOL
	FrgbReserved [32]BYTE
}

type tagPAINTSTRUCT = PAINTSTRUCT

type PPAINTSTRUCT = uintptr

type NPPAINTSTRUCT = uintptr

type LPPAINTSTRUCT = uintptr

type CREATESTRUCTA = struct {
	FlpCreateParams LPVOID
	FhInstance      HINSTANCE
	FhMenu          HMENU
	FhwndParent     HWND
	Fcy             int32
	Fcx             int32
	Fy              int32
	Fx              int32
	Fstyle          LONG
	FlpszName       LPCSTR
	FlpszClass      LPCSTR
	FdwExStyle      DWORD
}

type tagCREATESTRUCTA = CREATESTRUCTA

type LPCREATESTRUCTA = uintptr

type CREATESTRUCTW = struct {
	FlpCreateParams LPVOID
	FhInstance      HINSTANCE
	FhMenu          HMENU
	FhwndParent     HWND
	Fcy             int32
	Fcx             int32
	Fy              int32
	Fx              int32
	Fstyle          LONG
	FlpszName       LPCWSTR
	FlpszClass      LPCWSTR
	FdwExStyle      DWORD
}

type tagCREATESTRUCTW = CREATESTRUCTW

type LPCREATESTRUCTW = uintptr

type CREATESTRUCT = struct {
	FlpCreateParams LPVOID
	FhInstance      HINSTANCE
	FhMenu          HMENU
	FhwndParent     HWND
	Fcy             int32
	Fcx             int32
	Fy              int32
	Fx              int32
	Fstyle          LONG
	FlpszName       LPCSTR
	FlpszClass      LPCSTR
	FdwExStyle      DWORD
}

type LPCREATESTRUCT = uintptr

type WINDOWPLACEMENT = struct {
	Flength           UINT
	Fflags            UINT
	FshowCmd          UINT
	FptMinPosition    POINT
	FptMaxPosition    POINT
	FrcNormalPosition RECT
}

type tagWINDOWPLACEMENT = WINDOWPLACEMENT

type PWINDOWPLACEMENT = uintptr

type LPWINDOWPLACEMENT = uintptr

type NMHDR = struct {
	FhwndFrom HWND
	FidFrom   UINT_PTR
	Fcode     UINT
}

type tagNMHDR = NMHDR

type LPNMHDR = uintptr

type STYLESTRUCT = struct {
	FstyleOld DWORD
	FstyleNew DWORD
}

type tagSTYLESTRUCT = STYLESTRUCT

type LPSTYLESTRUCT = uintptr

type MEASUREITEMSTRUCT = struct {
	FCtlType    UINT
	FCtlID      UINT
	FitemID     UINT
	FitemWidth  UINT
	FitemHeight UINT
	FitemData   ULONG_PTR
}

type tagMEASUREITEMSTRUCT = MEASUREITEMSTRUCT

type PMEASUREITEMSTRUCT = uintptr

type LPMEASUREITEMSTRUCT = uintptr

type DRAWITEMSTRUCT = struct {
	FCtlType    UINT
	FCtlID      UINT
	FitemID     UINT
	FitemAction UINT
	FitemState  UINT
	FhwndItem   HWND
	FhDC        HDC
	FrcItem     RECT
	FitemData   ULONG_PTR
}

type tagDRAWITEMSTRUCT = DRAWITEMSTRUCT

type PDRAWITEMSTRUCT = uintptr

type LPDRAWITEMSTRUCT = uintptr

type DELETEITEMSTRUCT = struct {
	FCtlType  UINT
	FCtlID    UINT
	FitemID   UINT
	FhwndItem HWND
	FitemData ULONG_PTR
}

type tagDELETEITEMSTRUCT = DELETEITEMSTRUCT

type PDELETEITEMSTRUCT = uintptr

type LPDELETEITEMSTRUCT = uintptr

type COMPAREITEMSTRUCT = struct {
	FCtlType    UINT
	FCtlID      UINT
	FhwndItem   HWND
	FitemID1    UINT
	FitemData1  ULONG_PTR
	FitemID2    UINT
	FitemData2  ULONG_PTR
	FdwLocaleId DWORD
}

type tagCOMPAREITEMSTRUCT = COMPAREITEMSTRUCT

type PCOMPAREITEMSTRUCT = uintptr

type LPCOMPAREITEMSTRUCT = uintptr

type BSMINFO = struct {
	FcbSize UINT
	Fhdesk  HDESK
	Fhwnd   HWND
	Fluid   LUID
}

type PBSMINFO = uintptr

type HDEVNOTIFY = uintptr

type PHDEVNOTIFY = uintptr

type HPOWERNOTIFY = uintptr

type PHPOWERNOTIFY = uintptr

type PREGISTERCLASSNAMEW = uintptr

type UPDATELAYEREDWINDOWINFO = struct {
	FcbSize   DWORD
	FhdcDst   HDC
	FpptDst   uintptr
	Fpsize    uintptr
	FhdcSrc   HDC
	FpptSrc   uintptr
	FcrKey    COLORREF
	Fpblend   uintptr
	FdwFlags  DWORD
	FprcDirty uintptr
}

type tagUPDATELAYEREDWINDOWINFO = UPDATELAYEREDWINDOWINFO

type PUPDATELAYEREDWINDOWINFO = uintptr

type FLASHWINFO = struct {
	FcbSize    UINT
	Fhwnd      HWND
	FdwFlags   DWORD
	FuCount    UINT
	FdwTimeout DWORD
}

type PFLASHWINFO = uintptr

type DLGTEMPLATE = struct {
	Fstyle           DWORD
	FdwExtendedStyle DWORD
	Fcdit            WORD
	Fx               int16
	Fy               int16
	Fcx              int16
	Fcy              int16
}

type LPDLGTEMPLATEA = uintptr

type LPDLGTEMPLATEW = uintptr

type LPDLGTEMPLATE = uintptr

type LPCDLGTEMPLATEA = uintptr

type LPCDLGTEMPLATEW = uintptr

type LPCDLGTEMPLATE = uintptr

type DLGITEMTEMPLATE = struct {
	Fstyle           DWORD
	FdwExtendedStyle DWORD
	Fx               int16
	Fy               int16
	Fcx              int16
	Fcy              int16
	Fid              WORD
}

type PDLGITEMTEMPLATEA = uintptr

type PDLGITEMTEMPLATEW = uintptr

type PDLGITEMTEMPLATE = uintptr

type LPDLGITEMTEMPLATEA = uintptr

type LPDLGITEMTEMPLATEW = uintptr

type LPDLGITEMTEMPLATE = uintptr

type DIALOG_CONTROL_DPI_CHANGE_BEHAVIORS1 = int32

type DIALOG_CONTROL_DPI_CHANGE_BEHAVIORS = int32

const DCDC_DEFAULT = 0
const DCDC_DISABLE_FONT_UPDATE = 1
const DCDC_DISABLE_RELAYOUT = 2

type DIALOG_DPI_CHANGE_BEHAVIORS1 = int32

type DIALOG_DPI_CHANGE_BEHAVIORS = int32

const DDC_DEFAULT = 0
const DDC_DISABLE_ALL = 1
const DDC_DISABLE_RESIZE = 2
const DDC_DISABLE_CONTROL_RELAYOUT = 4

type MOUSEINPUT = struct {
	Fdx          LONG
	Fdy          LONG
	FmouseData   DWORD
	FdwFlags     DWORD
	Ftime        DWORD
	FdwExtraInfo ULONG_PTR
}

type tagMOUSEINPUT = MOUSEINPUT

type PMOUSEINPUT = uintptr

type LPMOUSEINPUT = uintptr

type KEYBDINPUT = struct {
	FwVk         WORD
	FwScan       WORD
	FdwFlags     DWORD
	Ftime        DWORD
	FdwExtraInfo ULONG_PTR
}

type tagKEYBDINPUT = KEYBDINPUT

type PKEYBDINPUT = uintptr

type LPKEYBDINPUT = uintptr

type HARDWAREINPUT = struct {
	FuMsg    DWORD
	FwParamL WORD
	FwParamH WORD
}

type tagHARDWAREINPUT = HARDWAREINPUT

type PHARDWAREINPUT = uintptr

type LPHARDWAREINPUT = uintptr

type INPUT = struct {
	Ftype1     DWORD
	F__ccgo1_8 struct {
		Fki [0]KEYBDINPUT
		Fhi [0]HARDWAREINPUT
		Fmi MOUSEINPUT
	}
}

type tagINPUT = INPUT

type PINPUT = uintptr

type LPINPUT = uintptr

type LASTINPUTINFO = struct {
	FcbSize UINT
	FdwTime DWORD
}

type tagLASTINPUTINFO = LASTINPUTINFO

type PLASTINPUTINFO = uintptr

type TPMPARAMS = struct {
	FcbSize    UINT
	FrcExclude RECT
}

type tagTPMPARAMS = TPMPARAMS

type MENUINFO = struct {
	FcbSize          DWORD
	FfMask           DWORD
	FdwStyle         DWORD
	FcyMax           UINT
	FhbrBack         HBRUSH
	FdwContextHelpID DWORD
	FdwMenuData      ULONG_PTR
}

type tagMENUINFO = MENUINFO

type LPMENUINFO = uintptr

type LPTPMPARAMS = uintptr

type LPCMENUINFO = uintptr

type MENUGETOBJECTINFO = struct {
	FdwFlags DWORD
	FuPos    UINT
	Fhmenu   HMENU
	Friid    PVOID
	FpvObj   PVOID
}

type tagMENUGETOBJECTINFO = MENUGETOBJECTINFO

type PMENUGETOBJECTINFO = uintptr

type MENUITEMINFOA = struct {
	FcbSize        UINT
	FfMask         UINT
	FfType         UINT
	FfState        UINT
	FwID           UINT
	FhSubMenu      HMENU
	FhbmpChecked   HBITMAP
	FhbmpUnchecked HBITMAP
	FdwItemData    ULONG_PTR
	FdwTypeData    LPSTR
	Fcch           UINT
	FhbmpItem      HBITMAP
}

type tagMENUITEMINFOA = MENUITEMINFOA

type LPMENUITEMINFOA = uintptr

type MENUITEMINFOW = struct {
	FcbSize        UINT
	FfMask         UINT
	FfType         UINT
	FfState        UINT
	FwID           UINT
	FhSubMenu      HMENU
	FhbmpChecked   HBITMAP
	FhbmpUnchecked HBITMAP
	FdwItemData    ULONG_PTR
	FdwTypeData    LPWSTR
	Fcch           UINT
	FhbmpItem      HBITMAP
}

type tagMENUITEMINFOW = MENUITEMINFOW

type LPMENUITEMINFOW = uintptr

type MENUITEMINFO = struct {
	FcbSize        UINT
	FfMask         UINT
	FfType         UINT
	FfState        UINT
	FwID           UINT
	FhSubMenu      HMENU
	FhbmpChecked   HBITMAP
	FhbmpUnchecked HBITMAP
	FdwItemData    ULONG_PTR
	FdwTypeData    LPSTR
	Fcch           UINT
	FhbmpItem      HBITMAP
}

type LPMENUITEMINFO = uintptr

type LPCMENUITEMINFOA = uintptr

type LPCMENUITEMINFOW = uintptr

type LPCMENUITEMINFO = uintptr

type DROPSTRUCT = struct {
	FhwndSource    HWND
	FhwndSink      HWND
	FwFmt          DWORD
	FdwData        ULONG_PTR
	FptDrop        POINT
	FdwControlData DWORD
}

type tagDROPSTRUCT = DROPSTRUCT

type PDROPSTRUCT = uintptr

type LPDROPSTRUCT = uintptr

type DRAWTEXTPARAMS = struct {
	FcbSize        UINT
	FiTabLength    int32
	FiLeftMargin   int32
	FiRightMargin  int32
	FuiLengthDrawn UINT
}

type tagDRAWTEXTPARAMS = DRAWTEXTPARAMS

type LPDRAWTEXTPARAMS = uintptr

type HELPINFO = struct {
	FcbSize       UINT
	FiContextType int32
	FiCtrlId      int32
	FhItemHandle  HANDLE
	FdwContextId  DWORD_PTR
	FMousePos     POINT
}

type tagHELPINFO = HELPINFO

type LPHELPINFO = uintptr

type MSGBOXCALLBACK = uintptr

type MSGBOXPARAMSA = struct {
	FcbSize             UINT
	FhwndOwner          HWND
	FhInstance          HINSTANCE
	FlpszText           LPCSTR
	FlpszCaption        LPCSTR
	FdwStyle            DWORD
	FlpszIcon           LPCSTR
	FdwContextHelpId    DWORD_PTR
	FlpfnMsgBoxCallback MSGBOXCALLBACK
	FdwLanguageId       DWORD
}

type tagMSGBOXPARAMSA = MSGBOXPARAMSA

type PMSGBOXPARAMSA = uintptr

type LPMSGBOXPARAMSA = uintptr

type MSGBOXPARAMSW = struct {
	FcbSize             UINT
	FhwndOwner          HWND
	FhInstance          HINSTANCE
	FlpszText           LPCWSTR
	FlpszCaption        LPCWSTR
	FdwStyle            DWORD
	FlpszIcon           LPCWSTR
	FdwContextHelpId    DWORD_PTR
	FlpfnMsgBoxCallback MSGBOXCALLBACK
	FdwLanguageId       DWORD
}

type tagMSGBOXPARAMSW = MSGBOXPARAMSW

type PMSGBOXPARAMSW = uintptr

type LPMSGBOXPARAMSW = uintptr

type MSGBOXPARAMS = struct {
	FcbSize             UINT
	FhwndOwner          HWND
	FhInstance          HINSTANCE
	FlpszText           LPCSTR
	FlpszCaption        LPCSTR
	FdwStyle            DWORD
	FlpszIcon           LPCSTR
	FdwContextHelpId    DWORD_PTR
	FlpfnMsgBoxCallback MSGBOXCALLBACK
	FdwLanguageId       DWORD
}

type PMSGBOXPARAMS = uintptr

type LPMSGBOXPARAMS = uintptr

type MENUITEMTEMPLATEHEADER = struct {
	FversionNumber WORD
	Foffset        WORD
}

type PMENUITEMTEMPLATEHEADER = uintptr

type MENUITEMTEMPLATE = struct {
	FmtOption WORD
	FmtID     WORD
	FmtString [1]WCHAR
}

type PMENUITEMTEMPLATE = uintptr

type ICONINFO = struct {
	FfIcon    WINBOOL
	FxHotspot DWORD
	FyHotspot DWORD
	FhbmMask  HBITMAP
	FhbmColor HBITMAP
}

type _ICONINFO = ICONINFO

type PICONINFO = uintptr

type CURSORSHAPE = struct {
	FxHotSpot  int32
	FyHotSpot  int32
	Fcx        int32
	Fcy        int32
	FcbWidth   int32
	FPlanes    BYTE
	FBitsPixel BYTE
}

type tagCURSORSHAPE = CURSORSHAPE

type LPCURSORSHAPE = uintptr

type ICONINFOEXA = struct {
	FcbSize    DWORD
	FfIcon     WINBOOL
	FxHotspot  DWORD
	FyHotspot  DWORD
	FhbmMask   HBITMAP
	FhbmColor  HBITMAP
	FwResID    WORD
	FszModName [260]CHAR
	FszResName [260]CHAR
}

type _ICONINFOEXA = ICONINFOEXA

type PICONINFOEXA = uintptr

type ICONINFOEXW = struct {
	FcbSize    DWORD
	FfIcon     WINBOOL
	FxHotspot  DWORD
	FyHotspot  DWORD
	FhbmMask   HBITMAP
	FhbmColor  HBITMAP
	FwResID    WORD
	FszModName [260]WCHAR
	FszResName [260]WCHAR
}

type _ICONINFOEXW = ICONINFOEXW

type PICONINFOEXW = uintptr

type ICONINFOEX = struct {
	FcbSize    DWORD
	FfIcon     WINBOOL
	FxHotspot  DWORD
	FyHotspot  DWORD
	FhbmMask   HBITMAP
	FhbmColor  HBITMAP
	FwResID    WORD
	FszModName [260]CHAR
	FszResName [260]CHAR
}

type PICONINFOEX = uintptr

type SCROLLINFO = struct {
	FcbSize    UINT
	FfMask     UINT
	FnMin      int32
	FnMax      int32
	FnPage     UINT
	FnPos      int32
	FnTrackPos int32
}

type tagSCROLLINFO = SCROLLINFO

type LPSCROLLINFO = uintptr

type LPCSCROLLINFO = uintptr

type MDICREATESTRUCTA = struct {
	FszClass LPCSTR
	FszTitle LPCSTR
	FhOwner  HANDLE
	Fx       int32
	Fy       int32
	Fcx      int32
	Fcy      int32
	Fstyle   DWORD
	FlParam  LPARAM
}

type tagMDICREATESTRUCTA = MDICREATESTRUCTA

type LPMDICREATESTRUCTA = uintptr

type MDICREATESTRUCTW = struct {
	FszClass LPCWSTR
	FszTitle LPCWSTR
	FhOwner  HANDLE
	Fx       int32
	Fy       int32
	Fcx      int32
	Fcy      int32
	Fstyle   DWORD
	FlParam  LPARAM
}

type tagMDICREATESTRUCTW = MDICREATESTRUCTW

type LPMDICREATESTRUCTW = uintptr

type MDICREATESTRUCT = struct {
	FszClass LPCSTR
	FszTitle LPCSTR
	FhOwner  HANDLE
	Fx       int32
	Fy       int32
	Fcx      int32
	Fcy      int32
	Fstyle   DWORD
	FlParam  LPARAM
}

type LPMDICREATESTRUCT = uintptr

type CLIENTCREATESTRUCT = struct {
	FhWindowMenu  HANDLE
	FidFirstChild UINT
}

type tagCLIENTCREATESTRUCT = CLIENTCREATESTRUCT

type LPCLIENTCREATESTRUCT = uintptr

type HELPPOLY = uint32

type MULTIKEYHELPA = struct {
	FmkSize      DWORD
	FmkKeylist   CHAR
	FszKeyphrase [1]CHAR
}

type tagMULTIKEYHELPA = MULTIKEYHELPA

type PMULTIKEYHELPA = uintptr

type LPMULTIKEYHELPA = uintptr

type MULTIKEYHELPW = struct {
	FmkSize      DWORD
	FmkKeylist   WCHAR
	FszKeyphrase [1]WCHAR
}

type tagMULTIKEYHELPW = MULTIKEYHELPW

type PMULTIKEYHELPW = uintptr

type LPMULTIKEYHELPW = uintptr

type MULTIKEYHELP = struct {
	FmkSize      DWORD
	FmkKeylist   CHAR
	FszKeyphrase [1]CHAR
}

type PMULTIKEYHELP = uintptr

type LPMULTIKEYHELP = uintptr

type HELPWININFOA = struct {
	FwStructSize int32
	Fx           int32
	Fy           int32
	Fdx          int32
	Fdy          int32
	FwMax        int32
	FrgchMember  [2]CHAR
}

type tagHELPWININFOA = HELPWININFOA

type PHELPWININFOA = uintptr

type LPHELPWININFOA = uintptr

type HELPWININFOW = struct {
	FwStructSize int32
	Fx           int32
	Fy           int32
	Fdx          int32
	Fdy          int32
	FwMax        int32
	FrgchMember  [2]WCHAR
}

type tagHELPWININFOW = HELPWININFOW

type PHELPWININFOW = uintptr

type LPHELPWININFOW = uintptr

type HELPWININFO = struct {
	FwStructSize int32
	Fx           int32
	Fy           int32
	Fdx          int32
	Fdy          int32
	FwMax        int32
	FrgchMember  [2]CHAR
}

type PHELPWININFO = uintptr

type LPHELPWININFO = uintptr

type NONCLIENTMETRICSA = struct {
	FcbSize             UINT
	FiBorderWidth       int32
	FiScrollWidth       int32
	FiScrollHeight      int32
	FiCaptionWidth      int32
	FiCaptionHeight     int32
	FlfCaptionFont      LOGFONTA
	FiSmCaptionWidth    int32
	FiSmCaptionHeight   int32
	FlfSmCaptionFont    LOGFONTA
	FiMenuWidth         int32
	FiMenuHeight        int32
	FlfMenuFont         LOGFONTA
	FlfStatusFont       LOGFONTA
	FlfMessageFont      LOGFONTA
	FiPaddedBorderWidth int32
}

type tagNONCLIENTMETRICSA = NONCLIENTMETRICSA

type PNONCLIENTMETRICSA = uintptr

type LPNONCLIENTMETRICSA = uintptr

type NONCLIENTMETRICSW = struct {
	FcbSize             UINT
	FiBorderWidth       int32
	FiScrollWidth       int32
	FiScrollHeight      int32
	FiCaptionWidth      int32
	FiCaptionHeight     int32
	FlfCaptionFont      LOGFONTW
	FiSmCaptionWidth    int32
	FiSmCaptionHeight   int32
	FlfSmCaptionFont    LOGFONTW
	FiMenuWidth         int32
	FiMenuHeight        int32
	FlfMenuFont         LOGFONTW
	FlfStatusFont       LOGFONTW
	FlfMessageFont      LOGFONTW
	FiPaddedBorderWidth int32
}

type tagNONCLIENTMETRICSW = NONCLIENTMETRICSW

type PNONCLIENTMETRICSW = uintptr

type LPNONCLIENTMETRICSW = uintptr

type NONCLIENTMETRICS = struct {
	FcbSize             UINT
	FiBorderWidth       int32
	FiScrollWidth       int32
	FiScrollHeight      int32
	FiCaptionWidth      int32
	FiCaptionHeight     int32
	FlfCaptionFont      LOGFONTA
	FiSmCaptionWidth    int32
	FiSmCaptionHeight   int32
	FlfSmCaptionFont    LOGFONTA
	FiMenuWidth         int32
	FiMenuHeight        int32
	FlfMenuFont         LOGFONTA
	FlfStatusFont       LOGFONTA
	FlfMessageFont      LOGFONTA
	FiPaddedBorderWidth int32
}

type PNONCLIENTMETRICS = uintptr

type LPNONCLIENTMETRICS = uintptr

type MINIMIZEDMETRICS = struct {
	FcbSize   UINT
	FiWidth   int32
	FiHorzGap int32
	FiVertGap int32
	FiArrange int32
}

type tagMINIMIZEDMETRICS = MINIMIZEDMETRICS

type PMINIMIZEDMETRICS = uintptr

type LPMINIMIZEDMETRICS = uintptr

type ICONMETRICSA = struct {
	FcbSize       UINT
	FiHorzSpacing int32
	FiVertSpacing int32
	FiTitleWrap   int32
	FlfFont       LOGFONTA
}

type tagICONMETRICSA = ICONMETRICSA

type PICONMETRICSA = uintptr

type LPICONMETRICSA = uintptr

type ICONMETRICSW = struct {
	FcbSize       UINT
	FiHorzSpacing int32
	FiVertSpacing int32
	FiTitleWrap   int32
	FlfFont       LOGFONTW
}

type tagICONMETRICSW = ICONMETRICSW

type PICONMETRICSW = uintptr

type LPICONMETRICSW = uintptr

type ICONMETRICS = struct {
	FcbSize       UINT
	FiHorzSpacing int32
	FiVertSpacing int32
	FiTitleWrap   int32
	FlfFont       LOGFONTA
}

type PICONMETRICS = uintptr

type LPICONMETRICS = uintptr

type ANIMATIONINFO = struct {
	FcbSize      UINT
	FiMinAnimate int32
}

type tagANIMATIONINFO = ANIMATIONINFO

type LPANIMATIONINFO = uintptr

type SERIALKEYSA = struct {
	FcbSize         UINT
	FdwFlags        DWORD
	FlpszActivePort LPSTR
	FlpszPort       LPSTR
	FiBaudRate      UINT
	FiPortState     UINT
	FiActive        UINT
}

type tagSERIALKEYSA = SERIALKEYSA

type LPSERIALKEYSA = uintptr

type SERIALKEYSW = struct {
	FcbSize         UINT
	FdwFlags        DWORD
	FlpszActivePort LPWSTR
	FlpszPort       LPWSTR
	FiBaudRate      UINT
	FiPortState     UINT
	FiActive        UINT
}

type tagSERIALKEYSW = SERIALKEYSW

type LPSERIALKEYSW = uintptr

type SERIALKEYS = struct {
	FcbSize         UINT
	FdwFlags        DWORD
	FlpszActivePort LPSTR
	FlpszPort       LPSTR
	FiBaudRate      UINT
	FiPortState     UINT
	FiActive        UINT
}

type LPSERIALKEYS = uintptr

type HIGHCONTRASTA = struct {
	FcbSize            UINT
	FdwFlags           DWORD
	FlpszDefaultScheme LPSTR
}

type tagHIGHCONTRASTA = HIGHCONTRASTA

type LPHIGHCONTRASTA = uintptr

type HIGHCONTRASTW = struct {
	FcbSize            UINT
	FdwFlags           DWORD
	FlpszDefaultScheme LPWSTR
}

type tagHIGHCONTRASTW = HIGHCONTRASTW

type LPHIGHCONTRASTW = uintptr

type HIGHCONTRAST = struct {
	FcbSize            UINT
	FdwFlags           DWORD
	FlpszDefaultScheme LPSTR
}

type LPHIGHCONTRAST = uintptr

type VIDEOPARAMETERS = struct {
	FGuid                  GUID
	FdwOffset              ULONG
	FdwCommand             ULONG
	FdwFlags               ULONG
	FdwMode                ULONG
	FdwTVStandard          ULONG
	FdwAvailableModes      ULONG
	FdwAvailableTVStandard ULONG
	FdwFlickerFilter       ULONG
	FdwOverScanX           ULONG
	FdwOverScanY           ULONG
	FdwMaxUnscaledX        ULONG
	FdwMaxUnscaledY        ULONG
	FdwPositionX           ULONG
	FdwPositionY           ULONG
	FdwBrightness          ULONG
	FdwContrast            ULONG
	FdwCPType              ULONG
	FdwCPCommand           ULONG
	FdwCPStandard          ULONG
	FdwCPKey               ULONG
	FbCP_APSTriggerBits    ULONG
	FbOEMCopyProtection    [256]UCHAR
}

type _VIDEOPARAMETERS = VIDEOPARAMETERS

type PVIDEOPARAMETERS = uintptr

type LPVIDEOPARAMETERS = uintptr

type FILTERKEYS = struct {
	FcbSize      UINT
	FdwFlags     DWORD
	FiWaitMSec   DWORD
	FiDelayMSec  DWORD
	FiRepeatMSec DWORD
	FiBounceMSec DWORD
}

type tagFILTERKEYS = FILTERKEYS

type LPFILTERKEYS = uintptr

type STICKYKEYS = struct {
	FcbSize  UINT
	FdwFlags DWORD
}

type tagSTICKYKEYS = STICKYKEYS

type LPSTICKYKEYS = uintptr

type MOUSEKEYS = struct {
	FcbSize          UINT
	FdwFlags         DWORD
	FiMaxSpeed       DWORD
	FiTimeToMaxSpeed DWORD
	FiCtrlSpeed      DWORD
	FdwReserved1     DWORD
	FdwReserved2     DWORD
}

type tagMOUSEKEYS = MOUSEKEYS

type LPMOUSEKEYS = uintptr

type ACCESSTIMEOUT = struct {
	FcbSize       UINT
	FdwFlags      DWORD
	FiTimeOutMSec DWORD
}

type tagACCESSTIMEOUT = ACCESSTIMEOUT

type LPACCESSTIMEOUT = uintptr

type SOUNDSENTRYA = struct {
	FcbSize                 UINT
	FdwFlags                DWORD
	FiFSTextEffect          DWORD
	FiFSTextEffectMSec      DWORD
	FiFSTextEffectColorBits DWORD
	FiFSGrafEffect          DWORD
	FiFSGrafEffectMSec      DWORD
	FiFSGrafEffectColor     DWORD
	FiWindowsEffect         DWORD
	FiWindowsEffectMSec     DWORD
	FlpszWindowsEffectDLL   LPSTR
	FiWindowsEffectOrdinal  DWORD
}

type tagSOUNDSENTRYA = SOUNDSENTRYA

type LPSOUNDSENTRYA = uintptr

type SOUNDSENTRYW = struct {
	FcbSize                 UINT
	FdwFlags                DWORD
	FiFSTextEffect          DWORD
	FiFSTextEffectMSec      DWORD
	FiFSTextEffectColorBits DWORD
	FiFSGrafEffect          DWORD
	FiFSGrafEffectMSec      DWORD
	FiFSGrafEffectColor     DWORD
	FiWindowsEffect         DWORD
	FiWindowsEffectMSec     DWORD
	FlpszWindowsEffectDLL   LPWSTR
	FiWindowsEffectOrdinal  DWORD
}

type tagSOUNDSENTRYW = SOUNDSENTRYW

type LPSOUNDSENTRYW = uintptr

type SOUNDSENTRY = struct {
	FcbSize                 UINT
	FdwFlags                DWORD
	FiFSTextEffect          DWORD
	FiFSTextEffectMSec      DWORD
	FiFSTextEffectColorBits DWORD
	FiFSGrafEffect          DWORD
	FiFSGrafEffectMSec      DWORD
	FiFSGrafEffectColor     DWORD
	FiWindowsEffect         DWORD
	FiWindowsEffectMSec     DWORD
	FlpszWindowsEffectDLL   LPSTR
	FiWindowsEffectOrdinal  DWORD
}

type LPSOUNDSENTRY = uintptr

type TOGGLEKEYS = struct {
	FcbSize  UINT
	FdwFlags DWORD
}

type tagTOGGLEKEYS = TOGGLEKEYS

type LPTOGGLEKEYS = uintptr

type MONITORINFO = struct {
	FcbSize    DWORD
	FrcMonitor RECT
	FrcWork    RECT
	FdwFlags   DWORD
}

type tagMONITORINFO = MONITORINFO

type LPMONITORINFO = uintptr

type AUDIODESCRIPTION = struct {
	FcbSize  UINT
	FEnabled WINBOOL
	FLocale  LCID
}

type tagAUDIODESCRIPTION = AUDIODESCRIPTION

type LPAUDIODESCRIPTION = uintptr

type MONITORINFOEXA = struct {
	F__ccgo0_0 struct {
		FcbSize    DWORD
		FrcMonitor RECT
		FrcWork    RECT
		FdwFlags   DWORD
	}
	FszDevice [32]CHAR
}

type tagMONITORINFOEXA = MONITORINFOEXA

type LPMONITORINFOEXA = uintptr

type MONITORINFOEXW = struct {
	F__ccgo0_0 struct {
		FcbSize    DWORD
		FrcMonitor RECT
		FrcWork    RECT
		FdwFlags   DWORD
	}
	FszDevice [32]WCHAR
}

type tagMONITORINFOEXW = MONITORINFOEXW

type LPMONITORINFOEXW = uintptr

type MONITORINFOEX = struct {
	F__ccgo0_0 struct {
		FcbSize    DWORD
		FrcMonitor RECT
		FrcWork    RECT
		FdwFlags   DWORD
	}
	FszDevice [32]CHAR
}

type LPMONITORINFOEX = uintptr

type MONITORENUMPROC = uintptr

type WINEVENTPROC = uintptr

type GUITHREADINFO = struct {
	FcbSize        DWORD
	Fflags         DWORD
	FhwndActive    HWND
	FhwndFocus     HWND
	FhwndCapture   HWND
	FhwndMenuOwner HWND
	FhwndMoveSize  HWND
	FhwndCaret     HWND
	FrcCaret       RECT
}

type tagGUITHREADINFO = GUITHREADINFO

type PGUITHREADINFO = uintptr

type LPGUITHREADINFO = uintptr

type CURSORINFO = struct {
	FcbSize      DWORD
	Fflags       DWORD
	FhCursor     HCURSOR
	FptScreenPos POINT
}

type tagCURSORINFO = CURSORINFO

type PCURSORINFO = uintptr

type LPCURSORINFO = uintptr

type WINDOWINFO = struct {
	FcbSize          DWORD
	FrcWindow        RECT
	FrcClient        RECT
	FdwStyle         DWORD
	FdwExStyle       DWORD
	FdwWindowStatus  DWORD
	FcxWindowBorders UINT
	FcyWindowBorders UINT
	FatomWindowType  ATOM
	FwCreatorVersion WORD
}

type tagWINDOWINFO = WINDOWINFO

type PWINDOWINFO = uintptr

type LPWINDOWINFO = uintptr

type TITLEBARINFO = struct {
	FcbSize     DWORD
	FrcTitleBar RECT
	Frgstate    [6]DWORD
}

type tagTITLEBARINFO = TITLEBARINFO

type PTITLEBARINFO = uintptr

type LPTITLEBARINFO = uintptr

type TITLEBARINFOEX = struct {
	FcbSize     DWORD
	FrcTitleBar RECT
	Frgstate    [6]DWORD
	Frgrect     [6]RECT
}

type tagTITLEBARINFOEX = TITLEBARINFOEX

type PTITLEBARINFOEX = uintptr

type LPTITLEBARINFOEX = uintptr

type MENUBARINFO = struct {
	FcbSize   DWORD
	FrcBar    RECT
	FhMenu    HMENU
	FhwndMenu HWND
	F__ccgo40 uint8
}

type tagMENUBARINFO = MENUBARINFO

type PMENUBARINFO = uintptr

type LPMENUBARINFO = uintptr

type SCROLLBARINFO = struct {
	FcbSize        DWORD
	FrcScrollBar   RECT
	FdxyLineButton int32
	FxyThumbTop    int32
	FxyThumbBottom int32
	Freserved      int32
	Frgstate       [6]DWORD
}

type tagSCROLLBARINFO = SCROLLBARINFO

type PSCROLLBARINFO = uintptr

type LPSCROLLBARINFO = uintptr

type COMBOBOXINFO = struct {
	FcbSize      DWORD
	FrcItem      RECT
	FrcButton    RECT
	FstateButton DWORD
	FhwndCombo   HWND
	FhwndItem    HWND
	FhwndList    HWND
}

type tagCOMBOBOXINFO = COMBOBOXINFO

type PCOMBOBOXINFO = uintptr

type LPCOMBOBOXINFO = uintptr

type ALTTABINFO = struct {
	FcbSize    DWORD
	FcItems    int32
	FcColumns  int32
	FcRows     int32
	FiColFocus int32
	FiRowFocus int32
	FcxItem    int32
	FcyItem    int32
	FptStart   POINT
}

type tagALTTABINFO = ALTTABINFO

type PALTTABINFO = uintptr

type LPALTTABINFO = uintptr

type HRAWINPUT__ = struct {
	Funused int32
}

type HRAWINPUT = uintptr

type RAWINPUTHEADER = struct {
	FdwType  DWORD
	FdwSize  DWORD
	FhDevice HANDLE
	FwParam  WPARAM
}

type tagRAWINPUTHEADER = RAWINPUTHEADER

type PRAWINPUTHEADER = uintptr

type LPRAWINPUTHEADER = uintptr

type RAWMOUSE = struct {
	FusFlags   USHORT
	F__ccgo1_4 struct {
		F__ccgo1_0 [0]struct {
			FusButtonFlags USHORT
			FusButtonData  USHORT
		}
		FulButtons ULONG
	}
	FulRawButtons       ULONG
	FlLastX             LONG
	FlLastY             LONG
	FulExtraInformation ULONG
}

type tagRAWMOUSE = RAWMOUSE

type PRAWMOUSE = uintptr

type LPRAWMOUSE = uintptr

type RAWKEYBOARD = struct {
	FMakeCode         USHORT
	FFlags            USHORT
	FReserved         USHORT
	FVKey             USHORT
	FMessage          UINT
	FExtraInformation ULONG
}

type tagRAWKEYBOARD = RAWKEYBOARD

type PRAWKEYBOARD = uintptr

type LPRAWKEYBOARD = uintptr

type RAWHID = struct {
	FdwSizeHid DWORD
	FdwCount   DWORD
	FbRawData  [1]BYTE
}

type tagRAWHID = RAWHID

type PRAWHID = uintptr

type LPRAWHID = uintptr

type RAWINPUT = struct {
	Fheader RAWINPUTHEADER
	Fdata   struct {
		Fkeyboard [0]RAWKEYBOARD
		Fhid      [0]RAWHID
		Fmouse    RAWMOUSE
	}
}

type tagRAWINPUT = RAWINPUT

type PRAWINPUT = uintptr

type LPRAWINPUT = uintptr

type RID_DEVICE_INFO_MOUSE = struct {
	FdwId                DWORD
	FdwNumberOfButtons   DWORD
	FdwSampleRate        DWORD
	FfHasHorizontalWheel WINBOOL
}

type tagRID_DEVICE_INFO_MOUSE = RID_DEVICE_INFO_MOUSE

type PRID_DEVICE_INFO_MOUSE = uintptr

type RID_DEVICE_INFO_KEYBOARD = struct {
	FdwType                 DWORD
	FdwSubType              DWORD
	FdwKeyboardMode         DWORD
	FdwNumberOfFunctionKeys DWORD
	FdwNumberOfIndicators   DWORD
	FdwNumberOfKeysTotal    DWORD
}

type tagRID_DEVICE_INFO_KEYBOARD = RID_DEVICE_INFO_KEYBOARD

type PRID_DEVICE_INFO_KEYBOARD = uintptr

type RID_DEVICE_INFO_HID = struct {
	FdwVendorId      DWORD
	FdwProductId     DWORD
	FdwVersionNumber DWORD
	FusUsagePage     USHORT
	FusUsage         USHORT
}

type tagRID_DEVICE_INFO_HID = RID_DEVICE_INFO_HID

type PRID_DEVICE_INFO_HID = uintptr

type RID_DEVICE_INFO = struct {
	FcbSize    DWORD
	FdwType    DWORD
	F__ccgo2_8 struct {
		Fkeyboard    [0]RID_DEVICE_INFO_KEYBOARD
		Fhid         [0]RID_DEVICE_INFO_HID
		Fmouse       RID_DEVICE_INFO_MOUSE
		F__ccgo_pad3 [8]byte
	}
}

type tagRID_DEVICE_INFO = RID_DEVICE_INFO

type PRID_DEVICE_INFO = uintptr

type LPRID_DEVICE_INFO = uintptr

type RAWINPUTDEVICE = struct {
	FusUsagePage USHORT
	FusUsage     USHORT
	FdwFlags     DWORD
	FhwndTarget  HWND
}

type tagRAWINPUTDEVICE = RAWINPUTDEVICE

type PRAWINPUTDEVICE = uintptr

type LPRAWINPUTDEVICE = uintptr

type PCRAWINPUTDEVICE = uintptr

type RAWINPUTDEVICELIST = struct {
	FhDevice HANDLE
	FdwType  DWORD
}

type tagRAWINPUTDEVICELIST = RAWINPUTDEVICELIST

type PRAWINPUTDEVICELIST = uintptr

type LGRPID = uint32

type LCTYPE = uint32

type CALTYPE = uint32

type CALID = uint32

type CPINFO = struct {
	FMaxCharSize UINT
	FDefaultChar [2]BYTE
	FLeadByte    [12]BYTE
}

type _cpinfo = CPINFO

type LPCPINFO = uintptr

type CPINFOEXA = struct {
	FMaxCharSize        UINT
	FDefaultChar        [2]BYTE
	FLeadByte           [12]BYTE
	FUnicodeDefaultChar WCHAR
	FCodePage           UINT
	FCodePageName       [260]CHAR
}

type _cpinfoexA = CPINFOEXA

type LPCPINFOEXA = uintptr

type CPINFOEXW = struct {
	FMaxCharSize        UINT
	FDefaultChar        [2]BYTE
	FLeadByte           [12]BYTE
	FUnicodeDefaultChar WCHAR
	FCodePage           UINT
	FCodePageName       [260]WCHAR
}

type _cpinfoexW = CPINFOEXW

type LPCPINFOEXW = uintptr

type CPINFOEX = struct {
	FMaxCharSize        UINT
	FDefaultChar        [2]BYTE
	FLeadByte           [12]BYTE
	FUnicodeDefaultChar WCHAR
	FCodePage           UINT
	FCodePageName       [260]CHAR
}

type LPCPINFOEX = uintptr

type NUMBERFMTA = struct {
	FNumDigits     UINT
	FLeadingZero   UINT
	FGrouping      UINT
	FlpDecimalSep  LPSTR
	FlpThousandSep LPSTR
	FNegativeOrder UINT
}

type _numberfmtA = NUMBERFMTA

type LPNUMBERFMTA = uintptr

type NUMBERFMTW = struct {
	FNumDigits     UINT
	FLeadingZero   UINT
	FGrouping      UINT
	FlpDecimalSep  LPWSTR
	FlpThousandSep LPWSTR
	FNegativeOrder UINT
}

type _numberfmtW = NUMBERFMTW

type LPNUMBERFMTW = uintptr

type NUMBERFMT = struct {
	FNumDigits     UINT
	FLeadingZero   UINT
	FGrouping      UINT
	FlpDecimalSep  LPSTR
	FlpThousandSep LPSTR
	FNegativeOrder UINT
}

type LPNUMBERFMT = uintptr

type CURRENCYFMTA = struct {
	FNumDigits        UINT
	FLeadingZero      UINT
	FGrouping         UINT
	FlpDecimalSep     LPSTR
	FlpThousandSep    LPSTR
	FNegativeOrder    UINT
	FPositiveOrder    UINT
	FlpCurrencySymbol LPSTR
}

type _currencyfmtA = CURRENCYFMTA

type LPCURRENCYFMTA = uintptr

type CURRENCYFMTW = struct {
	FNumDigits        UINT
	FLeadingZero      UINT
	FGrouping         UINT
	FlpDecimalSep     LPWSTR
	FlpThousandSep    LPWSTR
	FNegativeOrder    UINT
	FPositiveOrder    UINT
	FlpCurrencySymbol LPWSTR
}

type _currencyfmtW = CURRENCYFMTW

type LPCURRENCYFMTW = uintptr

type CURRENCYFMT = struct {
	FNumDigits        UINT
	FLeadingZero      UINT
	FGrouping         UINT
	FlpDecimalSep     LPSTR
	FlpThousandSep    LPSTR
	FNegativeOrder    UINT
	FPositiveOrder    UINT
	FlpCurrencySymbol LPSTR
}

type LPCURRENCYFMT = uintptr

type SYSNLS_FUNCTION = int32

const COMPARE_STRING = 1

type NLS_FUNCTION = uint32

type NLSVERSIONINFO = struct {
	FdwNLSVersionInfoSize DWORD
	FdwNLSVersion         DWORD
	FdwDefinedVersion     DWORD
}

type _nlsversioninfo = NLSVERSIONINFO

type LPNLSVERSIONINFO = uintptr

type NLSVERSIONINFOEX = struct {
	FdwNLSVersionInfoSize DWORD
	FdwNLSVersion         DWORD
	FdwDefinedVersion     DWORD
	FdwEffectiveId        DWORD
	FguidCustomVersion    GUID
}

type _nlsversioninfoex = NLSVERSIONINFOEX

type LPNLSVERSIONINFOEX = uintptr

type GEOID = int32

type GEOTYPE = uint32

type GEOCLASS = uint32

type SYSGEOTYPE = int32

const GEO_NATION = 1
const GEO_LATITUDE = 2
const GEO_LONGITUDE = 3
const GEO_ISO2 = 4
const GEO_ISO3 = 5
const GEO_RFC1766 = 6
const GEO_LCID = 7
const GEO_FRIENDLYNAME = 8
const GEO_OFFICIALNAME = 9
const GEO_TIMEZONES = 10
const GEO_OFFICIALLANGUAGES = 11
const GEO_ISO_UN_NUMBER = 12
const GEO_PARENT = 13
const GEO_DIALINGCODE = 14
const GEO_CURRENCYCODE = 15
const GEO_CURRENCYSYMBOL = 16

type SYSGEOCLASS = int32

const GEOCLASS_NATION = 16
const GEOCLASS_REGION = 14
const GEOCLASS_ALL = 0

type NORM_FORM = int32

type _NORM_FORM = int32

const NormalizationOther = 0
const NormalizationC = 1
const NormalizationD = 2
const NormalizationKC = 5
const NormalizationKD = 6

type LANGUAGEGROUP_ENUMPROCA = uintptr

type LANGGROUPLOCALE_ENUMPROCA = uintptr

type UILANGUAGE_ENUMPROCA = uintptr

type CODEPAGE_ENUMPROCA = uintptr

type DATEFMT_ENUMPROCA = uintptr

type DATEFMT_ENUMPROCEXA = uintptr

type TIMEFMT_ENUMPROCA = uintptr

type CALINFO_ENUMPROCA = uintptr

type CALINFO_ENUMPROCEXA = uintptr

type LOCALE_ENUMPROCA = uintptr

type LOCALE_ENUMPROCW = uintptr

type LANGUAGEGROUP_ENUMPROCW = uintptr

type LANGGROUPLOCALE_ENUMPROCW = uintptr

type UILANGUAGE_ENUMPROCW = uintptr

type CODEPAGE_ENUMPROCW = uintptr

type DATEFMT_ENUMPROCW = uintptr

type DATEFMT_ENUMPROCEXW = uintptr

type TIMEFMT_ENUMPROCW = uintptr

type CALINFO_ENUMPROCW = uintptr

type CALINFO_ENUMPROCEXW = uintptr

type GEO_ENUMPROC = uintptr

type FILEMUIINFO = struct {
	FdwSize               DWORD
	FdwVersion            DWORD
	FdwFileType           DWORD
	FpChecksum            [16]BYTE
	FpServiceChecksum     [16]BYTE
	FdwLanguageNameOffset DWORD
	FdwTypeIDMainSize     DWORD
	FdwTypeIDMainOffset   DWORD
	FdwTypeNameMainOffset DWORD
	FdwTypeIDMUISize      DWORD
	FdwTypeIDMUIOffset    DWORD
	FdwTypeNameMUIOffset  DWORD
	FabBuffer             [8]BYTE
}

type _FILEMUIINFO = FILEMUIINFO

type PFILEMUIINFO = uintptr

type CALINFO_ENUMPROCEXEX = uintptr

type DATEFMT_ENUMPROCEXEX = uintptr

type TIMEFMT_ENUMPROCEX = uintptr

type LOCALE_ENUMPROCEX = uintptr

type COORD = struct {
	FX SHORT
	FY SHORT
}

type _COORD = COORD

type PCOORD = uintptr

type SMALL_RECT = struct {
	FLeft   SHORT
	FTop    SHORT
	FRight  SHORT
	FBottom SHORT
}

type _SMALL_RECT = SMALL_RECT

type PSMALL_RECT = uintptr

type KEY_EVENT_RECORD = struct {
	FbKeyDown         WINBOOL
	FwRepeatCount     WORD
	FwVirtualKeyCode  WORD
	FwVirtualScanCode WORD
	FuChar            struct {
		FAsciiChar   [0]CHAR
		FUnicodeChar WCHAR
	}
	FdwControlKeyState DWORD
}

type _KEY_EVENT_RECORD = KEY_EVENT_RECORD

type PKEY_EVENT_RECORD = uintptr

type MOUSE_EVENT_RECORD = struct {
	FdwMousePosition   COORD
	FdwButtonState     DWORD
	FdwControlKeyState DWORD
	FdwEventFlags      DWORD
}

type _MOUSE_EVENT_RECORD = MOUSE_EVENT_RECORD

type PMOUSE_EVENT_RECORD = uintptr

type WINDOW_BUFFER_SIZE_RECORD = struct {
	FdwSize COORD
}

type _WINDOW_BUFFER_SIZE_RECORD = WINDOW_BUFFER_SIZE_RECORD

type PWINDOW_BUFFER_SIZE_RECORD = uintptr

type MENU_EVENT_RECORD = struct {
	FdwCommandId UINT
}

type _MENU_EVENT_RECORD = MENU_EVENT_RECORD

type PMENU_EVENT_RECORD = uintptr

type FOCUS_EVENT_RECORD = struct {
	FbSetFocus WINBOOL
}

type _FOCUS_EVENT_RECORD = FOCUS_EVENT_RECORD

type PFOCUS_EVENT_RECORD = uintptr

type INPUT_RECORD = struct {
	FEventType WORD
	FEvent     struct {
		FMouseEvent            [0]MOUSE_EVENT_RECORD
		FWindowBufferSizeEvent [0]WINDOW_BUFFER_SIZE_RECORD
		FMenuEvent             [0]MENU_EVENT_RECORD
		FFocusEvent            [0]FOCUS_EVENT_RECORD
		FKeyEvent              KEY_EVENT_RECORD
	}
}

type _INPUT_RECORD = INPUT_RECORD

type PINPUT_RECORD = uintptr

type CHAR_INFO = struct {
	FChar struct {
		FAsciiChar   [0]CHAR
		FUnicodeChar WCHAR
	}
	FAttributes WORD
}

type _CHAR_INFO = CHAR_INFO

type PCHAR_INFO = uintptr

type CONSOLE_FONT_INFO = struct {
	FnFont      DWORD
	FdwFontSize COORD
}

type _CONSOLE_FONT_INFO = CONSOLE_FONT_INFO

type PCONSOLE_FONT_INFO = uintptr

type HPCON = uintptr

type CONSOLE_READCONSOLE_CONTROL = struct {
	FnLength           ULONG
	FnInitialChars     ULONG
	FdwCtrlWakeupMask  ULONG
	FdwControlKeyState ULONG
}

type _CONSOLE_READCONSOLE_CONTROL = CONSOLE_READCONSOLE_CONTROL

type PCONSOLE_READCONSOLE_CONTROL = uintptr

type PHANDLER_ROUTINE = uintptr

type CONSOLE_CURSOR_INFO = struct {
	FdwSize   DWORD
	FbVisible WINBOOL
}

type _CONSOLE_CURSOR_INFO = CONSOLE_CURSOR_INFO

type PCONSOLE_CURSOR_INFO = uintptr

type CONSOLE_SCREEN_BUFFER_INFO = struct {
	FdwSize              COORD
	FdwCursorPosition    COORD
	FwAttributes         WORD
	FsrWindow            SMALL_RECT
	FdwMaximumWindowSize COORD
}

type _CONSOLE_SCREEN_BUFFER_INFO = CONSOLE_SCREEN_BUFFER_INFO

type PCONSOLE_SCREEN_BUFFER_INFO = uintptr

type CONSOLE_SCREEN_BUFFER_INFOEX = struct {
	FcbSize               ULONG
	FdwSize               COORD
	FdwCursorPosition     COORD
	FwAttributes          WORD
	FsrWindow             SMALL_RECT
	FdwMaximumWindowSize  COORD
	FwPopupAttributes     WORD
	FbFullscreenSupported WINBOOL
	FColorTable           [16]COLORREF
}

type _CONSOLE_SCREEN_BUFFER_INFOEX = CONSOLE_SCREEN_BUFFER_INFOEX

type PCONSOLE_SCREEN_BUFFER_INFOEX = uintptr

type CONSOLE_FONT_INFOEX = struct {
	FcbSize     ULONG
	FnFont      DWORD
	FdwFontSize COORD
	FFontFamily UINT
	FFontWeight UINT
	FFaceName   [32]WCHAR
}

type _CONSOLE_FONT_INFOEX = CONSOLE_FONT_INFOEX

type PCONSOLE_FONT_INFOEX = uintptr

type CONSOLE_SELECTION_INFO = struct {
	FdwFlags           DWORD
	FdwSelectionAnchor COORD
	FsrSelection       SMALL_RECT
}

type _CONSOLE_SELECTION_INFO = CONSOLE_SELECTION_INFO

type PCONSOLE_SELECTION_INFO = uintptr

type CONSOLE_HISTORY_INFO = struct {
	FcbSize                 UINT
	FHistoryBufferSize      UINT
	FNumberOfHistoryBuffers UINT
	FdwFlags                DWORD
}

type _CONSOLE_HISTORY_INFO = CONSOLE_HISTORY_INFO

type PCONSOLE_HISTORY_INFO = uintptr

type VS_FIXEDFILEINFO = struct {
	FdwSignature        DWORD
	FdwStrucVersion     DWORD
	FdwFileVersionMS    DWORD
	FdwFileVersionLS    DWORD
	FdwProductVersionMS DWORD
	FdwProductVersionLS DWORD
	FdwFileFlagsMask    DWORD
	FdwFileFlags        DWORD
	FdwFileOS           DWORD
	FdwFileType         DWORD
	FdwFileSubtype      DWORD
	FdwFileDateMS       DWORD
	FdwFileDateLS       DWORD
}

type tagVS_FIXEDFILEINFO = VS_FIXEDFILEINFO

type REGSAM = uint32

type LSTATUS = int32

type val_context = struct {
	Fvaluelen      int32
	Fvalue_context LPVOID
	Fval_buff_ptr  LPVOID
}

type PVALCONTEXT = uintptr

type PVALUEA = struct {
	Fpv_valuename     LPSTR
	Fpv_valuelen      int32
	Fpv_value_context LPVOID
	Fpv_type          DWORD
}

type pvalueA = PVALUEA

type PPVALUEA = uintptr

type PVALUEW = struct {
	Fpv_valuename     LPWSTR
	Fpv_valuelen      int32
	Fpv_value_context LPVOID
	Fpv_type          DWORD
}

type pvalueW = PVALUEW

type PPVALUEW = uintptr

type PVALUE = struct {
	Fpv_valuename     LPSTR
	Fpv_valuelen      int32
	Fpv_value_context LPVOID
	Fpv_type          DWORD
}

type PPVALUE = uintptr

type PQUERYHANDLER = uintptr

type REG_PROVIDER = struct {
	Fpi_R0_1val     PQUERYHANDLER
	Fpi_R0_allvals  PQUERYHANDLER
	Fpi_R3_1val     PQUERYHANDLER
	Fpi_R3_allvals  PQUERYHANDLER
	Fpi_flags       DWORD
	Fpi_key_context LPVOID
}

type provider_info = REG_PROVIDER

type PPROVIDER = uintptr

type VALENTA = struct {
	Fve_valuename LPSTR
	Fve_valuelen  DWORD
	Fve_valueptr  DWORD_PTR
	Fve_type      DWORD
}

type value_entA = VALENTA

type PVALENTA = uintptr

type VALENTW = struct {
	Fve_valuename LPWSTR
	Fve_valuelen  DWORD
	Fve_valueptr  DWORD_PTR
	Fve_type      DWORD
}

type value_entW = VALENTW

type PVALENTW = uintptr

type VALENT = struct {
	Fve_valuename LPSTR
	Fve_valuelen  DWORD
	Fve_valueptr  DWORD_PTR
	Fve_type      DWORD
}

type PVALENT = uintptr

type NETRESOURCEA = struct {
	FdwScope       DWORD
	FdwType        DWORD
	FdwDisplayType DWORD
	FdwUsage       DWORD
	FlpLocalName   LPSTR
	FlpRemoteName  LPSTR
	FlpComment     LPSTR
	FlpProvider    LPSTR
}

type _NETRESOURCEA = NETRESOURCEA

type LPNETRESOURCEA = uintptr

type NETRESOURCEW = struct {
	FdwScope       DWORD
	FdwType        DWORD
	FdwDisplayType DWORD
	FdwUsage       DWORD
	FlpLocalName   LPWSTR
	FlpRemoteName  LPWSTR
	FlpComment     LPWSTR
	FlpProvider    LPWSTR
}

type _NETRESOURCEW = NETRESOURCEW

type LPNETRESOURCEW = uintptr

type NETRESOURCE = struct {
	FdwScope       DWORD
	FdwType        DWORD
	FdwDisplayType DWORD
	FdwUsage       DWORD
	FlpLocalName   LPSTR
	FlpRemoteName  LPSTR
	FlpComment     LPSTR
	FlpProvider    LPSTR
}

type LPNETRESOURCE = uintptr

type CONNECTDLGSTRUCTA = struct {
	FcbStructure DWORD
	FhwndOwner   HWND
	FlpConnRes   LPNETRESOURCEA
	FdwFlags     DWORD
	FdwDevNum    DWORD
}

type _CONNECTDLGSTRUCTA = CONNECTDLGSTRUCTA

type LPCONNECTDLGSTRUCTA = uintptr

type CONNECTDLGSTRUCTW = struct {
	FcbStructure DWORD
	FhwndOwner   HWND
	FlpConnRes   LPNETRESOURCEW
	FdwFlags     DWORD
	FdwDevNum    DWORD
}

type _CONNECTDLGSTRUCTW = CONNECTDLGSTRUCTW

type LPCONNECTDLGSTRUCTW = uintptr

type CONNECTDLGSTRUCT = struct {
	FcbStructure DWORD
	FhwndOwner   HWND
	FlpConnRes   LPNETRESOURCEA
	FdwFlags     DWORD
	FdwDevNum    DWORD
}

type LPCONNECTDLGSTRUCT = uintptr

type DISCDLGSTRUCTA = struct {
	FcbStructure  DWORD
	FhwndOwner    HWND
	FlpLocalName  LPSTR
	FlpRemoteName LPSTR
	FdwFlags      DWORD
}

type _DISCDLGSTRUCTA = DISCDLGSTRUCTA

type LPDISCDLGSTRUCTA = uintptr

type DISCDLGSTRUCTW = struct {
	FcbStructure  DWORD
	FhwndOwner    HWND
	FlpLocalName  LPWSTR
	FlpRemoteName LPWSTR
	FdwFlags      DWORD
}

type _DISCDLGSTRUCTW = DISCDLGSTRUCTW

type LPDISCDLGSTRUCTW = uintptr

type DISCDLGSTRUCT = struct {
	FcbStructure  DWORD
	FhwndOwner    HWND
	FlpLocalName  LPSTR
	FlpRemoteName LPSTR
	FdwFlags      DWORD
}

type LPDISCDLGSTRUCT = uintptr

type UNIVERSAL_NAME_INFOA = struct {
	FlpUniversalName LPSTR
}

type _UNIVERSAL_NAME_INFOA = UNIVERSAL_NAME_INFOA

type LPUNIVERSAL_NAME_INFOA = uintptr

type UNIVERSAL_NAME_INFOW = struct {
	FlpUniversalName LPWSTR
}

type _UNIVERSAL_NAME_INFOW = UNIVERSAL_NAME_INFOW

type LPUNIVERSAL_NAME_INFOW = uintptr

type UNIVERSAL_NAME_INFO = struct {
	FlpUniversalName LPSTR
}

type LPUNIVERSAL_NAME_INFO = uintptr

type REMOTE_NAME_INFOA = struct {
	FlpUniversalName  LPSTR
	FlpConnectionName LPSTR
	FlpRemainingPath  LPSTR
}

type _REMOTE_NAME_INFOA = REMOTE_NAME_INFOA

type LPREMOTE_NAME_INFOA = uintptr

type REMOTE_NAME_INFOW = struct {
	FlpUniversalName  LPWSTR
	FlpConnectionName LPWSTR
	FlpRemainingPath  LPWSTR
}

type _REMOTE_NAME_INFOW = REMOTE_NAME_INFOW

type LPREMOTE_NAME_INFOW = uintptr

type REMOTE_NAME_INFO = struct {
	FlpUniversalName  LPSTR
	FlpConnectionName LPSTR
	FlpRemainingPath  LPSTR
}

type LPREMOTE_NAME_INFO = uintptr

type NETINFOSTRUCT = struct {
	FcbStructure       DWORD
	FdwProviderVersion DWORD
	FdwStatus          DWORD
	FdwCharacteristics DWORD
	FdwHandle          ULONG_PTR
	FwNetType          WORD
	FdwPrinters        DWORD
	FdwDrives          DWORD
}

type _NETINFOSTRUCT = NETINFOSTRUCT

type LPNETINFOSTRUCT = uintptr

type PFNGETPROFILEPATHA = uintptr

type PFNGETPROFILEPATHW = uintptr

type PFNRECONCILEPROFILEA = uintptr

type PFNRECONCILEPROFILEW = uintptr

type PFNPROCESSPOLICIESA = uintptr

type PFNPROCESSPOLICIESW = uintptr

type NETCONNECTINFOSTRUCT = struct {
	FcbStructure   DWORD
	FdwFlags       DWORD
	FdwSpeed       DWORD
	FdwDelay       DWORD
	FdwOptDataSize DWORD
}

type _NETCONNECTINFOSTRUCT = NETCONNECTINFOSTRUCT

type LPNETCONNECTINFOSTRUCT = uintptr

type PUWSTR_C = uintptr

type SERVICE_DESCRIPTIONA = struct {
	FlpDescription LPSTR
}

type _SERVICE_DESCRIPTIONA = SERVICE_DESCRIPTIONA

type LPSERVICE_DESCRIPTIONA = uintptr

type SERVICE_DESCRIPTIONW = struct {
	FlpDescription LPWSTR
}

type _SERVICE_DESCRIPTIONW = SERVICE_DESCRIPTIONW

type LPSERVICE_DESCRIPTIONW = uintptr

type SERVICE_DESCRIPTION = struct {
	FlpDescription LPSTR
}

type LPSERVICE_DESCRIPTION = uintptr

type SC_ACTION_TYPE = int32

type _SC_ACTION_TYPE = int32

const SC_ACTION_NONE = 0
const SC_ACTION_RESTART = 1
const SC_ACTION_REBOOT = 2
const SC_ACTION_RUN_COMMAND = 3

type SC_ACTION = struct {
	FType  SC_ACTION_TYPE
	FDelay DWORD
}

type _SC_ACTION = SC_ACTION

type LPSC_ACTION = uintptr

type SERVICE_FAILURE_ACTIONSA = struct {
	FdwResetPeriod DWORD
	FlpRebootMsg   LPSTR
	FlpCommand     LPSTR
	FcActions      DWORD
	FlpsaActions   uintptr
}

type _SERVICE_FAILURE_ACTIONSA = SERVICE_FAILURE_ACTIONSA

type LPSERVICE_FAILURE_ACTIONSA = uintptr

type SERVICE_FAILURE_ACTIONSW = struct {
	FdwResetPeriod DWORD
	FlpRebootMsg   LPWSTR
	FlpCommand     LPWSTR
	FcActions      DWORD
	FlpsaActions   uintptr
}

type _SERVICE_FAILURE_ACTIONSW = SERVICE_FAILURE_ACTIONSW

type LPSERVICE_FAILURE_ACTIONSW = uintptr

type SERVICE_FAILURE_ACTIONS = struct {
	FdwResetPeriod DWORD
	FlpRebootMsg   LPSTR
	FlpCommand     LPSTR
	FcActions      DWORD
	FlpsaActions   uintptr
}

type LPSERVICE_FAILURE_ACTIONS = uintptr

type SC_HANDLE__ = struct {
	Funused int32
}

type SC_HANDLE = uintptr

type LPSC_HANDLE = uintptr

type SERVICE_STATUS_HANDLE__ = struct {
	Funused int32
}

type SERVICE_STATUS_HANDLE = uintptr

type SC_STATUS_TYPE = int32

type _SC_STATUS_TYPE = int32

const SC_STATUS_PROCESS_INFO = 0

type SC_ENUM_TYPE = int32

type _SC_ENUM_TYPE = int32

const SC_ENUM_PROCESS_INFO = 0

type SERVICE_STATUS = struct {
	FdwServiceType             DWORD
	FdwCurrentState            DWORD
	FdwControlsAccepted        DWORD
	FdwWin32ExitCode           DWORD
	FdwServiceSpecificExitCode DWORD
	FdwCheckPoint              DWORD
	FdwWaitHint                DWORD
}

type _SERVICE_STATUS = SERVICE_STATUS

type LPSERVICE_STATUS = uintptr

type SERVICE_STATUS_PROCESS = struct {
	FdwServiceType             DWORD
	FdwCurrentState            DWORD
	FdwControlsAccepted        DWORD
	FdwWin32ExitCode           DWORD
	FdwServiceSpecificExitCode DWORD
	FdwCheckPoint              DWORD
	FdwWaitHint                DWORD
	FdwProcessId               DWORD
	FdwServiceFlags            DWORD
}

type _SERVICE_STATUS_PROCESS = SERVICE_STATUS_PROCESS

type LPSERVICE_STATUS_PROCESS = uintptr

type ENUM_SERVICE_STATUSA = struct {
	FlpServiceName LPSTR
	FlpDisplayName LPSTR
	FServiceStatus SERVICE_STATUS
}

type _ENUM_SERVICE_STATUSA = ENUM_SERVICE_STATUSA

type LPENUM_SERVICE_STATUSA = uintptr

type ENUM_SERVICE_STATUSW = struct {
	FlpServiceName LPWSTR
	FlpDisplayName LPWSTR
	FServiceStatus SERVICE_STATUS
}

type _ENUM_SERVICE_STATUSW = ENUM_SERVICE_STATUSW

type LPENUM_SERVICE_STATUSW = uintptr

type ENUM_SERVICE_STATUS = struct {
	FlpServiceName LPSTR
	FlpDisplayName LPSTR
	FServiceStatus SERVICE_STATUS
}

type LPENUM_SERVICE_STATUS = uintptr

type ENUM_SERVICE_STATUS_PROCESSA = struct {
	FlpServiceName        LPSTR
	FlpDisplayName        LPSTR
	FServiceStatusProcess SERVICE_STATUS_PROCESS
}

type _ENUM_SERVICE_STATUS_PROCESSA = ENUM_SERVICE_STATUS_PROCESSA

type LPENUM_SERVICE_STATUS_PROCESSA = uintptr

type ENUM_SERVICE_STATUS_PROCESSW = struct {
	FlpServiceName        LPWSTR
	FlpDisplayName        LPWSTR
	FServiceStatusProcess SERVICE_STATUS_PROCESS
}

type _ENUM_SERVICE_STATUS_PROCESSW = ENUM_SERVICE_STATUS_PROCESSW

type LPENUM_SERVICE_STATUS_PROCESSW = uintptr

type ENUM_SERVICE_STATUS_PROCESS = struct {
	FlpServiceName        LPSTR
	FlpDisplayName        LPSTR
	FServiceStatusProcess SERVICE_STATUS_PROCESS
}

type LPENUM_SERVICE_STATUS_PROCESS = uintptr

type SC_LOCK = uintptr

type QUERY_SERVICE_LOCK_STATUSA = struct {
	FfIsLocked      DWORD
	FlpLockOwner    LPSTR
	FdwLockDuration DWORD
}

type _QUERY_SERVICE_LOCK_STATUSA = QUERY_SERVICE_LOCK_STATUSA

type LPQUERY_SERVICE_LOCK_STATUSA = uintptr

type QUERY_SERVICE_LOCK_STATUSW = struct {
	FfIsLocked      DWORD
	FlpLockOwner    LPWSTR
	FdwLockDuration DWORD
}

type _QUERY_SERVICE_LOCK_STATUSW = QUERY_SERVICE_LOCK_STATUSW

type LPQUERY_SERVICE_LOCK_STATUSW = uintptr

type QUERY_SERVICE_LOCK_STATUS = struct {
	FfIsLocked      DWORD
	FlpLockOwner    LPSTR
	FdwLockDuration DWORD
}

type LPQUERY_SERVICE_LOCK_STATUS = uintptr

type QUERY_SERVICE_CONFIGA = struct {
	FdwServiceType      DWORD
	FdwStartType        DWORD
	FdwErrorControl     DWORD
	FlpBinaryPathName   LPSTR
	FlpLoadOrderGroup   LPSTR
	FdwTagId            DWORD
	FlpDependencies     LPSTR
	FlpServiceStartName LPSTR
	FlpDisplayName      LPSTR
}

type _QUERY_SERVICE_CONFIGA = QUERY_SERVICE_CONFIGA

type LPQUERY_SERVICE_CONFIGA = uintptr

type QUERY_SERVICE_CONFIGW = struct {
	FdwServiceType      DWORD
	FdwStartType        DWORD
	FdwErrorControl     DWORD
	FlpBinaryPathName   LPWSTR
	FlpLoadOrderGroup   LPWSTR
	FdwTagId            DWORD
	FlpDependencies     LPWSTR
	FlpServiceStartName LPWSTR
	FlpDisplayName      LPWSTR
}

type _QUERY_SERVICE_CONFIGW = QUERY_SERVICE_CONFIGW

type LPQUERY_SERVICE_CONFIGW = uintptr

type QUERY_SERVICE_CONFIG = struct {
	FdwServiceType      DWORD
	FdwStartType        DWORD
	FdwErrorControl     DWORD
	FlpBinaryPathName   LPSTR
	FlpLoadOrderGroup   LPSTR
	FdwTagId            DWORD
	FlpDependencies     LPSTR
	FlpServiceStartName LPSTR
	FlpDisplayName      LPSTR
}

type LPQUERY_SERVICE_CONFIG = uintptr

type LPSERVICE_MAIN_FUNCTIONW = uintptr

type LPSERVICE_MAIN_FUNCTIONA = uintptr

type SERVICE_TABLE_ENTRYA = struct {
	FlpServiceName LPSTR
	FlpServiceProc LPSERVICE_MAIN_FUNCTIONA
}

type _SERVICE_TABLE_ENTRYA = SERVICE_TABLE_ENTRYA

type LPSERVICE_TABLE_ENTRYA = uintptr

type SERVICE_TABLE_ENTRYW = struct {
	FlpServiceName LPWSTR
	FlpServiceProc LPSERVICE_MAIN_FUNCTIONW
}

type _SERVICE_TABLE_ENTRYW = SERVICE_TABLE_ENTRYW

type LPSERVICE_TABLE_ENTRYW = uintptr

type SERVICE_TABLE_ENTRY = struct {
	FlpServiceName LPSTR
	FlpServiceProc LPSERVICE_MAIN_FUNCTIONA
}

type LPSERVICE_TABLE_ENTRY = uintptr

type LPHANDLER_FUNCTION = uintptr

type LPHANDLER_FUNCTION_EX = uintptr

type PFN_SC_NOTIFY_CALLBACK = uintptr

type SERVICE_CONTROL_STATUS_REASON_PARAMSA = struct {
	FdwReason      DWORD
	FpszComment    LPSTR
	FServiceStatus SERVICE_STATUS_PROCESS
}

type _SERVICE_CONTROL_STATUS_REASON_PARAMSA = SERVICE_CONTROL_STATUS_REASON_PARAMSA

type PSERVICE_CONTROL_STATUS_REASON_PARAMSA = uintptr

type SERVICE_CONTROL_STATUS_REASON_PARAMSW = struct {
	FdwReason      DWORD
	FpszComment    LPWSTR
	FServiceStatus SERVICE_STATUS_PROCESS
}

type _SERVICE_CONTROL_STATUS_REASON_PARAMSW = SERVICE_CONTROL_STATUS_REASON_PARAMSW

type PSERVICE_CONTROL_STATUS_REASON_PARAMSW = uintptr

type SERVICE_CONTROL_STATUS_REASON_PARAMS = struct {
	FdwReason      DWORD
	FpszComment    LPSTR
	FServiceStatus SERVICE_STATUS_PROCESS
}

type PSERVICE_CONTROL_STATUS_REASON_PARAMS = uintptr

type SERVICE_NOTIFYA = struct {
	FdwVersion               DWORD
	FpfnNotifyCallback       PFN_SC_NOTIFY_CALLBACK
	FpContext                PVOID
	FdwNotificationStatus    DWORD
	FServiceStatus           SERVICE_STATUS_PROCESS
	FdwNotificationTriggered DWORD
	FpszServiceNames         LPSTR
}

type _SERVICE_NOTIFYA = SERVICE_NOTIFYA

type PSERVICE_NOTIFYA = uintptr

type SERVICE_NOTIFYW = struct {
	FdwVersion               DWORD
	FpfnNotifyCallback       PFN_SC_NOTIFY_CALLBACK
	FpContext                PVOID
	FdwNotificationStatus    DWORD
	FServiceStatus           SERVICE_STATUS_PROCESS
	FdwNotificationTriggered DWORD
	FpszServiceNames         LPWSTR
}

type _SERVICE_NOTIFYW = SERVICE_NOTIFYW

type PSERVICE_NOTIFYW = uintptr

type SERVICE_NOTIFY = struct {
	FdwVersion               DWORD
	FpfnNotifyCallback       PFN_SC_NOTIFY_CALLBACK
	FpContext                PVOID
	FdwNotificationStatus    DWORD
	FServiceStatus           SERVICE_STATUS_PROCESS
	FdwNotificationTriggered DWORD
	FpszServiceNames         LPSTR
}

type PSERVICE_NOTIFY = uintptr

type SERVICE_DELAYED_AUTO_START_INFO = struct {
	FfDelayedAutostart WINBOOL
}

type _SERVICE_DELAYED_AUTO_START_INFO = SERVICE_DELAYED_AUTO_START_INFO

type LPSERVICE_DELAYED_AUTO_START_INFO = uintptr

type SERVICE_FAILURE_ACTIONS_FLAG = struct {
	FfFailureActionsOnNonCrashFailures WINBOOL
}

type _SERVICE_FAILURE_ACTIONS_FLAG = SERVICE_FAILURE_ACTIONS_FLAG

type LPSERVICE_FAILURE_ACTIONS_FLAG = uintptr

type SERVICE_PRESHUTDOWN_INFO = struct {
	FdwPreshutdownTimeout DWORD
}

type _SERVICE_PRESHUTDOWN_INFO = SERVICE_PRESHUTDOWN_INFO

type LPSERVICE_PRESHUTDOWN_INFO = uintptr

type SERVICE_REQUIRED_PRIVILEGES_INFOA = struct {
	FpmszRequiredPrivileges LPSTR
}

type _SERVICE_REQUIRED_PRIVILEGES_INFOA = SERVICE_REQUIRED_PRIVILEGES_INFOA

type LPSERVICE_REQUIRED_PRIVILEGES_INFOA = uintptr

type SERVICE_REQUIRED_PRIVILEGES_INFOW = struct {
	FpmszRequiredPrivileges LPWSTR
}

type _SERVICE_REQUIRED_PRIVILEGES_INFOW = SERVICE_REQUIRED_PRIVILEGES_INFOW

type LPSERVICE_REQUIRED_PRIVILEGES_INFOW = uintptr

type SERVICE_REQUIRED_PRIVILEGES_INFO = struct {
	FpmszRequiredPrivileges LPSTR
}

type SERVICE_SID_INFO = struct {
	FdwServiceSidType DWORD
}

type _SERVICE_SID_INFO = SERVICE_SID_INFO

type LPSERVICE_SID_INFO = uintptr

type MODEMDEVCAPS = struct {
	FdwActualSize              DWORD
	FdwRequiredSize            DWORD
	FdwDevSpecificOffset       DWORD
	FdwDevSpecificSize         DWORD
	FdwModemProviderVersion    DWORD
	FdwModemManufacturerOffset DWORD
	FdwModemManufacturerSize   DWORD
	FdwModemModelOffset        DWORD
	FdwModemModelSize          DWORD
	FdwModemVersionOffset      DWORD
	FdwModemVersionSize        DWORD
	FdwDialOptions             DWORD
	FdwCallSetupFailTimer      DWORD
	FdwInactivityTimeout       DWORD
	FdwSpeakerVolume           DWORD
	FdwSpeakerMode             DWORD
	FdwModemOptions            DWORD
	FdwMaxDTERate              DWORD
	FdwMaxDCERate              DWORD
	FabVariablePortion         [1]BYTE
}

type _MODEMDEVCAPS = MODEMDEVCAPS

type PMODEMDEVCAPS = uintptr

type LPMODEMDEVCAPS = uintptr

type MODEMSETTINGS = struct {
	FdwActualSize             DWORD
	FdwRequiredSize           DWORD
	FdwDevSpecificOffset      DWORD
	FdwDevSpecificSize        DWORD
	FdwCallSetupFailTimer     DWORD
	FdwInactivityTimeout      DWORD
	FdwSpeakerVolume          DWORD
	FdwSpeakerMode            DWORD
	FdwPreferredModemOptions  DWORD
	FdwNegotiatedModemOptions DWORD
	FdwNegotiatedDCERate      DWORD
	FabVariablePortion        [1]BYTE
}

type _MODEMSETTINGS = MODEMSETTINGS

type PMODEMSETTINGS = uintptr

type LPMODEMSETTINGS = uintptr

type HIMC__ = struct {
	Funused int32
}

type HIMC = uintptr

type HIMCC__ = struct {
	Funused int32
}

type HIMCC = uintptr

type LPHKL = uintptr

type LPUINT = uintptr

type COMPOSITIONFORM = struct {
	FdwStyle      DWORD
	FptCurrentPos POINT
	FrcArea       RECT
}

type tagCOMPOSITIONFORM = COMPOSITIONFORM

type PCOMPOSITIONFORM = uintptr

type NPCOMPOSITIONFORM = uintptr

type LPCOMPOSITIONFORM = uintptr

type CANDIDATEFORM = struct {
	FdwIndex      DWORD
	FdwStyle      DWORD
	FptCurrentPos POINT
	FrcArea       RECT
}

type tagCANDIDATEFORM = CANDIDATEFORM

type PCANDIDATEFORM = uintptr

type NPCANDIDATEFORM = uintptr

type LPCANDIDATEFORM = uintptr

type CANDIDATELIST = struct {
	FdwSize      DWORD
	FdwStyle     DWORD
	FdwCount     DWORD
	FdwSelection DWORD
	FdwPageStart DWORD
	FdwPageSize  DWORD
	FdwOffset    [1]DWORD
}

type tagCANDIDATELIST = CANDIDATELIST

type PCANDIDATELIST = uintptr

type NPCANDIDATELIST = uintptr

type LPCANDIDATELIST = uintptr

type REGISTERWORDA = struct {
	FlpReading LPSTR
	FlpWord    LPSTR
}

type tagREGISTERWORDA = REGISTERWORDA

type PREGISTERWORDA = uintptr

type NPREGISTERWORDA = uintptr

type LPREGISTERWORDA = uintptr

type REGISTERWORDW = struct {
	FlpReading LPWSTR
	FlpWord    LPWSTR
}

type tagREGISTERWORDW = REGISTERWORDW

type PREGISTERWORDW = uintptr

type NPREGISTERWORDW = uintptr

type LPREGISTERWORDW = uintptr

type REGISTERWORD = struct {
	FlpReading LPSTR
	FlpWord    LPSTR
}

type PREGISTERWORD = uintptr

type NPREGISTERWORD = uintptr

type LPREGISTERWORD = uintptr

type RECONVERTSTRING = struct {
	FdwSize            DWORD
	FdwVersion         DWORD
	FdwStrLen          DWORD
	FdwStrOffset       DWORD
	FdwCompStrLen      DWORD
	FdwCompStrOffset   DWORD
	FdwTargetStrLen    DWORD
	FdwTargetStrOffset DWORD
}

type tagRECONVERTSTRING = RECONVERTSTRING

type PRECONVERTSTRING = uintptr

type NPRECONVERTSTRING = uintptr

type LPRECONVERTSTRING = uintptr

type STYLEBUFA = struct {
	FdwStyle       DWORD
	FszDescription [32]CHAR
}

type tagSTYLEBUFA = STYLEBUFA

type PSTYLEBUFA = uintptr

type NPSTYLEBUFA = uintptr

type LPSTYLEBUFA = uintptr

type STYLEBUFW = struct {
	FdwStyle       DWORD
	FszDescription [32]WCHAR
}

type tagSTYLEBUFW = STYLEBUFW

type PSTYLEBUFW = uintptr

type NPSTYLEBUFW = uintptr

type LPSTYLEBUFW = uintptr

type STYLEBUF = struct {
	FdwStyle       DWORD
	FszDescription [32]CHAR
}

type PSTYLEBUF = uintptr

type NPSTYLEBUF = uintptr

type LPSTYLEBUF = uintptr

type IMEMENUITEMINFOA = struct {
	FcbSize        UINT
	FfType         UINT
	FfState        UINT
	FwID           UINT
	FhbmpChecked   HBITMAP
	FhbmpUnchecked HBITMAP
	FdwItemData    DWORD
	FszString      [80]CHAR
	FhbmpItem      HBITMAP
}

type tagIMEMENUITEMINFOA = IMEMENUITEMINFOA

type PIMEMENUITEMINFOA = uintptr

type NPIMEMENUITEMINFOA = uintptr

type LPIMEMENUITEMINFOA = uintptr

type IMEMENUITEMINFOW = struct {
	FcbSize        UINT
	FfType         UINT
	FfState        UINT
	FwID           UINT
	FhbmpChecked   HBITMAP
	FhbmpUnchecked HBITMAP
	FdwItemData    DWORD
	FszString      [80]WCHAR
	FhbmpItem      HBITMAP
}

type tagIMEMENUITEMINFOW = IMEMENUITEMINFOW

type PIMEMENUITEMINFOW = uintptr

type NPIMEMENUITEMINFOW = uintptr

type LPIMEMENUITEMINFOW = uintptr

type IMEMENUITEMINFO = struct {
	FcbSize        UINT
	FfType         UINT
	FfState        UINT
	FwID           UINT
	FhbmpChecked   HBITMAP
	FhbmpUnchecked HBITMAP
	FdwItemData    DWORD
	FszString      [80]CHAR
	FhbmpItem      HBITMAP
}

type PIMEMENUITEMINFO = uintptr

type NPIMEMENUITEMINFO = uintptr

type LPIMEMENUITEMINFO = uintptr

type IMECHARPOSITION = struct {
	FdwSize      DWORD
	FdwCharPos   DWORD
	Fpt          POINT
	FcLineHeight UINT
	FrcDocument  RECT
}

type tagIMECHARPOSITION = IMECHARPOSITION

type PIMECHARPOSITION = uintptr

type NPIMECHARPOSITION = uintptr

type LPIMECHARPOSITION = uintptr

type IMCENUMPROC = uintptr

type REGISTERWORDENUMPROCA = uintptr

type REGISTERWORDENUMPROCW = uintptr

/* Restore old value of interface for Obj-C.  See above.  */

/* mutex */

/* condition variable */

// C documentation
//
//	/* ZSTD_pthread_create() and ZSTD_pthread_join() */
type ZSTD_pthread_t = uintptr

type _crt_app_type1 = int32

type _crt_app_type = int32

const _crt_unknown_app = 0
const _crt_console_app = 1
const _crt_gui_app = 2

type _crt_argv_mode1 = int32

type _crt_argv_mode = int32

const _crt_argv_no_arguments = 0
const _crt_argv_unexpanded_arguments = 1
const _crt_argv_expanded_arguments = 2

type _PVFV = uintptr

type _PIFV = uintptr

type _PVFI = uintptr

type _onexit_table_t = struct {
	F_first uintptr
	F_last  uintptr
	F_end   uintptr
}

type _ino_t = uint16

type ino_t = uint16

type _dev_t = uint32

type dev_t = uint32

type _pid_t = int64

type pid_t = int64

type _mode_t = uint16

type mode_t = uint16

type _off_t = int32

type off32_t = int32

type _off64_t = int64

type off64_t = int64

type off_t = int32

type useconds_t = uint32

type timespec = struct {
	Ftv_sec  time_t
	Ftv_nsec int32
}

type itimerspec = struct {
	Fit_interval timespec
	Fit_value    timespec
}

type _sigset_t = uint64

type _beginthread_proc_type = uintptr

type _beginthreadex_proc_type = uintptr

type _tls_callback_type = uintptr

/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/* ===  Implementation  === */

type ZSTD_thread_params_t = struct {
	Fstart_routine     uintptr
	Farg               uintptr
	Finitialized       int32
	Finitialized_cond  CONDITION_VARIABLE
	Finitialized_mutex CRITICAL_SECTION
}

func worker(tls *libc.TLS, arg uintptr) (r uint32) {
	var start_routine, thread_arg, thread_param uintptr
	_, _, _ = start_routine, thread_arg, thread_param
	/* Initialized thread_arg and start_routine and signal main thread that we don't need it
	 * to wait any longer.
	 */
	thread_param = arg
	thread_arg = (*ZSTD_thread_params_t)(unsafe.Pointer(thread_param)).Farg
	start_routine = (*ZSTD_thread_params_t)(unsafe.Pointer(thread_param)).Fstart_routine
	/* Signal main thread that we are running and do not depend on its memory anymore */
	libc.XEnterCriticalSection(tls, thread_param+32)
	(*ZSTD_thread_params_t)(unsafe.Pointer(thread_param)).Finitialized = int32(1)
	WakeConditionVariable(tls, thread_param+24)
	libc.XLeaveCriticalSection(tls, thread_param+32)
	(*(*func(*libc.TLS, uintptr) uintptr)(unsafe.Pointer(&struct{ uintptr }{start_routine})))(tls, thread_arg)
	return uint32(0)
}

type __ccgo_fp__XZSTD_pthread_create_2 = func(*libc.TLS, uintptr) uintptr

func ZSTD_pthread_create(tls *libc.TLS, thread uintptr, unused uintptr, __ccgo_fp_start_routine uintptr, arg uintptr) (r int32) {
	bp := tls.Alloc(80)
	defer tls.Free(80)
	var _ /* thread_param at bp+0 */ ZSTD_thread_params_t
	_ = unused
	if thread == libc.UintptrFromInt32(0) {
		return -int32(1)
	}
	*(*ZSTD_pthread_t)(unsafe.Pointer(thread)) = libc.UintptrFromInt32(0)
	(*(*ZSTD_thread_params_t)(unsafe.Pointer(bp))).Fstart_routine = __ccgo_fp_start_routine
	(*(*ZSTD_thread_params_t)(unsafe.Pointer(bp))).Farg = arg
	(*(*ZSTD_thread_params_t)(unsafe.Pointer(bp))).Finitialized = 0
	/* Setup thread initialization synchronization */
	_ = libc.UintptrFromInt32(0)
	InitializeConditionVariable(tls, bp+24)
	if libc.Int32FromInt32(0) != 0 {
		/* Should never happen on Windows */
		return -int32(1)
	}
	_ = libc.UintptrFromInt32(0)
	libc.XInitializeCriticalSection(tls, bp+32)
	if libc.Int32FromInt32(0) != 0 {
		/* Should never happen on Windows */
		_ = bp + 24
		return -int32(1)
	}
	/* Spawn thread */
	*(*ZSTD_pthread_t)(unsafe.Pointer(thread)) = uintptr(libc.X_beginthreadex(tls, libc.UintptrFromInt32(0), uint32(0), __ccgo_fp(worker), bp, uint32(0), libc.UintptrFromInt32(0)))
	if *(*ZSTD_pthread_t)(unsafe.Pointer(thread)) == libc.UintptrFromInt32(0) {
		libc.XDeleteCriticalSection(tls, bp+32)
		_ = bp + 24
		return *(*int32)(unsafe.Pointer(libc.X_errno(tls)))
	}
	/* Wait for thread to be initialized */
	libc.XEnterCriticalSection(tls, bp+32)
	for !((*(*ZSTD_thread_params_t)(unsafe.Pointer(bp))).Finitialized != 0) {
		SleepConditionVariableCS(tls, bp+24, bp+32, uint32(INFINITE))
	}
	libc.XLeaveCriticalSection(tls, bp+32)
	libc.XDeleteCriticalSection(tls, bp+32)
	_ = bp + 24
	return 0
}

func ZSTD_pthread_join(tls *libc.TLS, thread ZSTD_pthread_t) (r int32) {
	var result DWORD
	_ = result
	if !(thread != 0) {
		return 0
	}
	result = libc.XWaitForSingleObject(tls, thread, uint32(INFINITE))
	libc.XCloseHandle(tls, thread)
	switch result {
	case libc.Uint32FromInt32(0x00000000) + libc.Uint32FromInt32(0):
		return 0
	case libc.Uint32FromInt32(0x00000080) + libc.Uint32FromInt32(0):
		return int32(EINVAL)
	default:
		return int32(libc.XGetLastError(tls))
	}
	return r
}

/*!< default compression level, specified by ZSTD_CLEVEL_DEFAULT, requires v1.5.0+ */

// C documentation
//
//	/***************************************
//	*  Explicit context
//	***************************************/
//	/*= Compression context
//	 *  When compressing many times,
//	 *  it is recommended to allocate a compression context just once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make the workload easier for system's memory.
//	 *  Note : re-using context is just a speed / resource optimization.
//	 *         It doesn't change the compression ratio, which remains identical.
//	 *  Note 2: For parallel execution in multi-threaded environments,
//	 *         use one different context per thread .
//	 */
type ZSTD_CCtx = struct {
	Fstage                 ZSTD_compressionStage_e
	FcParamsChanged        int32
	Fbmi2                  int32
	FrequestedParams       ZSTD_CCtx_params
	FappliedParams         ZSTD_CCtx_params
	FsimpleApiParams       ZSTD_CCtx_params
	FdictID                U32
	FdictContentSize       size_t
	Fworkspace             ZSTD_cwksp
	FblockSizeMax          size_t
	FpledgedSrcSizePlusOne uint64
	FconsumedSrcSize       uint64
	FproducedCSize         uint64
	FxxhState              XXH_NAMESPACEXXH64_state_t
	FcustomMem             ZSTD_customMem
	Fpool                  uintptr
	FstaticSize            size_t
	FseqCollector          SeqCollector
	FisFirstBlock          int32
	Finitialized           int32
	FseqStore              SeqStore_t
	FldmState              ldmState_t
	FldmSequences          uintptr
	FmaxNbLdmSequences     size_t
	FexternSeqStore        RawSeqStore_t
	FblockState            ZSTD_blockState_t
	FtmpWorkspace          uintptr
	FtmpWkspSize           size_t
	FbufferedPolicy        ZSTD_buffered_policy_e
	FinBuff                uintptr
	FinBuffSize            size_t
	FinToCompress          size_t
	FinBuffPos             size_t
	FinBuffTarget          size_t
	FoutBuff               uintptr
	FoutBuffSize           size_t
	FoutBuffContentSize    size_t
	FoutBuffFlushedSize    size_t
	FstreamStage           ZSTD_cStreamStage
	FframeEnded            U32
	FexpectedInBuffer      ZSTD_inBuffer
	FstableIn_notConsumed  size_t
	FexpectedOutBufferSize size_t
	FlocalDict             ZSTD_localDict
	Fcdict                 uintptr
	FprefixDict            ZSTD_prefixDict
	Fmtctx                 uintptr
	FblockSplitCtx         ZSTD_blockSplitCtx
	FextSeqBuf             uintptr
	FextSeqBufCapacity     size_t
}

/*!< default compression level, specified by ZSTD_CLEVEL_DEFAULT, requires v1.5.0+ */

// C documentation
//
//	/***************************************
//	*  Explicit context
//	***************************************/
//	/*= Compression context
//	 *  When compressing many times,
//	 *  it is recommended to allocate a compression context just once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make the workload easier for system's memory.
//	 *  Note : re-using context is just a speed / resource optimization.
//	 *         It doesn't change the compression ratio, which remains identical.
//	 *  Note 2: For parallel execution in multi-threaded environments,
//	 *         use one different context per thread .
//	 */
type ZSTD_CCtx_s = ZSTD_CCtx

const ZSTDcs_created = 0
const ZSTDcs_init = 1
const ZSTDcs_ongoing = 2
const ZSTDcs_ending = 3
const ZSTDb_not_buffered = 0
const ZSTDb_buffered = 1
const zcss_init = 0
const zcss_load = 1
const zcss_flush = 2

// C documentation
//
//	/*= Decompression context
//	 *  When decompressing many times,
//	 *  it is recommended to allocate a context only once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make workload friendlier for system's memory.
//	 *  Use one context per thread for parallel execution. */
type ZSTD_DCtx = struct {
	FLLTptr               uintptr
	FMLTptr               uintptr
	FOFTptr               uintptr
	FHUFptr               uintptr
	Fentropy              ZSTD_entropyDTables_t
	Fworkspace            [640]U32
	FpreviousDstEnd       uintptr
	FprefixStart          uintptr
	FvirtualStart         uintptr
	FdictEnd              uintptr
	Fexpected             size_t
	FfParams              ZSTD_FrameHeader
	FprocessedCSize       U64
	FdecodedSize          U64
	FbType                blockType_e
	Fstage                ZSTD_dStage
	FlitEntropy           U32
	FfseEntropy           U32
	FxxhState             XXH_NAMESPACEXXH64_state_t
	FheaderSize           size_t
	Fformat               ZSTD_format_e
	FforceIgnoreChecksum  ZSTD_forceIgnoreChecksum_e
	FvalidateChecksum     U32
	FlitPtr               uintptr
	FcustomMem            ZSTD_customMem
	FlitSize              size_t
	FrleSize              size_t
	FstaticSize           size_t
	FisFrameDecompression int32
	Fbmi2                 int32
	FddictLocal           uintptr
	Fddict                uintptr
	FdictID               U32
	FddictIsCold          int32
	FdictUses             ZSTD_dictUses_e
	FddictSet             uintptr
	FrefMultipleDDicts    ZSTD_refMultipleDDicts_e
	FdisableHufAsm        int32
	FmaxBlockSizeParam    int32
	FstreamStage          ZSTD_dStreamStage
	FinBuff               uintptr
	FinBuffSize           size_t
	FinPos                size_t
	FmaxWindowSize        size_t
	FoutBuff              uintptr
	FoutBuffSize          size_t
	FoutStart             size_t
	FoutEnd               size_t
	FlhSize               size_t
	FhostageByte          U32
	FnoForwardProgress    int32
	FoutBufferMode        ZSTD_bufferMode_e
	FexpectedOutBuffer    ZSTD_outBuffer
	FlitBuffer            uintptr
	FlitBufferEnd         uintptr
	FlitBufferLocation    ZSTD_litLocation_e
	FlitExtraBuffer       [65568]BYTE
	FheaderBuffer         [18]BYTE
	FoversizedDuration    size_t
}

// C documentation
//
//	/*= Decompression context
//	 *  When decompressing many times,
//	 *  it is recommended to allocate a context only once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make workload friendlier for system's memory.
//	 *  Use one context per thread for parallel execution. */
type ZSTD_DCtx_s = ZSTD_DCtx

const bt_raw = 0
const bt_rle = 1
const bt_compressed = 2
const bt_reserved = 3
const ZSTDds_getFrameHeaderSize = 0
const ZSTDds_decodeFrameHeader = 1
const ZSTDds_decodeBlockHeader = 2
const ZSTDds_decompressBlock = 3
const ZSTDds_decompressLastBlock = 4
const ZSTDds_checkChecksum = 5
const ZSTDds_decodeSkippableHeader = 6
const ZSTDds_skipFrame = 7
const ZSTD_f_zstd1 = 0
const /* zstd frame format, specified in zstd_compression_format.md (default) */
ZSTD_f_zstd1_magicless = 1
const
/* Note: this enum controls ZSTD_d_forceIgnoreChecksum */
ZSTD_d_validateChecksum = 0
const ZSTD_d_ignoreChecksum = 1
const ZSTD_use_indefinitely = -1
const /* Use the dictionary indefinitely */
ZSTD_dont_use = 0
const /* Do not use the dictionary (if one exists free it) */
ZSTD_use_once = 1
const
/* Note: this enum controls ZSTD_d_refMultipleDDicts */
ZSTD_rmd_refSingleDDict = 0
const ZSTD_rmd_refMultipleDDicts = 1
const zdss_init = 0
const zdss_loadHeader = 1
const zdss_read = 2
const zdss_load = 3
const zdss_flush = 4
const ZSTD_bm_buffered = 0
const /* Buffer the input/output */
ZSTD_bm_stable = 1
const ZSTD_not_in_dst = 0
const /* Stored entirely within litExtraBuffer */
ZSTD_in_dst = 1
const /* Stored entirely within dst (in memory after current output write) */
ZSTD_split = 2

/*********************************************
*  Advanced compression API (Requires v1.4.0+)
**********************************************/

/* API design :
 *   Parameters are pushed one by one into an existing context,
 *   using ZSTD_CCtx_set*() functions.
 *   Pushed parameters are sticky : they are valid for next compressed frame, and any subsequent frame.
 *   "sticky" parameters are applicable to `ZSTD_compress2()` and `ZSTD_compressStream*()` !
 *   __They do not apply to one-shot variants such as ZSTD_compressCCtx()__ .
 *
 *   It's possible to reset all parameters to "default" using ZSTD_CCtx_reset().
 *
 *   This API supersedes all other "advanced" API entry points in the experimental section.
 *   In the future, we expect to remove API entry points from experimental which are redundant with this API.
 */

// C documentation
//
//	/* Compression strategies, listed from fastest to strongest */
type ZSTD_strategy = int32

const ZSTD_fast = 1
const ZSTD_dfast = 2
const ZSTD_greedy = 3
const ZSTD_lazy = 4
const ZSTD_lazy2 = 5
const ZSTD_btlazy2 = 6
const ZSTD_btopt = 7
const ZSTD_btultra = 8
const ZSTD_btultra2 = 9

type ZSTD_cParameter = int32

const

/* compression parameters
 * Note: When compressing with a ZSTD_CDict these parameters are superseded
 * by the parameters used to construct the ZSTD_CDict.
 * See ZSTD_CCtx_refCDict() for more info (superseded-by-cdict). */
ZSTD_c_compressionLevel = 100
const /* Set compression parameters according to pre-defined cLevel table.
 * Note that exact compression parameters are dynamically determined,
 * depending on both compression level and srcSize (when known).
 * Default level is ZSTD_CLEVEL_DEFAULT==3.
 * Special: value 0 means default, which is controlled by ZSTD_CLEVEL_DEFAULT.
 * Note 1 : it's possible to pass a negative compression level.
 * Note 2 : setting a level does not automatically set all other compression parameters
 *   to default. Setting this will however eventually dynamically impact the compression
 *   parameters which have not been manually set. The manually set
 *   ones will 'stick'. */
/* Advanced compression parameters :
 * It's possible to pin down compression parameters to some specific values.
 * In which case, these values are no longer dynamically selected by the compressor */
ZSTD_c_windowLog = 101
const /* Maximum allowed back-reference distance, expressed as power of 2.
 * This will set a memory budget for streaming decompression,
 * with larger values requiring more memory
 * and typically compressing more.
 * Must be clamped between ZSTD_WINDOWLOG_MIN and ZSTD_WINDOWLOG_MAX.
 * Special: value 0 means "use default windowLog".
 * Note: Using a windowLog greater than ZSTD_WINDOWLOG_LIMIT_DEFAULT
 *       requires explicitly allowing such size at streaming decompression stage. */
ZSTD_c_hashLog = 102
const /* Size of the initial probe table, as a power of 2.
 * Resulting memory usage is (1 << (hashLog+2)).
 * Must be clamped between ZSTD_HASHLOG_MIN and ZSTD_HASHLOG_MAX.
 * Larger tables improve compression ratio of strategies <= dFast,
 * and improve speed of strategies > dFast.
 * Special: value 0 means "use default hashLog". */
ZSTD_c_chainLog = 103
const /* Size of the multi-probe search table, as a power of 2.
 * Resulting memory usage is (1 << (chainLog+2)).
 * Must be clamped between ZSTD_CHAINLOG_MIN and ZSTD_CHAINLOG_MAX.
 * Larger tables result in better and slower compression.
 * This parameter is useless for "fast" strategy.
 * It's still useful when using "dfast" strategy,
 * in which case it defines a secondary probe table.
 * Special: value 0 means "use default chainLog". */
ZSTD_c_searchLog = 104
const /* Number of search attempts, as a power of 2.
 * More attempts result in better and slower compression.
 * This parameter is useless for "fast" and "dFast" strategies.
 * Special: value 0 means "use default searchLog". */
ZSTD_c_minMatch = 105
const /* Minimum size of searched matches.
 * Note that Zstandard can still find matches of smaller size,
 * it just tweaks its search algorithm to look for this size and larger.
 * Larger values increase compression and decompression speed, but decrease ratio.
 * Must be clamped between ZSTD_MINMATCH_MIN and ZSTD_MINMATCH_MAX.
 * Note that currently, for all strategies < btopt, effective minimum is 4.
 *                    , for all strategies > fast, effective maximum is 6.
 * Special: value 0 means "use default minMatchLength". */
ZSTD_c_targetLength = 106
const /* Impact of this field depends on strategy.
 * For strategies btopt, btultra & btultra2:
 *     Length of Match considered "good enough" to stop search.
 *     Larger values make compression stronger, and slower.
 * For strategy fast:
 *     Distance between match sampling.
 *     Larger values make compression faster, and weaker.
 * Special: value 0 means "use default targetLength". */
ZSTD_c_strategy = 107
const /* See ZSTD_strategy enum definition.
 * The higher the value of selected strategy, the more complex it is,
 * resulting in stronger and slower compression.
 * Special: value 0 means "use default strategy". */

ZSTD_c_targetCBlockSize = 130
const /* v1.5.6+
 * Attempts to fit compressed block size into approximately targetCBlockSize.
 * Bound by ZSTD_TARGETCBLOCKSIZE_MIN and ZSTD_TARGETCBLOCKSIZE_MAX.
 * Note that it's not a guarantee, just a convergence target (default:0).
 * No target when targetCBlockSize == 0.
 * This is helpful in low bandwidth streaming environments to improve end-to-end latency,
 * when a client can make use of partial documents (a prominent example being Chrome).
 * Note: this parameter is stable since v1.5.6.
 * It was present as an experimental parameter in earlier versions,
 * but it's not recommended using it with earlier library versions
 * due to massive performance regressions.
 */
/* LDM mode parameters */
ZSTD_c_enableLongDistanceMatching = 160
const /* Enable long distance matching.
 * This parameter is designed to improve compression ratio
 * for large inputs, by finding large matches at long distance.
 * It increases memory usage and window size.
 * Note: enabling this parameter increases default ZSTD_c_windowLog to 128 MB
 * except when expressly set to a different value.
 * Note: will be enabled by default if ZSTD_c_windowLog >= 128 MB and
 * compression strategy >= ZSTD_btopt (== compression level 16+) */
ZSTD_c_ldmHashLog = 161
const /* Size of the table for long distance matching, as a power of 2.
 * Larger values increase memory usage and compression ratio,
 * but decrease compression speed.
 * Must be clamped between ZSTD_HASHLOG_MIN and ZSTD_HASHLOG_MAX
 * default: windowlog - 7.
 * Special: value 0 means "automatically determine hashlog". */
ZSTD_c_ldmMinMatch = 162
const /* Minimum match size for long distance matcher.
 * Larger/too small values usually decrease compression ratio.
 * Must be clamped between ZSTD_LDM_MINMATCH_MIN and ZSTD_LDM_MINMATCH_MAX.
 * Special: value 0 means "use default value" (default: 64). */
ZSTD_c_ldmBucketSizeLog = 163
const /* Log size of each bucket in the LDM hash table for collision resolution.
 * Larger values improve collision resolution but decrease compression speed.
 * The maximum value is ZSTD_LDM_BUCKETSIZELOG_MAX.
 * Special: value 0 means "use default value" (default: 3). */
ZSTD_c_ldmHashRateLog = 164
const /* Frequency of inserting/looking up entries into the LDM hash table.
 * Must be clamped between 0 and (ZSTD_WINDOWLOG_MAX - ZSTD_HASHLOG_MIN).
 * Default is MAX(0, (windowLog - ldmHashLog)), optimizing hash table usage.
 * Larger values improve compression speed.
 * Deviating far from default value will likely result in a compression ratio decrease.
 * Special: value 0 means "automatically determine hashRateLog". */

/* frame parameters */
ZSTD_c_contentSizeFlag = 200
const /* Content size will be written into frame header _whenever known_ (default:1)
 * Content size must be known at the beginning of compression.
 * This is automatically the case when using ZSTD_compress2(),
 * For streaming scenarios, content size must be provided with ZSTD_CCtx_setPledgedSrcSize() */
ZSTD_c_checksumFlag = 201
const /* A 32-bits checksum of content is written at end of frame (default:0) */
ZSTD_c_dictIDFlag = 202
const /* When applicable, dictionary's ID is written into frame header (default:1) */

/* multi-threading parameters */
/* These parameters are only active if multi-threading is enabled (compiled with build macro ZSTD_MULTITHREAD).
 * Otherwise, trying to set any other value than default (0) will be a no-op and return an error.
 * In a situation where it's unknown if the linked library supports multi-threading or not,
 * setting ZSTD_c_nbWorkers to any value >= 1 and consulting the return value provides a quick way to check this property.
 */
ZSTD_c_nbWorkers = 400
const /* Select how many threads will be spawned to compress in parallel.
 * When nbWorkers >= 1, triggers asynchronous mode when invoking ZSTD_compressStream*() :
 * ZSTD_compressStream*() consumes input and flush output if possible, but immediately gives back control to caller,
 * while compression is performed in parallel, within worker thread(s).
 * (note : a strong exception to this rule is when first invocation of ZSTD_compressStream2() sets ZSTD_e_end :
 *  in which case, ZSTD_compressStream2() delegates to ZSTD_compress2(), which is always a blocking call).
 * More workers improve speed, but also increase memory usage.
 * Default value is `0`, aka "single-threaded mode" : no worker is spawned,
 * compression is performed inside Caller's thread, and all invocations are blocking */
ZSTD_c_jobSize = 401
const /* Size of a compression job. This value is enforced only when nbWorkers >= 1.
 * Each compression job is completed in parallel, so this value can indirectly impact the nb of active threads.
 * 0 means default, which is dynamically determined based on compression parameters.
 * Job size must be a minimum of overlap size, or ZSTDMT_JOBSIZE_MIN (= 512 KB), whichever is largest.
 * The minimum size is automatically and transparently enforced. */
ZSTD_c_overlapLog = 402
const /* Control the overlap size, as a fraction of window size.
 * The overlap size is an amount of data reloaded from previous job at the beginning of a new job.
 * It helps preserve compression ratio, while each job is compressed in parallel.
 * This value is enforced only when nbWorkers >= 1.
 * Larger values increase compression ratio, but decrease speed.
 * Possible values range from 0 to 9 :
 * - 0 means "default" : value will be determined by the library, depending on strategy
 * - 1 means "no overlap"
 * - 9 means "full overlap", using a full window size.
 * Each intermediate rank increases/decreases load size by a factor 2 :
 * 9: full window;  8: w/2;  7: w/4;  6: w/8;  5:w/16;  4: w/32;  3:w/64;  2:w/128;  1:no overlap;  0:default
 * default value varies between 6 and 9, depending on strategy */

/* note : additional experimental parameters are also available
 * within the experimental section of the API.
 * At the time of this writing, they include :
 * ZSTD_c_rsyncable
 * ZSTD_c_format
 * ZSTD_c_forceMaxWindow
 * ZSTD_c_forceAttachDict
 * ZSTD_c_literalCompressionMode
 * ZSTD_c_srcSizeHint
 * ZSTD_c_enableDedicatedDictSearch
 * ZSTD_c_stableInBuffer
 * ZSTD_c_stableOutBuffer
 * ZSTD_c_blockDelimiters
 * ZSTD_c_validateSequences
 * ZSTD_c_blockSplitterLevel
 * ZSTD_c_splitAfterSequences
 * ZSTD_c_useRowMatchFinder
 * ZSTD_c_prefetchCDictTables
 * ZSTD_c_enableSeqProducerFallback
 * ZSTD_c_maxBlockSize
 * Because they are not stable, it's necessary to define ZSTD_STATIC_LINKING_ONLY to access them.
 * note : never ever use experimentalParam? names directly;
 *        also, the enums values themselves are unstable and can still change.
 */
ZSTD_c_experimentalParam1 = 500
const ZSTD_c_experimentalParam2 = 10
const ZSTD_c_experimentalParam3 = 1000
const ZSTD_c_experimentalParam4 = 1001
const ZSTD_c_experimentalParam5 = 1002
const
/* was ZSTD_c_experimentalParam6=1003; is now ZSTD_c_targetCBlockSize */
ZSTD_c_experimentalParam7 = 1004
const ZSTD_c_experimentalParam8 = 1005
const ZSTD_c_experimentalParam9 = 1006
const ZSTD_c_experimentalParam10 = 1007
const ZSTD_c_experimentalParam11 = 1008
const ZSTD_c_experimentalParam12 = 1009
const ZSTD_c_experimentalParam13 = 1010
const ZSTD_c_experimentalParam14 = 1011
const ZSTD_c_experimentalParam15 = 1012
const ZSTD_c_experimentalParam16 = 1013
const ZSTD_c_experimentalParam17 = 1014
const ZSTD_c_experimentalParam18 = 1015
const ZSTD_c_experimentalParam19 = 1016
const ZSTD_c_experimentalParam20 = 1017

type ZSTD_bounds = struct {
	Ferror1     size_t
	FlowerBound int32
	FupperBound int32
}

type ZSTD_ResetDirective = int32

const ZSTD_reset_session_only = 1
const ZSTD_reset_parameters = 2
const ZSTD_reset_session_and_parameters = 3

/***********************************************
*  Advanced decompression API (Requires v1.4.0+)
************************************************/

/* The advanced API pushes parameters one by one into an existing DCtx context.
 * Parameters are sticky, and remain valid for all following frames
 * using the same DCtx context.
 * It's possible to reset parameters to default values using ZSTD_DCtx_reset().
 * Note : This API is compatible with existing ZSTD_decompressDCtx() and ZSTD_decompressStream().
 *        Therefore, no new decompression function is necessary.
 */

type ZSTD_dParameter = int32

const ZSTD_d_windowLogMax = 100
const /* Select a size limit (in power of 2) beyond which
 * the streaming API will refuse to allocate memory buffer
 * in order to protect the host from unreasonable memory requirements.
 * This parameter is only useful in streaming mode, since no internal buffer is allocated in single-pass mode.
 * By default, a decompression context accepts window sizes <= (1 << ZSTD_WINDOWLOG_LIMIT_DEFAULT).
 * Special: value 0 means "use default maximum windowLog". */

/* note : additional experimental parameters are also available
 * within the experimental section of the API.
 * At the time of this writing, they include :
 * ZSTD_d_format
 * ZSTD_d_stableOutBuffer
 * ZSTD_d_forceIgnoreChecksum
 * ZSTD_d_refMultipleDDicts
 * ZSTD_d_disableHuffmanAssembly
 * ZSTD_d_maxBlockSize
 * Because they are not stable, it's necessary to define ZSTD_STATIC_LINKING_ONLY to access them.
 * note : never ever use experimentalParam? names directly
 */
ZSTD_d_experimentalParam1 = 1000
const ZSTD_d_experimentalParam2 = 1001
const ZSTD_d_experimentalParam3 = 1002
const ZSTD_d_experimentalParam4 = 1003
const ZSTD_d_experimentalParam5 = 1004
const ZSTD_d_experimentalParam6 = 1005

/****************************
*  Streaming
****************************/

type ZSTD_inBuffer = struct {
	Fsrc  uintptr
	Fsize size_t
	Fpos  size_t
}

/****************************
*  Streaming
****************************/

type ZSTD_inBuffer_s = ZSTD_inBuffer

type ZSTD_outBuffer = struct {
	Fdst  uintptr
	Fsize size_t
	Fpos  size_t
}

type ZSTD_outBuffer_s = ZSTD_outBuffer

/*-***********************************************************************
*  Streaming compression - HowTo
*
*  A ZSTD_CStream object is required to track streaming operation.
*  Use ZSTD_createCStream() and ZSTD_freeCStream() to create/release resources.
*  ZSTD_CStream objects can be reused multiple times on consecutive compression operations.
*  It is recommended to reuse ZSTD_CStream since it will play nicer with system's memory, by re-using already allocated memory.
*
*  For parallel execution, use one separate ZSTD_CStream per thread.
*
*  note : since v1.3.0, ZSTD_CStream and ZSTD_CCtx are the same thing.
*
*  Parameters are sticky : when starting a new compression on the same context,
*  it will reuse the same sticky parameters as previous compression session.
*  When in doubt, it's recommended to fully initialize the context before usage.
*  Use ZSTD_CCtx_reset() to reset the context and ZSTD_CCtx_setParameter(),
*  ZSTD_CCtx_setPledgedSrcSize(), or ZSTD_CCtx_loadDictionary() and friends to
*  set more specific parameters, the pledged source size, or load a dictionary.
*
*  Use ZSTD_compressStream2() with ZSTD_e_continue as many times as necessary to
*  consume input stream. The function will automatically update both `pos`
*  fields within `input` and `output`.
*  Note that the function may not consume the entire input, for example, because
*  the output buffer is already full, in which case `input.pos < input.size`.
*  The caller must check if input has been entirely consumed.
*  If not, the caller must make some room to receive more compressed data,
*  and then present again remaining input data.
*  note: ZSTD_e_continue is guaranteed to make some forward progress when called,
*        but doesn't guarantee maximal forward progress. This is especially relevant
*        when compressing with multiple threads. The call won't block if it can
*        consume some input, but if it can't it will wait for some, but not all,
*        output to be flushed.
* @return : provides a minimum amount of data remaining to be flushed from internal buffers
*           or an error code, which can be tested using ZSTD_isError().
*
*  At any moment, it's possible to flush whatever data might remain stuck within internal buffer,
*  using ZSTD_compressStream2() with ZSTD_e_flush. `output->pos` will be updated.
*  Note that, if `output->size` is too small, a single invocation with ZSTD_e_flush might not be enough (return code > 0).
*  In which case, make some room to receive more compressed data, and call again ZSTD_compressStream2() with ZSTD_e_flush.
*  You must continue calling ZSTD_compressStream2() with ZSTD_e_flush until it returns 0, at which point you can change the
*  operation.
*  note: ZSTD_e_flush will flush as much output as possible, meaning when compressing with multiple threads, it will
*        block until the flush is complete or the output buffer is full.
*  @return : 0 if internal buffers are entirely flushed,
*            >0 if some data still present within internal buffer (the value is minimal estimation of remaining size),
*            or an error code, which can be tested using ZSTD_isError().
*
*  Calling ZSTD_compressStream2() with ZSTD_e_end instructs to finish a frame.
*  It will perform a flush and write frame epilogue.
*  The epilogue is required for decoders to consider a frame completed.
*  flush operation is the same, and follows same rules as calling ZSTD_compressStream2() with ZSTD_e_flush.
*  You must continue calling ZSTD_compressStream2() with ZSTD_e_end until it returns 0, at which point you are free to
*  start a new frame.
*  note: ZSTD_e_end will flush as much output as possible, meaning when compressing with multiple threads, it will
*        block until the flush is complete or the output buffer is full.
*  @return : 0 if frame fully completed and fully flushed,
*            >0 if some data still present within internal buffer (the value is minimal estimation of remaining size),
*            or an error code, which can be tested using ZSTD_isError().
*
* *******************************************************************/

type ZSTD_CStream = struct {
	Fstage                 ZSTD_compressionStage_e
	FcParamsChanged        int32
	Fbmi2                  int32
	FrequestedParams       ZSTD_CCtx_params
	FappliedParams         ZSTD_CCtx_params
	FsimpleApiParams       ZSTD_CCtx_params
	FdictID                U32
	FdictContentSize       size_t
	Fworkspace             ZSTD_cwksp
	FblockSizeMax          size_t
	FpledgedSrcSizePlusOne uint64
	FconsumedSrcSize       uint64
	FproducedCSize         uint64
	FxxhState              XXH_NAMESPACEXXH64_state_t
	FcustomMem             ZSTD_customMem
	Fpool                  uintptr
	FstaticSize            size_t
	FseqCollector          SeqCollector
	FisFirstBlock          int32
	Finitialized           int32
	FseqStore              SeqStore_t
	FldmState              ldmState_t
	FldmSequences          uintptr
	FmaxNbLdmSequences     size_t
	FexternSeqStore        RawSeqStore_t
	FblockState            ZSTD_blockState_t
	FtmpWorkspace          uintptr
	FtmpWkspSize           size_t
	FbufferedPolicy        ZSTD_buffered_policy_e
	FinBuff                uintptr
	FinBuffSize            size_t
	FinToCompress          size_t
	FinBuffPos             size_t
	FinBuffTarget          size_t
	FoutBuff               uintptr
	FoutBuffSize           size_t
	FoutBuffContentSize    size_t
	FoutBuffFlushedSize    size_t
	FstreamStage           ZSTD_cStreamStage
	FframeEnded            U32
	FexpectedInBuffer      ZSTD_inBuffer
	FstableIn_notConsumed  size_t
	FexpectedOutBufferSize size_t
	FlocalDict             ZSTD_localDict
	Fcdict                 uintptr
	FprefixDict            ZSTD_prefixDict
	Fmtctx                 uintptr
	FblockSplitCtx         ZSTD_blockSplitCtx
	FextSeqBuf             uintptr
	FextSeqBufCapacity     size_t
}

/* accept NULL pointer */

// C documentation
//
//	/*===== Streaming compression functions =====*/
type ZSTD_EndDirective = int32

const ZSTD_e_continue = 0
const /* collect more data, encoder decides when to output compressed result, for optimal compression ratio */
ZSTD_e_flush = 1
const /* flush any data provided so far,
 * it creates (at least) one new block, that can be decoded immediately on reception;
 * frame will continue: any future data can still reference previously compressed data, improving compression.
 * note : multithreaded compression will block to flush as much output as possible. */
ZSTD_e_end = 2

/*-***************************************************************************
*  Streaming decompression - HowTo
*
*  A ZSTD_DStream object is required to track streaming operations.
*  Use ZSTD_createDStream() and ZSTD_freeDStream() to create/release resources.
*  ZSTD_DStream objects can be re-employed multiple times.
*
*  Use ZSTD_initDStream() to start a new decompression operation.
* @return : recommended first input size
*  Alternatively, use advanced API to set specific properties.
*
*  Use ZSTD_decompressStream() repetitively to consume your input.
*  The function will update both `pos` fields.
*  If `input.pos < input.size`, some input has not been consumed.
*  It's up to the caller to present again remaining data.
*
*  The function tries to flush all data decoded immediately, respecting output buffer size.
*  If `output.pos < output.size`, decoder has flushed everything it could.
*
*  However, when `output.pos == output.size`, it's more difficult to know.
*  If @return > 0, the frame is not complete, meaning
*  either there is still some data left to flush within internal buffers,
*  or there is more input to read to complete the frame (or both).
*  In which case, call ZSTD_decompressStream() again to flush whatever remains in the buffer.
*  Note : with no additional input provided, amount of data flushed is necessarily <= ZSTD_BLOCKSIZE_MAX.
* @return : 0 when a frame is completely decoded and fully flushed,
*        or an error code, which can be tested using ZSTD_isError(),
*        or any other value > 0, which means there is still some decoding or flushing to do to complete current frame :
*                                the return value is a suggested next input size (just a hint for better latency)
*                                that will never request more than the remaining content of the compressed frame.
* *******************************************************************************/

type ZSTD_DStream = struct {
	FLLTptr               uintptr
	FMLTptr               uintptr
	FOFTptr               uintptr
	FHUFptr               uintptr
	Fentropy              ZSTD_entropyDTables_t
	Fworkspace            [640]U32
	FpreviousDstEnd       uintptr
	FprefixStart          uintptr
	FvirtualStart         uintptr
	FdictEnd              uintptr
	Fexpected             size_t
	FfParams              ZSTD_FrameHeader
	FprocessedCSize       U64
	FdecodedSize          U64
	FbType                blockType_e
	Fstage                ZSTD_dStage
	FlitEntropy           U32
	FfseEntropy           U32
	FxxhState             XXH_NAMESPACEXXH64_state_t
	FheaderSize           size_t
	Fformat               ZSTD_format_e
	FforceIgnoreChecksum  ZSTD_forceIgnoreChecksum_e
	FvalidateChecksum     U32
	FlitPtr               uintptr
	FcustomMem            ZSTD_customMem
	FlitSize              size_t
	FrleSize              size_t
	FstaticSize           size_t
	FisFrameDecompression int32
	Fbmi2                 int32
	FddictLocal           uintptr
	Fddict                uintptr
	FdictID               U32
	FddictIsCold          int32
	FdictUses             ZSTD_dictUses_e
	FddictSet             uintptr
	FrefMultipleDDicts    ZSTD_refMultipleDDicts_e
	FdisableHufAsm        int32
	FmaxBlockSizeParam    int32
	FstreamStage          ZSTD_dStreamStage
	FinBuff               uintptr
	FinBuffSize           size_t
	FinPos                size_t
	FmaxWindowSize        size_t
	FoutBuff              uintptr
	FoutBuffSize          size_t
	FoutStart             size_t
	FoutEnd               size_t
	FlhSize               size_t
	FhostageByte          U32
	FnoForwardProgress    int32
	FoutBufferMode        ZSTD_bufferMode_e
	FexpectedOutBuffer    ZSTD_outBuffer
	FlitBuffer            uintptr
	FlitBufferEnd         uintptr
	FlitBufferLocation    ZSTD_litLocation_e
	FlitExtraBuffer       [65568]BYTE
	FheaderBuffer         [18]BYTE
	FoversizedDuration    size_t
}

// C documentation
//
//	/***********************************
//	 *  Bulk processing dictionary API
//	 **********************************/
type ZSTD_CDict = struct {
	FdictContent       uintptr
	FdictContentSize   size_t
	FdictContentType   ZSTD_dictContentType_e
	FentropyWorkspace  uintptr
	Fworkspace         ZSTD_cwksp
	FmatchState        ZSTD_MatchState_t
	FcBlockState       ZSTD_compressedBlockState_t
	FcustomMem         ZSTD_customMem
	FdictID            U32
	FcompressionLevel  int32
	FuseRowMatchFinder ZSTD_ParamSwitch_e
}

// C documentation
//
//	/***********************************
//	 *  Bulk processing dictionary API
//	 **********************************/
type ZSTD_CDict_s = ZSTD_CDict

const ZSTD_dct_auto = 0
const /* dictionary is "full" when starting with ZSTD_MAGIC_DICTIONARY, otherwise it is "rawContent" */
ZSTD_dct_rawContent = 1
const /* ensures dictionary is always loaded as rawContent, even if it starts with ZSTD_MAGIC_DICTIONARY */
ZSTD_dct_fullDict = 2
const
/* Note: This enum controls features which are conditionally beneficial.
 * Zstd can take a decision on whether or not to enable the feature (ZSTD_ps_auto),
 * but setting the switch to ZSTD_ps_enable or ZSTD_ps_disable force enable/disable the feature.
 */
ZSTD_ps_auto = 0
const /* Let the library automatically determine whether the feature shall be enabled */
ZSTD_ps_enable = 1
const /* Force-enable the feature */
ZSTD_ps_disable = 2

type ZSTD_DDict = struct {
	FdictBuffer     uintptr
	FdictContent    uintptr
	FdictSize       size_t
	Fentropy        ZSTD_entropyDTables_t
	FdictID         U32
	FentropyPresent U32
	FcMem           ZSTD_customMem
}

type ZSTD_DDict_s = ZSTD_DDict

/* **************************************************************************************
 *   ADVANCED AND EXPERIMENTAL FUNCTIONS
 ****************************************************************************************
 * The definitions in the following section are considered experimental.
 * They are provided for advanced scenarios.
 * They should never be used with a dynamic library, as prototypes may change in the future.
 * Use them only in association with static linking.
 * ***************************************************************************************/

/* This can be overridden externally to hide static symbols. */

/****************************************************************************************
 *   experimental API (static linking only)
 ****************************************************************************************
 * The following symbols and constants
 * are not planned to join "stable API" status in the near future.
 * They can still change in future versions.
 * Some of them are planned to remain in the static_only section indefinitely.
 * Some of them might be removed in the future (especially when redundant with existing stable functions)
 * ***************************************************************************************/

/* compression parameter bounds */

/* LDM parameter bounds */

/* Advanced parameter bounds */

/* ---  Advanced types  --- */

type ZSTD_CCtx_params = struct {
	Fformat                    ZSTD_format_e
	FcParams                   ZSTD_compressionParameters
	FfParams                   ZSTD_frameParameters
	FcompressionLevel          int32
	FforceWindow               int32
	FtargetCBlockSize          size_t
	FsrcSizeHint               int32
	FattachDictPref            ZSTD_dictAttachPref_e
	FliteralCompressionMode    ZSTD_ParamSwitch_e
	FnbWorkers                 int32
	FjobSize                   size_t
	FoverlapLog                int32
	Frsyncable                 int32
	FldmParams                 ldmParams_t
	FenableDedicatedDictSearch int32
	FinBufferMode              ZSTD_bufferMode_e
	FoutBufferMode             ZSTD_bufferMode_e
	FblockDelimiters           ZSTD_SequenceFormat_e
	FvalidateSequences         int32
	FpostBlockSplitter         ZSTD_ParamSwitch_e
	FpreBlockSplitter_level    int32
	FmaxBlockSize              size_t
	FuseRowMatchFinder         ZSTD_ParamSwitch_e
	FdeterministicRefPrefix    int32
	FcustomMem                 ZSTD_customMem
	FprefetchCDictTables       ZSTD_ParamSwitch_e
	FenableMatchFinderFallback int32
	FextSeqProdState           uintptr
	FextSeqProdFunc            ZSTD_sequenceProducer_F
	FsearchForExternalRepcodes ZSTD_ParamSwitch_e
}

/* **************************************************************************************
 *   ADVANCED AND EXPERIMENTAL FUNCTIONS
 ****************************************************************************************
 * The definitions in the following section are considered experimental.
 * They are provided for advanced scenarios.
 * They should never be used with a dynamic library, as prototypes may change in the future.
 * Use them only in association with static linking.
 * ***************************************************************************************/

/* This can be overridden externally to hide static symbols. */

/****************************************************************************************
 *   experimental API (static linking only)
 ****************************************************************************************
 * The following symbols and constants
 * are not planned to join "stable API" status in the near future.
 * They can still change in future versions.
 * Some of them are planned to remain in the static_only section indefinitely.
 * Some of them might be removed in the future (especially when redundant with existing stable functions)
 * ***************************************************************************************/

/* compression parameter bounds */

/* LDM parameter bounds */

/* Advanced parameter bounds */

/* ---  Advanced types  --- */

type ZSTD_CCtx_params_s = ZSTD_CCtx_params

const
/* Note: this enum and the behavior it controls are effectively internal
 * implementation details of the compressor. They are expected to continue
 * to evolve and should be considered only in the context of extremely
 * advanced performance tuning.
 *
 * Zstd currently supports the use of a CDict in three ways:
 *
 * - The contents of the CDict can be copied into the working context. This
 *   means that the compression can search both the dictionary and input
 *   while operating on a single set of internal tables. This makes
 *   the compression faster per-byte of input. However, the initial copy of
 *   the CDict's tables incurs a fixed cost at the beginning of the
 *   compression. For small compressions (< 8 KB), that copy can dominate
 *   the cost of the compression.
 *
 * - The CDict's tables can be used in-place. In this model, compression is
 *   slower per input byte, because the compressor has to search two sets of
 *   tables. However, this model incurs no start-up cost (as long as the
 *   working context's tables can be reused). For small inputs, this can be
 *   faster than copying the CDict's tables.
 *
 * - The CDict's tables are not used at all, and instead we use the working
 *   context alone to reload the dictionary and use params based on the source
 *   size. See ZSTD_compress_insertDictionary() and ZSTD_compress_usingDict().
 *   This method is effective when the dictionary sizes are very small relative
 *   to the input size, and the input size is fairly large to begin with.
 *
 * Zstd has a simple internal heuristic that selects which strategy to use
 * at the beginning of a compression. However, if experimentation shows that
 * Zstd is making poor choices, it is possible to override that choice with
 * this enum.
 */
ZSTD_dictDefaultAttach = 0
const /* Use the default heuristic. */
ZSTD_dictForceAttach = 1
const /* Never copy the dictionary. */
ZSTD_dictForceCopy = 2
const /* Always copy the dictionary. */
ZSTD_dictForceLoad = 3
const ZSTD_sf_noBlockDelimiters = 0
const /* ZSTD_Sequence[] has no block delimiters, just sequences */
ZSTD_sf_explicitBlockDelimiters = 1

type ZSTD_Sequence = struct {
	Foffset      uint32
	FlitLength   uint32
	FmatchLength uint32
	Frep         uint32
}

type ZSTD_compressionParameters = struct {
	FwindowLog    uint32
	FchainLog     uint32
	FhashLog      uint32
	FsearchLog    uint32
	FminMatch     uint32
	FtargetLength uint32
	Fstrategy     ZSTD_strategy
}

type ZSTD_frameParameters = struct {
	FcontentSizeFlag int32
	FchecksumFlag    int32
	FnoDictIDFlag    int32
}

type ZSTD_parameters = struct {
	FcParams ZSTD_compressionParameters
	FfParams ZSTD_frameParameters
}

type ZSTD_dictContentType_e = int32

type ZSTD_dictLoadMethod_e = int32

const ZSTD_dlm_byCopy = 0
const /**< Copy dictionary content internally */
ZSTD_dlm_byRef = 1

type ZSTD_format_e = int32

type ZSTD_forceIgnoreChecksum_e = int32

type ZSTD_refMultipleDDicts_e = int32

type ZSTD_dictAttachPref_e = int32

type ZSTD_literalCompressionMode_e = int32

const ZSTD_lcm_auto = 0
const /**< Automatically determine the compression mode based on the compression level.
 *   Negative compression levels will be uncompressed, and positive compression
 *   levels will be compressed. */
ZSTD_lcm_huffman = 1
const /**< Always attempt Huffman compression. Uncompressed literals will still be
 *   emitted if Huffman compression is not profitable. */
ZSTD_lcm_uncompressed = 2

type ZSTD_ParamSwitch_e = int32

type ZSTD_FrameType_e = int32

const ZSTD_frame = 0
const ZSTD_skippableFrame = 1

type ZSTD_FrameHeader = struct {
	FframeContentSize uint64
	FwindowSize       uint64
	FblockSizeMax     uint32
	FframeType        ZSTD_FrameType_e
	FheaderSize       uint32
	FdictID           uint32
	FchecksumFlag     uint32
	F_reserved1       uint32
	F_reserved2       uint32
}

/*! ZSTD_DECOMPRESS_MARGIN() :
 * Similar to ZSTD_decompressionMargin(), but instead of computing the margin from
 * the compressed frame, compute it from the original size and the blockSizeLog.
 * See ZSTD_decompressionMargin() for details.
 *
 * WARNING: This macro does not support multi-frame input, the input must be a single
 * zstd frame. If you need that support use the function, or implement it yourself.
 *
 * @param originalSize The original uncompressed size of the data.
 * @param blockSize    The block size == MIN(windowSize, ZSTD_BLOCKSIZE_MAX).
 *                     Unless you explicitly set the windowLog smaller than
 *                     ZSTD_BLOCKSIZELOG_MAX you can just use ZSTD_BLOCKSIZE_MAX.
 */

type ZSTD_SequenceFormat_e = int32

// C documentation
//
//	/*! Custom memory allocation :
//	 *  These prototypes make it possible to pass your own allocation/free functions.
//	 *  ZSTD_customMem is provided at creation time, using ZSTD_create*_advanced() variants listed below.
//	 *  All allocation/free operations will be completed using these custom variants instead of regular <stdlib.h> ones.
//	 */
type ZSTD_allocFunction = uintptr

type ZSTD_freeFunction = uintptr

type ZSTD_customMem = struct {
	FcustomAlloc ZSTD_allocFunction
	FcustomFree  ZSTD_freeFunction
	Fopaque      uintptr
}

var ZSTD_defaultCMem = ZSTD_customMem{}

// C documentation
//
//	/*! Thread pool :
//	 *  These prototypes make it possible to share a thread pool among multiple compression contexts.
//	 *  This can limit resources for applications with multiple threads where each one uses
//	 *  a threaded compression mode (via ZSTD_c_nbWorkers parameter).
//	 *  ZSTD_createThreadPool creates a new thread pool with a given number of threads.
//	 *  Note that the lifetime of such pool must exist while being used.
//	 *  ZSTD_CCtx_refThreadPool assigns a thread pool to a context (use NULL argument value
//	 *  to use an internal thread pool).
//	 *  ZSTD_freeThreadPool frees a thread pool, accepts NULL pointer.
//	 */
type ZSTD_threadPool = struct {
	FcustomMem      ZSTD_customMem
	Fthreads        uintptr
	FthreadCapacity size_t
	FthreadLimit    size_t
	Fqueue          uintptr
	FqueueHead      size_t
	FqueueTail      size_t
	FqueueSize      size_t
	FnumThreadsBusy size_t
	FqueueEmpty     int32
	FqueueMutex     CRITICAL_SECTION
	FqueuePushCond  CONDITION_VARIABLE
	FqueuePopCond   CONDITION_VARIABLE
	Fshutdown       int32
}

// C documentation
//
//	/*! Thread pool :
//	 *  These prototypes make it possible to share a thread pool among multiple compression contexts.
//	 *  This can limit resources for applications with multiple threads where each one uses
//	 *  a threaded compression mode (via ZSTD_c_nbWorkers parameter).
//	 *  ZSTD_createThreadPool creates a new thread pool with a given number of threads.
//	 *  Note that the lifetime of such pool must exist while being used.
//	 *  ZSTD_CCtx_refThreadPool assigns a thread pool to a context (use NULL argument value
//	 *  to use an internal thread pool).
//	 *  ZSTD_freeThreadPool frees a thread pool, accepts NULL pointer.
//	 */
type POOL_ctx_s = ZSTD_threadPool

type ZSTD_frameProgression = struct {
	Fingested        uint64
	Fconsumed        uint64
	Fproduced        uint64
	Fflushed         uint64
	FcurrentJobID    uint32
	FnbActiveWorkers uint32
}

/* ********************* BLOCK-LEVEL SEQUENCE PRODUCER API *********************
 *
 * *** OVERVIEW ***
 * The Block-Level Sequence Producer API allows users to provide their own custom
 * sequence producer which libzstd invokes to process each block. The produced list
 * of sequences (literals and matches) is then post-processed by libzstd to produce
 * valid compressed blocks.
 *
 * This block-level offload API is a more granular complement of the existing
 * frame-level offload API compressSequences() (introduced in v1.5.1). It offers
 * an easier migration story for applications already integrated with libzstd: the
 * user application continues to invoke the same compression functions
 * ZSTD_compress2() or ZSTD_compressStream2() as usual, and transparently benefits
 * from the specific advantages of the external sequence producer. For example,
 * the sequence producer could be tuned to take advantage of known characteristics
 * of the input, to offer better speed / ratio, or could leverage hardware
 * acceleration not available within libzstd itself.
 *
 * See contrib/externalSequenceProducer for an example program employing the
 * Block-Level Sequence Producer API.
 *
 * *** USAGE ***
 * The user is responsible for implementing a function of type
 * ZSTD_sequenceProducer_F. For each block, zstd will pass the following
 * arguments to the user-provided function:
 *
 *   - sequenceProducerState: a pointer to a user-managed state for the sequence
 *     producer.
 *
 *   - outSeqs, outSeqsCapacity: an output buffer for the sequence producer.
 *     outSeqsCapacity is guaranteed >= ZSTD_sequenceBound(srcSize). The memory
 *     backing outSeqs is managed by the CCtx.
 *
 *   - src, srcSize: an input buffer for the sequence producer to parse.
 *     srcSize is guaranteed to be <= ZSTD_BLOCKSIZE_MAX.
 *
 *   - dict, dictSize: a history buffer, which may be empty, which the sequence
 *     producer may reference as it parses the src buffer. Currently, zstd will
 *     always pass dictSize == 0 into external sequence producers, but this will
 *     change in the future.
 *
 *   - compressionLevel: a signed integer representing the zstd compression level
 *     set by the user for the current operation. The sequence producer may choose
 *     to use this information to change its compression strategy and speed/ratio
 *     tradeoff. Note: the compression level does not reflect zstd parameters set
 *     through the advanced API.
 *
 *   - windowSize: a size_t representing the maximum allowed offset for external
 *     sequences. Note that sequence offsets are sometimes allowed to exceed the
 *     windowSize if a dictionary is present, see doc/zstd_compression_format.md
 *     for details.
 *
 * The user-provided function shall return a size_t representing the number of
 * sequences written to outSeqs. This return value will be treated as an error
 * code if it is greater than outSeqsCapacity. The return value must be non-zero
 * if srcSize is non-zero. The ZSTD_SEQUENCE_PRODUCER_ERROR macro is provided
 * for convenience, but any value greater than outSeqsCapacity will be treated as
 * an error code.
 *
 * If the user-provided function does not return an error code, the sequences
 * written to outSeqs must be a valid parse of the src buffer. Data corruption may
 * occur if the parse is not valid. A parse is defined to be valid if the
 * following conditions hold:
 *   - The sum of matchLengths and literalLengths must equal srcSize.
 *   - All sequences in the parse, except for the final sequence, must have
 *     matchLength >= ZSTD_MINMATCH_MIN. The final sequence must have
 *     matchLength >= ZSTD_MINMATCH_MIN or matchLength == 0.
 *   - All offsets must respect the windowSize parameter as specified in
 *     doc/zstd_compression_format.md.
 *   - If the final sequence has matchLength == 0, it must also have offset == 0.
 *
 * zstd will only validate these conditions (and fail compression if they do not
 * hold) if the ZSTD_c_validateSequences cParam is enabled. Note that sequence
 * validation has a performance cost.
 *
 * If the user-provided function returns an error, zstd will either fall back
 * to an internal sequence producer or fail the compression operation. The user can
 * choose between the two behaviors by setting the ZSTD_c_enableSeqProducerFallback
 * cParam. Fallback compression will follow any other cParam settings, such as
 * compression level, the same as in a normal compression operation.
 *
 * The user shall instruct zstd to use a particular ZSTD_sequenceProducer_F
 * function by calling
 *         ZSTD_registerSequenceProducer(cctx,
 *                                       sequenceProducerState,
 *                                       sequenceProducer)
 * This setting will persist until the next parameter reset of the CCtx.
 *
 * The sequenceProducerState must be initialized by the user before calling
 * ZSTD_registerSequenceProducer(). The user is responsible for destroying the
 * sequenceProducerState.
 *
 * *** LIMITATIONS ***
 * This API is compatible with all zstd compression APIs which respect advanced parameters.
 * However, there are three limitations:
 *
 * First, the ZSTD_c_enableLongDistanceMatching cParam is not currently supported.
 * COMPRESSION WILL FAIL if it is enabled and the user tries to compress with a block-level
 * external sequence producer.
 *   - Note that ZSTD_c_enableLongDistanceMatching is auto-enabled by default in some
 *     cases (see its documentation for details). Users must explicitly set
 *     ZSTD_c_enableLongDistanceMatching to ZSTD_ps_disable in such cases if an external
 *     sequence producer is registered.
 *   - As of this writing, ZSTD_c_enableLongDistanceMatching is disabled by default
 *     whenever ZSTD_c_windowLog < 128MB, but that's subject to change. Users should
 *     check the docs on ZSTD_c_enableLongDistanceMatching whenever the Block-Level Sequence
 *     Producer API is used in conjunction with advanced settings (like ZSTD_c_windowLog).
 *
 * Second, history buffers are not currently supported. Concretely, zstd will always pass
 * dictSize == 0 to the external sequence producer (for now). This has two implications:
 *   - Dictionaries are not currently supported. Compression will *not* fail if the user
 *     references a dictionary, but the dictionary won't have any effect.
 *   - Stream history is not currently supported. All advanced compression APIs, including
 *     streaming APIs, work with external sequence producers, but each block is treated as
 *     an independent chunk without history from previous blocks.
 *
 * Third, multi-threading within a single compression is not currently supported. In other words,
 * COMPRESSION WILL FAIL if ZSTD_c_nbWorkers > 0 and an external sequence producer is registered.
 * Multi-threading across compressions is fine: simply create one CCtx per thread.
 *
 * Long-term, we plan to overcome all three limitations. There is no technical blocker to
 * overcoming them. It is purely a question of engineering effort.
 */

type ZSTD_sequenceProducer_F = uintptr

type ZSTD_nextInputType_e = int32

const ZSTDnit_frameHeader = 0
const ZSTDnit_blockHeader = 1
const ZSTDnit_block = 2
const ZSTDnit_lastBlock = 3
const ZSTDnit_checksum = 4
const ZSTDnit_skippableFrame = 5 /**< insert uncompressed block into `dctx` history. Useful for multi-blocks decompression. */

/**** ended inlining ../zstd.h ****/

/* custom memory allocation functions */
func ZSTD_customMalloc(tls *libc.TLS, size size_t, customMem ZSTD_customMem) (r uintptr) {
	if customMem.FcustomAlloc != 0 {
		return (*(*func(*libc.TLS, uintptr, size_t) uintptr)(unsafe.Pointer(&struct{ uintptr }{customMem.FcustomAlloc})))(tls, customMem.Fopaque, size)
	}
	return libc.Xmalloc(tls, size)
}

func ZSTD_customCalloc(tls *libc.TLS, size size_t, customMem ZSTD_customMem) (r uintptr) {
	var ptr uintptr
	_ = ptr
	if customMem.FcustomAlloc != 0 {
		/* calloc implemented as malloc+memset;
		 * not as efficient as calloc, but next best guess for custom malloc */
		ptr = (*(*func(*libc.TLS, uintptr, size_t) uintptr)(unsafe.Pointer(&struct{ uintptr }{customMem.FcustomAlloc})))(tls, customMem.Fopaque, size)
		libc.Xmemset(tls, ptr, 0, size)
		return ptr
	}
	return libc.Xcalloc(tls, uint64(libc.Int32FromInt32(1)), size)
}

func ZSTD_customFree(tls *libc.TLS, ptr uintptr, customMem ZSTD_customMem) {
	if ptr != libc.UintptrFromInt32(0) {
		if customMem.FcustomFree != 0 {
			(*(*func(*libc.TLS, uintptr, uintptr))(unsafe.Pointer(&struct{ uintptr }{customMem.FcustomFree})))(tls, customMem.Fopaque, ptr)
		} else {
			libc.Xfree(tls, ptr)
		}
	}
}

/**** ended inlining ../common/allocations.h ****/
/**** skipping file: zstd_deps.h ****/
/**** skipping file: debug.h ****/
/**** start inlining pool.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/

type POOL_ctx = struct {
	FcustomMem      ZSTD_customMem
	Fthreads        uintptr
	FthreadCapacity size_t
	FthreadLimit    size_t
	Fqueue          uintptr
	FqueueHead      size_t
	FqueueTail      size_t
	FqueueSize      size_t
	FnumThreadsBusy size_t
	FqueueEmpty     int32
	FqueueMutex     CRITICAL_SECTION
	FqueuePushCond  CONDITION_VARIABLE
	FqueuePopCond   CONDITION_VARIABLE
	Fshutdown       int32
}

// C documentation
//
//	/*! POOL_function :
//	 *  The function type that can be added to a thread pool.
//	 */
type POOL_function = uintptr

/**** ended inlining pool.h ****/

/* ======   Compiler specifics   ====== */

/**** skipping file: threading.h ****/

// C documentation
//
//	/* A job is a function and an opaque argument */
type POOL_job = struct {
	Ffunction POOL_function
	Fopaque   uintptr
}

/**** ended inlining pool.h ****/

/* ======   Compiler specifics   ====== */

/**** skipping file: threading.h ****/

// C documentation
//
//	/* A job is a function and an opaque argument */
type POOL_job_s = POOL_job

// C documentation
//
//	/* POOL_thread() :
//	 * Work thread for the thread pool.
//	 * Waits for jobs and executes them.
//	 * @returns : NULL on failure else non-null.
//	 */
func POOL_thread(tls *libc.TLS, opaque uintptr) (r uintptr) {
	var ctx uintptr
	var job POOL_job
	_, _ = ctx, job
	ctx = opaque
	if !(ctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	for {
		/* Lock the mutex and wait for a non-empty queue or until shutdown */
		libc.XEnterCriticalSection(tls, ctx+96)
		for (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty != 0 || (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy >= (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit {
			if (*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown != 0 {
				/* even if !queueEmpty, (possible if numThreadsBusy >= threadLimit),
				 * a few threads will be shutdown while !queueEmpty,
				 * but enough threads will remain active to finish the queue */
				libc.XLeaveCriticalSection(tls, ctx+96)
				return opaque
			}
			SleepConditionVariableCS(tls, ctx+144, ctx+96, uint32(INFINITE))
		}
		/* Pop a job off the queue */
		job = *(*POOL_job)(unsafe.Pointer((*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue + uintptr((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead)*16))
		(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead = ((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead + uint64(1)) % (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize
		(*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy = (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy + 1
		(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty = libc.BoolInt32((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead == (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail)
		/* Unlock the mutex, signal a pusher, and run the job */
		WakeConditionVariable(tls, ctx+136)
		libc.XLeaveCriticalSection(tls, ctx+96)
		(*(*func(*libc.TLS, uintptr))(unsafe.Pointer(&struct{ uintptr }{job.Ffunction})))(tls, job.Fopaque)
		/* If the intended queue size was 0, signal after finishing job */
		libc.XEnterCriticalSection(tls, ctx+96)
		(*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy = (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy - 1
		WakeConditionVariable(tls, ctx+136)
		libc.XLeaveCriticalSection(tls, ctx+96)
		goto _1
	_1:
	} /* for (;;) */
	/* Unreachable */
	return r
}

// C documentation
//
//	/* ZSTD_createThreadPool() : public access point */
func ZSTD_createThreadPool(tls *libc.TLS, numThreads size_t) (r uintptr) {
	return POOL_create(tls, numThreads, uint64(0))
}

func POOL_create(tls *libc.TLS, numThreads size_t, queueSize size_t) (r uintptr) {
	return POOL_create_advanced(tls, numThreads, queueSize, ZSTD_defaultCMem)
}

func POOL_create_advanced(tls *libc.TLS, numThreads size_t, queueSize size_t, customMem ZSTD_customMem) (r uintptr) {
	var ctx uintptr
	var error1 int32
	var i size_t
	_, _, _ = ctx, error1, i
	/* Check parameters */
	if !(numThreads != 0) {
		return libc.UintptrFromInt32(0)
	}
	/* Allocate the context and zero initialize */
	ctx = ZSTD_customCalloc(tls, uint64(160), customMem)
	if !(ctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	/* Initialize the job queue.
	 * It needs one extra space since one space is wasted to differentiate
	 * empty and full queues.
	 */
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize = queueSize + uint64(1)
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue = ZSTD_customCalloc(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize*uint64(16), customMem)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty = int32(1)
	error1 = 0
	_ = libc.UintptrFromInt32(0)
	libc.XInitializeCriticalSection(tls, ctx+96)
	error1 = error1 | libc.Int32FromInt32(0)
	_ = libc.UintptrFromInt32(0)
	InitializeConditionVariable(tls, ctx+136)
	error1 = error1 | libc.Int32FromInt32(0)
	_ = libc.UintptrFromInt32(0)
	InitializeConditionVariable(tls, ctx+144)
	error1 = error1 | libc.Int32FromInt32(0)
	if error1 != 0 {
		POOL_free(tls, ctx)
		return libc.UintptrFromInt32(0)
	}
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown = 0
	/* Allocate space for the thread handles */
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads = ZSTD_customCalloc(tls, numThreads*uint64(8), customMem)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem = customMem
	/* Check for errors */
	if !((*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads != 0) || !((*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue != 0) {
		POOL_free(tls, ctx)
		return libc.UintptrFromInt32(0)
	}
	/* Initialize the threads */
	i = uint64(0)
	for {
		if !(i < numThreads) {
			break
		}
		if ZSTD_pthread_create(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads+uintptr(i)*8, libc.UintptrFromInt32(0), __ccgo_fp(POOL_thread), ctx) != 0 {
			(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = i
			POOL_free(tls, ctx)
			return libc.UintptrFromInt32(0)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = numThreads
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit = numThreads
	return ctx
}

// C documentation
//
//	/*! POOL_join() :
//	    Shutdown the queue, wake any sleeping threads, and join all of the threads.
//	*/
func POOL_join(tls *libc.TLS, ctx uintptr) {
	var i size_t
	_ = i
	/* Shut down the queue */
	libc.XEnterCriticalSection(tls, ctx+96)
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown = int32(1)
	libc.XLeaveCriticalSection(tls, ctx+96)
	/* Wake up sleeping threads */
	WakeAllConditionVariable(tls, ctx+136)
	WakeAllConditionVariable(tls, ctx+144)
	/* Join all of the threads */
	i = uint64(0)
	for {
		if !(i < (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity) {
			break
		}
		ZSTD_pthread_join(tls, *(*ZSTD_pthread_t)(unsafe.Pointer((*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads + uintptr(i)*8))) /* note : could fail */
		goto _1
	_1:
		;
		i = i + 1
	}
}

func POOL_free(tls *libc.TLS, ctx uintptr) {
	if !(ctx != 0) {
		return
	}
	POOL_join(tls, ctx)
	libc.XDeleteCriticalSection(tls, ctx+96)
	_ = ctx + 136
	_ = ctx + 144
	ZSTD_customFree(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	ZSTD_customFree(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	ZSTD_customFree(tls, ctx, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
}

// C documentation
//
//	/*! POOL_joinJobs() :
//	 *  Waits for all queued jobs to finish executing.
//	 */
func POOL_joinJobs(tls *libc.TLS, ctx uintptr) {
	libc.XEnterCriticalSection(tls, ctx+96)
	for !((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty != 0) || (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy > uint64(0) {
		SleepConditionVariableCS(tls, ctx+136, ctx+96, uint32(INFINITE))
	}
	libc.XLeaveCriticalSection(tls, ctx+96)
}

func ZSTD_freeThreadPool(tls *libc.TLS, pool uintptr) {
	POOL_free(tls, pool)
}

func POOL_sizeof(tls *libc.TLS, ctx uintptr) (r size_t) {
	if ctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* supports sizeof NULL */
	return uint64(160) + (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize*uint64(16) + (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity*uint64(8)
}

// C documentation
//
//	/* @return : 0 on success, 1 on error */
func POOL_resize_internal(tls *libc.TLS, ctx uintptr, numThreads size_t) (r int32) {
	var threadId size_t
	var threadPool uintptr
	_, _ = threadId, threadPool
	if numThreads <= (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity {
		if !(numThreads != 0) {
			return int32(1)
		}
		(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit = numThreads
		return 0
	}
	/* numThreads > threadCapacity */
	threadPool = ZSTD_customCalloc(tls, numThreads*uint64(8), (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	if !(threadPool != 0) {
		return int32(1)
	}
	/* replace existing thread pool */
	libc.Xmemcpy(tls, threadPool, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads, (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity*libc.Uint64FromInt64(8))
	ZSTD_customFree(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads = threadPool
	/* Initialize additional threads */
	threadId = (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity
	for {
		if !(threadId < numThreads) {
			break
		}
		if ZSTD_pthread_create(tls, threadPool+uintptr(threadId)*8, libc.UintptrFromInt32(0), __ccgo_fp(POOL_thread), ctx) != 0 {
			(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = threadId
			return int32(1)
		}
		goto _1
	_1:
		;
		threadId = threadId + 1
	}
	/* successfully expanded */
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = numThreads
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit = numThreads
	return 0
}

// C documentation
//
//	/* @return : 0 on success, 1 on error */
func POOL_resize(tls *libc.TLS, ctx uintptr, numThreads size_t) (r int32) {
	var result int32
	_ = result
	if ctx == libc.UintptrFromInt32(0) {
		return int32(1)
	}
	libc.XEnterCriticalSection(tls, ctx+96)
	result = POOL_resize_internal(tls, ctx, numThreads)
	WakeAllConditionVariable(tls, ctx+144)
	libc.XLeaveCriticalSection(tls, ctx+96)
	return result
}

// C documentation
//
//	/**
//	 * Returns 1 if the queue is full and 0 otherwise.
//	 *
//	 * When queueSize is 1 (pool was created with an intended queueSize of 0),
//	 * then a queue is empty if there is a thread free _and_ no job is waiting.
//	 */
func isQueueFull(tls *libc.TLS, ctx uintptr) (r int32) {
	if (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize > uint64(1) {
		return libc.BoolInt32((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead == ((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail+uint64(1))%(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize)
	} else {
		return libc.BoolInt32((*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy == (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit || !((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty != 0))
	}
	return r
}

func POOL_add_internal(tls *libc.TLS, ctx uintptr, __ccgo_fp_function POOL_function, opaque uintptr) {
	var job POOL_job
	_ = job
	job.Ffunction = __ccgo_fp_function
	job.Fopaque = opaque
	if (*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown != 0 {
		return
	}
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty = 0
	*(*POOL_job)(unsafe.Pointer((*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue + uintptr((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail)*16)) = job
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail = ((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail + uint64(1)) % (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize
	WakeConditionVariable(tls, ctx+144)
}

type __ccgo_fp__XPOOL_add_1 = func(*libc.TLS, uintptr)

func POOL_add(tls *libc.TLS, ctx uintptr, __ccgo_fp_function POOL_function, opaque uintptr) {
	libc.XEnterCriticalSection(tls, ctx+96)
	/* Wait until there is space in the queue for the new job */
	for isQueueFull(tls, ctx) != 0 && !((*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown != 0) {
		SleepConditionVariableCS(tls, ctx+136, ctx+96, uint32(INFINITE))
	}
	POOL_add_internal(tls, ctx, __ccgo_fp_function, opaque)
	libc.XLeaveCriticalSection(tls, ctx+96)
}

type __ccgo_fp__XPOOL_tryAdd_1 = func(*libc.TLS, uintptr)

func POOL_tryAdd(tls *libc.TLS, ctx uintptr, __ccgo_fp_function POOL_function, opaque uintptr) (r int32) {
	libc.XEnterCriticalSection(tls, ctx+96)
	if isQueueFull(tls, ctx) != 0 {
		libc.XLeaveCriticalSection(tls, ctx+96)
		return 0
	}
	POOL_add_internal(tls, ctx, __ccgo_fp_function, opaque)
	libc.XLeaveCriticalSection(tls, ctx+96)
	return int32(1)
}

/**** ended inlining common/pool.c ****/
/**** start inlining common/zstd_common.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: error_private.h ****/
/**** start inlining zstd_internal.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* this module contains definitions which must be identical
 * across compression, decompression and dictBuilder.
 * It also contains a few functions useful to at least 2 of them
 * and which benefit from being inlined */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: compiler.h ****/
/**** start inlining cpu.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**
 * Implementation taken from folly/CpuId.h
 * https://github.com/facebook/folly/blob/master/folly/CpuId.h
 */

/**** skipping file: mem.h ****/

type ZSTD_cpuid_t = struct {
	Ff1c U32
	Ff1d U32
	Ff7b U32
	Ff7c U32
}

func ZSTD_cpuid(tls *libc.TLS) (r ZSTD_cpuid_t) {
	var cpuid ZSTD_cpuid_t
	var f1c, f1d, f7b, f7c U32
	_, _, _, _, _ = cpuid, f1c, f1d, f7b, f7c
	f1c = uint32(0)
	f1d = uint32(0)
	f7b = uint32(0)
	f7c = uint32(0)
	cpuid.Ff1c = f1c
	cpuid.Ff1d = f1d
	cpuid.Ff7b = f7b
	cpuid.Ff7c = f7c
	return cpuid
	return r
}

// C documentation
//
//	/* cpuid(1): Processor Info and Feature Bits. */
func ZSTD_cpuid_sse3(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(0)) != uint32(0))
}

func ZSTD_cpuid_pclmuldq(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(1)) != uint32(0))
}

func ZSTD_cpuid_dtes64(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(2)) != uint32(0))
}

func ZSTD_cpuid_monitor(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(3)) != uint32(0))
}

func ZSTD_cpuid_dscpl(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(4)) != uint32(0))
}

func ZSTD_cpuid_vmx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(5)) != uint32(0))
}

func ZSTD_cpuid_smx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(6)) != uint32(0))
}

func ZSTD_cpuid_eist(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(7)) != uint32(0))
}

func ZSTD_cpuid_tm2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(8)) != uint32(0))
}

func ZSTD_cpuid_ssse3(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(9)) != uint32(0))
}

func ZSTD_cpuid_cnxtid(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(10)) != uint32(0))
}

func ZSTD_cpuid_fma(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(12)) != uint32(0))
}

func ZSTD_cpuid_cx16(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(13)) != uint32(0))
}

func ZSTD_cpuid_xtpr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(14)) != uint32(0))
}

func ZSTD_cpuid_pdcm(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(15)) != uint32(0))
}

func ZSTD_cpuid_pcid(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(17)) != uint32(0))
}

func ZSTD_cpuid_dca(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(18)) != uint32(0))
}

func ZSTD_cpuid_sse41(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) != uint32(0))
}

func ZSTD_cpuid_sse42(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(20)) != uint32(0))
}

func ZSTD_cpuid_x2apic(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(21)) != uint32(0))
}

func ZSTD_cpuid_movbe(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(22)) != uint32(0))
}

func ZSTD_cpuid_popcnt(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(23)) != uint32(0))
}

func ZSTD_cpuid_tscdeadline(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) != uint32(0))
}

func ZSTD_cpuid_aes(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(25)) != uint32(0))
}

func ZSTD_cpuid_xsave(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(26)) != uint32(0))
}

func ZSTD_cpuid_osxsave(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(27)) != uint32(0))
}

func ZSTD_cpuid_avx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(28)) != uint32(0))
}

func ZSTD_cpuid_f16c(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(29)) != uint32(0))
}

func ZSTD_cpuid_rdrand(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(30)) != uint32(0))
}

func ZSTD_cpuid_fpu(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(0)) != uint32(0))
}

func ZSTD_cpuid_vme(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(1)) != uint32(0))
}

func ZSTD_cpuid_de(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(2)) != uint32(0))
}

func ZSTD_cpuid_pse(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(3)) != uint32(0))
}

func ZSTD_cpuid_tsc(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(4)) != uint32(0))
}

func ZSTD_cpuid_msr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(5)) != uint32(0))
}

func ZSTD_cpuid_pae(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(6)) != uint32(0))
}

func ZSTD_cpuid_mce(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(7)) != uint32(0))
}

func ZSTD_cpuid_cx8(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(8)) != uint32(0))
}

func ZSTD_cpuid_apic(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(9)) != uint32(0))
}

func ZSTD_cpuid_sep(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(11)) != uint32(0))
}

func ZSTD_cpuid_mtrr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(12)) != uint32(0))
}

func ZSTD_cpuid_pge(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(13)) != uint32(0))
}

func ZSTD_cpuid_mca(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(14)) != uint32(0))
}

func ZSTD_cpuid_cmov(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(15)) != uint32(0))
}

func ZSTD_cpuid_pat(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(16)) != uint32(0))
}

func ZSTD_cpuid_pse36(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(17)) != uint32(0))
}

func ZSTD_cpuid_psn(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(18)) != uint32(0))
}

func ZSTD_cpuid_clfsh(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) != uint32(0))
}

func ZSTD_cpuid_ds(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(21)) != uint32(0))
}

func ZSTD_cpuid_acpi(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(22)) != uint32(0))
}

func ZSTD_cpuid_mmx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(23)) != uint32(0))
}

func ZSTD_cpuid_fxsr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) != uint32(0))
}

func ZSTD_cpuid_sse(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(25)) != uint32(0))
}

func ZSTD_cpuid_sse2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(26)) != uint32(0))
}

func ZSTD_cpuid_ss(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(27)) != uint32(0))
}

func ZSTD_cpuid_htt(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(28)) != uint32(0))
}

func ZSTD_cpuid_tm(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(29)) != uint32(0))
}

func ZSTD_cpuid_pbe(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)) != uint32(0))
}

// C documentation
//
//	/* cpuid(7): Extended Features. */
func ZSTD_cpuid_bmi1(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(3)) != uint32(0))
}

func ZSTD_cpuid_hle(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(4)) != uint32(0))
}

func ZSTD_cpuid_avx2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(5)) != uint32(0))
}

func ZSTD_cpuid_smep(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(7)) != uint32(0))
}

func ZSTD_cpuid_bmi2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(8)) != uint32(0))
}

func ZSTD_cpuid_erms(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(9)) != uint32(0))
}

func ZSTD_cpuid_invpcid(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(10)) != uint32(0))
}

func ZSTD_cpuid_rtm(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(11)) != uint32(0))
}

func ZSTD_cpuid_mpx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(14)) != uint32(0))
}

func ZSTD_cpuid_avx512f(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(16)) != uint32(0))
}

func ZSTD_cpuid_avx512dq(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(17)) != uint32(0))
}

func ZSTD_cpuid_rdseed(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(18)) != uint32(0))
}

func ZSTD_cpuid_adx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) != uint32(0))
}

func ZSTD_cpuid_smap(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(20)) != uint32(0))
}

func ZSTD_cpuid_avx512ifma(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(21)) != uint32(0))
}

func ZSTD_cpuid_pcommit(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(22)) != uint32(0))
}

func ZSTD_cpuid_clflushopt(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(23)) != uint32(0))
}

func ZSTD_cpuid_clwb(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) != uint32(0))
}

func ZSTD_cpuid_avx512pf(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(26)) != uint32(0))
}

func ZSTD_cpuid_avx512er(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(27)) != uint32(0))
}

func ZSTD_cpuid_avx512cd(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(28)) != uint32(0))
}

func ZSTD_cpuid_sha(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(29)) != uint32(0))
}

func ZSTD_cpuid_avx512bw(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(30)) != uint32(0))
}

func ZSTD_cpuid_avx512vl(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)) != uint32(0))
}

func ZSTD_cpuid_prefetchwt1(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(0)) != uint32(0))
}

func ZSTD_cpuid_avx512vbmi(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(1)) != uint32(0))
}

/* ****************************
*  Common basic types
******************************/
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/*
 * ISO C Standard:  7.17  Common definitions  <stddef.h>
 */

/* Copyright (C) 1989-2025 Free Software Foundation, Inc.

This file is part of GCC.

GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.

GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.

You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */

// C documentation
//
//	/*
//	 * ISO C Standard:  7.17  Common definitions  <stddef.h>
//	 */
//	/*!
//	 * @brief Exit code for the streaming API.
//	 */
type XXH_NAMESPACEXXH_errorcode = int32

const XXH_NAMESPACEXXH_OK = 0
const /*!< OK */
XXH_NAMESPACEXXH_ERROR = 1

// C documentation
//
//	/*-**********************************************************************
//	*  32-bit hash
//	************************************************************************/
type XXH32_hash_t = uint32

// C documentation
//
//	/*!
//	 * @typedef struct XXH32_state_s XXH32_state_t
//	 * @brief The opaque state struct for the XXH32 streaming API.
//	 *
//	 * @see XXH32_state_s for details.
//	 */
type XXH_NAMESPACEXXH32_state_t = struct {
	Ftotal_len_32 XXH32_hash_t
	Flarge_len    XXH32_hash_t
	Fv            [4]XXH32_hash_t
	Fmem32        [4]XXH32_hash_t
	Fmemsize      XXH32_hash_t
	Freserved     XXH32_hash_t
}

// C documentation
//
//	/*!
//	 * @typedef struct XXH32_state_s XXH32_state_t
//	 * @brief The opaque state struct for the XXH32 streaming API.
//	 *
//	 * @see XXH32_state_s for details.
//	 */
type XXH_NAMESPACEXXH32_state_s = XXH_NAMESPACEXXH32_state_t

/*******   Canonical representation   *******/

// C documentation
//
//	/*!
//	 * @brief Canonical (big endian) representation of @ref XXH32_hash_t.
//	 */
type XXH_NAMESPACEXXH32_canonical_t = struct {
	Fdigest [4]uint8
}

/*! @cond Doxygen ignores this part */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*
 * C23 __STDC_VERSION__ number hasn't been specified yet. For now
 * leave as `201711L` (C17 + 1).
 * TODO: Update to correct value when its been specified.
 */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/* C-language Attributes are added in C23. */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*
 * Define XXH_FALLTHROUGH macro for annotating switch case with the 'fallthrough' attribute
 * introduced in CPP17 and C23.
 * CPP17 : https://en.cppreference.com/w/cpp/language/attributes/fallthrough
 * C23   : https://en.cppreference.com/w/c/language/attributes/fallthrough
 */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*
 * Define XXH_NOESCAPE for annotated pointers in public API.
 * https://clang.llvm.org/docs/AttributeReference.html#noescape
 * As of writing this, only supported by clang.
 */
/*! @endcond */

/*!
 * @}
 * @ingroup public
 * @{
 */

// C documentation
//
//	/*-**********************************************************************
//	*  64-bit hash
//	************************************************************************/
type XXH64_hash_t = uint64

// C documentation
//
//	/*******   Streaming   *******/
//	/*!
//	 * @brief The opaque state struct for the XXH64 streaming API.
//	 *
//	 * @see XXH64_state_s for details.
//	 */
type XXH_NAMESPACEXXH64_state_t = struct {
	Ftotal_len  XXH64_hash_t
	Fv          [4]XXH64_hash_t
	Fmem64      [4]XXH64_hash_t
	Fmemsize    XXH32_hash_t
	Freserved32 XXH32_hash_t
	Freserved64 XXH64_hash_t
}

// C documentation
//
//	/*******   Streaming   *******/
//	/*!
//	 * @brief The opaque state struct for the XXH64 streaming API.
//	 *
//	 * @see XXH64_state_s for details.
//	 */
type XXH_NAMESPACEXXH64_state_s = XXH_NAMESPACEXXH64_state_t

/*******   Canonical representation   *******/

// C documentation
//
//	/*!
//	 * @brief Canonical (big endian) representation of @ref XXH64_hash_t.
//	 */
type XXH_NAMESPACEXXH64_canonical_t = struct {
	Fdigest [8]uint8
} /* typedef'd to XXH64_state_t */

/* ======================================================================== */
/* ======================================================================== */
/* ======================================================================== */

/*-**********************************************************************
 * xxHash implementation
 *-**********************************************************************
 * xxHash's implementation used to be hosted inside xxhash.c.
 *
 * However, inlining requires implementation to be visible to the compiler,
 * hence be included alongside the header.
 * Previously, implementation was hosted inside xxhash.c,
 * which was then #included when inlining was activated.
 * This construction created issues with a few build and install systems,
 * as it required xxhash.c to be stored in /include directory.
 *
 * xxHash implementation is now directly integrated within xxhash.h.
 * As a consequence, xxhash.c is no longer needed in /include.
 *
 * xxhash.c is still available and is still useful.
 * In a "normal" setup, when xxhash is not inlined,
 * xxhash.h only exposes the prototypes and public symbols,
 * while xxhash.c can be built into an object file xxhash.o
 * which can then be linked into the final binary.
 ************************************************************************/

/* *************************************
*  Tuning parameters
***************************************/

/*!
 * @defgroup tuning Tuning parameters
 * @{
 *
 * Various macros to control xxHash's behavior.
 */
/*!
 * @}
 */

/* prefer __packed__ structures (method 1) for GCC
 * < ARMv7 with unaligned access (e.g. Raspbian armhf) still uses byte shifting, so we use memcpy
 * which for some reason does unaligned loads. */

/* default to 1 for -Os or -Oz */

/* don't check on sizeopt, x86, aarch64, or arm when unaligned access is available */

/* generally preferable for performance */

/*!
 * @defgroup impl Implementation
 * @{
 */

/* *************************************
*  Includes & Memory related functions
***************************************/
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */
/* Copyright (C) 1992-2025 Free Software Foundation, Inc.

This file is part of GCC.

GCC is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 3, or (at your option) any later
version.

GCC is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.

You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */

/* This administrivia gets added to the beginning of limits.h
   if the system has its own version of limits.h.  */

/* We use _GCC_LIMITS_H_ because we want this not to match
   any macros that the system's limits.h uses for its own purposes.  */

/*
 * Modify the local functions below should you wish to use
 * different memory routines for malloc() and free()
 */
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Modify this function to use a different routine than malloc().
//	 */
func XXH_malloc(tls *libc.TLS, s size_t) (r uintptr) {
	return libc.Xmalloc(tls, s)
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Modify this function to use a different routine than free().
//	 */
func XXH_free(tls *libc.TLS, p uintptr) {
	libc.Xfree(tls, p)
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Modify this function to use a different routine than memcpy().
//	 */
func XXH_memcpy(tls *libc.TLS, dest uintptr, src uintptr, size size_t) (r uintptr) {
	return libc.Xmemcpy(tls, dest, src, size)
}

/* *************************************
*  Compiler Specific Options
***************************************/

/* enable inlining hints */

/* *************************************
*  Debug
***************************************/
/*!
 * @ingroup tuning
 * @def XXH_DEBUGLEVEL
 * @brief Sets the debugging level.
 *
 * XXH_DEBUGLEVEL is expected to be defined externally, typically via the
 * compiler's command line options. The value must be a number.
 */

/* note: use after variable declarations */

/*!
 * @internal
 * @def XXH_COMPILER_GUARD(var)
 * @brief Used to prevent unwanted optimizations for @p var.
 *
 * It uses an empty GCC inline assembly statement with a register constraint
 * which forces @p var into a general purpose register (eg eax, ebx, ecx
 * on x86) and marks it as modified.
 *
 * This is used in a few places to avoid unwanted autovectorization (e.g.
 * XXH32_round()). All vectorization we want is explicit via intrinsics,
 * and _usually_ isn't wanted elsewhere.
 *
 * We also use it to prevent unwanted constant folding for AArch64 in
 * XXH3_initCustomSecret_scalar().
 */

/* Specifically for NEON vectors which use the "w" constraint, on
 * Clang. */

// C documentation
//
//	/* *************************************
//	*  Basic Types
//	***************************************/
type xxh_u8 = uint8

type xxh_u32 = uint32

/* ***   Memory access   *** */

/*!
 * @internal
 * @fn xxh_u32 XXH_read32(const void* ptr)
 * @brief Reads an unaligned 32-bit integer from @p ptr in native endianness.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 *
 * @param ptr The pointer to read from.
 * @return The 32-bit native endian integer from the bytes at @p ptr.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_readLE32(const void* ptr)
 * @brief Reads an unaligned 32-bit little endian integer from @p ptr.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 *
 * @param ptr The pointer to read from.
 * @return The 32-bit little endian integer from the bytes at @p ptr.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_readBE32(const void* ptr)
 * @brief Reads an unaligned 32-bit big endian integer from @p ptr.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 *
 * @param ptr The pointer to read from.
 * @return The 32-bit big endian integer from the bytes at @p ptr.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_readLE32_align(const void* ptr, XXH_alignment align)
 * @brief Like @ref XXH_readLE32(), but has an option for aligned reads.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 * Note that when @ref XXH_FORCE_ALIGN_CHECK == 0, the @p align parameter is
 * always @ref XXH_alignment::XXH_unaligned.
 *
 * @param ptr The pointer to read from.
 * @param align Whether @p ptr is aligned.
 * @pre
 *   If @p align == @ref XXH_alignment::XXH_aligned, @p ptr must be 4 byte
 *   aligned.
 * @return The 32-bit little endian integer from the bytes at @p ptr.
 */

// C documentation
//
//	/*
//	 * __attribute__((aligned(1))) is supported by gcc and clang. Originally the
//	 * documentation claimed that it only increased the alignment, but actually it
//	 * can decrease it on gcc, clang, and icc:
//	 * https://gcc.gnu.org/bugzilla/show_bug.cgi?id=69502,
//	 * https://gcc.godbolt.org/z/xYez1j67Y.
//	 */
func XXH_read32(tls *libc.TLS, ptr uintptr) (r xxh_u32) {
	return *(*uint32)(unsafe.Pointer(ptr))
}

/* ***   Endianness   *** */

/*!
 * @ingroup tuning
 * @def XXH_CPU_LITTLE_ENDIAN
 * @brief Whether the target is little endian.
 *
 * Defined to 1 if the target is little endian, or 0 if it is big endian.
 * It can be defined externally, for example on the compiler command line.
 *
 * If it is not defined,
 * a runtime check (which is usually constant folded) is used instead.
 *
 * @note
 *   This is not necessarily defined to an integer constant.
 *
 * @see XXH_isLittleEndian() for the runtime check.
 */
/*
 * Try to detect endianness automatically, to avoid the nonstandard behavior
 * in `XXH_isLittleEndian()`
 */

/* ****************************************
*  Compiler-specific Functions and Macros
******************************************/

/*
 * C23 and future versions have standard "unreachable()".
 * Once it has been implemented reliably we can add it as an
 * additional case:
 *
 * ```
 * #if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= XXH_C23_VN)
 * #  include <stddef.h>
 * #  ifdef unreachable
 * #    define XXH_UNREACHABLE() unreachable()
 * #  endif
 * #endif
 * ```
 *
 * Note C++23 also has std::unreachable() which can be detected
 * as follows:
 * ```
 * #if defined(__cpp_lib_unreachable) && (__cpp_lib_unreachable >= 202202L)
 * #  include <utility>
 * #  define XXH_UNREACHABLE() std::unreachable()
 * #endif
 * ```
 * NB: `__cpp_lib_unreachable` is defined in the `<version>` header.
 * We don't use that as including `<utility>` in `extern "C"` blocks
 * doesn't work on GCC12
 */

/*!
 * @internal
 * @def XXH_rotl32(x,r)
 * @brief 32-bit rotate left.
 *
 * @param x The 32-bit integer to be rotated.
 * @param r The number of bits to rotate.
 * @pre
 *   @p r > 0 && @p r < 32
 * @note
 *   @p x and @p r may be evaluated multiple times.
 * @return The rotated result.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_swap32(xxh_u32 x)
 * @brief A 32-bit byteswap.
 *
 * @param x The 32-bit integer to byteswap.
 * @return @p x, byteswapped.
 */

/* ***************************
*  Memory reads
*****************************/

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Enum to indicate whether a pointer is aligned.
//	 */
type XXH_alignment = int32

const XXH_aligned = 0
const /*!< Aligned */
XXH_unaligned = 1

// C documentation
//
//	/*
//	 * XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load.
//	 *
//	 * This is ideal for older compilers which don't inline memcpy.
//	 */
func XXH_readLE32(tls *libc.TLS, ptr uintptr) (r xxh_u32) {
	return XXH_read32(tls, ptr)
}

func XXH_readBE32(tls *libc.TLS, ptr uintptr) (r xxh_u32) {
	return libc.X__builtin_bswap32(tls, XXH_read32(tls, ptr))
}

func XXH_readLE32_align(tls *libc.TLS, ptr uintptr, align XXH_alignment) (r xxh_u32) {
	if align == int32(XXH_unaligned) {
		return XXH_readLE32(tls, ptr)
	} else {
		return *(*xxh_u32)(unsafe.Pointer(ptr))
	}
	return r
}

// C documentation
//
//	/* *************************************
//	*  Misc
//	***************************************/
//	/*! @ingroup public */
func XXH_INLINE_XXH_versionNumber(tls *libc.TLS) (r uint32) {
	return uint32(libc.Int32FromInt32(XXH_VERSION_MAJOR)*libc.Int32FromInt32(100)*libc.Int32FromInt32(100) + libc.Int32FromInt32(XXH_VERSION_MINOR)*libc.Int32FromInt32(100) + libc.Int32FromInt32(XXH_VERSION_RELEASE))
}

/* *******************************************************************
*  32-bit hash functions
*********************************************************************/
/*!
 * @}
 * @defgroup XXH32_impl XXH32 implementation
 * @ingroup impl
 *
 * Details on the XXH32 implementation.
 * @{
 */
/* #define instead of static const, to be used as initializers */

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Normal stripe processing routine.
//	 *
//	 * This shuffles the bits so that any bit from @p input impacts several bits in
//	 * @p acc.
//	 *
//	 * @param acc The accumulator lane.
//	 * @param input The stripe of input to mix.
//	 * @return The mixed accumulator lane.
//	 */
func XXH32_round(tls *libc.TLS, acc xxh_u32, input xxh_u32) (r xxh_u32) {
	acc = acc + input*uint32(0x85EBCA77)
	acc = acc<<libc.Int32FromInt32(13) | acc>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(13))
	acc = acc * uint32(0x9E3779B1)
	return acc
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Mixes all bits to finalize the hash.
//	 *
//	 * The final mix ensures that all input bits have a chance to impact any bit in
//	 * the output digest, resulting in an unbiased distribution.
//	 *
//	 * @param hash The hash to avalanche.
//	 * @return The avalanched hash.
//	 */
func XXH32_avalanche(tls *libc.TLS, hash xxh_u32) (r xxh_u32) {
	hash = hash ^ hash>>int32(15)
	hash = hash * uint32(0x85EBCA77)
	hash = hash ^ hash>>int32(13)
	hash = hash * uint32(0xC2B2AE3D)
	hash = hash ^ hash>>int32(16)
	return hash
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Processes the last 0-15 bytes of @p ptr.
//	 *
//	 * There may be up to 15 bytes remaining to consume from the input.
//	 * This final stage will digest them to ensure that all input bytes are present
//	 * in the final mix.
//	 *
//	 * @param hash The hash to finalize.
//	 * @param ptr The pointer to the remaining input.
//	 * @param len The remaining length, modulo 16.
//	 * @param align Whether @p ptr is aligned.
//	 * @return The finalized hash.
//	 * @see XXH64_finalize().
//	 */
func XXH32_finalize(tls *libc.TLS, hash xxh_u32, ptr uintptr, len1 size_t, align XXH_alignment) (r xxh_u32) {
	var v1 uintptr
	_ = v1
	if ptr == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	/* Compact rerolled version; generally faster */
	if !(libc.Int32FromInt32(XXH32_ENDJMP) != 0) {
		len1 = len1 & uint64(15)
		for len1 >= uint64(4) {
			hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
			ptr = ptr + uintptr(4)
			hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
			len1 = len1 - uint64(4)
		}
		for len1 > uint64(0) {
			v1 = ptr
			ptr = ptr + 1
			hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
			hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
			len1 = len1 - 1
		}
		return XXH32_avalanche(tls, hash)
	} else {
		switch len1 & libc.Uint64FromInt32(15) {
		case uint64(12):
			goto _2
		case uint64(8):
			goto _3
		case uint64(4):
			goto _4
		case uint64(13):
			goto _5
		case uint64(9):
			goto _6
		case uint64(5):
			goto _7
		case uint64(14):
			goto _8
		case uint64(10):
			goto _9
		case uint64(6):
			goto _10
		case uint64(15):
			goto _11
		case uint64(11):
			goto _12
		case uint64(7):
			goto _13
		case uint64(3):
			goto _14
		case uint64(2):
			goto _15
		case uint64(1):
			goto _16
		case uint64(0):
			goto _17
		}
		goto _18 /* or switch(bEnd - p) */
	_2:
		;
	_21:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		goto _20
	_20:
		;
		if 0 != 0 {
			goto _21
		}
		goto _19
	_19:
		;
		/* fallthrough */
	_3:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_4:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		return XXH32_avalanche(tls, hash)
	_5:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_6:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_7:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		return XXH32_avalanche(tls, hash)
	_8:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_9:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_10:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		return XXH32_avalanche(tls, hash)
	_11:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_12:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_13:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_14:
		;
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		/* fallthrough */
	_15:
		;
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		/* fallthrough */
	_16:
		;
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		/* fallthrough */
	_17:
		;
		return XXH32_avalanche(tls, hash)
	_18:
		;
		if !(libc.Int32FromInt32(0) != 0) {
		}
		return hash /* reaching this point is deemed impossible */
	}
	return r
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief The implementation for @ref XXH32().
//	 *
//	 * @param input , len , seed Directly passed from @ref XXH32().
//	 * @param align Whether @p input is aligned.
//	 * @return The calculated hash.
//	 */
func XXH32_endian_align(tls *libc.TLS, input uintptr, len1 size_t, seed xxh_u32, align XXH_alignment) (r xxh_u32) {
	var bEnd, limit uintptr
	var h32, v1, v2, v3, v4 xxh_u32
	_, _, _, _, _, _, _ = bEnd, h32, limit, v1, v2, v3, v4
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	if len1 >= uint64(16) {
		bEnd = input + uintptr(len1)
		limit = bEnd - uintptr(15)
		v1 = seed + uint32(0x9E3779B1) + uint32(0x85EBCA77)
		v2 = seed + uint32(0x85EBCA77)
		v3 = seed + uint32(0)
		v4 = seed - uint32(0x9E3779B1)
		for cond := true; cond; cond = input < limit {
			v1 = XXH32_round(tls, v1, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
			v2 = XXH32_round(tls, v2, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
			v3 = XXH32_round(tls, v3, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
			v4 = XXH32_round(tls, v4, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
		}
		h32 = v1<<libc.Int32FromInt32(1) | v1>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(1)) + (v2<<libc.Int32FromInt32(7) | v2>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(7))) + (v3<<libc.Int32FromInt32(12) | v3>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(12))) + (v4<<libc.Int32FromInt32(18) | v4>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(18)))
	} else {
		h32 = seed + uint32(0x165667B1)
	}
	h32 = h32 + uint32(len1)
	return XXH32_finalize(tls, h32, input, len1&uint64(15), align)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32(tls *libc.TLS, input uintptr, len1 size_t, seed XXH32_hash_t) (r XXH32_hash_t) {
	if XXH_FORCE_ALIGN_CHECK != 0 {
		if uint64(input)&uint64(3) == uint64(0) { /* Input is 4-bytes aligned, leverage the speed benefit */
			return XXH32_endian_align(tls, input, len1, seed, int32(XXH_aligned))
		}
	}
	return XXH32_endian_align(tls, input, len1, seed, int32(XXH_unaligned))
}

// C documentation
//
//	/*******   Hash streaming   *******/
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_createState(tls *libc.TLS) (r uintptr) {
	return XXH_malloc(tls, uint64(48))
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_freeState(tls *libc.TLS, statePtr uintptr) (r XXH_NAMESPACEXXH_errorcode) {
	XXH_free(tls, statePtr)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_copyState(tls *libc.TLS, dstState uintptr, srcState uintptr) {
	XXH_memcpy(tls, dstState, srcState, uint64(48))
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_reset(tls *libc.TLS, statePtr uintptr, seed XXH32_hash_t) (r XXH_NAMESPACEXXH_errorcode) {
	if !(statePtr != libc.UintptrFromInt32(0)) {
	}
	libc.Xmemset(tls, statePtr, 0, uint64(48))
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8)) = seed + uint32(0x9E3779B1) + uint32(0x85EBCA77)
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8 + 1*4)) = seed + uint32(0x85EBCA77)
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8 + 2*4)) = seed + uint32(0)
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8 + 3*4)) = seed - uint32(0x9E3779B1)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_update(tls *libc.TLS, state uintptr, input uintptr, len1 size_t) (r XXH_NAMESPACEXXH_errorcode) {
	var bEnd, limit, p, p32 uintptr
	_, _, _, _ = bEnd, limit, p, p32
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
		return int32(XXH_NAMESPACEXXH_OK)
	}
	p = input
	bEnd = p + uintptr(len1)
	*(*XXH32_hash_t)(unsafe.Pointer(state)) += uint32(len1)
	*(*XXH32_hash_t)(unsafe.Pointer(state + 4)) |= uint32(libc.BoolInt32(len1 >= libc.Uint64FromInt32(16)) | libc.BoolInt32((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Ftotal_len_32 >= libc.Uint32FromInt32(16)))
	if uint64((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize)+len1 < uint64(16) { /* fill in tmp buffer */
		XXH_memcpy(tls, state+24+uintptr((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize), input, len1)
		*(*XXH32_hash_t)(unsafe.Pointer(state + 40)) += uint32(len1)
		return int32(XXH_NAMESPACEXXH_OK)
	}
	if (*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize != 0 { /* some data left from previous update */
		XXH_memcpy(tls, state+24+uintptr((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize), input, uint64(uint32(16)-(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize))
		p32 = state + 24
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE32(tls, p32))
		p32 += 4
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)), XXH_readLE32(tls, p32))
		p32 += 4
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)), XXH_readLE32(tls, p32))
		p32 += 4
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)), XXH_readLE32(tls, p32))
		p = p + uintptr(uint32(16)-(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize)
		(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize = uint32(0)
	}
	if p <= bEnd-uintptr(16) {
		limit = bEnd - uintptr(16)
		for cond := true; cond; cond = p <= limit {
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
		}
	}
	if p < bEnd {
		XXH_memcpy(tls, state+24, p, uint64(int64(bEnd)-int64(p)))
		(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize = uint32(int64(bEnd) - int64(p))
	}
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_digest(tls *libc.TLS, state uintptr) (r XXH32_hash_t) {
	var h32 xxh_u32
	_ = h32
	if (*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Flarge_len != 0 {
		h32 = *(*XXH32_hash_t)(unsafe.Pointer(state + 8))<<libc.Int32FromInt32(1) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(1)) + (*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4))<<libc.Int32FromInt32(7) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(7))) + (*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4))<<libc.Int32FromInt32(12) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(12))) + (*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4))<<libc.Int32FromInt32(18) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(18)))
	} else {
		h32 = *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)) + uint32(0x165667B1)
	}
	h32 = h32 + (*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Ftotal_len_32
	return XXH32_finalize(tls, h32, state+24, uint64((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize), int32(XXH_aligned))
}

/*******   Canonical representation   *******/

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_canonicalFromHash(tls *libc.TLS, dst uintptr, _hash XXH32_hash_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*XXH32_hash_t)(unsafe.Pointer(bp)) = _hash
	if int32(XXH_CPU_LITTLE_ENDIAN) != 0 {
		*(*XXH32_hash_t)(unsafe.Pointer(bp)) = libc.X__builtin_bswap32(tls, *(*XXH32_hash_t)(unsafe.Pointer(bp)))
	}
	XXH_memcpy(tls, dst, bp, uint64(4))
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_hashFromCanonical(tls *libc.TLS, src uintptr) (r XXH32_hash_t) {
	return XXH_readBE32(tls, src)
}

/* *******************************************************************
*  64-bit hash functions
*********************************************************************/
/*!
 * @}
 * @ingroup impl
 * @{
 */
/*******   Memory access   *******/

type xxh_u64 = uint64

// C documentation
//
//	/*
//	 * __attribute__((aligned(1))) is supported by gcc and clang. Originally the
//	 * documentation claimed that it only increased the alignment, but actually it
//	 * can decrease it on gcc, clang, and icc:
//	 * https://gcc.gnu.org/bugzilla/show_bug.cgi?id=69502,
//	 * https://gcc.godbolt.org/z/xYez1j67Y.
//	 */
func XXH_read64(tls *libc.TLS, ptr uintptr) (r xxh_u64) {
	return *(*uint64)(unsafe.Pointer(ptr))
}

// C documentation
//
//	/* XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load. */
func XXH_readLE64(tls *libc.TLS, ptr uintptr) (r xxh_u64) {
	return XXH_read64(tls, ptr)
}

func XXH_readBE64(tls *libc.TLS, ptr uintptr) (r xxh_u64) {
	return libc.X__builtin_bswap64(tls, XXH_read64(tls, ptr))
}

func XXH_readLE64_align(tls *libc.TLS, ptr uintptr, align XXH_alignment) (r xxh_u64) {
	if align == int32(XXH_unaligned) {
		return XXH_readLE64(tls, ptr)
	} else {
		return *(*xxh_u64)(unsafe.Pointer(ptr))
	}
	return r
}

/*******   xxh64   *******/
/*!
 * @}
 * @defgroup XXH64_impl XXH64 implementation
 * @ingroup impl
 *
 * Details on the XXH64 implementation.
 * @{
 */
/* #define rather that static const, to be used as initializers */

// C documentation
//
//	/*! @copydoc XXH32_round */
func XXH64_round(tls *libc.TLS, acc xxh_u64, input xxh_u64) (r xxh_u64) {
	acc = acc + input*uint64(0xC2B2AE3D27D4EB4F)
	acc = acc<<libc.Int32FromInt32(31) | acc>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(31))
	acc = acc * uint64(0x9E3779B185EBCA87)
	return acc
}

func XXH64_mergeRound(tls *libc.TLS, acc xxh_u64, val xxh_u64) (r xxh_u64) {
	val = XXH64_round(tls, uint64(0), val)
	acc = acc ^ val
	acc = acc*uint64(0x9E3779B185EBCA87) + uint64(0x85EBCA77C2B2AE63)
	return acc
}

// C documentation
//
//	/*! @copydoc XXH32_avalanche */
func XXH64_avalanche(tls *libc.TLS, hash xxh_u64) (r xxh_u64) {
	hash = hash ^ hash>>int32(33)
	hash = hash * uint64(0xC2B2AE3D27D4EB4F)
	hash = hash ^ hash>>int32(29)
	hash = hash * uint64(0x165667B19E3779F9)
	hash = hash ^ hash>>int32(32)
	return hash
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Processes the last 0-31 bytes of @p ptr.
//	 *
//	 * There may be up to 31 bytes remaining to consume from the input.
//	 * This final stage will digest them to ensure that all input bytes are present
//	 * in the final mix.
//	 *
//	 * @param hash The hash to finalize.
//	 * @param ptr The pointer to the remaining input.
//	 * @param len The remaining length, modulo 32.
//	 * @param align Whether @p ptr is aligned.
//	 * @return The finalized hash
//	 * @see XXH32_finalize().
//	 */
func XXH64_finalize(tls *libc.TLS, hash xxh_u64, ptr uintptr, len1 size_t, align XXH_alignment) (r xxh_u64) {
	var k1 xxh_u64
	var v1 uintptr
	_, _ = k1, v1
	if ptr == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	len1 = len1 & uint64(31)
	for len1 >= uint64(8) {
		k1 = XXH64_round(tls, uint64(0), XXH_readLE64_align(tls, ptr, align))
		ptr = ptr + uintptr(8)
		hash = hash ^ k1
		hash = (hash<<libc.Int32FromInt32(27)|hash>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(27)))*uint64(0x9E3779B185EBCA87) + uint64(0x85EBCA77C2B2AE63)
		len1 = len1 - uint64(8)
	}
	if len1 >= uint64(4) {
		hash = hash ^ uint64(XXH_readLE32_align(tls, ptr, align))*uint64(0x9E3779B185EBCA87)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(23)|hash>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(23)))*uint64(0xC2B2AE3D27D4EB4F) + uint64(0x165667B19E3779F9)
		len1 = len1 - uint64(4)
	}
	for len1 > uint64(0) {
		v1 = ptr
		ptr = ptr + 1
		hash = hash ^ uint64(*(*xxh_u8)(unsafe.Pointer(v1)))*uint64(0x27D4EB2F165667C5)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(11))) * uint64(0x9E3779B185EBCA87)
		len1 = len1 - 1
	}
	return XXH64_avalanche(tls, hash)
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief The implementation for @ref XXH64().
//	 *
//	 * @param input , len , seed Directly passed from @ref XXH64().
//	 * @param align Whether @p input is aligned.
//	 * @return The calculated hash.
//	 */
func XXH64_endian_align(tls *libc.TLS, input uintptr, len1 size_t, seed xxh_u64, align XXH_alignment) (r xxh_u64) {
	var bEnd, limit uintptr
	var h64, v1, v2, v3, v4 xxh_u64
	_, _, _, _, _, _, _ = bEnd, h64, limit, v1, v2, v3, v4
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	if len1 >= uint64(32) {
		bEnd = input + uintptr(len1)
		limit = bEnd - uintptr(31)
		v1 = seed + uint64(0x9E3779B185EBCA87) + uint64(0xC2B2AE3D27D4EB4F)
		v2 = seed + uint64(0xC2B2AE3D27D4EB4F)
		v3 = seed + uint64(0)
		v4 = seed - uint64(0x9E3779B185EBCA87)
		for cond := true; cond; cond = input < limit {
			v1 = XXH64_round(tls, v1, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
			v2 = XXH64_round(tls, v2, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
			v3 = XXH64_round(tls, v3, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
			v4 = XXH64_round(tls, v4, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
		}
		h64 = v1<<libc.Int32FromInt32(1) | v1>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(1)) + (v2<<libc.Int32FromInt32(7) | v2>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(7))) + (v3<<libc.Int32FromInt32(12) | v3>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(12))) + (v4<<libc.Int32FromInt32(18) | v4>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(18)))
		h64 = XXH64_mergeRound(tls, h64, v1)
		h64 = XXH64_mergeRound(tls, h64, v2)
		h64 = XXH64_mergeRound(tls, h64, v3)
		h64 = XXH64_mergeRound(tls, h64, v4)
	} else {
		h64 = seed + uint64(0x27D4EB2F165667C5)
	}
	h64 = h64 + len1
	return XXH64_finalize(tls, h64, input, len1, align)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64(tls *libc.TLS, input uintptr, len1 size_t, seed XXH64_hash_t) (r XXH64_hash_t) {
	if XXH_FORCE_ALIGN_CHECK != 0 {
		if uint64(input)&uint64(7) == uint64(0) { /* Input is aligned, let's leverage the speed advantage */
			return XXH64_endian_align(tls, input, len1, seed, int32(XXH_aligned))
		}
	}
	return XXH64_endian_align(tls, input, len1, seed, int32(XXH_unaligned))
}

// C documentation
//
//	/*******   Hash Streaming   *******/
//	/*! @ingroup XXH64_family*/
func XXH_INLINE_XXH64_createState(tls *libc.TLS) (r uintptr) {
	return XXH_malloc(tls, uint64(88))
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_freeState(tls *libc.TLS, statePtr uintptr) (r XXH_NAMESPACEXXH_errorcode) {
	XXH_free(tls, statePtr)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_copyState(tls *libc.TLS, dstState uintptr, srcState uintptr) {
	XXH_memcpy(tls, dstState, srcState, uint64(88))
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_reset(tls *libc.TLS, statePtr uintptr, seed XXH64_hash_t) (r XXH_NAMESPACEXXH_errorcode) {
	if !(statePtr != libc.UintptrFromInt32(0)) {
	}
	libc.Xmemset(tls, statePtr, 0, uint64(88))
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8)) = seed + uint64(0x9E3779B185EBCA87) + uint64(0xC2B2AE3D27D4EB4F)
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8 + 1*8)) = seed + uint64(0xC2B2AE3D27D4EB4F)
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8 + 2*8)) = seed + uint64(0)
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8 + 3*8)) = seed - uint64(0x9E3779B185EBCA87)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_update(tls *libc.TLS, state uintptr, input uintptr, len1 size_t) (r XXH_NAMESPACEXXH_errorcode) {
	var bEnd, limit, p uintptr
	_, _, _ = bEnd, limit, p
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
		return int32(XXH_NAMESPACEXXH_OK)
	}
	p = input
	bEnd = p + uintptr(len1)
	*(*XXH64_hash_t)(unsafe.Pointer(state)) += len1
	if uint64((*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize)+len1 < uint64(32) { /* fill in tmp buffer */
		XXH_memcpy(tls, state+40+uintptr((*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize), input, len1)
		*(*XXH32_hash_t)(unsafe.Pointer(state + 72)) += uint32(len1)
		return int32(XXH_NAMESPACEXXH_OK)
	}
	if (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize != 0 { /* tmp buffer is full */
		XXH_memcpy(tls, state+40+uintptr((*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize), input, uint64(uint32(32)-(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE64(tls, state+40+uintptr(0)*8))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)), XXH_readLE64(tls, state+40+uintptr(1)*8))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)), XXH_readLE64(tls, state+40+uintptr(2)*8))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)), XXH_readLE64(tls, state+40+uintptr(3)*8))
		p = p + uintptr(uint32(32)-(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize)
		(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize = uint32(0)
	}
	if p+uintptr(32) <= bEnd {
		limit = bEnd - uintptr(32)
		for cond := true; cond; cond = p <= limit {
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
		}
	}
	if p < bEnd {
		XXH_memcpy(tls, state+40, p, uint64(int64(bEnd)-int64(p)))
		(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize = uint32(int64(bEnd) - int64(p))
	}
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_digest(tls *libc.TLS, state uintptr) (r XXH64_hash_t) {
	var h64 xxh_u64
	_ = h64
	if (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Ftotal_len >= uint64(32) {
		h64 = *(*XXH64_hash_t)(unsafe.Pointer(state + 8))<<libc.Int32FromInt32(1) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(1)) + (*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8))<<libc.Int32FromInt32(7) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(7))) + (*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8))<<libc.Int32FromInt32(12) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(12))) + (*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8))<<libc.Int32FromInt32(18) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(18)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)))
	} else {
		h64 = *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)) + uint64(0x27D4EB2F165667C5)
	}
	h64 = h64 + (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Ftotal_len
	return XXH64_finalize(tls, h64, state+40, (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Ftotal_len, int32(XXH_aligned))
}

/******* Canonical representation   *******/

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_canonicalFromHash(tls *libc.TLS, dst uintptr, _hash XXH64_hash_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*XXH64_hash_t)(unsafe.Pointer(bp)) = _hash
	if int32(XXH_CPU_LITTLE_ENDIAN) != 0 {
		*(*XXH64_hash_t)(unsafe.Pointer(bp)) = libc.X__builtin_bswap64(tls, *(*XXH64_hash_t)(unsafe.Pointer(bp)))
	}
	XXH_memcpy(tls, dst, bp, uint64(8))
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_hashFromCanonical(tls *libc.TLS, src uintptr) (r XXH64_hash_t) {
	return XXH_readBE64(tls, src)
}

/*!
 * @}
 */
/**** ended inlining xxhash.h ****/
/**** start inlining zstd_trace.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/*
 * ISO C Standard:  7.17  Common definitions  <stddef.h>
 */

/* Copyright (C) 1989-2025 Free Software Foundation, Inc.

This file is part of GCC.

GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.

GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.

You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */

/*
 * ISO C Standard:  7.17  Common definitions  <stddef.h>
 */

/* weak symbol support
 * For now, enable conservatively:
 * - Only GNUC
 * - Only ELF
 * - Only x86-64, i386, aarch64 and risc-v.
 * Also, explicitly disable on platforms known not to work so they aren't
 * forgotten in the future.
 */

/* Only enable tracing when weak symbols are available. */

/**** ended inlining zstd_trace.h ****/

/* ---- static assert (debug) --- */

/*-*************************************
*  shared macros
***************************************/

/*-*************************************
*  Common constants
***************************************/

var repStartValue = [3]U32{
	0: uint32(1),
	1: uint32(4),
	2: uint32(8),
}

var ZSTD_fcs_fieldSize = [4]size_t{
	1: uint64(2),
	2: uint64(4),
	3: uint64(8),
}
var ZSTD_did_fieldSize = [4]size_t{
	1: uint64(1),
	2: uint64(2),
	3: uint64(4),
}

var ZSTD_blockHeaderSize = uint64(ZSTD_BLOCKHEADERSIZE)

type blockType_e = int32

type SymbolEncodingType_e = int32

const set_basic = 0
const set_rle = 1
const set_compressed = 2
const set_repeat = 3

/* Each table cannot take more than #symbols * FSELog bits */

var LL_bits = [36]U8{
	16: uint8(1),
	17: uint8(1),
	18: uint8(1),
	19: uint8(1),
	20: uint8(2),
	21: uint8(2),
	22: uint8(3),
	23: uint8(3),
	24: uint8(4),
	25: uint8(6),
	26: uint8(7),
	27: uint8(8),
	28: uint8(9),
	29: uint8(10),
	30: uint8(11),
	31: uint8(12),
	32: uint8(13),
	33: uint8(14),
	34: uint8(15),
	35: uint8(16),
}
var LL_defaultNorm = [36]S16{
	0:  int16(4),
	1:  int16(3),
	2:  int16(2),
	3:  int16(2),
	4:  int16(2),
	5:  int16(2),
	6:  int16(2),
	7:  int16(2),
	8:  int16(2),
	9:  int16(2),
	10: int16(2),
	11: int16(2),
	12: int16(2),
	13: int16(1),
	14: int16(1),
	15: int16(1),
	16: int16(2),
	17: int16(2),
	18: int16(2),
	19: int16(2),
	20: int16(2),
	21: int16(2),
	22: int16(2),
	23: int16(2),
	24: int16(2),
	25: int16(3),
	26: int16(2),
	27: int16(1),
	28: int16(1),
	29: int16(1),
	30: int16(1),
	31: int16(1),
	32: int16(-int32(1)),
	33: int16(-int32(1)),
	34: int16(-int32(1)),
	35: int16(-int32(1)),
}
var LL_defaultNormLog = uint32(LL_DEFAULTNORMLOG)

var ML_bits = [53]U8{
	32: uint8(1),
	33: uint8(1),
	34: uint8(1),
	35: uint8(1),
	36: uint8(2),
	37: uint8(2),
	38: uint8(3),
	39: uint8(3),
	40: uint8(4),
	41: uint8(4),
	42: uint8(5),
	43: uint8(7),
	44: uint8(8),
	45: uint8(9),
	46: uint8(10),
	47: uint8(11),
	48: uint8(12),
	49: uint8(13),
	50: uint8(14),
	51: uint8(15),
	52: uint8(16),
}
var ML_defaultNorm = [53]S16{
	0:  int16(1),
	1:  int16(4),
	2:  int16(3),
	3:  int16(2),
	4:  int16(2),
	5:  int16(2),
	6:  int16(2),
	7:  int16(2),
	8:  int16(2),
	9:  int16(1),
	10: int16(1),
	11: int16(1),
	12: int16(1),
	13: int16(1),
	14: int16(1),
	15: int16(1),
	16: int16(1),
	17: int16(1),
	18: int16(1),
	19: int16(1),
	20: int16(1),
	21: int16(1),
	22: int16(1),
	23: int16(1),
	24: int16(1),
	25: int16(1),
	26: int16(1),
	27: int16(1),
	28: int16(1),
	29: int16(1),
	30: int16(1),
	31: int16(1),
	32: int16(1),
	33: int16(1),
	34: int16(1),
	35: int16(1),
	36: int16(1),
	37: int16(1),
	38: int16(1),
	39: int16(1),
	40: int16(1),
	41: int16(1),
	42: int16(1),
	43: int16(1),
	44: int16(1),
	45: int16(1),
	46: int16(-int32(1)),
	47: int16(-int32(1)),
	48: int16(-int32(1)),
	49: int16(-int32(1)),
	50: int16(-int32(1)),
	51: int16(-int32(1)),
	52: int16(-int32(1)),
}
var ML_defaultNormLog = uint32(ML_DEFAULTNORMLOG)

var OF_defaultNorm = [29]S16{
	0:  int16(1),
	1:  int16(1),
	2:  int16(1),
	3:  int16(1),
	4:  int16(1),
	5:  int16(1),
	6:  int16(2),
	7:  int16(2),
	8:  int16(2),
	9:  int16(1),
	10: int16(1),
	11: int16(1),
	12: int16(1),
	13: int16(1),
	14: int16(1),
	15: int16(1),
	16: int16(1),
	17: int16(1),
	18: int16(1),
	19: int16(1),
	20: int16(1),
	21: int16(1),
	22: int16(1),
	23: int16(1),
	24: int16(-int32(1)),
	25: int16(-int32(1)),
	26: int16(-int32(1)),
	27: int16(-int32(1)),
	28: int16(-int32(1)),
}
var OF_defaultNormLog = uint32(OF_DEFAULTNORMLOG)

// C documentation
//
//	/*-*******************************************
//	*  Shared functions to include for inlining
//	*********************************************/
func ZSTD_copy8(tls *libc.TLS, dst uintptr, src uintptr) {
	libc.Xmemcpy(tls, dst, src, uint64(libc.Int32FromInt32(8)))
}

// C documentation
//
//	/* Need to use memmove here since the literal buffer can now be located within
//	   the dst buffer. In circumstances where the op "catches up" to where the
//	   literal buffer is, there can be partial overlaps in this call on the final
//	   copy if the literal is being shifted by less than 16 bytes. */
func ZSTD_copy16(tls *libc.TLS, dst uintptr, src uintptr) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* copy16_buf at bp+0 */ [16]BYTE
	libc.Xmemcpy(tls, bp, src, uint64(libc.Int32FromInt32(16)))
	libc.Xmemcpy(tls, dst, bp, uint64(libc.Int32FromInt32(16)))
}

type ZSTD_overlap_e = int32

const ZSTD_no_overlap = 0
const ZSTD_overlap_src_before_dst = 1

// C documentation
//
//	/*! ZSTD_wildcopy() :
//	 *  Custom version of ZSTD_memcpy(), can over read/write up to WILDCOPY_OVERLENGTH bytes (if length==0)
//	 *  @param ovtype controls the overlap detection
//	 *         - ZSTD_no_overlap: The source and destination are guaranteed to be at least WILDCOPY_VECLEN bytes apart.
//	 *         - ZSTD_overlap_src_before_dst: The src and dst may overlap, but they MUST be at least 8 bytes apart.
//	 *           The src buffer must be before the dst buffer.
//	 */
func ZSTD_wildcopy(tls *libc.TLS, dst uintptr, src uintptr, length ptrdiff_t, ovtype ZSTD_overlap_e) {
	var diff ptrdiff_t
	var ip, oend, op uintptr
	_, _, _, _ = diff, ip, oend, op
	diff = int64(dst) - int64(src)
	ip = src
	op = dst
	oend = op + uintptr(length)
	if ovtype == int32(ZSTD_overlap_src_before_dst) && diff < int64(WILDCOPY_VECLEN) {
		/* Handle short offset copies. */
		for cond := true; cond; cond = op < oend {
			ZSTD_copy8(tls, op, ip)
			op = op + uintptr(8)
			ip = ip + uintptr(8)
		}
	} else {
		/* Separate out the first COPY16() call because the copy length is
		 * almost certain to be short, so the branches have different
		 * probabilities. Since it is almost certain to be short, only do
		 * one COPY16() in the first call. Then, do two calls per loop since
		 * at that point it is more likely to have a high trip count.
		 */
		ZSTD_copy16(tls, op, ip)
		if int64(16) >= length {
			return
		}
		op = op + uintptr(16)
		ip = ip + uintptr(16)
		for cond := true; cond; cond = op < oend {
			ZSTD_copy16(tls, op, ip)
			op = op + uintptr(16)
			ip = ip + uintptr(16)
			ZSTD_copy16(tls, op, ip)
			op = op + uintptr(16)
			ip = ip + uintptr(16)
		}
	}
}

func ZSTD_limitCopy(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var length size_t
	var v1 uint64
	_, _ = length, v1
	if dstCapacity < srcSize {
		v1 = dstCapacity
	} else {
		v1 = srcSize
	}
	length = v1
	if length > uint64(0) {
		libc.Xmemcpy(tls, dst, src, length)
	}
	return length
}

/* define "workspace is too large" as this number of times larger than needed */

/* when workspace is continuously too large
 * during at least this number of times,
 * context's memory usage is considered wasteful,
 * because it's sized to handle a worst case scenario which rarely happens.
 * In which case, resize it down to free some memory */

// C documentation
//
//	/* Controls whether the input/output buffer is buffered or stable. */
type ZSTD_bufferMode_e = int32

/*-*******************************************
*  Private declarations
*********************************************/

// C documentation
//
//	/**
//	 * Contains the compressed frame size and an upper-bound for the decompressed frame size.
//	 * Note: before using `compressedSize`, check for errors using ZSTD_isError().
//	 *       similarly, before using `decompressedBound`, check for errors using:
//	 *          `decompressedBound != ZSTD_CONTENTSIZE_ERROR`
//	 */
type ZSTD_frameSizeInfo = struct {
	FnbBlocks          size_t
	FcompressedSize    size_t
	FdecompressedBound uint64
}

/* zstdmt, adaptive_compression (shouldn't get this definition from here) */

type blockProperties_t = struct {
	FblockType blockType_e
	FlastBlock U32
	ForigSize  U32
}

// C documentation
//
//	/**
//	 * @returns true iff the CPU supports dynamic BMI2 dispatch.
//	 */
func ZSTD_cpuSupportsBmi2(tls *libc.TLS) (r int32) {
	var cpuid ZSTD_cpuid_t
	_ = cpuid
	cpuid = ZSTD_cpuid(tls)
	return libc.BoolInt32(ZSTD_cpuid_bmi1(tls, cpuid) != 0 && ZSTD_cpuid_bmi2(tls, cpuid) != 0)
}

/**** ended inlining zstd_internal.h ****/

// C documentation
//
//	/*-****************************************
//	*  Version
//	******************************************/
func ZSTD_versionNumber(tls *libc.TLS) (r uint32) {
	return uint32(libc.Int32FromInt32(ZSTD_VERSION_MAJOR)*libc.Int32FromInt32(100)*libc.Int32FromInt32(100) + libc.Int32FromInt32(ZSTD_VERSION_MINOR)*libc.Int32FromInt32(100) + libc.Int32FromInt32(ZSTD_VERSION_RELEASE))
}

func ZSTD_versionString(tls *libc.TLS) (r uintptr) {
	return __ccgo_ts + 1320
}

// C documentation
//
//	/*-****************************************
//	*  ZSTD Error Management
//	******************************************/
//	/*! ZSTD_isError() :
//	 *  tells if a return value is an error code
//	 *  symbol is required for external callers */
func ZSTD_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

// C documentation
//
//	/*! ZSTD_getErrorName() :
//	 *  provides error code string from function result (useful for debugging) */
func ZSTD_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorName(tls, code)
}

// C documentation
//
//	/*! ZSTD_getError() :
//	 *  convert a `size_t` function result into a proper ZSTD_errorCode enum */
func ZSTD_getErrorCode(tls *libc.TLS, code size_t) (r ZSTD_ErrorCode) {
	return ERR_getErrorCode(tls, code)
}

// C documentation
//
//	/*! ZSTD_getErrorString() :
//	 *  provides error code string from enum */
func ZSTD_getErrorString(tls *libc.TLS, code ZSTD_ErrorCode) (r uintptr) {
	return ERR_getErrorString(tls, code)
}

/**** ended inlining hist.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

/* **************************************************************
*  Templates
****************************************************************/
/*
  designed to be included
  for type-specific functions (template emulation in C)
  Objective is to write these functions only once, for improved maintenance
*/

/* safety checks */

/* Function names */

/* Function templates */

// C documentation
//
//	/* FSE_buildCTable_wksp() :
//	 * Same as FSE_buildCTable(), but using an externally allocated scratch buffer (`workSpace`).
//	 * wkspSize should be sized to handle worst case situation, which is `1<<max_tableLog * sizeof(FSE_FUNCTION_TYPE)`
//	 * workSpace must also be properly aligned with FSE_FUNCTION_TYPE requirements
//	 */
func FSE_buildCTable_wksp(tls *libc.TLS, ct uintptr, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	var FSCT, cumul, ptr, spread, symbolTT, tableSymbol, tableU16, v12 uintptr
	var add, sv U64
	var freq, i, n, nbOccurrences int32
	var highThreshold, maxBitsOut, maxSV1, minStatePlus, position1, s, step, symbol, tableMask, tableSize, u, u2, v3 U32
	var pos, position, s1, u1, uPosition, unroll size_t
	var s2 BYTE
	var s3, total, v1 uint32
	var v11 U16
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = FSCT, add, cumul, freq, highThreshold, i, maxBitsOut, maxSV1, minStatePlus, n, nbOccurrences, pos, position, position1, ptr, s, s1, s2, s3, spread, step, sv, symbol, symbolTT, tableMask, tableSize, tableSymbol, tableU16, total, u, u1, u2, uPosition, unroll, v1, v11, v12, v3
	tableSize = uint32(int32(1) << tableLog)
	tableMask = tableSize - uint32(1)
	ptr = ct
	tableU16 = ptr + uintptr(2)*2
	if tableLog != 0 {
		v1 = tableSize >> int32(1)
	} else {
		v1 = uint32(1)
	}
	FSCT = ptr + uintptr(1)*4 + uintptr(v1)*4
	symbolTT = FSCT
	step = tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3)
	maxSV1 = maxSymbolValue + uint32(1)
	cumul = workSpace                                               /* size = maxSV1 */
	tableSymbol = cumul + uintptr(maxSV1+libc.Uint32FromInt32(1))*2 /* size = tableSize */
	highThreshold = tableSize - uint32(1)
	/* Must be 2 bytes-aligned */
	if uint64(4)*((uint64(maxSymbolValue+libc.Uint32FromInt32(2))+uint64(1)<<tableLog)/uint64(2)+libc.Uint64FromInt64(8)/libc.Uint64FromInt64(4)) > wkspSize {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	/* CTable header */
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(2))*2)) = uint16(tableLog)
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(1))*2)) = uint16(maxSymbolValue)
	/* required for threshold strategy to work */
	/* For explanations on how to distribute symbol values over the table :
	 * https://fastcompression.blogspot.fr/2014/02/fse-distributing-symbol-values.html */
	/* symbol start positions */
	*(*U16)(unsafe.Pointer(cumul)) = uint16(0)
	u = uint32(1)
	for {
		if !(u <= maxSV1) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(u-uint32(1))*2))) == -int32(1) { /* Low proba symbol */
			*(*U16)(unsafe.Pointer(cumul + uintptr(u)*2)) = uint16(int32(*(*U16)(unsafe.Pointer(cumul + uintptr(u-uint32(1))*2))) + int32(1))
			v3 = highThreshold
			highThreshold = highThreshold - 1
			*(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(v3))) = uint8(u - libc.Uint32FromInt32(1))
		} else {
			*(*U16)(unsafe.Pointer(cumul + uintptr(u)*2)) = uint16(int32(*(*U16)(unsafe.Pointer(cumul + uintptr(u-uint32(1))*2))) + int32(uint16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(u-uint32(1))*2)))))
			/* no overflow */
		}
		goto _2
	_2:
		;
		u = u + 1
	}
	*(*U16)(unsafe.Pointer(cumul + uintptr(maxSV1)*2)) = uint16(tableSize + libc.Uint32FromInt32(1))
	/* Spread symbols */
	if highThreshold == tableSize-uint32(1) {
		/* Case for no low prob count symbols. Lay down 8 bytes at a time
		 * to reduce branch misses since we are operating on a small block
		 */
		spread = tableSymbol + uintptr(tableSize) /* size = tableSize + 8 (may write beyond tableSize) */
		add = uint64(0x0101010101010101)
		pos = uint64(0)
		sv = uint64(0)
		s = uint32(0)
		for {
			if !(s < maxSV1) {
				break
			}
			n = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)))
			MEM_write64(tls, spread+uintptr(pos), sv)
			i = int32(8)
			for {
				if !(i < n) {
					break
				}
				MEM_write64(tls, spread+uintptr(pos)+uintptr(i), sv)
				goto _5
			_5:
				;
				i = i + int32(8)
			}
			pos = pos + uint64(n)
			goto _4
		_4:
			;
			s = s + 1
			sv = sv + add
		}
		/* Spread symbols across the table. Lack of lowprob symbols means that
		 * we don't need variable sized inner loop, so we can unroll the loop and
		 * reduce branch misses.
		 */
		position = uint64(0)
		unroll = uint64(2) /* Experimentally determined optimal unroll */
		/* FSE_MIN_TABLELOG is 5 */
		s1 = uint64(0)
		for {
			if !(s1 < uint64(tableSize)) {
				break
			}
			u1 = uint64(0)
			for {
				if !(u1 < unroll) {
					break
				}
				uPosition = (position + u1*uint64(step)) & uint64(tableMask)
				*(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(uPosition))) = *(*BYTE)(unsafe.Pointer(spread + uintptr(s1+u1)))
				goto _7
			_7:
				;
				u1 = u1 + 1
			}
			position = (position + unroll*uint64(step)) & uint64(tableMask)
			goto _6
		_6:
			;
			s1 = s1 + unroll
		}
		/* Must have initialized all positions */
	} else {
		position1 = uint32(0)
		symbol = uint32(0)
		for {
			if !(symbol < maxSV1) {
				break
			}
			freq = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(symbol)*2)))
			nbOccurrences = 0
			for {
				if !(nbOccurrences < freq) {
					break
				}
				*(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(position1))) = uint8(symbol)
				position1 = (position1 + step) & tableMask
				for position1 > highThreshold {
					position1 = (position1 + step) & tableMask
				} /* Low proba area */
				goto _9
			_9:
				;
				nbOccurrences = nbOccurrences + 1
			}
			goto _8
		_8:
			;
			symbol = symbol + 1
		}
		/* Must have initialized all positions */
	}
	/* Build table */
	u2 = uint32(0)
	for {
		if !(u2 < tableSize) {
			break
		}
		s2 = *(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(u2))) /* note : static analyzer may not understand tableSymbol is properly initialized */
		v12 = cumul + uintptr(s2)*2
		v11 = *(*U16)(unsafe.Pointer(v12))
		*(*U16)(unsafe.Pointer(v12)) = *(*U16)(unsafe.Pointer(v12)) + 1
		*(*U16)(unsafe.Pointer(tableU16 + uintptr(v11)*2)) = uint16(tableSize + u2) /* TableU16 : sorted by symbol order; gives next state value */
		goto _10
	_10:
		;
		u2 = u2 + 1
	}
	/* Build Symbol Transformation Table */
	total = uint32(0)
	s3 = uint32(0)
	for {
		if !(s3 <= maxSymbolValue) {
			break
		}
		switch int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2))) {
		case 0:
			/* filling nonetheless, for compatibility with FSE_getMaxNbBits() */
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaNbBits = (tableLog+uint32(1))<<int32(16) - uint32(libc.Int32FromInt32(1)<<tableLog)
		case -int32(1):
			fallthrough
		case int32(1):
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaNbBits = tableLog<<int32(16) - uint32(libc.Int32FromInt32(1)<<tableLog)
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaFindState = int32(total - libc.Uint32FromInt32(1))
			total = total + 1
		default:
			maxBitsOut = tableLog - ZSTD_highbit32(tls, uint32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))-uint32(1))
			minStatePlus = uint32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2))) << maxBitsOut
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaNbBits = maxBitsOut<<libc.Int32FromInt32(16) - minStatePlus
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaFindState = int32(total - uint32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2))))
			total = total + uint32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))
		}
		goto _13
	_13:
		;
		s3 = s3 + 1
	}
	return uint64(0)
}

// C documentation
//
//	/*-**************************************************************
//	*  FSE NCount encoding
//	****************************************************************/
func FSE_NCountWriteBound(tls *libc.TLS, maxSymbolValue uint32, tableLog uint32) (r size_t) {
	var maxHeaderSize size_t
	var v1 uint64
	_, _ = maxHeaderSize, v1
	maxHeaderSize = uint64(((maxSymbolValue+uint32(1))*tableLog+uint32(4)+uint32(2))/uint32(8) + uint32(1) + uint32(2))
	if maxSymbolValue != 0 {
		v1 = maxHeaderSize
	} else {
		v1 = uint64(FSE_NCOUNTBOUND)
	}
	return v1 /* maxSymbolValue==0 ? use default */
}

func FSE_writeNCount_generic(tls *libc.TLS, header uintptr, headerBufferSize size_t, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, writeIsSafe uint32) (r size_t) {
	var alphabetSize, start, symbol, v1 uint32
	var bitCount, count, max, nbBits, previousIs0, remaining, tableSize, threshold, v2 int32
	var bitStream U32
	var oend, ostart, out uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = alphabetSize, bitCount, bitStream, count, max, nbBits, oend, ostart, out, previousIs0, remaining, start, symbol, tableSize, threshold, v1, v2
	ostart = header
	out = ostart
	oend = ostart + uintptr(headerBufferSize)
	tableSize = int32(1) << tableLog
	bitStream = uint32(0)
	bitCount = 0
	symbol = uint32(0)
	alphabetSize = maxSymbolValue + uint32(1)
	previousIs0 = 0
	/* Table Size */
	bitStream = bitStream + (tableLog-uint32(FSE_MIN_TABLELOG))<<bitCount
	bitCount = bitCount + int32(4)
	/* Init */
	remaining = tableSize + int32(1) /* +1 for extra accuracy */
	threshold = tableSize
	nbBits = int32(tableLog) + int32(1)
	for symbol < alphabetSize && remaining > int32(1) { /* stops at 1 */
		if previousIs0 != 0 {
			start = symbol
			for symbol < alphabetSize && !(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(symbol)*2)) != 0) {
				symbol = symbol + 1
			}
			if symbol == alphabetSize {
				break
			} /* incorrect distribution */
			for symbol >= start+uint32(24) {
				start = start + uint32(24)
				bitStream = bitStream + uint32(0xFFFF)<<bitCount
				if !(writeIsSafe != 0) && out > oend-uintptr(2) {
					return uint64(-int32(ZSTD_error_dstSize_tooSmall))
				} /* Buffer overflow */
				*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
				*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
				out = out + uintptr(2)
				bitStream = bitStream >> uint32(16)
			}
			for symbol >= start+uint32(3) {
				start = start + uint32(3)
				bitStream = bitStream + uint32(3)<<bitCount
				bitCount = bitCount + int32(2)
			}
			bitStream = bitStream + (symbol-start)<<bitCount
			bitCount = bitCount + int32(2)
			if bitCount > int32(16) {
				if !(writeIsSafe != 0) && out > oend-uintptr(2) {
					return uint64(-int32(ZSTD_error_dstSize_tooSmall))
				} /* Buffer overflow */
				*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
				*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
				out = out + uintptr(2)
				bitStream = bitStream >> uint32(16)
				bitCount = bitCount - int32(16)
			}
		}
		v1 = symbol
		symbol = symbol + 1
		count = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(v1)*2)))
		max = int32(2)*threshold - int32(1) - remaining
		if count < 0 {
			v2 = -count
		} else {
			v2 = count
		}
		remaining = remaining - v2
		count = count + 1 /* +1 for extra accuracy */
		if count >= threshold {
			count = count + max
		} /* [0..max[ [max..threshold[ (...) [threshold+max 2*threshold[ */
		bitStream = bitStream + uint32(count)<<bitCount
		bitCount = bitCount + nbBits
		bitCount = bitCount - libc.BoolInt32(count < max)
		previousIs0 = libc.BoolInt32(count == int32(1))
		if remaining < int32(1) {
			return uint64(-int32(ZSTD_error_GENERIC))
		}
		for remaining < threshold {
			nbBits = nbBits - 1
			threshold = threshold >> int32(1)
		}
		if bitCount > int32(16) {
			if !(writeIsSafe != 0) && out > oend-uintptr(2) {
				return uint64(-int32(ZSTD_error_dstSize_tooSmall))
			} /* Buffer overflow */
			*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
			*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
			out = out + uintptr(2)
			bitStream = bitStream >> uint32(16)
			bitCount = bitCount - int32(16)
		}
	}
	if remaining != int32(1) {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* incorrect normalized distribution */
	/* flush remaining bitStream */
	if !(writeIsSafe != 0) && out > oend-uintptr(2) {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	} /* Buffer overflow */
	*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
	*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
	out = out + uintptr((bitCount+int32(7))/int32(8))
	return uint64(int64(out) - int64(ostart))
}

func FSE_writeNCount(tls *libc.TLS, buffer uintptr, bufferSize size_t, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32) (r size_t) {
	if tableLog > uint32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	} /* Unsupported */
	if tableLog < uint32(FSE_MIN_TABLELOG) {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* Unsupported */
	if bufferSize < FSE_NCountWriteBound(tls, maxSymbolValue, tableLog) {
		return FSE_writeNCount_generic(tls, buffer, bufferSize, normalizedCounter, maxSymbolValue, tableLog, uint32(0))
	}
	return FSE_writeNCount_generic(tls, buffer, bufferSize, normalizedCounter, maxSymbolValue, tableLog, uint32(1))
}

/*-**************************************************************
*  FSE Compression Code
****************************************************************/

// C documentation
//
//	/* provides the minimum logSize to safely represent a distribution */
func FSE_minTableLog(tls *libc.TLS, srcSize size_t, maxSymbolValue uint32) (r uint32) {
	var minBits, minBitsSrc, minBitsSymbols U32
	var v1 uint32
	_, _, _, _ = minBits, minBitsSrc, minBitsSymbols, v1
	minBitsSrc = ZSTD_highbit32(tls, uint32(srcSize)) + uint32(1)
	minBitsSymbols = ZSTD_highbit32(tls, maxSymbolValue) + uint32(2)
	if minBitsSrc < minBitsSymbols {
		v1 = minBitsSrc
	} else {
		v1 = minBitsSymbols
	}
	minBits = v1
	/* Not supported, RLE should be used instead */
	return minBits
}

func FSE_optimalTableLog_internal(tls *libc.TLS, maxTableLog uint32, srcSize size_t, maxSymbolValue uint32, minus uint32) (r uint32) {
	var maxBitsSrc, minBits, tableLog U32
	_, _, _ = maxBitsSrc, minBits, tableLog
	maxBitsSrc = ZSTD_highbit32(tls, uint32(srcSize-libc.Uint64FromInt32(1))) - minus
	tableLog = maxTableLog
	minBits = FSE_minTableLog(tls, srcSize, maxSymbolValue)
	/* Not supported, RLE should be used instead */
	if tableLog == uint32(0) {
		tableLog = uint32(libc.Int32FromInt32(FSE_DEFAULT_MEMORY_USAGE) - libc.Int32FromInt32(2))
	}
	if maxBitsSrc < tableLog {
		tableLog = maxBitsSrc
	} /* Accuracy can be reduced */
	if minBits > tableLog {
		tableLog = minBits
	} /* Need a minimum to safely represent all symbol values */
	if tableLog < uint32(FSE_MIN_TABLELOG) {
		tableLog = uint32(FSE_MIN_TABLELOG)
	}
	if tableLog > uint32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		tableLog = uint32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE) - libc.Int32FromInt32(2))
	}
	return tableLog
}

func FSE_optimalTableLog(tls *libc.TLS, maxTableLog uint32, srcSize size_t, maxSymbolValue uint32) (r uint32) {
	return FSE_optimalTableLog_internal(tls, maxTableLog, srcSize, maxSymbolValue, uint32(2))
}

/* Secondary normalization method.
   To be used when primary method fails. */

func FSE_normalizeM2(tls *libc.TLS, norm uintptr, tableLog U32, count uintptr, total size_t, maxSymbolValue U32, lowProbCount int16) (r size_t) {
	var NOT_YET_ASSIGNED int16
	var ToDistribute, distributed, lowOne, lowThreshold, maxC, maxV, s, sEnd, sStart, weight U32
	var end, mid, rStep, tmpTotal, vStepLog U64
	var v4 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = NOT_YET_ASSIGNED, ToDistribute, distributed, end, lowOne, lowThreshold, maxC, maxV, mid, rStep, s, sEnd, sStart, tmpTotal, vStepLog, weight, v4
	NOT_YET_ASSIGNED = int16(-int32(2))
	distributed = uint32(0)
	/* Init */
	lowThreshold = uint32(total >> tableLog)
	lowOne = uint32(total * libc.Uint64FromInt32(3) >> (tableLog + libc.Uint32FromInt32(1)))
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) == uint32(0) {
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = 0
			goto _1
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowThreshold {
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = lowProbCount
			distributed = distributed + 1
			total = total - uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
			goto _1
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowOne {
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = int16(1)
			distributed = distributed + 1
			total = total - uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
			goto _1
		}
		*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = NOT_YET_ASSIGNED
		goto _1
	_1:
		;
		s = s + 1
	}
	ToDistribute = uint32(libc.Int32FromInt32(1)<<tableLog) - distributed
	if ToDistribute == uint32(0) {
		return uint64(0)
	}
	if total/uint64(ToDistribute) > uint64(lowOne) {
		/* risk of rounding to zero */
		lowOne = uint32(total * libc.Uint64FromInt32(3) / uint64(ToDistribute*libc.Uint32FromInt32(2)))
		s = uint32(0)
		for {
			if !(s <= maxSymbolValue) {
				break
			}
			if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) == int32(NOT_YET_ASSIGNED) && *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowOne {
				*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = int16(1)
				distributed = distributed + 1
				total = total - uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
				goto _2
			}
			goto _2
		_2:
			;
			s = s + 1
		}
		ToDistribute = uint32(libc.Int32FromInt32(1)<<tableLog) - distributed
	}
	if distributed == maxSymbolValue+uint32(1) {
		/* all values are pretty poor;
		   probably incompressible data (should have already been detected);
		   find max, then give all remaining points to max */
		maxV = uint32(0)
		maxC = uint32(0)
		s = uint32(0)
		for {
			if !(s <= maxSymbolValue) {
				break
			}
			if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) > maxC {
				maxV = s
				maxC = *(*uint32)(unsafe.Pointer(count + uintptr(s)*4))
			}
			goto _3
		_3:
			;
			s = s + 1
		}
		v4 = norm + uintptr(maxV)*2
		*(*int16)(unsafe.Pointer(v4)) = int16(int32(*(*int16)(unsafe.Pointer(v4))) + int32(int16(ToDistribute)))
		return uint64(0)
	}
	if total == uint64(0) {
		/* all of the symbols were low enough for the lowOne or lowThreshold */
		s = uint32(0)
		for {
			if !(ToDistribute > uint32(0)) {
				break
			}
			if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) > 0 {
				ToDistribute = ToDistribute - 1
				*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = *(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) + 1
			}
			goto _5
		_5:
			;
			s = (s + uint32(1)) % (maxSymbolValue + uint32(1))
		}
		return uint64(0)
	}
	vStepLog = uint64(uint32(62) - tableLog)
	mid = uint64(1)<<(vStepLog-uint64(1)) - uint64(1)
	rStep = (libc.Uint64FromInt32(1)<<vStepLog*uint64(ToDistribute) + mid) / uint64(uint32(total)) /* scale on remaining */
	tmpTotal = mid
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) == int32(NOT_YET_ASSIGNED) {
			end = tmpTotal + uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))*rStep
			sStart = uint32(tmpTotal >> vStepLog)
			sEnd = uint32(end >> vStepLog)
			weight = sEnd - sStart
			if weight < uint32(1) {
				return uint64(-int32(ZSTD_error_GENERIC))
			}
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = int16(weight)
			tmpTotal = end
		}
		goto _6
	_6:
		;
		s = s + 1
	}
	return uint64(0)
}

func FSE_normalizeCount(tls *libc.TLS, normalizedCounter uintptr, tableLog uint32, count uintptr, total size_t, maxSymbolValue uint32, useLowProbCount uint32) (r size_t) {
	var errorCode size_t
	var largest, s uint32
	var largestP, lowProbCount, proba int16
	var lowThreshold U32
	var restToBeat, scale, step, vStep U64
	var stillToDistribute, v1 int32
	var v3 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = errorCode, largest, largestP, lowProbCount, lowThreshold, proba, restToBeat, s, scale, step, stillToDistribute, vStep, v1, v3
	/* Sanity checks */
	if tableLog == uint32(0) {
		tableLog = uint32(libc.Int32FromInt32(FSE_DEFAULT_MEMORY_USAGE) - libc.Int32FromInt32(2))
	}
	if tableLog < uint32(FSE_MIN_TABLELOG) {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* Unsupported size */
	if tableLog > uint32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	} /* Unsupported size */
	if tableLog < FSE_minTableLog(tls, total, maxSymbolValue) {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* Too small tableLog, compression potentially impossible */
	if useLowProbCount != 0 {
		v1 = -int32(1)
	} else {
		v1 = int32(1)
	}
	lowProbCount = int16(v1)
	scale = uint64(uint32(62) - tableLog)
	step = libc.Uint64FromInt32(1) << libc.Int32FromInt32(62) / uint64(uint32(total)) /* <== here, one division ! */
	vStep = uint64(1) << (scale - uint64(20))
	stillToDistribute = int32(1) << tableLog
	largest = uint32(0)
	largestP = 0
	lowThreshold = uint32(total >> tableLog)
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4))) == total {
			return uint64(0)
		} /* rle special case */
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) == uint32(0) {
			*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)) = 0
			goto _2
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowThreshold {
			*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)) = lowProbCount
			stillToDistribute = stillToDistribute - 1
		} else {
			proba = int16(uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4))) * step >> scale)
			if int32(proba) < int32(8) {
				restToBeat = vStep * uint64(rtbTable[proba])
				proba = int16(int32(proba) + libc.BoolInt32(uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))*step-uint64(proba)<<scale > restToBeat))
			}
			if int32(proba) > int32(largestP) {
				largestP = proba
				largest = s
			}
			*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)) = proba
			stillToDistribute = stillToDistribute - int32(proba)
		}
		goto _2
	_2:
		;
		s = s + 1
	}
	if -stillToDistribute >= int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(largest)*2)))>>int32(1) {
		/* corner case, need another normalization method */
		errorCode = FSE_normalizeM2(tls, normalizedCounter, tableLog, count, total, maxSymbolValue, lowProbCount)
		if ERR_isError(tls, errorCode) != 0 {
			return errorCode
		}
	} else {
		v3 = normalizedCounter + uintptr(largest)*2
		*(*int16)(unsafe.Pointer(v3)) = int16(int32(*(*int16)(unsafe.Pointer(v3))) + int32(int16(stillToDistribute)))
	}
	return uint64(tableLog)
}

var rtbTable = [8]U32{
	1: uint32(473195),
	2: uint32(504333),
	3: uint32(520860),
	4: uint32(550000),
	5: uint32(700000),
	6: uint32(750000),
	7: uint32(830000),
}

// C documentation
//
//	/* fake FSE_CTable, for rle input (always same symbol) */
func FSE_buildCTable_rle(tls *libc.TLS, ct uintptr, symbolValue BYTE) (r size_t) {
	var FSCTptr, ptr, symbolTT, tableU16 uintptr
	_, _, _, _ = FSCTptr, ptr, symbolTT, tableU16
	ptr = ct
	tableU16 = ptr + uintptr(2)*2
	FSCTptr = ptr + uintptr(2)*4
	symbolTT = FSCTptr
	/* header */
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(2))*2)) = libc.Uint16FromInt32(0)
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(1))*2)) = uint16(symbolValue)
	/* Build table */
	*(*U16)(unsafe.Pointer(tableU16)) = uint16(0)
	*(*U16)(unsafe.Pointer(tableU16 + 1*2)) = uint16(0) /* just in case */
	/* Build Symbol Transformation Table */
	(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits = uint32(0)
	(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaFindState = 0
	return uint64(0)
}

func FSE_compress_usingCTable_generic(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, ct uintptr, fast uint32) (r size_t) {
	bp := tls.Alloc(112)
	defer tls.Free(112)
	var iend, ip, istart, v1 uintptr
	var initError size_t
	var _ /* CState1 at bp+40 */ FSE_CState_t
	var _ /* CState2 at bp+72 */ FSE_CState_t
	var _ /* bitC at bp+0 */ BIT_CStream_t
	_, _, _, _, _ = iend, initError, ip, istart, v1
	istart = src
	iend = istart + uintptr(srcSize)
	ip = iend
	/* init */
	if srcSize <= uint64(2) {
		return uint64(0)
	}
	initError = BIT_initCStream(tls, bp, dst, dstSize)
	if ERR_isError(tls, initError) != 0 {
		return uint64(0)
	} /* not enough space available to write a bitstream */
	if srcSize&uint64(1) != 0 {
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+40, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+72, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if fast != 0 {
			BIT_flushBitsFast(tls, bp)
		} else {
			BIT_flushBits(tls, bp)
		}
	} else {
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+72, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+40, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
	}
	/* join to mod 4 */
	srcSize = srcSize - uint64(2)
	if libc.Bool(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) > uint64((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(4)+libc.Int32FromInt32(7))) && srcSize&uint64(2) != 0 { /* test bit 2 */
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+72, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if fast != 0 {
			BIT_flushBitsFast(tls, bp)
		} else {
			BIT_flushBits(tls, bp)
		}
	}
	/* 2 or 4 encoding per loop */
	for ip > istart {
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+72, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) < uint64((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(2)+libc.Int32FromInt32(7)) { /* this test must be static */
			if fast != 0 {
				BIT_flushBitsFast(tls, bp)
			} else {
				BIT_flushBits(tls, bp)
			}
		}
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) > uint64((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(4)+libc.Int32FromInt32(7)) { /* this test must be static */
			ip = ip - 1
			v1 = ip
			FSE_encodeSymbol(tls, bp, bp+72, uint32(*(*BYTE)(unsafe.Pointer(v1))))
			ip = ip - 1
			v1 = ip
			FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		}
		if fast != 0 {
			BIT_flushBitsFast(tls, bp)
		} else {
			BIT_flushBits(tls, bp)
		}
	}
	FSE_flushCState(tls, bp, bp+72)
	FSE_flushCState(tls, bp, bp+40)
	return BIT_closeCStream(tls, bp)
}

func FSE_compress_usingCTable(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, ct uintptr) (r size_t) {
	var fast uint32
	_ = fast
	fast = libc.BoolUint32(dstSize >= srcSize+srcSize>>libc.Int32FromInt32(7)+libc.Uint64FromInt32(4)+libc.Uint64FromInt64(8))
	if fast != 0 {
		return FSE_compress_usingCTable_generic(tls, dst, dstSize, src, srcSize, ct, uint32(1))
	} else {
		return FSE_compress_usingCTable_generic(tls, dst, dstSize, src, srcSize, ct, uint32(0))
	}
	return r
}

func FSE_compressBound(tls *libc.TLS, size size_t) (r size_t) {
	return libc.Uint64FromInt32(FSE_NCOUNTBOUND) + (size + size>>libc.Int32FromInt32(7) + libc.Uint64FromInt32(4) + libc.Uint64FromInt64(8))
}

/**** ended inlining compress/fse_compress.c ****/
/**** start inlining compress/hist.c ****/
/* ******************************************************************
 * hist : Histogram functions
 * part of Finite State Entropy project
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* --- dependencies --- */
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/debug.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: hist.h ****/

// C documentation
//
//	/* --- Error management --- */
func HIST_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

// C documentation
//
//	/*-**************************************************************
//	 *  Histogram functions
//	 ****************************************************************/
func HIST_add(tls *libc.TLS, count uintptr, src uintptr, srcSize size_t) {
	var end, ip, v1 uintptr
	_, _, _ = end, ip, v1
	ip = src
	end = ip + uintptr(srcSize)
	for ip < end {
		v1 = ip
		ip = ip + 1
		*(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) = *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) + 1
	}
}

func HIST_count_simple(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, src uintptr, srcSize size_t) (r uint32) {
	var end, ip, v1 uintptr
	var largestCount, maxSymbolValue uint32
	var s U32
	_, _, _, _, _, _ = end, ip, largestCount, maxSymbolValue, s, v1
	ip = src
	end = ip + uintptr(srcSize)
	maxSymbolValue = *(*uint32)(unsafe.Pointer(maxSymbolValuePtr))
	largestCount = uint32(0)
	libc.Xmemset(tls, count, 0, uint64(maxSymbolValue+libc.Uint32FromInt32(1))*libc.Uint64FromInt64(4))
	if srcSize == uint64(0) {
		*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = uint32(0)
		return uint32(0)
	}
	for ip < end {
		v1 = ip
		ip = ip + 1
		*(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) = *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) + 1
	}
	for !(*(*uint32)(unsafe.Pointer(count + uintptr(maxSymbolValue)*4)) != 0) {
		maxSymbolValue = maxSymbolValue - 1
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = maxSymbolValue
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) > largestCount {
			largestCount = *(*uint32)(unsafe.Pointer(count + uintptr(s)*4))
		}
		goto _2
	_2:
		;
		s = s + 1
	}
	return largestCount
}

type HIST_checkInput_e = int32

const trustInput = 0
const checkMaxSymbolValue = 1

// C documentation
//
//	/* HIST_count_parallel_wksp() :
//	 * store histogram into 4 intermediate tables, recombined at the end.
//	 * this design makes better use of OoO cpus,
//	 * and is noticeably faster when some values are heavily repeated.
//	 * But it needs some additional workspace for intermediate tables.
//	 * `workSpace` must be a U32 table of size >= HIST_WKSP_SIZE_U32.
//	 * @return : largest histogram frequency,
//	 *           or an error code (notably when histogram's alphabet is larger than *maxSymbolValuePtr) */
func HIST_count_parallel_wksp(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t, check HIST_checkInput_e, workSpace uintptr) (r size_t) {
	var Counting1, Counting2, Counting3, Counting4, iend, ip, v1 uintptr
	var c, cached, s U32
	var countSize size_t
	var max, maxSymbolValue uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = Counting1, Counting2, Counting3, Counting4, c, cached, countSize, iend, ip, max, maxSymbolValue, s, v1
	ip = source
	iend = ip + uintptr(sourceSize)
	countSize = uint64(*(*uint32)(unsafe.Pointer(maxSymbolValuePtr))+libc.Uint32FromInt32(1)) * uint64(4)
	max = uint32(0)
	Counting1 = workSpace
	Counting2 = Counting1 + uintptr(256)*4
	Counting3 = Counting2 + uintptr(256)*4
	Counting4 = Counting3 + uintptr(256)*4
	/* safety checks */
	if !(sourceSize != 0) {
		libc.Xmemset(tls, count, 0, countSize)
		*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = uint32(0)
		return uint64(0)
	}
	libc.Xmemset(tls, workSpace, 0, uint64(libc.Int32FromInt32(4)*libc.Int32FromInt32(256))*libc.Uint64FromInt64(4))
	/* by stripes of 16 bytes */
	cached = MEM_read32(tls, ip)
	ip = ip + uintptr(4)
	for ip < iend-uintptr(15) {
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
	}
	ip = ip - uintptr(4)
	/* finish last symbols */
	for ip < iend {
		v1 = ip
		ip = ip + 1
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) + 1
	}
	s = uint32(0)
	for {
		if !(s < uint32(256)) {
			break
		}
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(s)*4)) += *(*U32)(unsafe.Pointer(Counting2 + uintptr(s)*4)) + *(*U32)(unsafe.Pointer(Counting3 + uintptr(s)*4)) + *(*U32)(unsafe.Pointer(Counting4 + uintptr(s)*4))
		if *(*U32)(unsafe.Pointer(Counting1 + uintptr(s)*4)) > max {
			max = *(*U32)(unsafe.Pointer(Counting1 + uintptr(s)*4))
		}
		goto _2
	_2:
		;
		s = s + 1
	}
	maxSymbolValue = uint32(255)
	for !(*(*U32)(unsafe.Pointer(Counting1 + uintptr(maxSymbolValue)*4)) != 0) {
		maxSymbolValue = maxSymbolValue - 1
	}
	if check != 0 && maxSymbolValue > *(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooSmall))
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = maxSymbolValue
	libc.Xmemmove(tls, count, Counting1, countSize) /* in case count & Counting1 are overlapping */
	return uint64(max)
}

// C documentation
//
//	/* HIST_countFast_wksp() :
//	 * Same as HIST_countFast(), but using an externally provided scratch buffer.
//	 * `workSpace` is a writable buffer which must be 4-bytes aligned,
//	 * `workSpaceSize` must be >= HIST_WKSP_SIZE
//	 */
func HIST_countFast_wksp(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t, workSpace uintptr, workSpaceSize size_t) (r size_t) {
	if sourceSize < uint64(1500) { /* heuristic threshold */
		return uint64(HIST_count_simple(tls, count, maxSymbolValuePtr, source, sourceSize))
	}
	if uint64(workSpace)&uint64(3) != 0 {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* must be aligned on 4-bytes boundaries */
	if workSpaceSize < libc.Uint64FromInt32(HIST_WKSP_SIZE_U32)*libc.Uint64FromInt64(4) {
		return uint64(-int32(ZSTD_error_workSpace_tooSmall))
	}
	return HIST_count_parallel_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, int32(trustInput), workSpace)
}

// C documentation
//
//	/* HIST_count_wksp() :
//	 * Same as HIST_count(), but using an externally provided scratch buffer.
//	 * `workSpace` size must be table of >= HIST_WKSP_SIZE_U32 unsigned */
func HIST_count_wksp(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t, workSpace uintptr, workSpaceSize size_t) (r size_t) {
	if uint64(workSpace)&uint64(3) != 0 {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* must be aligned on 4-bytes boundaries */
	if workSpaceSize < libc.Uint64FromInt32(HIST_WKSP_SIZE_U32)*libc.Uint64FromInt64(4) {
		return uint64(-int32(ZSTD_error_workSpace_tooSmall))
	}
	if *(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) < uint32(255) {
		return HIST_count_parallel_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, int32(checkMaxSymbolValue), workSpace)
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = uint32(255)
	return HIST_countFast_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, workSpace, workSpaceSize)
}

// C documentation
//
//	/* fast variant (unsafe : won't check if src contains values beyond count[] limit) */
func HIST_countFast(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t) (r size_t) {
	bp := tls.Alloc(4096)
	defer tls.Free(4096)
	var _ /* tmpCounters at bp+0 */ [1024]uint32
	return HIST_countFast_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, bp, uint64(4096))
}

func HIST_count(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(4096)
	defer tls.Free(4096)
	var _ /* tmpCounters at bp+0 */ [1024]uint32
	return HIST_count_wksp(tls, count, maxSymbolValuePtr, src, srcSize, bp, uint64(4096))
}

/**** ended inlining compress/hist.c ****/
/**** start inlining compress/huf_compress.c ****/
/* ******************************************************************
 * Huffman encoder, part of New Generation Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Compiler specifics
****************************************************************/

/* **************************************************************
*  Includes
****************************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  Required declarations
//	****************************************************************/
type nodeElt = struct {
	Fcount  U32
	Fparent U16
	Fbyte1  BYTE
	FnbBits BYTE
}

/**** ended inlining compress/hist.c ****/
/**** start inlining compress/huf_compress.c ****/
/* ******************************************************************
 * Huffman encoder, part of New Generation Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Compiler specifics
****************************************************************/

/* **************************************************************
*  Includes
****************************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  Required declarations
//	****************************************************************/
type nodeElt_s = nodeElt

/* **************************************************************
*  Debug Traces
****************************************************************/

/* *******************************************************
*  HUF : Huffman block compression
*********************************************************/

func HUF_alignUpWorkspace(tls *libc.TLS, workspace uintptr, workspaceSizePtr uintptr, align size_t) (r uintptr) {
	var add, mask, rem size_t
	var aligned uintptr
	_, _, _, _ = add, aligned, mask, rem
	mask = align - uint64(1)
	rem = uint64(workspace) & mask
	add = (align - rem) & mask
	aligned = workspace + uintptr(add)
	/* pow 2 */
	if *(*size_t)(unsafe.Pointer(workspaceSizePtr)) >= add {
		*(*size_t)(unsafe.Pointer(workspaceSizePtr)) -= add
		return aligned
	} else {
		*(*size_t)(unsafe.Pointer(workspaceSizePtr)) = uint64(0)
		return libc.UintptrFromInt32(0)
	}
	return r
}

/* HUF_compressWeights() :
 * Same as FSE_compress(), but dedicated to huff0's weights compression.
 * The use case needs much less stack memory.
 * Note : all elements within weightTable are supposed to be <= HUF_TABLELOG_MAX.
 */

type HUF_CompressWeightsWksp = struct {
	FCTable        [59]FSE_CTable
	FscratchBuffer [41]U32
	Fcount         [13]uint32
	Fnorm          [13]S16
}

func HUF_compressWeights(tls *libc.TLS, dst uintptr, dstSize size_t, weightTable uintptr, wtSize size_t, workspace uintptr, _workspaceSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*size_t)(unsafe.Pointer(bp)) = _workspaceSize
	var _var_err__, _var_err__1, cSize, hSize size_t
	var maxCount uint32
	var oend, op, ostart, wksp uintptr
	var tableLog U32
	var _ /* maxSymbolValue at bp+8 */ uint32
	_, _, _, _, _, _, _, _, _, _ = _var_err__, _var_err__1, cSize, hSize, maxCount, oend, op, ostart, tableLog, wksp
	ostart = dst
	op = ostart
	oend = ostart + uintptr(dstSize)
	*(*uint32)(unsafe.Pointer(bp + 8)) = uint32(HUF_TABLELOG_MAX)
	tableLog = uint32(MAX_FSE_TABLELOG_FOR_HUFF_HEADER)
	wksp = HUF_alignUpWorkspace(tls, workspace, bp, uint64(4))
	if *(*size_t)(unsafe.Pointer(bp)) < uint64(480) {
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	/* init conditions */
	if wtSize <= uint64(1) {
		return uint64(0)
	} /* Not compressible */
	/* Scan input and build symbol stats */
	maxCount = HIST_count_simple(tls, wksp+400, bp+8, weightTable, wtSize) /* never fails */
	if uint64(maxCount) == wtSize {
		return uint64(1)
	} /* only a single symbol in src : rle */
	if maxCount == uint32(1) {
		return uint64(0)
	} /* each symbol present maximum once => not compressible */
	tableLog = FSE_optimalTableLog(tls, tableLog, wtSize, *(*uint32)(unsafe.Pointer(bp + 8)))
	_var_err__ = FSE_normalizeCount(tls, wksp+452, tableLog, wksp+400, wtSize, *(*uint32)(unsafe.Pointer(bp + 8)), uint32(0))
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	/* Write table description header */
	hSize = FSE_writeNCount(tls, op, uint64(int64(oend)-int64(op)), wksp+452, *(*uint32)(unsafe.Pointer(bp + 8)), tableLog)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	op = op + uintptr(hSize)
	/* Compress */
	_var_err__1 = FSE_buildCTable_wksp(tls, wksp, wksp+452, *(*uint32)(unsafe.Pointer(bp + 8)), tableLog, wksp+236, uint64(164))
	if ERR_isError(tls, _var_err__1) != 0 {
		return _var_err__1
	}
	cSize = FSE_compress_usingCTable(tls, op, uint64(int64(oend)-int64(op)), weightTable, wtSize, wksp)
	if ERR_isError(tls, cSize) != 0 {
		return cSize
	}
	if cSize == uint64(0) {
		return uint64(0)
	} /* not enough space for compressed data */
	op = op + uintptr(cSize)
	return uint64(int64(op) - int64(ostart))
}

func HUF_getNbBits(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt & uint64(0xFF)
}

func HUF_getNbBitsFast(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt
}

func HUF_getValue(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt & ^libc.Uint64FromInt32(0xFF)
}

func HUF_getValueFast(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt
}

func HUF_setNbBits(tls *libc.TLS, elt uintptr, nbBits size_t) {
	*(*HUF_CElt)(unsafe.Pointer(elt)) = nbBits
}

func HUF_setValue(tls *libc.TLS, elt uintptr, value size_t) {
	var nbBits size_t
	_ = nbBits
	nbBits = HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(elt)))
	if nbBits > uint64(0) {
		*(*HUF_CElt)(unsafe.Pointer(elt)) |= value << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - nbBits)
	}
}

func HUF_readCTableHeader(tls *libc.TLS, ctable uintptr) (r HUF_CTableHeader) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* header at bp+0 */ HUF_CTableHeader
	libc.Xmemcpy(tls, bp, ctable, libc.Uint64FromInt64(8))
	return *(*HUF_CTableHeader)(unsafe.Pointer(bp))
}

func HUF_writeCTableHeader(tls *libc.TLS, ctable uintptr, tableLog U32, maxSymbolValue U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* header at bp+0 */ HUF_CTableHeader
	_ = libc.Uint64FromInt64(1)
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(8))
	(*(*HUF_CTableHeader)(unsafe.Pointer(bp))).FtableLog = uint8(tableLog)
	(*(*HUF_CTableHeader)(unsafe.Pointer(bp))).FmaxSymbolValue = uint8(maxSymbolValue)
	libc.Xmemcpy(tls, ctable, bp, libc.Uint64FromInt64(8))
}

type HUF_WriteCTableWksp = struct {
	Fwksp         HUF_CompressWeightsWksp
	FbitsToWeight [13]BYTE
	FhuffWeight   [255]BYTE
}

func HUF_writeCTable_wksp(tls *libc.TLS, dst uintptr, maxDstSize size_t, CTable uintptr, maxSymbolValue uint32, huffLog uint32, workspace uintptr, _workspaceSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*size_t)(unsafe.Pointer(bp)) = _workspaceSize
	var ct, op, wksp uintptr
	var hSize size_t
	var n U32
	_, _, _, _, _ = ct, hSize, n, op, wksp
	ct = CTable + uintptr(1)*8
	op = dst
	wksp = HUF_alignUpWorkspace(tls, workspace, bp, uint64(4))
	_ = libc.Uint64FromInt64(1)
	/* check conditions */
	if *(*size_t)(unsafe.Pointer(bp)) < uint64(748) {
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	if maxSymbolValue > uint32(HUF_SYMBOLVALUE_MAX) {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	/* convert to weight */
	*(*BYTE)(unsafe.Pointer(wksp + 480)) = uint8(0)
	n = uint32(1)
	for {
		if !(n < huffLog+uint32(1)) {
			break
		}
		*(*BYTE)(unsafe.Pointer(wksp + 480 + uintptr(n))) = uint8(huffLog + libc.Uint32FromInt32(1) - n)
		goto _1
	_1:
		;
		n = n + 1
	}
	n = uint32(0)
	for {
		if !(n < maxSymbolValue) {
			break
		}
		*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(n))) = *(*BYTE)(unsafe.Pointer(wksp + 480 + uintptr(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n)*8))))))
		goto _2
	_2:
		;
		n = n + 1
	}
	/* attempt weights compression by FSE */
	if maxDstSize < uint64(1) {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	hSize = HUF_compressWeights(tls, op+uintptr(1), maxDstSize-uint64(1), wksp+493, uint64(maxSymbolValue), wksp, uint64(480))
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if libc.BoolInt32(hSize > uint64(1))&libc.BoolInt32(hSize < uint64(maxSymbolValue/uint32(2))) != 0 { /* FSE compressed */
		*(*BYTE)(unsafe.Pointer(op)) = uint8(hSize)
		return hSize + uint64(1)
	}
	/* write raw values as 4-bits (max : 15) */
	if maxSymbolValue > uint32(libc.Int32FromInt32(256)-libc.Int32FromInt32(128)) {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* should not happen : likely means source cannot be compressed */
	if uint64((maxSymbolValue+uint32(1))/uint32(2)+uint32(1)) > maxDstSize {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	} /* not enough space within dst buffer */
	*(*BYTE)(unsafe.Pointer(op)) = uint8(libc.Uint32FromInt32(128) + (maxSymbolValue - libc.Uint32FromInt32(1)))
	*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(maxSymbolValue))) = uint8(0) /* to be sure it doesn't cause msan issue in final combination */
	n = uint32(0)
	for {
		if !(n < maxSymbolValue) {
			break
		}
		*(*BYTE)(unsafe.Pointer(op + uintptr(n/uint32(2)+uint32(1)))) = uint8(int32(*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(n))))<<libc.Int32FromInt32(4) + int32(*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(n+uint32(1))))))
		goto _3
	_3:
		;
		n = n + uint32(2)
	}
	return uint64((maxSymbolValue+uint32(1))/uint32(2) + uint32(1))
}

func HUF_readCTable(tls *libc.TLS, CTable uintptr, maxSymbolValuePtr uintptr, src uintptr, srcSize size_t, hasZeroWeights uintptr) (r size_t) {
	bp := tls.Alloc(352)
	defer tls.Free(352)
	var ct, v7 uintptr
	var curr, n, n1, n2, n3, n4, nextRankStart, w U32
	var min, v6 U16
	var nbPerRank [14]U16
	var readSize size_t
	var _ /* huffWeight at bp+0 */ [256]BYTE
	var _ /* nbSymbols at bp+312 */ U32
	var _ /* rankVal at bp+256 */ [13]U32
	var _ /* tableLog at bp+308 */ U32
	var _ /* valPerRank at bp+316 */ [14]U16
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = ct, curr, min, n, n1, n2, n3, n4, nbPerRank, nextRankStart, readSize, w, v6, v7 /* large enough for values from 0 to 16 */
	*(*U32)(unsafe.Pointer(bp + 308)) = uint32(0)
	*(*U32)(unsafe.Pointer(bp + 312)) = uint32(0)
	ct = CTable + uintptr(1)*8
	/* get symbol weights */
	readSize = HUF_readStats(tls, bp, uint64(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)), bp+256, bp+312, bp+308, src, srcSize)
	if ERR_isError(tls, readSize) != 0 {
		return readSize
	}
	*(*uint32)(unsafe.Pointer(hasZeroWeights)) = libc.BoolUint32((*(*[13]U32)(unsafe.Pointer(bp + 256)))[0] > libc.Uint32FromInt32(0))
	/* check result */
	if *(*U32)(unsafe.Pointer(bp + 308)) > uint32(HUF_TABLELOG_MAX) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	if *(*U32)(unsafe.Pointer(bp + 312)) > *(*uint32)(unsafe.Pointer(maxSymbolValuePtr))+uint32(1) {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooSmall))
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = *(*U32)(unsafe.Pointer(bp + 312)) - uint32(1)
	HUF_writeCTableHeader(tls, CTable, *(*U32)(unsafe.Pointer(bp + 308)), *(*uint32)(unsafe.Pointer(maxSymbolValuePtr)))
	/* Prepare base value per rank */
	nextRankStart = uint32(0)
	n = uint32(1)
	for {
		if !(n <= *(*U32)(unsafe.Pointer(bp + 308))) {
			break
		}
		curr = nextRankStart
		nextRankStart = nextRankStart + (*(*[13]U32)(unsafe.Pointer(bp + 256)))[n]<<(n-libc.Uint32FromInt32(1))
		(*(*[13]U32)(unsafe.Pointer(bp + 256)))[n] = curr
		goto _1
	_1:
		;
		n = n + 1
	}
	/* fill nbBits */
	n1 = uint32(0)
	for {
		if !(n1 < *(*U32)(unsafe.Pointer(bp + 312))) {
			break
		}
		w = uint32((*(*[256]BYTE)(unsafe.Pointer(bp)))[n1])
		HUF_setNbBits(tls, ct+uintptr(n1)*8, uint64(int32(uint8(*(*U32)(unsafe.Pointer(bp + 308))+libc.Uint32FromInt32(1)-w))&-libc.BoolInt32(w != uint32(0))))
		goto _2
	_2:
		;
		n1 = n1 + 1
	}
	/* fill val */
	nbPerRank = [14]U16{} /* support w=0=>n=tableLog+1 */
	*(*[14]U16)(unsafe.Pointer(bp + 316)) = [14]U16{}
	n2 = uint32(0)
	for {
		if !(n2 < *(*U32)(unsafe.Pointer(bp + 312))) {
			break
		}
		nbPerRank[HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n2)*8)))] = nbPerRank[HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n2)*8)))] + 1
		goto _3
	_3:
		;
		n2 = n2 + 1
	}
	/* determine stating value per rank */
	(*(*[14]U16)(unsafe.Pointer(bp + 316)))[*(*U32)(unsafe.Pointer(bp + 308))+uint32(1)] = uint16(0) /* for w==0 */
	min = uint16(0)
	n3 = *(*U32)(unsafe.Pointer(bp + 308))
	for {
		if !(n3 > uint32(0)) {
			break
		} /* start at n=tablelog <-> w=1 */
		(*(*[14]U16)(unsafe.Pointer(bp + 316)))[n3] = min /* get starting value within each rank */
		min = uint16(int32(min) + int32(nbPerRank[n3]))
		min = uint16(int32(min) >> libc.Int32FromInt32(1))
		goto _4
	_4:
		;
		n3 = n3 - 1
	}
	/* assign value within rank, symbol order */
	n4 = uint32(0)
	for {
		if !(n4 < *(*U32)(unsafe.Pointer(bp + 312))) {
			break
		}
		v7 = bp + 316 + uintptr(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n4)*8))))*2
		v6 = *(*U16)(unsafe.Pointer(v7))
		*(*U16)(unsafe.Pointer(v7)) = *(*U16)(unsafe.Pointer(v7)) + 1
		HUF_setValue(tls, ct+uintptr(n4)*8, uint64(v6))
		goto _5
	_5:
		;
		n4 = n4 + 1
	}
	return readSize
}

func HUF_getNbBitsFromCTable(tls *libc.TLS, CTable uintptr, symbolValue U32) (r U32) {
	var ct uintptr
	_ = ct
	ct = CTable + uintptr(1)*8
	if symbolValue > uint32(HUF_readCTableHeader(tls, CTable).FmaxSymbolValue) {
		return uint32(0)
	}
	return uint32(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(symbolValue)*8))))
}

// C documentation
//
//	/**
//	 * HUF_setMaxHeight():
//	 * Try to enforce @targetNbBits on the Huffman tree described in @huffNode.
//	 *
//	 * It attempts to convert all nodes with nbBits > @targetNbBits
//	 * to employ @targetNbBits instead. Then it adjusts the tree
//	 * so that it remains a valid canonical Huffman tree.
//	 *
//	 * @pre               The sum of the ranks of each symbol == 2^largestBits,
//	 *                    where largestBits == huffNode[lastNonNull].nbBits.
//	 * @post              The sum of the ranks of each symbol == 2^largestBits,
//	 *                    where largestBits is the return value (expected <= targetNbBits).
//	 *
//	 * @param huffNode    The Huffman tree modified in place to enforce targetNbBits.
//	 *                    It's presumed sorted, from most frequent to rarest symbol.
//	 * @param lastNonNull The symbol with the lowest count in the Huffman tree.
//	 * @param targetNbBits  The allowed number of bits, which the Huffman tree
//	 *                    may not respect. After this function the Huffman tree will
//	 *                    respect targetNbBits.
//	 * @return            The maximum number of bits of the Huffman tree after adjustment.
//	 */
func HUF_setMaxHeight(tls *libc.TLS, huffNode uintptr, lastNonNull U32, targetNbBits U32) (r U32) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var baseCost, currentNbBits, highPos, highTotal, largestBits, lowPos, lowTotal, nBitsToDecrease, noSymbol U32
	var n, pos, totalCost int32
	var _ /* rankLast at bp+0 */ [14]U32
	_, _, _, _, _, _, _, _, _, _, _, _ = baseCost, currentNbBits, highPos, highTotal, largestBits, lowPos, lowTotal, n, nBitsToDecrease, noSymbol, pos, totalCost
	largestBits = uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lastNonNull)*8))).FnbBits)
	/* early exit : no elt > targetNbBits, so the tree is already valid. */
	if largestBits <= targetNbBits {
		return largestBits
	}
	/* there are several too large elements (at least >= 2) */
	totalCost = 0
	baseCost = uint32(int32(1) << (largestBits - targetNbBits))
	n = int32(lastNonNull)
	/* Adjust any ranks > targetNbBits to targetNbBits.
	 * Compute totalCost, which is how far the sum of the ranks is
	 * we are over 2^largestBits after adjust the offending ranks.
	 */
	for uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits) > targetNbBits {
		totalCost = int32(uint32(totalCost) + (baseCost - uint32(libc.Int32FromInt32(1)<<(largestBits-uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits)))))
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits = uint8(targetNbBits)
		n = n - 1
	}
	/* n stops at huffNode[n].nbBits <= targetNbBits */
	/* n end at index of smallest symbol using < targetNbBits */
	for uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits) == targetNbBits {
		n = n - 1
	}
	/* renorm totalCost from 2^largestBits to 2^targetNbBits
	 * note : totalCost is necessarily a multiple of baseCost */
	totalCost = int32(uint32(totalCost) >> (largestBits - targetNbBits))
	/* repay normalized cost */
	noSymbol = uint32(0xF0F0F0F0)
	/* Get pos of last (smallest = lowest cum. count) symbol per rank */
	libc.Xmemset(tls, bp, int32(0xF0), libc.Uint64FromInt64(56))
	currentNbBits = targetNbBits
	pos = n
	for {
		if !(pos >= 0) {
			break
		}
		if uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).FnbBits) >= currentNbBits {
			goto _1
		}
		currentNbBits = uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).FnbBits) /* < targetNbBits */
		(*(*[14]U32)(unsafe.Pointer(bp)))[targetNbBits-currentNbBits] = uint32(pos)
		goto _1
	_1:
		;
		pos = pos - 1
	}
	for totalCost > 0 {
		/* Try to reduce the next power of 2 above totalCost because we
		 * gain back half the rank.
		 */
		nBitsToDecrease = ZSTD_highbit32(tls, uint32(totalCost)) + uint32(1)
		for {
			if !(nBitsToDecrease > uint32(1)) {
				break
			}
			highPos = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease]
			lowPos = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease-uint32(1)]
			if highPos == noSymbol {
				goto _2
			}
			/* Decrease highPos if no symbols of lowPos or if it is
			 * not cheaper to remove 2 lowPos than highPos.
			 */
			if lowPos == noSymbol {
				break
			}
			highTotal = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(highPos)*8))).Fcount
			lowTotal = uint32(2) * (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowPos)*8))).Fcount
			if highTotal <= lowTotal {
				break
			}
			goto _2
		_2:
			;
			nBitsToDecrease = nBitsToDecrease - 1
		}
		/* only triggered when no more rank 1 symbol left => find closest one (note : there is necessarily at least one !) */
		/* HUF_MAX_TABLELOG test just to please gcc 5+; but it should not be necessary */
		for nBitsToDecrease <= uint32(HUF_TABLELOG_MAX) && (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] == noSymbol {
			nBitsToDecrease = nBitsToDecrease + 1
		}
		/* Increase the number of bits to gain back half the rank cost. */
		totalCost = totalCost - int32(1)<<(nBitsToDecrease-uint32(1))
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease])*8))).FnbBits = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease])*8))).FnbBits + 1
		/* Fix up the new rank.
		 * If the new rank was empty, this symbol is now its smallest.
		 * Otherwise, this symbol will be the largest in the new rank so no adjustment.
		 */
		if (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease-uint32(1)] == noSymbol {
			(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease-uint32(1)] = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease]
		}
		/* Fix up the old rank.
		 * If the symbol was at position 0, meaning it was the highest weight symbol in the tree,
		 * it must be the only symbol in its rank, so the old rank now has no symbols.
		 * Otherwise, since the Huffman nodes are sorted by count, the previous position is now
		 * the smallest node in the rank. If the previous position belongs to a different rank,
		 * then the rank is now empty.
		 */
		if (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] == uint32(0) { /* special case, reached largest symbol */
			(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] = noSymbol
		} else {
			(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] - 1
			if uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease])*8))).FnbBits) != targetNbBits-nBitsToDecrease {
				(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] = noSymbol
			} /* this rank is now empty */
		}
	} /* while (totalCost > 0) */
	/* If we've removed too much weight, then we have to add it back.
	 * To avoid overshooting again, we only adjust the smallest rank.
	 * We take the largest nodes from the lowest rank 0 and move them
	 * to rank 1. There's guaranteed to be enough rank 0 symbols because
	 * TODO.
	 */
	for totalCost < 0 { /* Sometimes, cost correction overshoot */
		/* special case : no rank 1 symbol (using targetNbBits-1);
		 * let's create one from largest rank 0 (using targetNbBits).
		 */
		if (*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] == noSymbol {
			for uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits) == targetNbBits {
				n = n - 1
			}
			(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n+int32(1))*8))).FnbBits = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n+int32(1))*8))).FnbBits - 1
			(*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] = uint32(n + libc.Int32FromInt32(1))
			totalCost = totalCost + 1
			continue
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)]+uint32(1))*8))).FnbBits = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)]+uint32(1))*8))).FnbBits - 1
		(*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] = (*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] + 1
		totalCost = totalCost + 1
	}
	/* repay normalized cost */
	/* there are several too large elements (at least >= 2) */
	return targetNbBits
}

type rankPos = struct {
	Fbase U16
	Fcurr U16
}

type huffNodeTable = [512]nodeElt

/* Number of buckets available for HUF_sort() */

type HUF_buildCTable_wksp_tables = struct {
	FhuffNodeTbl  huffNodeTable
	FrankPosition [192]rankPos
}

/* RANK_POSITION_DISTINCT_COUNT_CUTOFF == Cutoff point in HUF_sort() buckets for which we use log2 bucketing.
 * Strategy is to use as many buckets as possible for representing distinct
 * counts while using the remainder to represent all "large" counts.
 *
 * To satisfy this requirement for 192 buckets, we can do the following:
 * Let buckets 0-166 represent distinct counts of [0, 166]
 * Let buckets 166 to 192 represent all remaining counts up to RANK_POSITION_MAX_COUNT_LOG using log2 bucketing.
 */

// C documentation
//
//	/* Return the appropriate bucket index for a given count. See definition of
//	 * RANK_POSITION_DISTINCT_COUNT_CUTOFF for explanation of bucketing strategy.
//	 */
func HUF_getIndex(tls *libc.TLS, count U32) (r U32) {
	var v1 uint32
	_ = v1
	if count < uint32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1))+ZSTD_highbit32(tls, uint32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1))) {
		v1 = count
	} else {
		v1 = ZSTD_highbit32(tls, count) + uint32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1))
	}
	return v1
}

// C documentation
//
//	/* Helper swap function for HUF_quickSortPartition() */
func HUF_swapNodes(tls *libc.TLS, a uintptr, b uintptr) {
	var tmp nodeElt
	_ = tmp
	tmp = *(*nodeElt)(unsafe.Pointer(a))
	*(*nodeElt)(unsafe.Pointer(a)) = *(*nodeElt)(unsafe.Pointer(b))
	*(*nodeElt)(unsafe.Pointer(b)) = tmp
}

// C documentation
//
//	/* Returns 0 if the huffNode array is not sorted by descending count */
func HUF_isSorted(tls *libc.TLS, huffNode uintptr, maxSymbolValue1 U32) (r int32) {
	var i U32
	_ = i
	i = uint32(1)
	for {
		if !(i < maxSymbolValue1) {
			break
		}
		if (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(i)*8))).Fcount > (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(i-uint32(1))*8))).Fcount {
			return 0
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return int32(1)
}

// C documentation
//
//	/* Insertion sort by descending order */
func HUF_insertionSort(tls *libc.TLS, huffNode uintptr, low int32, high int32) {
	var i, j, size int32
	var key nodeElt
	_, _, _, _ = i, j, key, size
	size = high - low + int32(1)
	huffNode = huffNode + uintptr(low)*8
	i = int32(1)
	for {
		if !(i < size) {
			break
		}
		key = *(*nodeElt)(unsafe.Pointer(huffNode + uintptr(i)*8))
		j = i - int32(1)
		for j >= 0 && (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j)*8))).Fcount < key.Fcount {
			*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j+int32(1))*8)) = *(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j)*8))
			j = j - 1
		}
		*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j+int32(1))*8)) = key
		goto _1
	_1:
		;
		i = i + 1
	}
}

// C documentation
//
//	/* Pivot helper function for quicksort. */
func HUF_quickSortPartition(tls *libc.TLS, arr uintptr, low int32, high int32) (r int32) {
	var i, j int32
	var pivot U32
	_, _, _ = i, j, pivot
	/* Simply select rightmost element as pivot. "Better" selectors like
	 * median-of-three don't experimentally appear to have any benefit.
	 */
	pivot = (*(*nodeElt)(unsafe.Pointer(arr + uintptr(high)*8))).Fcount
	i = low - int32(1)
	j = low
	for {
		if !(j < high) {
			break
		}
		if (*(*nodeElt)(unsafe.Pointer(arr + uintptr(j)*8))).Fcount > pivot {
			i = i + 1
			HUF_swapNodes(tls, arr+uintptr(i)*8, arr+uintptr(j)*8)
		}
		goto _1
	_1:
		;
		j = j + 1
	}
	HUF_swapNodes(tls, arr+uintptr(i+int32(1))*8, arr+uintptr(high)*8)
	return i + int32(1)
}

// C documentation
//
//	/* Classic quicksort by descending with partially iterative calls
//	 * to reduce worst case callstack size.
//	 */
func HUF_simpleQuickSort(tls *libc.TLS, arr uintptr, low int32, high int32) {
	var idx, kInsertionSortThreshold int32
	_, _ = idx, kInsertionSortThreshold
	kInsertionSortThreshold = int32(8)
	if high-low < kInsertionSortThreshold {
		HUF_insertionSort(tls, arr, low, high)
		return
	}
	for low < high {
		idx = HUF_quickSortPartition(tls, arr, low, high)
		if idx-low < high-idx {
			HUF_simpleQuickSort(tls, arr, low, idx-int32(1))
			low = idx + int32(1)
		} else {
			HUF_simpleQuickSort(tls, arr, idx+int32(1), high)
			high = idx - int32(1)
		}
	}
}

// C documentation
//
//	/**
//	 * HUF_sort():
//	 * Sorts the symbols [0, maxSymbolValue] by count[symbol] in decreasing order.
//	 * This is a typical bucket sorting strategy that uses either quicksort or insertion sort to sort each bucket.
//	 *
//	 * @param[out] huffNode       Sorted symbols by decreasing count. Only members `.count` and `.byte` are filled.
//	 *                            Must have (maxSymbolValue + 1) entries.
//	 * @param[in]  count          Histogram of the symbols.
//	 * @param[in]  maxSymbolValue Maximum symbol value.
//	 * @param      rankPosition   This is a scratch workspace. Must have RANK_POSITION_TABLE_SIZE entries.
//	 */
func HUF_sort(tls *libc.TLS, huffNode uintptr, count uintptr, maxSymbolValue U32, rankPosition uintptr) {
	var bucketSize int32
	var bucketStartIdx, c, lowerRank, maxSymbolValue1, n, pos, r U32
	var v3 uintptr
	var v5 U16
	_, _, _, _, _, _, _, _, _, _ = bucketSize, bucketStartIdx, c, lowerRank, maxSymbolValue1, n, pos, r, v3, v5
	maxSymbolValue1 = maxSymbolValue + uint32(1)
	/* Compute base and set curr to base.
	 * For symbol s let lowerRank = HUF_getIndex(count[n]) and rank = lowerRank + 1.
	 * See HUF_getIndex to see bucketing strategy.
	 * We attribute each symbol to lowerRank's base value, because we want to know where
	 * each rank begins in the output, so for rank R we want to count ranks R+1 and above.
	 */
	libc.Xmemset(tls, rankPosition, 0, libc.Uint64FromInt64(4)*libc.Uint64FromInt32(RANK_POSITION_TABLE_SIZE))
	n = uint32(0)
	for {
		if !(n < maxSymbolValue1) {
			break
		}
		lowerRank = HUF_getIndex(tls, *(*uint32)(unsafe.Pointer(count + uintptr(n)*4)))
		(*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(lowerRank)*4))).Fbase = (*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(lowerRank)*4))).Fbase + 1
		goto _1
	_1:
		;
		n = n + 1
	}
	/* Set up the rankPosition table */
	n = uint32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE) - libc.Int32FromInt32(1))
	for {
		if !(n > uint32(0)) {
			break
		}
		v3 = rankPosition + uintptr(n-uint32(1))*4
		*(*U16)(unsafe.Pointer(v3)) = U16(int32(*(*U16)(unsafe.Pointer(v3))) + int32((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fbase))
		(*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n-uint32(1))*4))).Fcurr = (*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n-uint32(1))*4))).Fbase
		goto _2
	_2:
		;
		n = n - 1
	}
	/* Insert each symbol into their appropriate bucket, setting up rankPosition table. */
	n = uint32(0)
	for {
		if !(n < maxSymbolValue1) {
			break
		}
		c = *(*uint32)(unsafe.Pointer(count + uintptr(n)*4))
		r = HUF_getIndex(tls, c) + uint32(1)
		v3 = rankPosition + uintptr(r)*4 + 2
		v5 = *(*U16)(unsafe.Pointer(v3))
		*(*U16)(unsafe.Pointer(v3)) = *(*U16)(unsafe.Pointer(v3)) + 1
		pos = uint32(v5)
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).Fcount = c
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).Fbyte1 = uint8(n)
		goto _4
	_4:
		;
		n = n + 1
	}
	/* Sort each bucket. */
	n = uint32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1)) + ZSTD_highbit32(tls, uint32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1)))
	for {
		if !(n < uint32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1))) {
			break
		}
		bucketSize = int32((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fcurr) - int32((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fbase)
		bucketStartIdx = uint32((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fbase)
		if bucketSize > int32(1) {
			HUF_simpleQuickSort(tls, huffNode+uintptr(bucketStartIdx)*8, 0, bucketSize-int32(1))
		}
		goto _7
	_7:
		;
		n = n + 1
	}
}

/** HUF_buildCTable_wksp() :
 *  Same as HUF_buildCTable(), but using externally allocated scratch buffer.
 *  `workSpace` must be aligned on 4-bytes boundaries, and be at least as large as sizeof(HUF_buildCTable_wksp_tables).
 */

// C documentation
//
//	/* HUF_buildTree():
//	 * Takes the huffNode array sorted by HUF_sort() and builds an unlimited-depth Huffman tree.
//	 *
//	 * @param huffNode        The array sorted by HUF_sort(). Builds the Huffman tree in this array.
//	 * @param maxSymbolValue  The maximum symbol value.
//	 * @return                The smallest node in the Huffman tree (by count).
//	 */
func HUF_buildTree(tls *libc.TLS, huffNode uintptr, maxSymbolValue U32) (r int32) {
	var huffNode0 uintptr
	var lowN, lowS, n, n1, n2, nodeNb, nodeRoot, nonNullRank, v3, v4, v5, v6, v7, v8 int32
	var v1 U16
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = huffNode0, lowN, lowS, n, n1, n2, nodeNb, nodeRoot, nonNullRank, v1, v3, v4, v5, v6, v7, v8
	huffNode0 = huffNode - uintptr(1)*8
	nodeNb = libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX) + libc.Int32FromInt32(1)
	/* init for parents */
	nonNullRank = int32(maxSymbolValue)
	for (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nonNullRank)*8))).Fcount == uint32(0) {
		nonNullRank = nonNullRank - 1
	}
	lowS = nonNullRank
	nodeRoot = nodeNb + lowS - int32(1)
	lowN = nodeNb
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nodeNb)*8))).Fcount = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fcount + (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS-int32(1))*8))).Fcount
	v1 = uint16(nodeNb)
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS-int32(1))*8))).Fparent = v1
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fparent = v1
	nodeNb = nodeNb + 1
	lowS = lowS - int32(2)
	n = nodeNb
	for {
		if !(n <= nodeRoot) {
			break
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fcount = libc.Uint32FromUint32(1) << libc.Int32FromInt32(30)
		goto _2
	_2:
		;
		n = n + 1
	}
	(*(*nodeElt)(unsafe.Pointer(huffNode0))).Fcount = libc.Uint32FromUint32(1) << libc.Int32FromInt32(31) /* fake entry, strong barrier */
	/* create parents */
	for nodeNb <= nodeRoot {
		if (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fcount < (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowN)*8))).Fcount {
			v4 = lowS
			lowS = lowS - 1
			v3 = v4
		} else {
			v5 = lowN
			lowN = lowN + 1
			v3 = v5
		}
		n1 = v3
		if (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fcount < (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowN)*8))).Fcount {
			v7 = lowS
			lowS = lowS - 1
			v6 = v7
		} else {
			v8 = lowN
			lowN = lowN + 1
			v6 = v8
		}
		n2 = v6
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nodeNb)*8))).Fcount = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n1)*8))).Fcount + (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n2)*8))).Fcount
		v1 = uint16(nodeNb)
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n2)*8))).Fparent = v1
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n1)*8))).Fparent = v1
		nodeNb = nodeNb + 1
	}
	/* distribute weights (unlimited tree height) */
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nodeRoot)*8))).FnbBits = uint8(0)
	n = nodeRoot - int32(1)
	for {
		if !(n >= libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)) {
			break
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits = uint8(int32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fparent)*8))).FnbBits) + int32(1))
		goto _10
	_10:
		;
		n = n - 1
	}
	n = 0
	for {
		if !(n <= nonNullRank) {
			break
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits = uint8(int32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fparent)*8))).FnbBits) + int32(1))
		goto _11
	_11:
		;
		n = n + 1
	}
	return nonNullRank
}

// C documentation
//
//	/**
//	 * HUF_buildCTableFromTree():
//	 * Build the CTable given the Huffman tree in huffNode.
//	 *
//	 * @param[out] CTable         The output Huffman CTable.
//	 * @param      huffNode       The Huffman tree.
//	 * @param      nonNullRank    The last and smallest node in the Huffman tree.
//	 * @param      maxSymbolValue The maximum symbol value.
//	 * @param      maxNbBits      The exact maximum number of bits used in the Huffman tree.
//	 */
func HUF_buildCTableFromTree(tls *libc.TLS, CTable uintptr, huffNode uintptr, nonNullRank int32, maxSymbolValue U32, maxNbBits U32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var alphabetSize, n int32
	var ct, v6 uintptr
	var min, v5 U16
	var nbPerRank [13]U16
	var _ /* valPerRank at bp+0 */ [13]U16
	_, _, _, _, _, _, _ = alphabetSize, ct, min, n, nbPerRank, v5, v6
	ct = CTable + uintptr(1)*8
	nbPerRank = [13]U16{}
	*(*[13]U16)(unsafe.Pointer(bp)) = [13]U16{}
	alphabetSize = int32(maxSymbolValue + libc.Uint32FromInt32(1))
	n = 0
	for {
		if !(n <= nonNullRank) {
			break
		}
		nbPerRank[(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits] = nbPerRank[(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits] + 1
		goto _1
	_1:
		;
		n = n + 1
	}
	/* determine starting value per rank */
	min = uint16(0)
	n = int32(maxNbBits)
	for {
		if !(n > 0) {
			break
		}
		(*(*[13]U16)(unsafe.Pointer(bp)))[n] = min /* get starting value within each rank */
		min = uint16(int32(min) + int32(nbPerRank[n]))
		min = uint16(int32(min) >> libc.Int32FromInt32(1))
		goto _2
	_2:
		;
		n = n - 1
	}
	n = 0
	for {
		if !(n < alphabetSize) {
			break
		}
		HUF_setNbBits(tls, ct+uintptr((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fbyte1)*8, uint64((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits))
		goto _3
	_3:
		;
		n = n + 1
	} /* push nbBits per symbol, symbol order */
	n = 0
	for {
		if !(n < alphabetSize) {
			break
		}
		v6 = bp + uintptr(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n)*8))))*2
		v5 = *(*U16)(unsafe.Pointer(v6))
		*(*U16)(unsafe.Pointer(v6)) = *(*U16)(unsafe.Pointer(v6)) + 1
		HUF_setValue(tls, ct+uintptr(n)*8, uint64(v5))
		goto _4
	_4:
		;
		n = n + 1
	} /* assign value within rank, symbol order */
	HUF_writeCTableHeader(tls, CTable, maxNbBits, maxSymbolValue)
}

func HUF_buildCTable_wksp(tls *libc.TLS, CTable uintptr, count uintptr, maxSymbolValue U32, maxNbBits U32, workSpace uintptr, _wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*size_t)(unsafe.Pointer(bp)) = _wkspSize
	var huffNode, huffNode0, wksp_tables uintptr
	var nonNullRank int32
	_, _, _, _ = huffNode, huffNode0, nonNullRank, wksp_tables
	wksp_tables = HUF_alignUpWorkspace(tls, workSpace, bp, uint64(4))
	huffNode0 = wksp_tables
	huffNode = huffNode0 + uintptr(1)*8
	_ = libc.Uint64FromInt64(1)
	/* safety checks */
	if *(*size_t)(unsafe.Pointer(bp)) < uint64(4864) {
		return uint64(-int32(ZSTD_error_workSpace_tooSmall))
	}
	if maxNbBits == uint32(0) {
		maxNbBits = uint32(HUF_TABLELOG_DEFAULT)
	}
	if maxSymbolValue > uint32(HUF_SYMBOLVALUE_MAX) {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	libc.Xmemset(tls, huffNode0, 0, libc.Uint64FromInt64(4096))
	/* sort, decreasing order */
	HUF_sort(tls, huffNode, count, maxSymbolValue, wksp_tables+4096)
	/* build tree */
	nonNullRank = HUF_buildTree(tls, huffNode, maxSymbolValue)
	/* determine and enforce maxTableLog */
	maxNbBits = HUF_setMaxHeight(tls, huffNode, uint32(nonNullRank), maxNbBits)
	if maxNbBits > uint32(HUF_TABLELOG_MAX) {
		return uint64(-int32(ZSTD_error_GENERIC))
	} /* check fit into table */
	HUF_buildCTableFromTree(tls, CTable, huffNode, nonNullRank, maxSymbolValue, maxNbBits)
	return uint64(maxNbBits)
}

func HUF_estimateCompressedSize(tls *libc.TLS, CTable uintptr, count uintptr, maxSymbolValue uint32) (r size_t) {
	var ct uintptr
	var nbBits size_t
	var s int32
	_, _, _ = ct, nbBits, s
	ct = CTable + uintptr(1)*8
	nbBits = uint64(0)
	s = 0
	for {
		if !(s <= int32(maxSymbolValue)) {
			break
		}
		nbBits = nbBits + HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(s)*8)))*uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
		goto _1
	_1:
		;
		s = s + 1
	}
	return nbBits >> int32(3)
}

func HUF_validateCTable(tls *libc.TLS, CTable uintptr, count uintptr, maxSymbolValue uint32) (r int32) {
	var bad, s int32
	var ct uintptr
	var header HUF_CTableHeader
	_, _, _, _ = bad, ct, header, s
	header = HUF_readCTableHeader(tls, CTable)
	ct = CTable + uintptr(1)*8
	bad = 0
	if uint32(header.FmaxSymbolValue) < maxSymbolValue {
		return 0
	}
	s = 0
	for {
		if !(s <= int32(maxSymbolValue)) {
			break
		}
		bad = bad | libc.BoolInt32(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) != uint32(0))&libc.BoolInt32(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(s)*8))) == uint64(0))
		goto _1
	_1:
		;
		s = s + 1
	}
	return libc.BoolInt32(!(bad != 0))
}

func HUF_compressBound(tls *libc.TLS, size size_t) (r size_t) {
	return libc.Uint64FromInt32(HUF_CTABLEBOUND) + (size + size>>libc.Int32FromInt32(8) + libc.Uint64FromInt32(8))
}

/** HUF_CStream_t:
 * Huffman uses its own BIT_CStream_t implementation.
 * There are three major differences from BIT_CStream_t:
 *   1. HUF_addBits() takes a HUF_CElt (size_t) which is
 *      the pair (nbBits, value) in the format:
 *      format:
 *        - Bits [0, 4)            = nbBits
 *        - Bits [4, 64 - nbBits)  = 0
 *        - Bits [64 - nbBits, 64) = value
 *   2. The bitContainer is built from the upper bits and
 *      right shifted. E.g. to add a new value of N bits
 *      you right shift the bitContainer by N, then or in
 *      the new value into the N upper bits.
 *   3. The bitstream has two bit containers. You can add
 *      bits to the second container and merge them into
 *      the first container.
 */

type HUF_CStream_t = struct {
	FbitContainer [2]size_t
	FbitPos       [2]size_t
	FstartPtr     uintptr
	Fptr          uintptr
	FendPtr       uintptr
}

// C documentation
//
//	/**! HUF_initCStream():
//	 * Initializes the bitstream.
//	 * @returns 0 or an error code.
//	 */
func HUF_initCStream(tls *libc.TLS, bitC uintptr, startPtr uintptr, dstCapacity size_t) (r size_t) {
	libc.Xmemset(tls, bitC, 0, libc.Uint64FromInt64(56))
	(*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr = startPtr
	(*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr
	(*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr = (*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr + uintptr(dstCapacity) - uintptr(8)
	if dstCapacity <= uint64(8) {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	return uint64(0)
}

// C documentation
//
//	/*! HUF_addBits():
//	 * Adds the symbol stored in HUF_CElt elt to the bitstream.
//	 *
//	 * @param elt   The element we're adding. This is a (nbBits, value) pair.
//	 *              See the HUF_CStream_t docs for the format.
//	 * @param idx   Insert into the bitstream at this idx.
//	 * @param kFast This is a template parameter. If the bitstream is guaranteed
//	 *              to have at least 4 unused bits after this call it may be 1,
//	 *              otherwise it must be 0. HUF_addBits() is faster when fast is set.
//	 */
func HUF_addBits(tls *libc.TLS, bitC uintptr, elt HUF_CElt, idx int32, kFast int32) {
	var v1 uint64
	_ = v1
	/* This is efficient on x86-64 with BMI2 because shrx
	 * only reads the low 6 bits of the register. The compiler
	 * knows this and elides the mask. When fast is set,
	 * every operation can use the same value loaded from elt.
	 */
	*(*size_t)(unsafe.Pointer(bitC + uintptr(idx)*8)) >>= HUF_getNbBits(tls, elt)
	if kFast != 0 {
		v1 = HUF_getValueFast(tls, elt)
	} else {
		v1 = HUF_getValue(tls, elt)
	}
	*(*size_t)(unsafe.Pointer(bitC + uintptr(idx)*8)) |= v1
	/* We only read the low 8 bits of bitC->bitPos[idx] so it
	 * doesn't matter that the high bits have noise from the value.
	 */
	*(*size_t)(unsafe.Pointer(bitC + 16 + uintptr(idx)*8)) += HUF_getNbBitsFast(tls, elt)
	/* The last 4-bits of elt are dirty if fast is set,
	 * so we must not be overwriting bits that have already been
	 * inserted into the bit container.
	 */
}

func HUF_zeroIndex1(tls *libc.TLS, bitC uintptr) {
	*(*size_t)(unsafe.Pointer(bitC + 1*8)) = uint64(0)
	*(*size_t)(unsafe.Pointer(bitC + 16 + 1*8)) = uint64(0)
}

// C documentation
//
//	/*! HUF_mergeIndex1() :
//	 * Merges the bit container @ index 1 into the bit container @ index 0
//	 * and zeros the bit container @ index 1.
//	 */
func HUF_mergeIndex1(tls *libc.TLS, bitC uintptr) {
	*(*size_t)(unsafe.Pointer(bitC)) >>= *(*size_t)(unsafe.Pointer(bitC + 16 + 1*8)) & uint64(0xFF)
	*(*size_t)(unsafe.Pointer(bitC)) |= *(*size_t)(unsafe.Pointer(bitC + 1*8))
	*(*size_t)(unsafe.Pointer(bitC + 16)) += *(*size_t)(unsafe.Pointer(bitC + 16 + 1*8))
}

// C documentation
//
//	/*! HUF_flushBits() :
//	* Flushes the bits in the bit container @ index 0.
//	*
//	* @post bitPos will be < 8.
//	* @param kFast If kFast is set then we must know a-priori that
//	*              the bit container will not overflow.
//	*/
func HUF_flushBits(tls *libc.TLS, bitC uintptr, kFast int32) {
	var bitContainer, nbBits, nbBytes size_t
	_, _, _ = bitContainer, nbBits, nbBytes
	/* The upper bits of bitPos are noisy, so we must mask by 0xFF. */
	nbBits = *(*size_t)(unsafe.Pointer(bitC + 16)) & uint64(0xFF)
	nbBytes = nbBits >> int32(3)
	/* The top nbBits bits of bitContainer are the ones we need. */
	bitContainer = *(*size_t)(unsafe.Pointer(bitC)) >> (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - nbBits)
	/* Mask bitPos to account for the bytes we consumed. */
	*(*size_t)(unsafe.Pointer(bitC + 16)) &= uint64(7)
	MEM_writeLEST(tls, (*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr, bitContainer)
	*(*uintptr)(unsafe.Pointer(bitC + 40)) += uintptr(nbBytes)
	if !(kFast != 0) && (*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr > (*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		(*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr
	}
	/* bitContainer doesn't need to be modified because the leftover
	 * bits are already the top bitPos bits. And we don't care about
	 * noise in the lower values.
	 */
}

// C documentation
//
//	/*! HUF_endMark()
//	 * @returns The Huffman stream end mark: A 1-bit value = 1.
//	 */
func HUF_endMark(tls *libc.TLS) (r HUF_CElt) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* endMark at bp+0 */ HUF_CElt
	HUF_setNbBits(tls, bp, uint64(1))
	HUF_setValue(tls, bp, uint64(1))
	return *(*HUF_CElt)(unsafe.Pointer(bp))
}

// C documentation
//
//	/*! HUF_closeCStream() :
//	 *  @return Size of CStream, in bytes,
//	 *          or 0 if it could not fit into dstBuffer */
func HUF_closeCStream(tls *libc.TLS, bitC uintptr) (r size_t) {
	var nbBits size_t
	_ = nbBits
	HUF_addBits(tls, bitC, HUF_endMark(tls), 0, 0)
	HUF_flushBits(tls, bitC, 0)
	nbBits = *(*size_t)(unsafe.Pointer(bitC + 16)) & uint64(0xFF)
	if (*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr >= (*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		return uint64(0)
	} /* overflow detected */
	return uint64(int64((*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr)-int64((*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr)) + libc.BoolUint64(nbBits > libc.Uint64FromInt32(0))
	return r
}

func HUF_encodeSymbol(tls *libc.TLS, bitCPtr uintptr, symbol U32, CTable uintptr, idx int32, fast int32) {
	HUF_addBits(tls, bitCPtr, *(*HUF_CElt)(unsafe.Pointer(CTable + uintptr(symbol)*8)), idx, fast)
}

func HUF_compress1X_usingCTable_internal_body_loop(tls *libc.TLS, bitC uintptr, ip uintptr, srcSize size_t, ct uintptr, kUnroll int32, kFastFlush int32, kLastFast int32) {
	var n, rem, u, u1, v2 int32
	_, _, _, _, _ = n, rem, u, u1, v2
	/* Join to kUnroll */
	n = int32(srcSize)
	rem = n % kUnroll
	if rem > 0 {
		for {
			if !(rem > 0) {
				break
			}
			n = n - 1
			v2 = n
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(v2)))), ct, 0, 0)
			goto _1
		_1:
			;
			rem = rem - 1
		}
		HUF_flushBits(tls, bitC, kFastFlush)
	}
	/* Join to 2 * kUnroll */
	if n%(int32(2)*kUnroll) != 0 {
		u = int32(1)
		for {
			if !(u < kUnroll) {
				break
			}
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-u)))), ct, 0, int32(1))
			goto _3
		_3:
			;
			u = u + 1
		}
		HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll)))), ct, 0, kLastFast)
		HUF_flushBits(tls, bitC, kFastFlush)
		n = n - kUnroll
	}
	for {
		if !(n > 0) {
			break
		}
		u1 = int32(1)
		for {
			if !(u1 < kUnroll) {
				break
			}
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-u1)))), ct, 0, int32(1))
			goto _5
		_5:
			;
			u1 = u1 + 1
		}
		HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll)))), ct, 0, kLastFast)
		HUF_flushBits(tls, bitC, kFastFlush)
		/* Encode kUnroll symbols into the bitstream @ index 1.
		 * This allows us to start filling the bit container
		 * without any data dependencies.
		 */
		HUF_zeroIndex1(tls, bitC)
		u1 = int32(1)
		for {
			if !(u1 < kUnroll) {
				break
			}
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll-u1)))), ct, int32(1), int32(1))
			goto _6
		_6:
			;
			u1 = u1 + 1
		}
		HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll-kUnroll)))), ct, int32(1), kLastFast)
		/* Merge bitstream @ index 1 into the bitstream @ index 0 */
		HUF_mergeIndex1(tls, bitC)
		HUF_flushBits(tls, bitC, kFastFlush)
		goto _4
	_4:
		;
		n = n - int32(2)*kUnroll
	}
}

// C documentation
//
//	/**
//	 * Returns a tight upper bound on the output space needed by Huffman
//	 * with 8 bytes buffer to handle over-writes. If the output is at least
//	 * this large we don't need to do bounds checks during Huffman encoding.
//	 */
func HUF_tightCompressBound(tls *libc.TLS, srcSize size_t, tableLog size_t) (r size_t) {
	return srcSize*tableLog>>int32(3) + uint64(8)
}

func HUF_compress1X_usingCTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr) (r size_t) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var ct, ip, oend, op, ostart uintptr
	var initErr size_t
	var tableLog U32
	var v1 int32
	var _ /* bitC at bp+0 */ HUF_CStream_t
	_, _, _, _, _, _, _, _ = ct, initErr, ip, oend, op, ostart, tableLog, v1
	tableLog = uint32(HUF_readCTableHeader(tls, CTable).FtableLog)
	ct = CTable + uintptr(1)*8
	ip = src
	ostart = dst
	oend = ostart + uintptr(dstSize)
	/* init */
	if dstSize < uint64(8) {
		return uint64(0)
	} /* not enough space to compress */
	op = ostart
	initErr = HUF_initCStream(tls, bp, op, uint64(int64(oend)-int64(op)))
	if ERR_isError(tls, initErr) != 0 {
		return uint64(0)
	}
	if dstSize < HUF_tightCompressBound(tls, srcSize, uint64(tableLog)) || tableLog > uint32(11) {
		if MEM_32bits(tls) != 0 {
			v1 = int32(2)
		} else {
			v1 = int32(4)
		}
		HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, v1, 0, 0)
	} else {
		if MEM_32bits(tls) != 0 {
			switch tableLog {
			case uint32(11):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(2), int32(1), 0)
			case uint32(10):
				fallthrough
			case uint32(9):
				fallthrough
			case uint32(8):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(2), int32(1), int32(1))
			case uint32(7):
				fallthrough
			default:
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(3), int32(1), int32(1))
				break
			}
		} else {
			switch tableLog {
			case uint32(11):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(5), int32(1), 0)
			case uint32(10):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(5), int32(1), int32(1))
			case uint32(9):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(6), int32(1), 0)
			case uint32(8):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(7), int32(1), 0)
			case uint32(7):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(8), int32(1), 0)
			case uint32(6):
				fallthrough
			default:
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(9), int32(1), int32(1))
				break
			}
		}
	}
	return HUF_closeCStream(tls, bp)
}

func HUF_compress1X_usingCTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr) (r size_t) {
	return HUF_compress1X_usingCTable_internal_body(tls, dst, dstSize, src, srcSize, CTable)
}

func HUF_compress1X_usingCTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr) (r size_t) {
	return HUF_compress1X_usingCTable_internal_body(tls, dst, dstSize, src, srcSize, CTable)
}

func HUF_compress1X_usingCTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_compress1X_usingCTable_internal_bmi2(tls, dst, dstSize, src, srcSize, CTable)
	}
	return HUF_compress1X_usingCTable_internal_default(tls, dst, dstSize, src, srcSize, CTable)
}

func HUF_compress1X_usingCTable(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	return HUF_compress1X_usingCTable_internal(tls, dst, dstSize, src, srcSize, CTable, flags)
}

func HUF_compress4X_usingCTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	var cSize, cSize1, cSize2, cSize3, segmentSize size_t
	var iend, ip, oend, op, ostart uintptr
	_, _, _, _, _, _, _, _, _, _ = cSize, cSize1, cSize2, cSize3, iend, ip, oend, op, ostart, segmentSize
	segmentSize = (srcSize + uint64(3)) / uint64(4) /* first 3 segments */
	ip = src
	iend = ip + uintptr(srcSize)
	ostart = dst
	oend = ostart + uintptr(dstSize)
	op = ostart
	if dstSize < uint64(libc.Int32FromInt32(6)+libc.Int32FromInt32(1)+libc.Int32FromInt32(1)+libc.Int32FromInt32(1)+libc.Int32FromInt32(8)) {
		return uint64(0)
	} /* minimum space to compress successfully */
	if srcSize < uint64(12) {
		return uint64(0)
	} /* no saving possible : too small input */
	op = op + uintptr(6) /* jumpTable */
	cSize = HUF_compress1X_usingCTable_internal(tls, op, uint64(int64(oend)-int64(op)), ip, segmentSize, CTable, flags)
	if ERR_isError(tls, cSize) != 0 {
		return cSize
	}
	if cSize == uint64(0) || cSize > uint64(65535) {
		return uint64(0)
	}
	MEM_writeLE16(tls, ostart, uint16(cSize))
	op = op + uintptr(cSize)
	ip = ip + uintptr(segmentSize)
	cSize1 = HUF_compress1X_usingCTable_internal(tls, op, uint64(int64(oend)-int64(op)), ip, segmentSize, CTable, flags)
	if ERR_isError(tls, cSize1) != 0 {
		return cSize1
	}
	if cSize1 == uint64(0) || cSize1 > uint64(65535) {
		return uint64(0)
	}
	MEM_writeLE16(tls, ostart+uintptr(2), uint16(cSize1))
	op = op + uintptr(cSize1)
	ip = ip + uintptr(segmentSize)
	cSize2 = HUF_compress1X_usingCTable_internal(tls, op, uint64(int64(oend)-int64(op)), ip, segmentSize, CTable, flags)
	if ERR_isError(tls, cSize2) != 0 {
		return cSize2
	}
	if cSize2 == uint64(0) || cSize2 > uint64(65535) {
		return uint64(0)
	}
	MEM_writeLE16(tls, ostart+uintptr(4), uint16(cSize2))
	op = op + uintptr(cSize2)
	ip = ip + uintptr(segmentSize)
	cSize3 = HUF_compress1X_usingCTable_internal(tls, op, uint64(int64(oend)-int64(op)), ip, uint64(int64(iend)-int64(ip)), CTable, flags)
	if ERR_isError(tls, cSize3) != 0 {
		return cSize3
	}
	if cSize3 == uint64(0) || cSize3 > uint64(65535) {
		return uint64(0)
	}
	op = op + uintptr(cSize3)
	return uint64(int64(op) - int64(ostart))
}

func HUF_compress4X_usingCTable(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	return HUF_compress4X_usingCTable_internal(tls, dst, dstSize, src, srcSize, CTable, flags)
}

type HUF_nbStreams_e = int32

const HUF_singleStream = 0
const HUF_fourStreams = 1

func HUF_compressCTable_internal(tls *libc.TLS, ostart uintptr, op uintptr, oend uintptr, src uintptr, srcSize size_t, nbStreams HUF_nbStreams_e, CTable uintptr, flags int32) (r size_t) {
	var cSize size_t
	var v1 uint64
	_, _ = cSize, v1
	if nbStreams == int32(HUF_singleStream) {
		v1 = HUF_compress1X_usingCTable_internal(tls, op, uint64(int64(oend)-int64(op)), src, srcSize, CTable, flags)
	} else {
		v1 = HUF_compress4X_usingCTable_internal(tls, op, uint64(int64(oend)-int64(op)), src, srcSize, CTable, flags)
	}
	cSize = v1
	if ERR_isError(tls, cSize) != 0 {
		return cSize
	}
	if cSize == uint64(0) {
		return uint64(0)
	} /* uncompressible */
	op = op + uintptr(cSize)
	/* check compressibility */
	if uint64(int64(op)-int64(ostart)) >= srcSize-uint64(1) {
		return uint64(0)
	}
	return uint64(int64(op) - int64(ostart))
}

type HUF_compress_tables_t = struct {
	Fcount  [256]uint32
	FCTable [257]HUF_CElt
	Fwksps  struct {
		FwriteCTable_wksp [0]HUF_WriteCTableWksp
		Fhist_wksp        [0][1024]U32
		FbuildCTable_wksp HUF_buildCTable_wksp_tables
	}
}

func HUF_cardinality(tls *libc.TLS, count uintptr, maxSymbolValue uint32) (r uint32) {
	var cardinality, i uint32
	_, _ = cardinality, i
	cardinality = uint32(0)
	i = uint32(0)
	for {
		if !(i < maxSymbolValue+uint32(1)) {
			break
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(i)*4)) != uint32(0) {
			cardinality = cardinality + uint32(1)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return cardinality
}

func HUF_minTableLog(tls *libc.TLS, symbolCardinality uint32) (r uint32) {
	var minBitsSymbols U32
	_ = minBitsSymbols
	minBitsSymbols = ZSTD_highbit32(tls, symbolCardinality) + uint32(1)
	return minBitsSymbols
}

func HUF_optimalTableLog(tls *libc.TLS, maxTableLog uint32, srcSize size_t, maxSymbolValue uint32, workSpace uintptr, wkspSize size_t, table uintptr, count uintptr, flags int32) (r uint32) {
	var dst uintptr
	var dstSize, hSize, maxBits, newSize, optSize size_t
	var minTableLog, optLog, optLogGuess, symbolCardinality uint32
	_, _, _, _, _, _, _, _, _, _ = dst, dstSize, hSize, maxBits, minTableLog, newSize, optLog, optLogGuess, optSize, symbolCardinality
	/* Not supported, RLE should be used instead */
	if !(flags&int32(HUF_flags_optimalDepth) != 0) {
		/* cheap evaluation, based on FSE */
		return FSE_optimalTableLog_internal(tls, maxTableLog, srcSize, maxSymbolValue, uint32(1))
	}
	dst = workSpace + uintptr(748)
	dstSize = wkspSize - uint64(748)
	symbolCardinality = HUF_cardinality(tls, count, maxSymbolValue)
	minTableLog = HUF_minTableLog(tls, symbolCardinality)
	optSize = uint64(^libc.Int32FromInt32(0)) - libc.Uint64FromInt32(1)
	optLog = maxTableLog
	/* Search until size increases */
	optLogGuess = minTableLog
	for {
		if !(optLogGuess <= maxTableLog) {
			break
		}
		maxBits = HUF_buildCTable_wksp(tls, table, count, maxSymbolValue, optLogGuess, workSpace, wkspSize)
		if ERR_isError(tls, maxBits) != 0 {
			goto _1
		}
		if maxBits < uint64(optLogGuess) && optLogGuess > minTableLog {
			break
		}
		hSize = HUF_writeCTable_wksp(tls, dst, dstSize, table, maxSymbolValue, uint32(maxBits), workSpace, wkspSize)
		if ERR_isError(tls, hSize) != 0 {
			goto _1
		}
		newSize = HUF_estimateCompressedSize(tls, table, count, maxSymbolValue) + hSize
		if newSize > optSize+uint64(1) {
			break
		}
		if newSize < optSize {
			optSize = newSize
			optLog = optLogGuess
		}
		goto _1
	_1:
		;
		optLogGuess = optLogGuess + 1
	}
	return optLog
	return r
}

// C documentation
//
//	/* HUF_compress_internal() :
//	 * `workSpace_align4` must be aligned on 4-bytes boundaries,
//	 * and occupies the same space as a table of HUF_WORKSPACE_SIZE_U64 unsigned */
func HUF_compress_internal(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, _maxSymbolValue uint32, huffLog uint32, nbStreams HUF_nbStreams_e, workSpace uintptr, _wkspSize size_t, oldHufTable uintptr, repeat uintptr, flags int32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	*(*uint32)(unsafe.Pointer(bp)) = _maxSymbolValue
	*(*size_t)(unsafe.Pointer(bp + 8)) = _wkspSize
	var _var_err__, hSize, largest, largestBegin, largestEnd, largestTotal, maxBits, newSize, oldSize size_t
	var oend, op, ostart, table uintptr
	var _ /* maxSymbolValueBegin at bp+16 */ uint32
	var _ /* maxSymbolValueEnd at bp+20 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = _var_err__, hSize, largest, largestBegin, largestEnd, largestTotal, maxBits, newSize, oend, oldSize, op, ostart, table
	table = HUF_alignUpWorkspace(tls, workSpace, bp+8, uint64(8))
	ostart = dst
	oend = ostart + uintptr(dstSize)
	op = ostart
	_ = libc.Uint64FromInt64(1)
	/* checks & inits */
	if *(*size_t)(unsafe.Pointer(bp + 8)) < uint64(7944) {
		return uint64(-int32(ZSTD_error_workSpace_tooSmall))
	}
	if !(srcSize != 0) {
		return uint64(0)
	} /* Uncompressed */
	if !(dstSize != 0) {
		return uint64(0)
	} /* cannot fit anything within dst budget */
	if srcSize > uint64(libc.Int32FromInt32(128)*libc.Int32FromInt32(1024)) {
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	} /* current block size limit */
	if huffLog > uint32(HUF_TABLELOG_MAX) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	if *(*uint32)(unsafe.Pointer(bp)) > uint32(HUF_SYMBOLVALUE_MAX) {
		return uint64(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	if !(*(*uint32)(unsafe.Pointer(bp)) != 0) {
		*(*uint32)(unsafe.Pointer(bp)) = uint32(HUF_SYMBOLVALUE_MAX)
	}
	if !(huffLog != 0) {
		huffLog = uint32(HUF_TABLELOG_DEFAULT)
	}
	/* Heuristic : If old table is valid, use it for small inputs */
	if flags&int32(HUF_flags_preferRepeat) != 0 && repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) == int32(HUF_repeat_valid) {
		return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, oldHufTable, flags)
	}
	/* If uncompressible data is suspected, do a smaller sampling first */
	_ = libc.Uint64FromInt64(1)
	if flags&int32(HUF_flags_suspectUncompressible) != 0 && srcSize >= uint64(libc.Int32FromInt32(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)*libc.Int32FromInt32(SUSPECT_INCOMPRESSIBLE_SAMPLE_RATIO)) {
		largestTotal = uint64(0)
		*(*uint32)(unsafe.Pointer(bp + 16)) = *(*uint32)(unsafe.Pointer(bp))
		largestBegin = uint64(HIST_count_simple(tls, table, bp+16, src, uint64(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)))
		if ERR_isError(tls, largestBegin) != 0 {
			return largestBegin
		}
		largestTotal = largestTotal + largestBegin
		*(*uint32)(unsafe.Pointer(bp + 20)) = *(*uint32)(unsafe.Pointer(bp))
		largestEnd = uint64(HIST_count_simple(tls, table, bp+20, src+uintptr(srcSize)-uintptr(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE), uint64(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)))
		if ERR_isError(tls, largestEnd) != 0 {
			return largestEnd
		}
		largestTotal = largestTotal + largestEnd
		if largestTotal <= uint64(libc.Int32FromInt32(2)*libc.Int32FromInt32(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)>>libc.Int32FromInt32(7)+libc.Int32FromInt32(4)) {
			return uint64(0)
		} /* heuristic : probably not compressible enough */
	}
	/* Scan input and build symbol stats */
	largest = HIST_count_wksp(tls, table, bp, src, srcSize, table+3080, uint64(4096))
	if ERR_isError(tls, largest) != 0 {
		return largest
	}
	if largest == srcSize {
		*(*BYTE)(unsafe.Pointer(ostart)) = *(*BYTE)(unsafe.Pointer(src))
		return uint64(1)
	} /* single symbol, rle */
	if largest <= srcSize>>libc.Int32FromInt32(7)+uint64(4) {
		return uint64(0)
	} /* heuristic : probably not compressible enough */
	/* Check validity of previous table */
	if repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) == int32(HUF_repeat_check) && !(HUF_validateCTable(tls, oldHufTable, table, *(*uint32)(unsafe.Pointer(bp))) != 0) {
		*(*HUF_repeat)(unsafe.Pointer(repeat)) = int32(HUF_repeat_none)
	}
	/* Heuristic : use existing table for small inputs */
	if flags&int32(HUF_flags_preferRepeat) != 0 && repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) != int32(HUF_repeat_none) {
		return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, oldHufTable, flags)
	}
	/* Build Huffman Tree */
	huffLog = HUF_optimalTableLog(tls, huffLog, srcSize, *(*uint32)(unsafe.Pointer(bp)), table+3080, uint64(4864), table+1024, table, flags)
	maxBits = HUF_buildCTable_wksp(tls, table+1024, table, *(*uint32)(unsafe.Pointer(bp)), huffLog, table+3080, uint64(4864))
	_var_err__ = maxBits
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	huffLog = uint32(maxBits)
	/* Write table description header */
	hSize = HUF_writeCTable_wksp(tls, op, dstSize, table+1024, *(*uint32)(unsafe.Pointer(bp)), huffLog, table+3080, uint64(748))
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	/* Check if using previous huffman table is beneficial */
	if repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) != int32(HUF_repeat_none) {
		oldSize = HUF_estimateCompressedSize(tls, oldHufTable, table, *(*uint32)(unsafe.Pointer(bp)))
		newSize = HUF_estimateCompressedSize(tls, table+1024, table, *(*uint32)(unsafe.Pointer(bp)))
		if oldSize <= hSize+newSize || hSize+uint64(12) >= srcSize {
			return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, oldHufTable, flags)
		}
	}
	/* Use the new huffman table */
	if hSize+uint64(12) >= srcSize {
		return uint64(0)
	}
	op = op + uintptr(hSize)
	if repeat != 0 {
		*(*HUF_repeat)(unsafe.Pointer(repeat)) = int32(HUF_repeat_none)
	}
	if oldHufTable != 0 {
		libc.Xmemcpy(tls, oldHufTable, table+1024, libc.Uint64FromInt64(2056))
	} /* Save new table */
	return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, table+1024, flags)
}

func HUF_compress1X_repeat(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, maxSymbolValue uint32, huffLog uint32, workSpace uintptr, wkspSize size_t, hufTable uintptr, repeat uintptr, flags int32) (r size_t) {
	return HUF_compress_internal(tls, dst, dstSize, src, srcSize, maxSymbolValue, huffLog, int32(HUF_singleStream), workSpace, wkspSize, hufTable, repeat, flags)
}

// C documentation
//
//	/* HUF_compress4X_repeat():
//	 * compress input using 4 streams.
//	 * consider skipping quickly
//	 * reuse an existing huffman compression table */
func HUF_compress4X_repeat(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, maxSymbolValue uint32, huffLog uint32, workSpace uintptr, wkspSize size_t, hufTable uintptr, repeat uintptr, flags int32) (r size_t) {
	return HUF_compress_internal(tls, dst, dstSize, src, srcSize, maxSymbolValue, huffLog, int32(HUF_fourStreams), workSpace, wkspSize, hufTable, repeat, flags)
}

/**** ended inlining compress/huf_compress.c ****/
/**** start inlining compress/zstd_compress_literals.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
 *  Dependencies
 ***************************************/
/**** start inlining zstd_compress_literals.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** start inlining zstd_compress_internal.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* This header contains definitions
 * that shall **only** be used by modules within lib/compress.
 */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: ../common/zstd_internal.h ****/
/**** start inlining zstd_cwksp.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/portability_macros.h ****/
/**** skipping file: ../common/compiler.h ****/

/*-*************************************
*  Constants
***************************************/

/* Since the workspace is effectively its own little malloc implementation /
 * arena, when we run under ASAN, we should similarly insert redzones between
 * each internal element of the workspace, so ASAN will catch overruns that
 * reach outside an object but that stay inside the workspace.
 *
 * This defines the size of that redzone.
 */

/* Set our tables and aligneds to align by 64 bytes */

// C documentation
//
//	/*-*************************************
//	*  Structures
//	***************************************/
type ZSTD_cwksp_alloc_phase_e = int32

const ZSTD_cwksp_alloc_objects = 0
const ZSTD_cwksp_alloc_aligned_init_once = 1
const ZSTD_cwksp_alloc_aligned = 2
const ZSTD_cwksp_alloc_buffers = 3

// C documentation
//
//	/**
//	 * Used to describe whether the workspace is statically allocated (and will not
//	 * necessarily ever be freed), or if it's dynamically allocated and we can
//	 * expect a well-formed caller to free this.
//	 */
type ZSTD_cwksp_static_alloc_e = int32

const ZSTD_cwksp_dynamic_alloc = 0
const ZSTD_cwksp_static_alloc = 1

// C documentation
//
//	/**
//	 * Zstd fits all its internal datastructures into a single continuous buffer,
//	 * so that it only needs to perform a single OS allocation (or so that a buffer
//	 * can be provided to it and it can perform no allocations at all). This buffer
//	 * is called the workspace.
//	 *
//	 * Several optimizations complicate that process of allocating memory ranges
//	 * from this workspace for each internal datastructure:
//	 *
//	 * - These different internal datastructures have different setup requirements:
//	 *
//	 *   - The static objects need to be cleared once and can then be trivially
//	 *     reused for each compression.
//	 *
//	 *   - Various buffers don't need to be initialized at all--they are always
//	 *     written into before they're read.
//	 *
//	 *   - The matchstate tables have a unique requirement that they don't need
//	 *     their memory to be totally cleared, but they do need the memory to have
//	 *     some bound, i.e., a guarantee that all values in the memory they've been
//	 *     allocated is less than some maximum value (which is the starting value
//	 *     for the indices that they will then use for compression). When this
//	 *     guarantee is provided to them, they can use the memory without any setup
//	 *     work. When it can't, they have to clear the area.
//	 *
//	 * - These buffers also have different alignment requirements.
//	 *
//	 * - We would like to reuse the objects in the workspace for multiple
//	 *   compressions without having to perform any expensive reallocation or
//	 *   reinitialization work.
//	 *
//	 * - We would like to be able to efficiently reuse the workspace across
//	 *   multiple compressions **even when the compression parameters change** and
//	 *   we need to resize some of the objects (where possible).
//	 *
//	 * To attempt to manage this buffer, given these constraints, the ZSTD_cwksp
//	 * abstraction was created. It works as follows:
//	 *
//	 * Workspace Layout:
//	 *
//	 * [                        ... workspace ...                           ]
//	 * [objects][tables ->] free space [<- buffers][<- aligned][<- init once]
//	 *
//	 * The various objects that live in the workspace are divided into the
//	 * following categories, and are allocated separately:
//	 *
//	 * - Static objects: this is optionally the enclosing ZSTD_CCtx or ZSTD_CDict,
//	 *   so that literally everything fits in a single buffer. Note: if present,
//	 *   this must be the first object in the workspace, since ZSTD_customFree{CCtx,
//	 *   CDict}() rely on a pointer comparison to see whether one or two frees are
//	 *   required.
//	 *
//	 * - Fixed size objects: these are fixed-size, fixed-count objects that are
//	 *   nonetheless "dynamically" allocated in the workspace so that we can
//	 *   control how they're initialized separately from the broader ZSTD_CCtx.
//	 *   Examples:
//	 *   - Entropy Workspace
//	 *   - 2 x ZSTD_compressedBlockState_t
//	 *   - CDict dictionary contents
//	 *
//	 * - Tables: these are any of several different datastructures (hash tables,
//	 *   chain tables, binary trees) that all respect a common format: they are
//	 *   uint32_t arrays, all of whose values are between 0 and (nextSrc - base).
//	 *   Their sizes depend on the cparams. These tables are 64-byte aligned.
//	 *
//	 * - Init once: these buffers require to be initialized at least once before
//	 *   use. They should be used when we want to skip memory initialization
//	 *   while not triggering memory checkers (like Valgrind) when reading from
//	 *   from this memory without writing to it first.
//	 *   These buffers should be used carefully as they might contain data
//	 *   from previous compressions.
//	 *   Buffers are aligned to 64 bytes.
//	 *
//	 * - Aligned: these buffers don't require any initialization before they're
//	 *   used. The user of the buffer should make sure they write into a buffer
//	 *   location before reading from it.
//	 *   Buffers are aligned to 64 bytes.
//	 *
//	 * - Buffers: these buffers are used for various purposes that don't require
//	 *   any alignment or initialization before they're used. This means they can
//	 *   be moved around at no cost for a new compression.
//	 *
//	 * Allocating Memory:
//	 *
//	 * The various types of objects must be allocated in order, so they can be
//	 * correctly packed into the workspace buffer. That order is:
//	 *
//	 * 1. Objects
//	 * 2. Init once / Tables
//	 * 3. Aligned / Tables
//	 * 4. Buffers / Tables
//	 *
//	 * Attempts to reserve objects of different types out of order will fail.
//	 */
type ZSTD_cwksp = struct {
	Fworkspace                  uintptr
	FworkspaceEnd               uintptr
	FobjectEnd                  uintptr
	FtableEnd                   uintptr
	FtableValidEnd              uintptr
	FallocStart                 uintptr
	FinitOnceStart              uintptr
	FallocFailed                BYTE
	FworkspaceOversizedDuration int32
	Fphase                      ZSTD_cwksp_alloc_phase_e
	FisStatic                   ZSTD_cwksp_static_alloc_e
}

func ZSTD_cwksp_assert_internal_consistency(tls *libc.TLS, ws uintptr) {
	_ = ws
}

// C documentation
//
//	/**
//	 * Align must be a power of 2.
//	 */
func ZSTD_cwksp_align(tls *libc.TLS, size size_t, align size_t) (r size_t) {
	var mask size_t
	_ = mask
	mask = align - uint64(1)
	return (size + mask) & ^mask
}

// C documentation
//
//	/**
//	 * Use this to determine how much space in the workspace we will consume to
//	 * allocate this object. (Normally it should be exactly the size of the object,
//	 * but under special conditions, like ASAN, where we pad each object, it might
//	 * be larger.)
//	 *
//	 * Since tables aren't currently redzoned, you don't need to call through this
//	 * to figure out how much space you need for the matchState tables. Everything
//	 * else is though.
//	 *
//	 * Do not use for sizing aligned buffers. Instead, use ZSTD_cwksp_aligned64_alloc_size().
//	 */
func ZSTD_cwksp_alloc_size(tls *libc.TLS, size size_t) (r size_t) {
	if size == uint64(0) {
		return uint64(0)
	}
	return size
}

func ZSTD_cwksp_aligned_alloc_size(tls *libc.TLS, size size_t, alignment size_t) (r size_t) {
	return ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, size, alignment))
}

// C documentation
//
//	/**
//	 * Returns an adjusted alloc size that is the nearest larger multiple of 64 bytes.
//	 * Used to determine the number of bytes required for a given "aligned".
//	 */
func ZSTD_cwksp_aligned64_alloc_size(tls *libc.TLS, size size_t) (r size_t) {
	return ZSTD_cwksp_aligned_alloc_size(tls, size, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES))
}

// C documentation
//
//	/**
//	 * Returns the amount of additional space the cwksp must allocate
//	 * for internal purposes (currently only alignment).
//	 */
func ZSTD_cwksp_slack_space_required(tls *libc.TLS) (r size_t) {
	var slackSpace size_t
	_ = slackSpace
	/* For alignment, the wksp will always allocate an additional 2*ZSTD_CWKSP_ALIGNMENT_BYTES
	 * bytes to align the beginning of tables section and end of buffers;
	 */
	slackSpace = uint64(libc.Int32FromInt32(ZSTD_CWKSP_ALIGNMENT_BYTES) * libc.Int32FromInt32(2))
	return slackSpace
}

// C documentation
//
//	/**
//	 * Return the number of additional bytes required to align a pointer to the given number of bytes.
//	 * alignBytes must be a power of two.
//	 */
func ZSTD_cwksp_bytes_to_align_ptr(tls *libc.TLS, ptr uintptr, alignBytes size_t) (r size_t) {
	var alignBytesMask, bytes size_t
	_, _ = alignBytesMask, bytes
	alignBytesMask = alignBytes - uint64(1)
	bytes = (alignBytes - uint64(ptr)&alignBytesMask) & alignBytesMask
	return bytes
}

// C documentation
//
//	/**
//	 * Returns the initial value for allocStart which is used to determine the position from
//	 * which we can allocate from the end of the workspace.
//	 */
func ZSTD_cwksp_initialAllocStart(tls *libc.TLS, ws uintptr) (r uintptr) {
	var endPtr uintptr
	_ = endPtr
	endPtr = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd
	endPtr = endPtr - uintptr(uint64(endPtr)%libc.Uint64FromInt32(ZSTD_CWKSP_ALIGNMENT_BYTES))
	return endPtr
}

// C documentation
//
//	/**
//	 * Internal function. Do not use directly.
//	 * Reserves the given number of bytes within the aligned/buffer segment of the wksp,
//	 * which counts from the end of the wksp (as opposed to the object/table segment).
//	 *
//	 * Returns a pointer to the beginning of that space.
//	 */
func ZSTD_cwksp_reserve_internal_buffer_space(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alloc, bottom uintptr
	_, _ = alloc, bottom
	alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart - uintptr(bytes)
	bottom = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
	if alloc < bottom {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(1)
		return libc.UintptrFromInt32(0)
	}
	/* the area is reserved from the end of wksp.
	 * If it overlaps with tableValidEnd, it voids guarantees on values' range */
	if alloc < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = alloc
	}
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart = alloc
	return alloc
}

// C documentation
//
//	/**
//	 * Moves the cwksp to the next phase, and does any necessary allocations.
//	 * cwksp initialization must necessarily go through each phase in order.
//	 * Returns a 0 on success, or zstd error
//	 */
func ZSTD_cwksp_internal_advance_phase(tls *libc.TLS, ws uintptr, phase ZSTD_cwksp_alloc_phase_e) (r size_t) {
	var alloc, objectEnd uintptr
	var bytesToAlign size_t
	_, _, _ = alloc, bytesToAlign, objectEnd
	if phase > (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase {
		/* Going from allocating objects to allocating initOnce / tables */
		if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase < int32(ZSTD_cwksp_alloc_aligned_init_once) && phase >= int32(ZSTD_cwksp_alloc_aligned_init_once) {
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart = ZSTD_cwksp_initialAllocStart(tls, ws)
			/* Align the start of the tables to 64 bytes. Use [0, 63] bytes */
			alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
			bytesToAlign = ZSTD_cwksp_bytes_to_align_ptr(tls, alloc, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES))
			objectEnd = alloc + uintptr(bytesToAlign)
			if objectEnd > (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1326, 0)
				}
				return uint64(-int32(ZSTD_error_memory_allocation))
			}
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd = objectEnd
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = objectEnd /* table area starts being empty */
			if (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd {
				(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
			}
		}
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase = phase
		ZSTD_cwksp_assert_internal_consistency(tls, ws)
	}
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Returns whether this object/buffer/etc was allocated in this workspace.
//	 */
func ZSTD_cwksp_owns_buffer(tls *libc.TLS, ws uintptr, ptr uintptr) (r int32) {
	return libc.BoolInt32(ptr != libc.UintptrFromInt32(0) && (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace <= ptr && ptr < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd)
}

// C documentation
//
//	/**
//	 * Internal function. Do not use directly.
//	 */
func ZSTD_cwksp_reserve_internal(tls *libc.TLS, ws uintptr, bytes size_t, phase ZSTD_cwksp_alloc_phase_e) (r uintptr) {
	var alloc uintptr
	_ = alloc
	if ZSTD_isError(tls, ZSTD_cwksp_internal_advance_phase(tls, ws, phase)) != 0 || bytes == uint64(0) {
		return libc.UintptrFromInt32(0)
	}
	alloc = ZSTD_cwksp_reserve_internal_buffer_space(tls, ws, bytes)
	return alloc
}

// C documentation
//
//	/**
//	 * Reserves and returns unaligned memory.
//	 */
func ZSTD_cwksp_reserve_buffer(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	return ZSTD_cwksp_reserve_internal(tls, ws, bytes, int32(ZSTD_cwksp_alloc_buffers))
}

// C documentation
//
//	/**
//	 * Reserves and returns memory sized on and aligned on ZSTD_CWKSP_ALIGNMENT_BYTES (64 bytes).
//	 * This memory has been initialized at least once in the past.
//	 * This doesn't mean it has been initialized this time, and it might contain data from previous
//	 * operations.
//	 * The main usage is for algorithms that might need read access into uninitialized memory.
//	 * The algorithm must maintain safety under these conditions and must make sure it doesn't
//	 * leak any of the past data (directly or in side channels).
//	 */
func ZSTD_cwksp_reserve_aligned_init_once(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alignedBytes size_t
	var ptr uintptr
	var v1 uint64
	_, _, _ = alignedBytes, ptr, v1
	alignedBytes = ZSTD_cwksp_align(tls, bytes, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES))
	ptr = ZSTD_cwksp_reserve_internal(tls, ws, alignedBytes, int32(ZSTD_cwksp_alloc_aligned_init_once))
	if ptr != 0 && ptr < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart {
		/* We assume the memory following the current allocation is either:
		 * 1. Not usable as initOnce memory (end of workspace)
		 * 2. Another initOnce buffer that has been allocated before (and so was previously memset)
		 * 3. An ASAN redzone, in which case we don't want to write on it
		 * For these reasons it should be fine to not explicitly zero every byte up to ws->initOnceStart.
		 * Note that we assume here that MSAN and ASAN cannot run in the same time. */
		if uint64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart)-int64(ptr)) < alignedBytes {
			v1 = uint64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart) - int64(ptr))
		} else {
			v1 = alignedBytes
		}
		libc.Xmemset(tls, ptr, 0, v1)
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart = ptr
	}
	return ptr
}

// C documentation
//
//	/**
//	 * Reserves and returns memory sized on and aligned on ZSTD_CWKSP_ALIGNMENT_BYTES (64 bytes).
//	 */
func ZSTD_cwksp_reserve_aligned64(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var ptr uintptr
	_ = ptr
	ptr = ZSTD_cwksp_reserve_internal(tls, ws, ZSTD_cwksp_align(tls, bytes, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES)), int32(ZSTD_cwksp_alloc_aligned))
	return ptr
}

// C documentation
//
//	/**
//	 * Aligned on 64 bytes. These buffers have the special property that
//	 * their values remain constrained, allowing us to reuse them without
//	 * memset()-ing them.
//	 */
func ZSTD_cwksp_reserve_table(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alloc, end, top uintptr
	var phase ZSTD_cwksp_alloc_phase_e
	_, _, _, _ = alloc, end, phase, top
	phase = int32(ZSTD_cwksp_alloc_aligned_init_once)
	/* We can only start allocating tables after we are done reserving space for objects at the
	 * start of the workspace */
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase < phase {
		if ZSTD_isError(tls, ZSTD_cwksp_internal_advance_phase(tls, ws, phase)) != 0 {
			return libc.UintptrFromInt32(0)
		}
	}
	alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
	end = alloc + uintptr(bytes)
	top = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
	if end > top {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(1)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = end
	return alloc
}

// C documentation
//
//	/**
//	 * Aligned on sizeof(void*).
//	 * Note : should happen only once, at workspace first initialization
//	 */
func ZSTD_cwksp_reserve_object(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alloc, end uintptr
	var roundedBytes size_t
	_, _, _ = alloc, end, roundedBytes
	roundedBytes = ZSTD_cwksp_align(tls, bytes, uint64(8))
	alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	end = alloc + uintptr(roundedBytes)
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
	/* we must be in the first phase, no advance is possible */
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase != int32(ZSTD_cwksp_alloc_objects) || end > (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(1)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd = end
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = end
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = end
	return alloc
}

// C documentation
//
//	/**
//	 * with alignment control
//	 * Note : should happen only once, at workspace first initialization
//	 */
func ZSTD_cwksp_reserve_object_aligned(tls *libc.TLS, ws uintptr, byteSize size_t, alignment size_t) (r uintptr) {
	var mask, surplus size_t
	var start uintptr
	var v1 uint64
	_, _, _, _ = mask, start, surplus, v1
	mask = alignment - uint64(1)
	if alignment > uint64(8) {
		v1 = alignment - uint64(8)
	} else {
		v1 = uint64(0)
	}
	surplus = v1
	start = ZSTD_cwksp_reserve_object(tls, ws, byteSize+surplus)
	if start == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	if surplus == uint64(0) {
		return start
	}
	return uintptr((uint64(start) + surplus) & ^mask)
}

func ZSTD_cwksp_mark_tables_dirty(tls *libc.TLS, ws uintptr) {
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

func ZSTD_cwksp_mark_tables_clean(tls *libc.TLS, ws uintptr) {
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
	}
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

// C documentation
//
//	/**
//	 * Zero the part of the allocated tables not already marked clean.
//	 */
func ZSTD_cwksp_clean_tables(tls *libc.TLS, ws uintptr) {
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd {
		libc.Xmemset(tls, (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd, 0, uint64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd)-int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd)))
	}
	ZSTD_cwksp_mark_tables_clean(tls, ws)
}

// C documentation
//
//	/**
//	 * Invalidates table allocations.
//	 * All other allocations remain valid.
//	 */
func ZSTD_cwksp_clear_tables(tls *libc.TLS, ws uintptr) {
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

// C documentation
//
//	/**
//	 * Invalidates all buffer, aligned, and table allocations.
//	 * Object allocations remain valid.
//	 */
func ZSTD_cwksp_clear(tls *libc.TLS, ws uintptr) {
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart = ZSTD_cwksp_initialAllocStart(tls, ws)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(0)
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase > int32(ZSTD_cwksp_alloc_aligned_init_once) {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase = int32(ZSTD_cwksp_alloc_aligned_init_once)
	}
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

func ZSTD_cwksp_sizeof(tls *libc.TLS, ws uintptr) (r size_t) {
	return uint64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd) - int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace))
}

func ZSTD_cwksp_used(tls *libc.TLS, ws uintptr) (r size_t) {
	return uint64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd)-int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace)) + uint64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd)-int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart))
}

// C documentation
//
//	/**
//	 * The provided workspace takes ownership of the buffer [start, start+size).
//	 * Any existing values in the workspace are ignored (the previously managed
//	 * buffer, if present, must be separately freed).
//	 */
func ZSTD_cwksp_init(tls *libc.TLS, ws uintptr, start uintptr, size size_t, isStatic ZSTD_cwksp_static_alloc_e) {
	/* ensure correct alignment */
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace = start
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd = start + uintptr(size)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart = ZSTD_cwksp_initialAllocStart(tls, ws)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase = int32(ZSTD_cwksp_alloc_objects)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FisStatic = isStatic
	ZSTD_cwksp_clear(tls, ws)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration = 0
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

func ZSTD_cwksp_create(tls *libc.TLS, ws uintptr, size size_t, customMem ZSTD_customMem) (r size_t) {
	var workspace uintptr
	_ = workspace
	workspace = ZSTD_customMalloc(tls, size, customMem)
	if workspace == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	ZSTD_cwksp_init(tls, ws, workspace, size, int32(ZSTD_cwksp_dynamic_alloc))
	return uint64(0)
}

func ZSTD_cwksp_free(tls *libc.TLS, ws uintptr, customMem ZSTD_customMem) {
	var ptr uintptr
	_ = ptr
	ptr = (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace
	libc.Xmemset(tls, ws, 0, libc.Uint64FromInt64(72))
	ZSTD_customFree(tls, ptr, customMem)
}

// C documentation
//
//	/**
//	 * Moves the management of a workspace from one cwksp to another. The src cwksp
//	 * is left in an invalid state (src must be re-init()'ed before it's used again).
//	 */
func ZSTD_cwksp_move(tls *libc.TLS, dst uintptr, src uintptr) {
	*(*ZSTD_cwksp)(unsafe.Pointer(dst)) = *(*ZSTD_cwksp)(unsafe.Pointer(src))
	libc.Xmemset(tls, src, 0, libc.Uint64FromInt64(72))
}

func ZSTD_cwksp_reserve_failed(tls *libc.TLS, ws uintptr) (r int32) {
	return int32((*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed)
}

/*-*************************************
*  Functions Checking Free Space
***************************************/

// C documentation
//
//	/* ZSTD_alignmentSpaceWithinBounds() :
//	 * Returns if the estimated space needed for a wksp is within an acceptable limit of the
//	 * actual amount of space used.
//	 */
func ZSTD_cwksp_estimated_space_within_bounds(tls *libc.TLS, ws uintptr, estimatedSpace size_t) (r int32) {
	/* We have an alignment space between objects and tables between tables and buffers, so we can have up to twice
	 * the alignment bytes difference between estimation and actual usage */
	return libc.BoolInt32(estimatedSpace-ZSTD_cwksp_slack_space_required(tls) <= ZSTD_cwksp_used(tls, ws) && ZSTD_cwksp_used(tls, ws) <= estimatedSpace)
}

func ZSTD_cwksp_available_space(tls *libc.TLS, ws uintptr) (r size_t) {
	return uint64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart) - int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd))
}

func ZSTD_cwksp_check_available(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) (r int32) {
	return libc.BoolInt32(ZSTD_cwksp_available_space(tls, ws) >= additionalNeededSpace)
}

func ZSTD_cwksp_check_too_large(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) (r int32) {
	return ZSTD_cwksp_check_available(tls, ws, additionalNeededSpace*uint64(ZSTD_WORKSPACETOOLARGE_FACTOR))
}

func ZSTD_cwksp_check_wasteful(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) (r int32) {
	return libc.BoolInt32(ZSTD_cwksp_check_too_large(tls, ws, additionalNeededSpace) != 0 && (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration > int32(ZSTD_WORKSPACETOOLARGE_MAXDURATION))
}

func ZSTD_cwksp_bump_oversized_duration(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) {
	if ZSTD_cwksp_check_too_large(tls, ws, additionalNeededSpace) != 0 {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration + 1
	} else {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration = 0
	}
}

/**** ended inlining zstd_cwksp.h ****/
/**** start inlining zstdmt_compress.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ===   Dependencies   === */
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/

/* Note : This is an internal API.
 *        These APIs used to be exposed with ZSTDLIB_API,
 *        because it used to be the only way to invoke MT compression.
 *        Now, you must use ZSTD_compress2 and ZSTD_compressStream2() instead.
 *
 *        This API requires ZSTD_MULTITHREAD to be defined during compilation,
 *        otherwise ZSTDMT_createCCtx*() will fail.
 */

/* ===   Constants   === */

/* ========================================================
 * ===  Private interface, for use by ZSTD_compress.c   ===
 * ===  Not exposed in libzstd. Never invoke directly   ===
 * ======================================================== */

// C documentation
//
//	/* ===   Memory management   === */
type ZSTDMT_CCtx = struct {
	Ffactory           uintptr
	Fjobs              uintptr
	FbufPool           uintptr
	FcctxPool          uintptr
	FseqPool           uintptr
	Fparams            ZSTD_CCtx_params
	FtargetSectionSize size_t
	FtargetPrefixSize  size_t
	FjobReady          int32
	FinBuff            InBuff_t
	FroundBuff         RoundBuff_t
	Fserial            SerialState
	Frsync             RSyncState_t
	FjobIDMask         uint32
	FdoneJobID         uint32
	FnextJobID         uint32
	FframeEnded        uint32
	FallJobsCompleted  uint32
	FframeContentSize  uint64
	Fconsumed          uint64
	Fproduced          uint64
	FcMem              ZSTD_customMem
	FcdictLocal        uintptr
	Fcdict             uintptr
	F__ccgo3032        uint8
}

/**** ended inlining zstd_cwksp.h ****/
/**** start inlining zstdmt_compress.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ===   Dependencies   === */
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/

/* Note : This is an internal API.
 *        These APIs used to be exposed with ZSTDLIB_API,
 *        because it used to be the only way to invoke MT compression.
 *        Now, you must use ZSTD_compress2 and ZSTD_compressStream2() instead.
 *
 *        This API requires ZSTD_MULTITHREAD to be defined during compilation,
 *        otherwise ZSTDMT_createCCtx*() will fail.
 */

/* ===   Constants   === */

/* ========================================================
 * ===  Private interface, for use by ZSTD_compress.c   ===
 * ===  Not exposed in libzstd. Never invoke directly   ===
 * ======================================================== */

// C documentation
//
//	/* ===   Memory management   === */
type ZSTDMT_CCtx_s = ZSTDMT_CCtx

/**** ended inlining zstd_preSplit.h ****/

/*-*************************************
*  Constants
***************************************/

// C documentation
//
//	/*-*************************************
//	*  Context memory management
//	***************************************/
type ZSTD_compressionStage_e = int32

type ZSTD_cStreamStage = int32

type ZSTD_prefixDict = struct {
	Fdict            uintptr
	FdictSize        size_t
	FdictContentType ZSTD_dictContentType_e
}

type ZSTD_prefixDict_s = ZSTD_prefixDict

type ZSTD_localDict = struct {
	FdictBuffer      uintptr
	Fdict            uintptr
	FdictSize        size_t
	FdictContentType ZSTD_dictContentType_e
	Fcdict           uintptr
}

type ZSTD_hufCTables_t = struct {
	FCTable     [257]HUF_CElt
	FrepeatMode HUF_repeat
}

type ZSTD_fseCTables_t = struct {
	FoffcodeCTable          [193]FSE_CTable
	FmatchlengthCTable      [363]FSE_CTable
	FlitlengthCTable        [329]FSE_CTable
	Foffcode_repeatMode     FSE_repeat
	Fmatchlength_repeatMode FSE_repeat
	Flitlength_repeatMode   FSE_repeat
}

type ZSTD_entropyCTables_t = struct {
	Fhuf ZSTD_hufCTables_t
	Ffse ZSTD_fseCTables_t
}

// C documentation
//
//	/***********************************************
//	*  Sequences *
//	***********************************************/
type SeqDef = struct {
	FoffBase   U32
	FlitLength U16
	FmlBase    U16
}

// C documentation
//
//	/***********************************************
//	*  Sequences *
//	***********************************************/
type SeqDef_s = SeqDef

// C documentation
//
//	/* Controls whether seqStore has a single "long" litLength or matchLength. See SeqStore_t. */
type ZSTD_longLengthType_e = int32

const ZSTD_llt_none = 0
const /* no longLengthType */
ZSTD_llt_literalLength = 1
const /* represents a long literal */
ZSTD_llt_matchLength = 2

type SeqStore_t = struct {
	FsequencesStart uintptr
	Fsequences      uintptr
	FlitStart       uintptr
	Flit            uintptr
	FllCode         uintptr
	FmlCode         uintptr
	FofCode         uintptr
	FmaxNbSeq       size_t
	FmaxNbLit       size_t
	FlongLengthType ZSTD_longLengthType_e
	FlongLengthPos  U32
}

type ZSTD_SequenceLength = struct {
	FlitLength   U32
	FmatchLength U32
}

// C documentation
//
//	/**
//	 * Returns the ZSTD_SequenceLength for the given sequences. It handles the decoding of long sequences
//	 * indicated by longLengthPos and longLengthType, and adds MINMATCH back to matchLength.
//	 */
func ZSTD_getSequenceLength(tls *libc.TLS, seqStore uintptr, seq uintptr) (r ZSTD_SequenceLength) {
	var seqLen ZSTD_SequenceLength
	_ = seqLen
	seqLen.FlitLength = uint32((*SeqDef)(unsafe.Pointer(seq)).FlitLength)
	seqLen.FmatchLength = uint32(int32((*SeqDef)(unsafe.Pointer(seq)).FmlBase) + int32(MINMATCH))
	if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos == uint32((int64(seq)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart))/8) {
		if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
			seqLen.FlitLength += uint32(0x10000)
		}
		if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_matchLength) {
			seqLen.FmatchLength += uint32(0x10000)
		}
	}
	return seqLen
}

/* compress, dictBuilder, decodeCorpus (shouldn't get its definition from here) */

// C documentation
//
//	/***********************************************
//	*  Entropy buffer statistics structs and funcs *
//	***********************************************/
//	/** ZSTD_hufCTablesMetadata_t :
//	 *  Stores Literals Block Type for a super-block in hType, and
//	 *  huffman tree description in hufDesBuffer.
//	 *  hufDesSize refers to the size of huffman tree description in bytes.
//	 *  This metadata is populated in ZSTD_buildBlockEntropyStats_literals() */
type ZSTD_hufCTablesMetadata_t = struct {
	FhType        SymbolEncodingType_e
	FhufDesBuffer [128]BYTE
	FhufDesSize   size_t
}

// C documentation
//
//	/** ZSTD_fseCTablesMetadata_t :
//	 *  Stores symbol compression modes for a super-block in {ll, ol, ml}Type, and
//	 *  fse tables in fseTablesBuffer.
//	 *  fseTablesSize refers to the size of fse tables in bytes.
//	 *  This metadata is populated in ZSTD_buildBlockEntropyStats_sequences() */
type ZSTD_fseCTablesMetadata_t = struct {
	FllType          SymbolEncodingType_e
	FofType          SymbolEncodingType_e
	FmlType          SymbolEncodingType_e
	FfseTablesBuffer [133]BYTE
	FfseTablesSize   size_t
	FlastCountSize   size_t
}

type ZSTD_entropyCTablesMetadata_t = struct {
	FhufMetadata ZSTD_hufCTablesMetadata_t
	FfseMetadata ZSTD_fseCTablesMetadata_t
}

/*********************************
*  Compression internals structs *
*********************************/

type ZSTD_match_t = struct {
	Foff  U32
	Flen1 U32
}

type rawSeq = struct {
	Foffset      U32
	FlitLength   U32
	FmatchLength U32
}

type RawSeqStore_t = struct {
	Fseq           uintptr
	Fpos           size_t
	FposInSequence size_t
	Fsize          size_t
	Fcapacity      size_t
}

var kNullRawSeqStore = RawSeqStore_t{}

type ZSTD_optimal_t = struct {
	Fprice  int32
	Foff    U32
	Fmlen   U32
	Flitlen U32
	Frep    [3]U32
}

type ZSTD_OptPrice_e = int32

const zop_dynamic = 0
const zop_predef = 1

type optState_t = struct {
	FlitFreq                 uintptr
	FlitLengthFreq           uintptr
	FmatchLengthFreq         uintptr
	FoffCodeFreq             uintptr
	FmatchTable              uintptr
	FpriceTable              uintptr
	FlitSum                  U32
	FlitLengthSum            U32
	FmatchLengthSum          U32
	FoffCodeSum              U32
	FlitSumBasePrice         U32
	FlitLengthSumBasePrice   U32
	FmatchLengthSumBasePrice U32
	FoffCodeSumBasePrice     U32
	FpriceType               ZSTD_OptPrice_e
	FsymbolCosts             uintptr
	FliteralCompressionMode  ZSTD_ParamSwitch_e
}

type ZSTD_compressedBlockState_t = struct {
	Fentropy ZSTD_entropyCTables_t
	Frep     [3]U32
}

type ZSTD_window_t = struct {
	FnextSrc               uintptr
	Fbase                  uintptr
	FdictBase              uintptr
	FdictLimit             U32
	FlowLimit              U32
	FnbOverflowCorrections U32
}

type ZSTD_MatchState_t = struct {
	Fwindow              ZSTD_window_t
	FloadedDictEnd       U32
	FnextToUpdate        U32
	FhashLog3            U32
	FrowHashLog          U32
	FtagTable            uintptr
	FhashCache           [8]U32
	FhashSalt            U64
	FhashSaltEntropy     U32
	FhashTable           uintptr
	FhashTable3          uintptr
	FchainTable          uintptr
	FforceNonContiguous  int32
	FdedicatedDictSearch int32
	Fopt                 optState_t
	FdictMatchState      uintptr
	FcParams             ZSTD_compressionParameters
	FldmSeqStore         uintptr
	FprefetchCDictTables int32
	FlazySkipping        int32
}

type ZSTD_blockState_t = struct {
	FprevCBlock uintptr
	FnextCBlock uintptr
	FmatchState ZSTD_MatchState_t
}

type ldmEntry_t = struct {
	Foffset   U32
	Fchecksum U32
}

type ldmMatchCandidate_t = struct {
	Fsplit    uintptr
	Fhash     U32
	Fchecksum U32
	Fbucket   uintptr
}

type ldmState_t = struct {
	Fwindow          ZSTD_window_t
	FhashTable       uintptr
	FloadedDictEnd   U32
	FbucketOffsets   uintptr
	FsplitIndices    [64]size_t
	FmatchCandidates [64]ldmMatchCandidate_t
}

type ldmParams_t = struct {
	FenableLdm      ZSTD_ParamSwitch_e
	FhashLog        U32
	FbucketSizeLog  U32
	FminMatchLength U32
	FhashRateLog    U32
	FwindowLog      U32
}

type SeqCollector = struct {
	FcollectSequences int32
	FseqStart         uintptr
	FseqIndex         size_t
	FmaxSequences     size_t
}

/* typedef'd to ZSTD_CCtx_params within "zstd.h" */

// C documentation
//
//	/**
//	 * Indicates whether this compression proceeds directly from user-provided
//	 * source buffer to user-provided destination buffer (ZSTDb_not_buffered), or
//	 * whether the context needs to buffer the input/output (ZSTDb_buffered).
//	 */
type ZSTD_buffered_policy_e = int32

// C documentation
//
//	/**
//	 * Struct that contains all elements of block splitter that should be allocated
//	 * in a wksp.
//	 */
type ZSTD_blockSplitCtx = struct {
	FfullSeqStoreChunk  SeqStore_t
	FfirstHalfSeqStore  SeqStore_t
	FsecondHalfSeqStore SeqStore_t
	FcurrSeqStore       SeqStore_t
	FnextSeqStore       SeqStore_t
	Fpartitions         [196]U32
	FentropyMetadata    ZSTD_entropyCTablesMetadata_t
}

type ZSTD_dictTableLoadMethod_e = int32

const ZSTD_dtlm_fast = 0
const ZSTD_dtlm_full = 1

type ZSTD_tableFillPurpose_e = int32

const ZSTD_tfp_forCCtx = 0
const ZSTD_tfp_forCDict = 1

type ZSTD_dictMode_e = int32

const ZSTD_noDict = 0
const ZSTD_extDict = 1
const ZSTD_dictMatchState = 2
const ZSTD_dedicatedDictSearch = 3

type ZSTD_CParamMode_e = int32

const ZSTD_cpm_noAttachDict = 0
const /* Compression with ZSTD_noDict or ZSTD_extDict.
 * In this mode we use both the srcSize and the dictSize
 * when selecting and adjusting parameters.
 */
ZSTD_cpm_attachDict = 1
const /* Compression with ZSTD_dictMatchState or ZSTD_dedicatedDictSearch.
 * In this mode we only take the srcSize into account when selecting
 * and adjusting parameters.
 */
ZSTD_cpm_createCDict = 2
const /* Creating a CDict.
 * In this mode we take both the source size and the dictionary size
 * into account when selecting and adjusting the parameters.
 */
ZSTD_cpm_unknown = 3

type ZSTD_BlockCompressor_f = uintptr

func ZSTD_LLcode(tls *libc.TLS, litLength U32) (r U32) {
	var v1 uint32
	_ = v1
	if litLength > uint32(63) {
		v1 = ZSTD_highbit32(tls, litLength) + LL_deltaCode
	} else {
		v1 = uint32(LL_Code[litLength])
	}
	return v1
}

var LL_Code = [64]BYTE{
	1:  uint8(1),
	2:  uint8(2),
	3:  uint8(3),
	4:  uint8(4),
	5:  uint8(5),
	6:  uint8(6),
	7:  uint8(7),
	8:  uint8(8),
	9:  uint8(9),
	10: uint8(10),
	11: uint8(11),
	12: uint8(12),
	13: uint8(13),
	14: uint8(14),
	15: uint8(15),
	16: uint8(16),
	17: uint8(16),
	18: uint8(17),
	19: uint8(17),
	20: uint8(18),
	21: uint8(18),
	22: uint8(19),
	23: uint8(19),
	24: uint8(20),
	25: uint8(20),
	26: uint8(20),
	27: uint8(20),
	28: uint8(21),
	29: uint8(21),
	30: uint8(21),
	31: uint8(21),
	32: uint8(22),
	33: uint8(22),
	34: uint8(22),
	35: uint8(22),
	36: uint8(22),
	37: uint8(22),
	38: uint8(22),
	39: uint8(22),
	40: uint8(23),
	41: uint8(23),
	42: uint8(23),
	43: uint8(23),
	44: uint8(23),
	45: uint8(23),
	46: uint8(23),
	47: uint8(23),
	48: uint8(24),
	49: uint8(24),
	50: uint8(24),
	51: uint8(24),
	52: uint8(24),
	53: uint8(24),
	54: uint8(24),
	55: uint8(24),
	56: uint8(24),
	57: uint8(24),
	58: uint8(24),
	59: uint8(24),
	60: uint8(24),
	61: uint8(24),
	62: uint8(24),
	63: uint8(24),
}

var LL_deltaCode = uint32(19)

// C documentation
//
//	/* ZSTD_MLcode() :
//	 * note : mlBase = matchLength - MINMATCH;
//	 *        because it's the format it's stored in seqStore->sequences */
func ZSTD_MLcode(tls *libc.TLS, mlBase U32) (r U32) {
	var v1 uint32
	_ = v1
	if mlBase > uint32(127) {
		v1 = ZSTD_highbit32(tls, mlBase) + ML_deltaCode
	} else {
		v1 = uint32(ML_Code[mlBase])
	}
	return v1
}

var ML_Code = [128]BYTE{
	1:   uint8(1),
	2:   uint8(2),
	3:   uint8(3),
	4:   uint8(4),
	5:   uint8(5),
	6:   uint8(6),
	7:   uint8(7),
	8:   uint8(8),
	9:   uint8(9),
	10:  uint8(10),
	11:  uint8(11),
	12:  uint8(12),
	13:  uint8(13),
	14:  uint8(14),
	15:  uint8(15),
	16:  uint8(16),
	17:  uint8(17),
	18:  uint8(18),
	19:  uint8(19),
	20:  uint8(20),
	21:  uint8(21),
	22:  uint8(22),
	23:  uint8(23),
	24:  uint8(24),
	25:  uint8(25),
	26:  uint8(26),
	27:  uint8(27),
	28:  uint8(28),
	29:  uint8(29),
	30:  uint8(30),
	31:  uint8(31),
	32:  uint8(32),
	33:  uint8(32),
	34:  uint8(33),
	35:  uint8(33),
	36:  uint8(34),
	37:  uint8(34),
	38:  uint8(35),
	39:  uint8(35),
	40:  uint8(36),
	41:  uint8(36),
	42:  uint8(36),
	43:  uint8(36),
	44:  uint8(37),
	45:  uint8(37),
	46:  uint8(37),
	47:  uint8(37),
	48:  uint8(38),
	49:  uint8(38),
	50:  uint8(38),
	51:  uint8(38),
	52:  uint8(38),
	53:  uint8(38),
	54:  uint8(38),
	55:  uint8(38),
	56:  uint8(39),
	57:  uint8(39),
	58:  uint8(39),
	59:  uint8(39),
	60:  uint8(39),
	61:  uint8(39),
	62:  uint8(39),
	63:  uint8(39),
	64:  uint8(40),
	65:  uint8(40),
	66:  uint8(40),
	67:  uint8(40),
	68:  uint8(40),
	69:  uint8(40),
	70:  uint8(40),
	71:  uint8(40),
	72:  uint8(40),
	73:  uint8(40),
	74:  uint8(40),
	75:  uint8(40),
	76:  uint8(40),
	77:  uint8(40),
	78:  uint8(40),
	79:  uint8(40),
	80:  uint8(41),
	81:  uint8(41),
	82:  uint8(41),
	83:  uint8(41),
	84:  uint8(41),
	85:  uint8(41),
	86:  uint8(41),
	87:  uint8(41),
	88:  uint8(41),
	89:  uint8(41),
	90:  uint8(41),
	91:  uint8(41),
	92:  uint8(41),
	93:  uint8(41),
	94:  uint8(41),
	95:  uint8(41),
	96:  uint8(42),
	97:  uint8(42),
	98:  uint8(42),
	99:  uint8(42),
	100: uint8(42),
	101: uint8(42),
	102: uint8(42),
	103: uint8(42),
	104: uint8(42),
	105: uint8(42),
	106: uint8(42),
	107: uint8(42),
	108: uint8(42),
	109: uint8(42),
	110: uint8(42),
	111: uint8(42),
	112: uint8(42),
	113: uint8(42),
	114: uint8(42),
	115: uint8(42),
	116: uint8(42),
	117: uint8(42),
	118: uint8(42),
	119: uint8(42),
	120: uint8(42),
	121: uint8(42),
	122: uint8(42),
	123: uint8(42),
	124: uint8(42),
	125: uint8(42),
	126: uint8(42),
	127: uint8(42),
}

var ML_deltaCode = uint32(36)

// C documentation
//
//	/* ZSTD_cParam_withinBounds:
//	 * @return 1 if value is within cParam bounds,
//	 * 0 otherwise */
func ZSTD_cParam_withinBounds(tls *libc.TLS, cParam ZSTD_cParameter, value int32) (r int32) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_cParam_getBounds(tls, cParam)
	if ZSTD_isError(tls, bounds.Ferror1) != 0 {
		return 0
	}
	if value < bounds.FlowerBound {
		return 0
	}
	if value > bounds.FupperBound {
		return 0
	}
	return int32(1)
}

// C documentation
//
//	/* ZSTD_selectAddr:
//	 * @return index >= lowLimit ? candidate : backup,
//	 * tries to force branchless codegen. */
func ZSTD_selectAddr(tls *libc.TLS, index U32, lowLimit U32, candidate uintptr, backup uintptr) (r uintptr) {
	var v1 uintptr
	_ = v1
	if index >= lowLimit {
		v1 = candidate
	} else {
		v1 = backup
	}
	return v1
}

// C documentation
//
//	/* ZSTD_noCompressBlock() :
//	 * Writes uncompressed block to dst buffer from given src.
//	 * Returns the size of the block */
func ZSTD_noCompressBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32) (r size_t) {
	var cBlockHeader24 U32
	_ = cBlockHeader24
	cBlockHeader24 = lastBlock + uint32(bt_raw)<<libc.Int32FromInt32(1) + uint32(srcSize<<libc.Int32FromInt32(3))
	if srcSize+ZSTD_blockHeaderSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1391, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	MEM_writeLE24(tls, dst, cBlockHeader24)
	libc.Xmemcpy(tls, dst+uintptr(ZSTD_blockHeaderSize), src, srcSize)
	return ZSTD_blockHeaderSize + srcSize
}

func ZSTD_rleCompressBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src BYTE, srcSize size_t, lastBlock U32) (r size_t) {
	var cBlockHeader U32
	var op uintptr
	_, _ = cBlockHeader, op
	op = dst
	cBlockHeader = lastBlock + uint32(bt_rle)<<libc.Int32FromInt32(1) + uint32(srcSize<<libc.Int32FromInt32(3))
	if dstCapacity < uint64(4) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	MEM_writeLE24(tls, op, cBlockHeader)
	*(*BYTE)(unsafe.Pointer(op + 3)) = src
	return uint64(4)
}

// C documentation
//
//	/* ZSTD_minGain() :
//	 * minimum compression required
//	 * to generate a compress block or a compressed literals section.
//	 * note : use same formula for both situations */
func ZSTD_minGain(tls *libc.TLS, srcSize size_t, strat ZSTD_strategy) (r size_t) {
	var minlog U32
	var v1 uint32
	_, _ = minlog, v1
	if strat >= int32(ZSTD_btultra) {
		v1 = uint32(strat) - uint32(1)
	} else {
		v1 = uint32(6)
	}
	minlog = v1
	_ = libc.Uint64FromInt64(1)
	return srcSize>>minlog + uint64(2)
}

func ZSTD_literalsCompressionIsDisabled(tls *libc.TLS, cctxParams uintptr) (r int32) {
	switch (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FliteralCompressionMode {
	case int32(ZSTD_ps_enable):
		return 0
	case int32(ZSTD_ps_disable):
		return int32(1)
	default:
		fallthrough
	case int32(ZSTD_ps_auto):
		return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy == int32(ZSTD_fast) && (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.FtargetLength > uint32(0))
	}
	return r
}

// C documentation
//
//	/*! ZSTD_safecopyLiterals() :
//	 *  memcpy() function that won't read beyond more than WILDCOPY_OVERLENGTH bytes past ilimit_w.
//	 *  Only called when the sequence ends past ilimit_w, so it only needs to be optimized for single
//	 *  large copies.
//	 */
func ZSTD_safecopyLiterals(tls *libc.TLS, op uintptr, ip uintptr, iend uintptr, ilimit_w uintptr) {
	var v1, v2 uintptr
	_, _ = v1, v2
	if ip <= ilimit_w {
		ZSTD_wildcopy(tls, op, ip, int64(ilimit_w)-int64(ip), int32(ZSTD_no_overlap))
		op = op + uintptr(int64(ilimit_w)-int64(ip))
		ip = ilimit_w
	}
	for ip < iend {
		v1 = op
		op = op + 1
		v2 = ip
		ip = ip + 1
		*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
	}
}

// C documentation
//
//	/*! ZSTD_storeSeqOnly() :
//	 *  Store a sequence (litlen, litPtr, offBase and matchLength) into SeqStore_t.
//	 *  Literals themselves are not copied, but @litPtr is updated.
//	 *  @offBase : Users should employ macros REPCODE_TO_OFFBASE() and OFFSET_TO_OFFBASE().
//	 *  @matchLength : must be >= MINMATCH
//	*/
func ZSTD_storeSeqOnly(tls *libc.TLS, seqStorePtr uintptr, litLength size_t, offBase U32, matchLength size_t) {
	var mlBase size_t
	_ = mlBase
	/* literal Length */
	if libc.BoolInt32(litLength > libc.Uint64FromInt32(0xFFFF)) != 0 {
		/* there can only be a single long length */
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType = int32(ZSTD_llt_literalLength)
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos = uint32((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	}
	(*(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences))).FlitLength = uint16(litLength)
	/* match offset */
	(*(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences))).FoffBase = offBase
	/* match Length */
	mlBase = matchLength - uint64(MINMATCH)
	if libc.BoolInt32(mlBase > libc.Uint64FromInt32(0xFFFF)) != 0 {
		/* there can only be a single long length */
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType = int32(ZSTD_llt_matchLength)
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos = uint32((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	}
	(*(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences))).FmlBase = uint16(mlBase)
	(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences += 8
}

// C documentation
//
//	/*! ZSTD_storeSeq() :
//	 *  Store a sequence (litlen, litPtr, offBase and matchLength) into SeqStore_t.
//	 *  @offBase : Users should employ macros REPCODE_TO_OFFBASE() and OFFSET_TO_OFFBASE().
//	 *  @matchLength : must be >= MINMATCH
//	 *  Allowed to over-read literals up to litLimit.
//	*/
func ZSTD_storeSeq(tls *libc.TLS, seqStorePtr uintptr, litLength size_t, literals uintptr, litLimit uintptr, offBase U32, matchLength size_t) {
	var litEnd, litLimit_w uintptr
	_, _ = litEnd, litLimit_w
	litLimit_w = litLimit - uintptr(WILDCOPY_OVERLENGTH)
	litEnd = literals + uintptr(litLength)
	/* copy Literals */
	if litEnd <= litLimit_w {
		/* Common case we can use wildcopy.
		 * First copy 16 bytes, because literals are likely short.
		 */
		_ = libc.Uint64FromInt64(1)
		ZSTD_copy16(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit, literals)
		if litLength > uint64(16) {
			ZSTD_wildcopy(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit+uintptr(16), literals+uintptr(16), int64(litLength)-int64(16), int32(ZSTD_no_overlap))
		}
	} else {
		ZSTD_safecopyLiterals(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit, literals, litEnd, litLimit_w)
	}
	*(*uintptr)(unsafe.Pointer(seqStorePtr + 24)) += uintptr(litLength)
	ZSTD_storeSeqOnly(tls, seqStorePtr, litLength, offBase, matchLength)
}

// C documentation
//
//	/* ZSTD_updateRep() :
//	 * updates in-place @rep (array of repeat offsets)
//	 * @offBase : sum-type, using numeric representation of ZSTD_storeSeq()
//	 */
func ZSTD_updateRep(tls *libc.TLS, rep uintptr, offBase U32, ll0 U32) {
	var currentOffset, repCode U32
	var v1, v2 uint32
	_, _, _, _ = currentOffset, repCode, v1, v2
	if offBase > uint32(ZSTD_REP_NUM) { /* full offset */
		*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep + 1*4))
		*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
		*(*U32)(unsafe.Pointer(rep)) = offBase - libc.Uint32FromInt32(ZSTD_REP_NUM)
	} else { /* repcode */
		repCode = offBase - uint32(1) + ll0
		if repCode > uint32(0) {
			if repCode == uint32(ZSTD_REP_NUM) {
				v1 = *(*U32)(unsafe.Pointer(rep)) - uint32(1)
			} else {
				v1 = *(*U32)(unsafe.Pointer(rep + uintptr(repCode)*4))
			} /* note : if repCode==0, no change */
			currentOffset = v1
			if repCode >= uint32(2) {
				v2 = *(*U32)(unsafe.Pointer(rep + 1*4))
			} else {
				v2 = *(*U32)(unsafe.Pointer(rep + 2*4))
			}
			*(*U32)(unsafe.Pointer(rep + 2*4)) = v2
			*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
			*(*U32)(unsafe.Pointer(rep)) = currentOffset
		} else { /* repCode == 0 */
			/* nothing to do */
		}
	}
}

type Repcodes_t = struct {
	Frep [3]U32
}

type repcodes_s = Repcodes_t

func ZSTD_newRep(tls *libc.TLS, rep uintptr, offBase U32, ll0 U32) (r Repcodes_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* newReps at bp+0 */ Repcodes_t
	libc.Xmemcpy(tls, bp, rep, libc.Uint64FromInt64(12))
	ZSTD_updateRep(tls, bp, offBase, ll0)
	return *(*Repcodes_t)(unsafe.Pointer(bp))
}

// C documentation
//
//	/*-*************************************
//	*  Match length counter
//	***************************************/
func ZSTD_count(tls *libc.TLS, pIn uintptr, pMatch uintptr, pInLimit uintptr) (r size_t) {
	var diff, diff1 size_t
	var pInLoopLimit, pStart uintptr
	_, _, _, _ = diff, diff1, pInLoopLimit, pStart
	pStart = pIn
	pInLoopLimit = pInLimit - uintptr(libc.Uint64FromInt64(8)-libc.Uint64FromInt32(1))
	if pIn < pInLoopLimit {
		diff = MEM_readST(tls, pMatch) ^ MEM_readST(tls, pIn)
		if diff != 0 {
			return uint64(ZSTD_NbCommonBytes(tls, diff))
		}
		pIn = pIn + uintptr(8)
		pMatch = pMatch + uintptr(8)
		for pIn < pInLoopLimit {
			diff1 = MEM_readST(tls, pMatch) ^ MEM_readST(tls, pIn)
			if !(diff1 != 0) {
				pIn = pIn + uintptr(8)
				pMatch = pMatch + uintptr(8)
				continue
			}
			pIn = pIn + uintptr(ZSTD_NbCommonBytes(tls, diff1))
			return uint64(int64(pIn) - int64(pStart))
		}
	}
	if MEM_64bits(tls) != 0 && pIn < pInLimit-libc.UintptrFromInt32(3) && MEM_read32(tls, pMatch) == MEM_read32(tls, pIn) {
		pIn = pIn + uintptr(4)
		pMatch = pMatch + uintptr(4)
	}
	if pIn < pInLimit-libc.UintptrFromInt32(1) && int32(MEM_read16(tls, pMatch)) == int32(MEM_read16(tls, pIn)) {
		pIn = pIn + uintptr(2)
		pMatch = pMatch + uintptr(2)
	}
	if pIn < pInLimit && int32(*(*BYTE)(unsafe.Pointer(pMatch))) == int32(*(*BYTE)(unsafe.Pointer(pIn))) {
		pIn = pIn + 1
	}
	return uint64(int64(pIn) - int64(pStart))
}

// C documentation
//
//	/** ZSTD_count_2segments() :
//	 *  can count match length with `ip` & `match` in 2 different segments.
//	 *  convention : on reaching mEnd, match count continue starting from iStart
//	 */
func ZSTD_count_2segments(tls *libc.TLS, ip uintptr, match uintptr, iEnd uintptr, mEnd uintptr, iStart uintptr) (r size_t) {
	var matchLength size_t
	var vEnd, v1 uintptr
	_, _, _ = matchLength, vEnd, v1
	if ip+uintptr(int64(mEnd)-int64(match)) < iEnd {
		v1 = ip + uintptr(int64(mEnd)-int64(match))
	} else {
		v1 = iEnd
	}
	vEnd = v1
	matchLength = ZSTD_count(tls, ip, match, vEnd)
	if match+uintptr(matchLength) != mEnd {
		return matchLength
	}
	return matchLength + ZSTD_count(tls, ip+uintptr(matchLength), iStart, iEnd)
}

// C documentation
//
//	/*-*************************************
//	 *  Hashes
//	 ***************************************/
var prime3bytes = uint32(506832829)

func ZSTD_hash3(tls *libc.TLS, u U32, h U32, s U32) (r U32) {
	return (u<<(libc.Int32FromInt32(32)-libc.Int32FromInt32(24))*prime3bytes ^ s) >> (uint32(32) - h)
}

func ZSTD_hash3Ptr(tls *libc.TLS, ptr uintptr, h U32) (r size_t) {
	return uint64(ZSTD_hash3(tls, MEM_readLE32(tls, ptr), h, uint32(0)))
}

/* only in zstd_opt.h */

func ZSTD_hash3PtrS(tls *libc.TLS, ptr uintptr, h U32, s U32) (r size_t) {
	return uint64(ZSTD_hash3(tls, MEM_readLE32(tls, ptr), h, s))
}

var prime4bytes = uint32(2654435761)

func ZSTD_hash4(tls *libc.TLS, u U32, h U32, s U32) (r U32) {
	return (u*prime4bytes ^ s) >> (uint32(32) - h)
}

func ZSTD_hash4Ptr(tls *libc.TLS, ptr uintptr, h U32) (r size_t) {
	return uint64(ZSTD_hash4(tls, MEM_readLE32(tls, ptr), h, uint32(0)))
}

func ZSTD_hash4PtrS(tls *libc.TLS, ptr uintptr, h U32, s U32) (r size_t) {
	return uint64(ZSTD_hash4(tls, MEM_readLE32(tls, ptr), h, s))
}

var prime5bytes = uint64(889523592379)

func ZSTD_hash5(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u<<(libc.Int32FromInt32(64)-libc.Int32FromInt32(40))*prime5bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash5Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash5(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash5PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash5(tls, MEM_readLE64(tls, p), h, s)
}

var prime6bytes = uint64(227718039650203)

func ZSTD_hash6(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u<<(libc.Int32FromInt32(64)-libc.Int32FromInt32(48))*prime6bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash6Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash6(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash6PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash6(tls, MEM_readLE64(tls, p), h, s)
}

var prime7bytes = uint64(58295818150454627)

func ZSTD_hash7(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u<<(libc.Int32FromInt32(64)-libc.Int32FromInt32(56))*prime7bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash7Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash7(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash7PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash7(tls, MEM_readLE64(tls, p), h, s)
}

var prime8bytes = uint64(0xCF1BBCDCB7A56463)

func ZSTD_hash8(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u*prime8bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash8Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash8(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash8PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash8(tls, MEM_readLE64(tls, p), h, s)
}

func ZSTD_hashPtr(tls *libc.TLS, p uintptr, hBits U32, mls U32) (r size_t) {
	/* Although some of these hashes do support hBits up to 64, some do not.
	 * To be on the safe side, always avoid hBits > 32. */
	switch mls {
	default:
		fallthrough
	case uint32(4):
		return ZSTD_hash4Ptr(tls, p, hBits)
	case uint32(5):
		return ZSTD_hash5Ptr(tls, p, hBits)
	case uint32(6):
		return ZSTD_hash6Ptr(tls, p, hBits)
	case uint32(7):
		return ZSTD_hash7Ptr(tls, p, hBits)
	case uint32(8):
		return ZSTD_hash8Ptr(tls, p, hBits)
	}
	return r
}

func ZSTD_hashPtrSalted(tls *libc.TLS, p uintptr, hBits U32, mls U32, hashSalt U64) (r size_t) {
	/* Although some of these hashes do support hBits up to 64, some do not.
	 * To be on the safe side, always avoid hBits > 32. */
	switch mls {
	default:
		fallthrough
	case uint32(4):
		return ZSTD_hash4PtrS(tls, p, hBits, uint32(hashSalt))
	case uint32(5):
		return ZSTD_hash5PtrS(tls, p, hBits, hashSalt)
	case uint32(6):
		return ZSTD_hash6PtrS(tls, p, hBits, hashSalt)
	case uint32(7):
		return ZSTD_hash7PtrS(tls, p, hBits, hashSalt)
	case uint32(8):
		return ZSTD_hash8PtrS(tls, p, hBits, hashSalt)
	}
	return r
}

// C documentation
//
//	/** ZSTD_ipow() :
//	 * Return base^exponent.
//	 */
func ZSTD_ipow(tls *libc.TLS, base U64, exponent U64) (r U64) {
	var power U64
	_ = power
	power = uint64(1)
	for exponent != 0 {
		if exponent&uint64(1) != 0 {
			power = power * base
		}
		exponent = exponent >> uint64(1)
		base = base * base
	}
	return power
}

// C documentation
//
//	/** ZSTD_rollingHash_append() :
//	 * Add the buffer to the hash value.
//	 */
func ZSTD_rollingHash_append(tls *libc.TLS, hash U64, buf uintptr, size size_t) (r U64) {
	var istart uintptr
	var pos size_t
	_, _ = istart, pos
	istart = buf
	pos = uint64(0)
	for {
		if !(pos < size) {
			break
		}
		hash = hash * prime8bytes
		hash = hash + uint64(int32(*(*BYTE)(unsafe.Pointer(istart + uintptr(pos))))+int32(ZSTD_ROLL_HASH_CHAR_OFFSET))
		goto _1
	_1:
		;
		pos = pos + 1
	}
	return hash
}

// C documentation
//
//	/** ZSTD_rollingHash_compute() :
//	 * Compute the rolling hash value of the buffer.
//	 */
func ZSTD_rollingHash_compute(tls *libc.TLS, buf uintptr, size size_t) (r U64) {
	return ZSTD_rollingHash_append(tls, uint64(0), buf, size)
}

// C documentation
//
//	/** ZSTD_rollingHash_primePower() :
//	 * Compute the primePower to be passed to ZSTD_rollingHash_rotate() for a hash
//	 * over a window of length bytes.
//	 */
func ZSTD_rollingHash_primePower(tls *libc.TLS, length U32) (r U64) {
	return ZSTD_ipow(tls, prime8bytes, uint64(length-uint32(1)))
}

// C documentation
//
//	/** ZSTD_rollingHash_rotate() :
//	 * Rotate the rolling hash by one byte.
//	 */
func ZSTD_rollingHash_rotate(tls *libc.TLS, hash U64, toRemove BYTE, toAdd BYTE, primePower U64) (r U64) {
	hash = hash - uint64(int32(toRemove)+libc.Int32FromInt32(ZSTD_ROLL_HASH_CHAR_OFFSET))*primePower
	hash = hash * prime8bytes
	hash = hash + uint64(int32(toAdd)+int32(ZSTD_ROLL_HASH_CHAR_OFFSET))
	return hash
}

/*-*************************************
*  Round buffer management
***************************************/
/* Max @current value allowed:
 * In 32-bit mode: we want to avoid crossing the 2 GB limit,
 *                 reducing risks of side effects in case of signed operations on indexes.
 * In 64-bit mode: we want to ensure that adding the maximum job size (512 MB)
 *                 doesn't overflow U32 index capacity (4 GB) */
/* Maximum chunk size before overflow correction needs to be called again */

// C documentation
//
//	/**
//	 * ZSTD_window_clear():
//	 * Clears the window containing the history by simply setting it to empty.
//	 */
func ZSTD_window_clear(tls *libc.TLS, window uintptr) {
	var end U32
	var endT size_t
	_, _ = end, endT
	endT = uint64(int64((*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	end = uint32(endT)
	(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = end
	(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = end
}

func ZSTD_window_isEmpty(tls *libc.TLS, window ZSTD_window_t) (r U32) {
	return libc.BoolUint32(window.FdictLimit == uint32(ZSTD_WINDOW_START_INDEX) && window.FlowLimit == uint32(ZSTD_WINDOW_START_INDEX) && int64(window.FnextSrc)-int64(window.Fbase) == int64(ZSTD_WINDOW_START_INDEX))
}

// C documentation
//
//	/**
//	 * ZSTD_window_hasExtDict():
//	 * Returns non-zero if the window has a non-empty extDict.
//	 */
func ZSTD_window_hasExtDict(tls *libc.TLS, window ZSTD_window_t) (r U32) {
	return libc.BoolUint32(window.FlowLimit < window.FdictLimit)
}

// C documentation
//
//	/**
//	 * ZSTD_matchState_dictMode():
//	 * Inspects the provided matchState and figures out what dictMode should be
//	 * passed to the compressor.
//	 */
func ZSTD_matchState_dictMode(tls *libc.TLS, ms uintptr) (r ZSTD_dictMode_e) {
	var v1, v2, v3 int32
	_, _, _ = v1, v2, v3
	if ZSTD_window_hasExtDict(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow) != 0 {
		v1 = int32(ZSTD_extDict)
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState != libc.UintptrFromInt32(0) {
			if (*ZSTD_MatchState_t)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState)).FdedicatedDictSearch != 0 {
				v3 = int32(ZSTD_dedicatedDictSearch)
			} else {
				v3 = int32(ZSTD_dictMatchState)
			}
			v2 = v3
		} else {
			v2 = int32(ZSTD_noDict)
		}
		v1 = v2
	}
	return v1
}

/* Defining this macro to non-zero tells zstd to run the overflow correction
 * code much more frequently. This is very inefficient, and should only be
 * used for tests and fuzzers.
 */

// C documentation
//
//	/**
//	 * ZSTD_window_canOverflowCorrect():
//	 * Returns non-zero if the indices are large enough for overflow correction
//	 * to work correctly without impacting compression ratio.
//	 */
func ZSTD_window_canOverflowCorrect(tls *libc.TLS, window ZSTD_window_t, cycleLog U32, maxDist U32, loadedDictEnd U32, src uintptr) (r U32) {
	var adjustedIndex, adjustment, curr, cycleSize, dictionaryInvalidated, indexLargeEnough, minIndexToOverflowCorrect U32
	var v1, v2 uint32
	_, _, _, _, _, _, _, _, _ = adjustedIndex, adjustment, curr, cycleSize, dictionaryInvalidated, indexLargeEnough, minIndexToOverflowCorrect, v1, v2
	cycleSize = uint32(1) << cycleLog
	curr = uint32(int64(src) - int64(window.Fbase))
	if maxDist > cycleSize {
		v1 = maxDist
	} else {
		v1 = cycleSize
	}
	minIndexToOverflowCorrect = cycleSize + v1 + uint32(ZSTD_WINDOW_START_INDEX)
	/* Adjust the min index to backoff the overflow correction frequency,
	 * so we don't waste too much CPU in overflow correction. If this
	 * computation overflows we don't really care, we just need to make
	 * sure it is at least minIndexToOverflowCorrect.
	 */
	adjustment = window.FnbOverflowCorrections + uint32(1)
	if minIndexToOverflowCorrect*adjustment > minIndexToOverflowCorrect {
		v2 = minIndexToOverflowCorrect * adjustment
	} else {
		v2 = minIndexToOverflowCorrect
	}
	adjustedIndex = v2
	indexLargeEnough = libc.BoolUint32(curr > adjustedIndex)
	/* Only overflow correct early if the dictionary is invalidated already,
	 * so we don't hurt compression ratio.
	 */
	dictionaryInvalidated = libc.BoolUint32(curr > maxDist+loadedDictEnd)
	return libc.BoolUint32(indexLargeEnough != 0 && dictionaryInvalidated != 0)
}

// C documentation
//
//	/**
//	 * ZSTD_window_needOverflowCorrection():
//	 * Returns non-zero if the indices are getting too large and need overflow
//	 * protection.
//	 */
func ZSTD_window_needOverflowCorrection(tls *libc.TLS, window ZSTD_window_t, cycleLog U32, maxDist U32, loadedDictEnd U32, src uintptr, srcEnd uintptr) (r U32) {
	var curr U32
	var v1 uint32
	_, _ = curr, v1
	curr = uint32(int64(srcEnd) - int64(window.Fbase))
	if ZSTD_WINDOW_OVERFLOW_CORRECT_FREQUENTLY != 0 {
		if ZSTD_window_canOverflowCorrect(tls, window, cycleLog, maxDist, loadedDictEnd, src) != 0 {
			return uint32(1)
		}
	}
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	return libc.BoolUint32(curr > v1)
}

// C documentation
//
//	/**
//	 * ZSTD_window_correctOverflow():
//	 * Reduces the indices to protect from index overflow.
//	 * Returns the correction made to the indices, which must be applied to every
//	 * stored index.
//	 *
//	 * The least significant cycleLog bits of the indices must remain the same,
//	 * which may be 0. Every index up to maxDist in the past must be valid.
//	 */
func ZSTD_window_correctOverflow(tls *libc.TLS, window uintptr, cycleLog U32, maxDist U32, src uintptr) (r U32) {
	var correction, curr, currentCycle, currentCycleCorrection, cycleMask, cycleSize, newCurrent U32
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _ = correction, curr, currentCycle, currentCycleCorrection, cycleMask, cycleSize, newCurrent, v1, v2, v3
	/* preemptive overflow correction:
	 * 1. correction is large enough:
	 *    lowLimit > (3<<29) ==> current > 3<<29 + 1<<windowLog
	 *    1<<windowLog <= newCurrent < 1<<chainLog + 1<<windowLog
	 *
	 *    current - newCurrent
	 *    > (3<<29 + 1<<windowLog) - (1<<windowLog + 1<<chainLog)
	 *    > (3<<29) - (1<<chainLog)
	 *    > (3<<29) - (1<<30)             (NOTE: chainLog <= 30)
	 *    > 1<<29
	 *
	 * 2. (ip+ZSTD_CHUNKSIZE_MAX - cctx->base) doesn't overflow:
	 *    After correction, current is less than (1<<chainLog + 1<<windowLog).
	 *    In 64-bit mode we are safe, because we have 64-bit ptrdiff_t.
	 *    In 32-bit mode we are safe, because (chainLog <= 29), so
	 *    ip+ZSTD_CHUNKSIZE_MAX - cctx->base < 1<<32.
	 * 3. (cctx->lowLimit + 1<<windowLog) < 1<<32:
	 *    windowLog <= 31 ==> 3<<29 + 1<<windowLog < 7<<29 < 1<<32.
	 */
	cycleSize = uint32(1) << cycleLog
	cycleMask = cycleSize - uint32(1)
	curr = uint32(int64(src) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	currentCycle = curr & cycleMask
	if currentCycle < uint32(ZSTD_WINDOW_START_INDEX) {
		if cycleSize > uint32(libc.Int32FromInt32(ZSTD_WINDOW_START_INDEX)) {
			v2 = cycleSize
		} else {
			v2 = uint32(libc.Int32FromInt32(ZSTD_WINDOW_START_INDEX))
		}
		v1 = v2
	} else {
		v1 = uint32(0)
	}
	/* Ensure newCurrent - maxDist >= ZSTD_WINDOW_START_INDEX. */
	currentCycleCorrection = v1
	if maxDist > cycleSize {
		v3 = maxDist
	} else {
		v3 = cycleSize
	}
	newCurrent = currentCycle + currentCycleCorrection + v3
	correction = curr - newCurrent
	/* maxDist must be a power of two so that:
	 *   (newCurrent & cycleMask) == (curr & cycleMask)
	 * This is required to not corrupt the chains / binary tree.
	 */
	if !(libc.Int32FromInt32(ZSTD_WINDOW_OVERFLOW_CORRECT_FREQUENTLY) != 0) {
		/* Loose bound, should be around 1<<29 (see above) */
	}
	*(*uintptr)(unsafe.Pointer(window + 8)) += uintptr(correction)
	*(*uintptr)(unsafe.Pointer(window + 16)) += uintptr(correction)
	if (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit < correction+uint32(ZSTD_WINDOW_START_INDEX) {
		(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = uint32(ZSTD_WINDOW_START_INDEX)
	} else {
		*(*U32)(unsafe.Pointer(window + 28)) -= correction
	}
	if (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit < correction+uint32(ZSTD_WINDOW_START_INDEX) {
		(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = uint32(ZSTD_WINDOW_START_INDEX)
	} else {
		*(*U32)(unsafe.Pointer(window + 24)) -= correction
	}
	/* Ensure we can still reference the full window. */
	/* Ensure that lowLimit and dictLimit didn't underflow. */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnbOverflowCorrections = (*ZSTD_window_t)(unsafe.Pointer(window)).FnbOverflowCorrections + 1
	return correction
}

// C documentation
//
//	/**
//	 * ZSTD_window_enforceMaxDist():
//	 * Updates lowLimit so that:
//	 *    (srcEnd - base) - lowLimit == maxDist + loadedDictEnd
//	 *
//	 * It ensures index is valid as long as index >= lowLimit.
//	 * This must be called before a block compression call.
//	 *
//	 * loadedDictEnd is only defined if a dictionary is in use for current compression.
//	 * As the name implies, loadedDictEnd represents the index at end of dictionary.
//	 * The value lies within context's referential, it can be directly compared to blockEndIdx.
//	 *
//	 * If loadedDictEndPtr is NULL, no dictionary is in use, and we use loadedDictEnd == 0.
//	 * If loadedDictEndPtr is not NULL, we set it to zero after updating lowLimit.
//	 * This is because dictionaries are allowed to be referenced fully
//	 * as long as the last byte of the dictionary is in the window.
//	 * Once input has progressed beyond window size, dictionary cannot be referenced anymore.
//	 *
//	 * In normal dict mode, the dictionary lies between lowLimit and dictLimit.
//	 * In dictMatchState mode, lowLimit and dictLimit are the same,
//	 * and the dictionary is below them.
//	 * forceWindow and dictMatchState are therefore incompatible.
//	 */
func ZSTD_window_enforceMaxDist(tls *libc.TLS, window uintptr, blockEnd uintptr, maxDist U32, loadedDictEndPtr uintptr, dictMatchStatePtr uintptr) {
	var blockEndIdx, loadedDictEnd, newLowLimit U32
	var v1 uint32
	_, _, _, _ = blockEndIdx, loadedDictEnd, newLowLimit, v1
	blockEndIdx = uint32(int64(blockEnd) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	if loadedDictEndPtr != libc.UintptrFromInt32(0) {
		v1 = *(*U32)(unsafe.Pointer(loadedDictEndPtr))
	} else {
		v1 = uint32(0)
	}
	loadedDictEnd = v1
	/* - When there is no dictionary : loadedDictEnd == 0.
	     In which case, the test (blockEndIdx > maxDist) is merely to avoid
	     overflowing next operation `newLowLimit = blockEndIdx - maxDist`.
	   - When there is a standard dictionary :
	     Index referential is copied from the dictionary,
	     which means it starts from 0.
	     In which case, loadedDictEnd == dictSize,
	     and it makes sense to compare `blockEndIdx > maxDist + dictSize`
	     since `blockEndIdx` also starts from zero.
	   - When there is an attached dictionary :
	     loadedDictEnd is expressed within the referential of the context,
	     so it can be directly compared against blockEndIdx.
	*/
	if blockEndIdx > maxDist+loadedDictEnd {
		newLowLimit = blockEndIdx - maxDist
		if (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit < newLowLimit {
			(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = newLowLimit
		}
		if (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit < (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit {
			(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit
		}
		/* On reaching window size, dictionaries are invalidated */
		if loadedDictEndPtr != 0 {
			*(*U32)(unsafe.Pointer(loadedDictEndPtr)) = uint32(0)
		}
		if dictMatchStatePtr != 0 {
			*(*uintptr)(unsafe.Pointer(dictMatchStatePtr)) = libc.UintptrFromInt32(0)
		}
	}
}

// C documentation
//
//	/* Similar to ZSTD_window_enforceMaxDist(),
//	 * but only invalidates dictionary
//	 * when input progresses beyond window size.
//	 * assumption : loadedDictEndPtr and dictMatchStatePtr are valid (non NULL)
//	 *              loadedDictEnd uses same referential as window->base
//	 *              maxDist is the window size */
func ZSTD_checkDictValidity(tls *libc.TLS, window uintptr, blockEnd uintptr, maxDist U32, loadedDictEndPtr uintptr, dictMatchStatePtr uintptr) {
	var blockEndIdx, loadedDictEnd U32
	_, _ = blockEndIdx, loadedDictEnd
	blockEndIdx = uint32(int64(blockEnd) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	loadedDictEnd = *(*U32)(unsafe.Pointer(loadedDictEndPtr))
	if blockEndIdx > loadedDictEnd+maxDist || loadedDictEnd != (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit {
		/* On reaching window size, dictionaries are invalidated.
		 * For simplification, if window size is reached anywhere within next block,
		 * the dictionary is invalidated for the full block.
		 *
		 * We also have to invalidate the dictionary if ZSTD_window_update() has detected
		 * non-contiguous segments, which means that loadedDictEnd != window->dictLimit.
		 * loadedDictEnd may be 0, if forceWindow is true, but in that case we never use
		 * dictMatchState, so setting it to NULL is not a problem.
		 */
		*(*U32)(unsafe.Pointer(loadedDictEndPtr)) = uint32(0)
		*(*uintptr)(unsafe.Pointer(dictMatchStatePtr)) = libc.UintptrFromInt32(0)
	} else {
		if *(*U32)(unsafe.Pointer(loadedDictEndPtr)) != uint32(0) {
		}
	}
}

func ZSTD_window_init(tls *libc.TLS, window uintptr) {
	libc.Xmemset(tls, window, 0, libc.Uint64FromInt64(40))
	(*ZSTD_window_t)(unsafe.Pointer(window)).Fbase = __ccgo_ts + 1432
	(*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase = __ccgo_ts + 1432
	_ = libc.Uint64FromInt64(1)                                                                                                           /* Start above ZSTD_DUBT_UNSORTED_MARK */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = uint32(ZSTD_WINDOW_START_INDEX)                                                 /* start from >0, so that 1st position is valid */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = uint32(ZSTD_WINDOW_START_INDEX)                                                  /* it ensures first and later CCtx usages compress the same */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc = (*ZSTD_window_t)(unsafe.Pointer(window)).Fbase + uintptr(ZSTD_WINDOW_START_INDEX) /* see issue #1241 */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnbOverflowCorrections = uint32(0)
}

// C documentation
//
//	/**
//	 * ZSTD_window_update():
//	 * Updates the window by appending [src, src + srcSize) to the window.
//	 * If it is not contiguous, the current prefix becomes the extDict, and we
//	 * forget about the extDict. Handles overlap of the prefix and extDict.
//	 * Returns non-zero if the segment is contiguous.
//	 */
func ZSTD_window_update(tls *libc.TLS, window uintptr, src uintptr, srcSize size_t, forceNonContiguous int32) (r U32) {
	var contiguous, lowLimitMax U32
	var distanceFromBase, highInputIdx size_t
	var ip uintptr
	var v1 uint32
	_, _, _, _, _, _ = contiguous, distanceFromBase, highInputIdx, ip, lowLimitMax, v1
	ip = src
	contiguous = uint32(1)
	if srcSize == uint64(0) {
		return contiguous
	}
	/* Check if blocks follow each other */
	if src != (*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc || forceNonContiguous != 0 {
		/* not contiguous */
		distanceFromBase = uint64(int64((*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
		(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit
		/* should never overflow */
		(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = uint32(distanceFromBase)
		(*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase = (*ZSTD_window_t)(unsafe.Pointer(window)).Fbase
		(*ZSTD_window_t)(unsafe.Pointer(window)).Fbase = ip - uintptr(distanceFromBase)
		/* ms->nextToUpdate = window->dictLimit; */
		if (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit-(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit < uint32(HASH_READ_SIZE) {
			(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit
		} /* too small extDict */
		contiguous = uint32(0)
	}
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc = ip + uintptr(srcSize)
	/* if input and dictionary overlap : reduce dictionary (area presumed modified by input) */
	if libc.BoolInt32(ip+uintptr(srcSize) > (*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase+uintptr((*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit))&libc.BoolInt32(ip < (*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase+uintptr((*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit)) != 0 {
		highInputIdx = uint64(int64(ip+uintptr(srcSize)) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase))
		if highInputIdx > uint64((*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit) {
			v1 = (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit
		} else {
			v1 = uint32(highInputIdx)
		}
		lowLimitMax = v1
		(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = lowLimitMax
	}
	return contiguous
}

// C documentation
//
//	/**
//	 * Returns the lowest allowed match index. It may either be in the ext-dict or the prefix.
//	 */
func ZSTD_getLowestMatchIndex(tls *libc.TLS, ms uintptr, curr U32, windowLog uint32) (r U32) {
	var isDictionary, lowestValid, matchLowest, maxDistance, withinWindow U32
	var v1, v2 uint32
	_, _, _, _, _, _, _ = isDictionary, lowestValid, matchLowest, maxDistance, withinWindow, v1, v2
	maxDistance = uint32(1) << windowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinWindow = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinWindow
	}
	/* When using a dictionary the entire dictionary is valid if a single byte of the dictionary
	 * is within the window. We invalidate the dictionary (and set loadedDictEnd to 0) when it isn't
	 * valid for the entire block. So this check is sufficient to find the lowest valid match index.
	 */
	matchLowest = v2
	return matchLowest
}

// C documentation
//
//	/**
//	 * Returns the lowest allowed match index in the prefix.
//	 */
func ZSTD_getLowestPrefixIndex(tls *libc.TLS, ms uintptr, curr U32, windowLog uint32) (r U32) {
	var isDictionary, lowestValid, matchLowest, maxDistance, withinWindow U32
	var v1, v2 uint32
	_, _, _, _, _, _, _ = isDictionary, lowestValid, matchLowest, maxDistance, withinWindow, v1, v2
	maxDistance = uint32(1) << windowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinWindow = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinWindow
	}
	/* When computing the lowest prefix index we need to take the dictionary into account to handle
	 * the edge case where the dictionary and the source are contiguous in memory.
	 */
	matchLowest = v2
	return matchLowest
}

// C documentation
//
//	/* index_safety_check:
//	 * intentional underflow : ensure repIndex isn't overlapping dict + prefix
//	 * @return 1 if values are not overlapping,
//	 * 0 otherwise */
func ZSTD_index_overlap_check(tls *libc.TLS, prefixLowestIndex U32, repIndex U32) (r int32) {
	return libc.BoolInt32(prefixLowestIndex-libc.Uint32FromInt32(1)-repIndex >= libc.Uint32FromInt32(3))
}

/* debug functions */

/* Short Cache */

/* Normally, zstd matchfinders follow this flow:
 *     1. Compute hash at ip
 *     2. Load index from hashTable[hash]
 *     3. Check if *ip == *(base + index)
 * In dictionary compression, loading *(base + index) is often an L2 or even L3 miss.
 *
 * Short cache is an optimization which allows us to avoid step 3 most of the time
 * when the data doesn't actually match. With short cache, the flow becomes:
 *     1. Compute (hash, currentTag) at ip. currentTag is an 8-bit independent hash at ip.
 *     2. Load (index, matchTag) from hashTable[hash]. See ZSTD_writeTaggedIndex to understand how this works.
 *     3. Only if currentTag == matchTag, check *ip == *(base + index). Otherwise, continue.
 *
 * Currently, short cache is only implemented in CDict hashtables. Thus, its use is limited to
 * dictMatchState matchfinders.
 */

// C documentation
//
//	/* Helper function for ZSTD_fillHashTable and ZSTD_fillDoubleHashTable.
//	 * Unpacks hashAndTag into (hash, tag), then packs (index, tag) into hashTable[hash]. */
func ZSTD_writeTaggedIndex(tls *libc.TLS, hashTable uintptr, hashAndTag size_t, index U32) {
	var hash size_t
	var tag U32
	_, _ = hash, tag
	hash = hashAndTag >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
	tag = uint32(hashAndTag & uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)-libc.Uint32FromInt32(1)))
	*(*U32)(unsafe.Pointer(hashTable + uintptr(hash)*4)) = index<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS) | tag
}

// C documentation
//
//	/* Helper function for short cache matchfinders.
//	 * Unpacks tag1 and tag2 from lower bits of packedTag1 and packedTag2, then checks if the tags match. */
func ZSTD_comparePackedTags(tls *libc.TLS, packedTag1 size_t, packedTag2 size_t) (r int32) {
	var tag1, tag2 U32
	_, _ = tag1, tag2
	tag1 = uint32(packedTag1 & uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)-libc.Uint32FromInt32(1)))
	tag2 = uint32(packedTag2 & uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)-libc.Uint32FromInt32(1)))
	return libc.BoolInt32(tag1 == tag2)
}

type ZSTD_SequencePosition = struct {
	Fidx           U32
	FposInSequence U32
	FposInSrc      size_t
}

type BlockSummary = struct {
	FnbSequences size_t
	FblockSize   size_t
	FlitSize     size_t
}

// C documentation
//
//	/* Returns 1 if an external sequence producer is registered, otherwise returns 0. */
func ZSTD_hasExtSeqProd(tls *libc.TLS, params uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdFunc != libc.UintptrFromInt32(0))
}

/**** ended inlining zstd_compress_literals.h ****/

/* **************************************************************
*  Debug Traces
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  Literals compression - special cases
//	****************************************************************/
func ZSTD_noCompressLiterals(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var flSize U32
	var ostart uintptr
	_, _ = flSize, ostart
	ostart = dst
	flSize = uint32(int32(1) + libc.BoolInt32(srcSize > uint64(31)) + libc.BoolInt32(srcSize > uint64(4095)))
	if srcSize+uint64(flSize) > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	switch flSize {
	case uint32(1): /* 2 - 1 - 5 */
		*(*BYTE)(unsafe.Pointer(ostart)) = uint8(uint64(uint32(set_basic)) + srcSize<<libc.Int32FromInt32(3))
	case uint32(2): /* 2 - 2 - 12 */
		MEM_writeLE16(tls, ostart, uint16(uint64(uint32(set_basic)+uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	case uint32(3): /* 2 - 2 - 20 */
		MEM_writeLE32(tls, ostart, uint32(uint64(uint32(set_basic)+uint32(libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	default: /* not necessary : flSize is {1,2,3} */
	}
	libc.Xmemcpy(tls, ostart+uintptr(flSize), src, srcSize)
	return srcSize + uint64(flSize)
}

func allBytesIdentical(tls *libc.TLS, src uintptr, srcSize size_t) (r int32) {
	var b BYTE
	var p size_t
	_, _ = b, p
	b = *(*BYTE)(unsafe.Pointer(src))
	p = uint64(1)
	for {
		if !(p < srcSize) {
			break
		}
		if int32(*(*BYTE)(unsafe.Pointer(src + uintptr(p)))) != int32(b) {
			return 0
		}
		goto _1
	_1:
		;
		p = p + 1
	}
	return int32(1)
	return r
}

func ZSTD_compressRleLiteralsBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var flSize U32
	var ostart uintptr
	_, _ = flSize, ostart
	ostart = dst
	flSize = uint32(int32(1) + libc.BoolInt32(srcSize > uint64(31)) + libc.BoolInt32(srcSize > uint64(4095)))
	_ = dstCapacity
	switch flSize {
	case uint32(1): /* 2 - 1 - 5 */
		*(*BYTE)(unsafe.Pointer(ostart)) = uint8(uint64(uint32(set_rle)) + srcSize<<libc.Int32FromInt32(3))
	case uint32(2): /* 2 - 2 - 12 */
		MEM_writeLE16(tls, ostart, uint16(uint64(uint32(set_rle)+uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	case uint32(3): /* 2 - 2 - 20 */
		MEM_writeLE32(tls, ostart, uint32(uint64(uint32(set_rle)+uint32(libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	default: /* not necessary : flSize is {1,2,3} */
	}
	*(*BYTE)(unsafe.Pointer(ostart + uintptr(flSize))) = *(*BYTE)(unsafe.Pointer(src))
	return uint64(flSize + uint32(1))
}

// C documentation
//
//	/* ZSTD_minLiteralsToCompress() :
//	 * returns minimal amount of literals
//	 * for literal compression to even be attempted.
//	 * Minimum is made tighter as compression strategy increases.
//	 */
func ZSTD_minLiteralsToCompress(tls *libc.TLS, strategy ZSTD_strategy, huf_repeat HUF_repeat) (r size_t) {
	var mintc size_t
	var shift, v1 int32
	var v2 uint64
	_, _, _, _ = mintc, shift, v1, v2
	/* btultra2 : min 8 bytes;
	 * then 2x larger for each successive compression strategy
	 * max threshold 64 bytes */
	if int32(9)-strategy < int32(3) {
		v1 = int32(9) - strategy
	} else {
		v1 = int32(3)
	}
	shift = v1
	if huf_repeat == int32(HUF_repeat_valid) {
		v2 = uint64(6)
	} else {
		v2 = libc.Uint64FromInt32(8) << shift
	}
	mintc = v2
	return mintc
	return r
}

func ZSTD_compressLiterals(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, entropyWorkspace uintptr, entropyWorkspaceSize size_t, prevHuf uintptr, nextHuf uintptr, strategy ZSTD_strategy, disableLiteralCompression int32, suspectUncompressible int32, bmi2 int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cLitSize, lhSize, minGain size_t
	var flags, v1, v2, v3, v4 int32
	var hType SymbolEncodingType_e
	var huf_compress, ostart, v5 uintptr
	var lhc, lhc1, lhc2, singleStream U32
	var _ /* repeat at bp+0 */ HUF_repeat
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cLitSize, flags, hType, huf_compress, lhSize, lhc, lhc1, lhc2, minGain, ostart, singleStream, v1, v2, v3, v4, v5
	lhSize = uint64(int32(3) + libc.BoolInt32(srcSize >= uint64(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(srcSize >= uint64(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	ostart = dst
	singleStream = libc.BoolUint32(srcSize < uint64(256))
	hType = int32(set_compressed)
	/* Prepare nextEntropy assuming reusing the existing table */
	libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
	if disableLiteralCompression != 0 {
		return ZSTD_noCompressLiterals(tls, dst, dstCapacity, src, srcSize)
	}
	/* if too small, don't even attempt compression (speed opt) */
	if srcSize < ZSTD_minLiteralsToCompress(tls, strategy, (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode) {
		return ZSTD_noCompressLiterals(tls, dst, dstCapacity, src, srcSize)
	}
	if dstCapacity < lhSize+uint64(1) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1434, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	*(*HUF_repeat)(unsafe.Pointer(bp)) = (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode
	if bmi2 != 0 {
		v1 = int32(HUF_flags_bmi2)
	} else {
		v1 = 0
	}
	if strategy < int32(ZSTD_lazy) && srcSize <= uint64(1024) {
		v2 = int32(HUF_flags_preferRepeat)
	} else {
		v2 = 0
	}
	if strategy >= int32(ZSTD_btultra) {
		v3 = int32(HUF_flags_optimalDepth)
	} else {
		v3 = 0
	}
	if suspectUncompressible != 0 {
		v4 = int32(HUF_flags_suspectUncompressible)
	} else {
		v4 = 0
	}
	flags = 0 | v1 | v2 | v3 | v4
	if *(*HUF_repeat)(unsafe.Pointer(bp)) == int32(HUF_repeat_valid) && lhSize == uint64(3) {
		singleStream = uint32(1)
	}
	if singleStream != 0 {
		v5 = __ccgo_fp(HUF_compress1X_repeat)
	} else {
		v5 = __ccgo_fp(HUF_compress4X_repeat)
	}
	huf_compress = v5
	cLitSize = (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uint32, uint32, uintptr, size_t, uintptr, uintptr, int32) size_t)(unsafe.Pointer(&struct{ uintptr }{huf_compress})))(tls, ostart+uintptr(lhSize), dstCapacity-lhSize, src, srcSize, uint32(HUF_SYMBOLVALUE_MAX), uint32(LitHufLog), entropyWorkspace, entropyWorkspaceSize, nextHuf, bp, flags)
	if *(*HUF_repeat)(unsafe.Pointer(bp)) != int32(HUF_repeat_none) {
		/* reused the existing table */
		hType = int32(set_repeat)
	}
	minGain = ZSTD_minGain(tls, srcSize, strategy)
	if cLitSize == uint64(0) || cLitSize >= srcSize-minGain || ERR_isError(tls, cLitSize) != 0 {
		libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
		return ZSTD_noCompressLiterals(tls, dst, dstCapacity, src, srcSize)
	}
	if cLitSize == uint64(1) {
		/* A return value of 1 signals that the alphabet consists of a single symbol.
		 * However, in some rare circumstances, it could be the compressed size (a single byte).
		 * For that outcome to have a chance to happen, it's necessary that `srcSize < 8`.
		 * (it's also necessary to not generate statistics).
		 * Therefore, in such a case, actively check that all bytes are identical. */
		if srcSize >= uint64(8) || allBytesIdentical(tls, src, srcSize) != 0 {
			libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
			return ZSTD_compressRleLiteralsBlock(tls, dst, dstCapacity, src, srcSize)
		}
	}
	if hType == int32(set_compressed) {
		/* using a newly constructed table */
		(*ZSTD_hufCTables_t)(unsafe.Pointer(nextHuf)).FrepeatMode = int32(HUF_repeat_check)
	}
	/* Build header */
	switch lhSize {
	case uint64(3): /* 2 - 2 - 10 - 10 */
		if !(singleStream != 0) {
		}
		lhc = uint32(hType) + libc.BoolUint32(!(singleStream != 0))<<libc.Int32FromInt32(2) + uint32(srcSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(14)
		MEM_writeLE24(tls, ostart, lhc)
	case uint64(4): /* 2 - 2 - 14 - 14 */
		lhc1 = uint32(hType+libc.Int32FromInt32(2)<<libc.Int32FromInt32(2)) + uint32(srcSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(18)
		MEM_writeLE32(tls, ostart, lhc1)
	case uint64(5): /* 2 - 2 - 18 - 18 */
		lhc2 = uint32(hType+libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)) + uint32(srcSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(22)
		MEM_writeLE32(tls, ostart, lhc2)
		*(*BYTE)(unsafe.Pointer(ostart + 4)) = uint8(cLitSize >> libc.Int32FromInt32(10))
	default: /* not possible : lhSize is {3,4,5} */
	}
	return lhSize + cLitSize
}

/**** ended inlining compress/zstd_compress_literals.c ****/
/**** start inlining compress/zstd_compress_sequences.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
 *  Dependencies
 ***************************************/
/**** start inlining zstd_compress_sequences.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/zstd_internal.h ****/

type ZSTD_DefaultPolicy_e = int32

const ZSTD_defaultDisallowed = 0
const ZSTD_defaultAllowed = 1

/**** ended inlining zstd_compress_sequences.h ****/

// C documentation
//
//	/**
//	 * -log2(x / 256) lookup table for x in [0, 256).
//	 * If x == 0: Return 0
//	 * Else: Return floor(-log2(x / 256) * 256)
//	 */
var kInverseProbabilityLog256 = [256]uint32{
	1:   uint32(2048),
	2:   uint32(1792),
	3:   uint32(1642),
	4:   uint32(1536),
	5:   uint32(1453),
	6:   uint32(1386),
	7:   uint32(1329),
	8:   uint32(1280),
	9:   uint32(1236),
	10:  uint32(1197),
	11:  uint32(1162),
	12:  uint32(1130),
	13:  uint32(1100),
	14:  uint32(1073),
	15:  uint32(1047),
	16:  uint32(1024),
	17:  uint32(1001),
	18:  uint32(980),
	19:  uint32(960),
	20:  uint32(941),
	21:  uint32(923),
	22:  uint32(906),
	23:  uint32(889),
	24:  uint32(874),
	25:  uint32(859),
	26:  uint32(844),
	27:  uint32(830),
	28:  uint32(817),
	29:  uint32(804),
	30:  uint32(791),
	31:  uint32(779),
	32:  uint32(768),
	33:  uint32(756),
	34:  uint32(745),
	35:  uint32(734),
	36:  uint32(724),
	37:  uint32(714),
	38:  uint32(704),
	39:  uint32(694),
	40:  uint32(685),
	41:  uint32(676),
	42:  uint32(667),
	43:  uint32(658),
	44:  uint32(650),
	45:  uint32(642),
	46:  uint32(633),
	47:  uint32(626),
	48:  uint32(618),
	49:  uint32(610),
	50:  uint32(603),
	51:  uint32(595),
	52:  uint32(588),
	53:  uint32(581),
	54:  uint32(574),
	55:  uint32(567),
	56:  uint32(561),
	57:  uint32(554),
	58:  uint32(548),
	59:  uint32(542),
	60:  uint32(535),
	61:  uint32(529),
	62:  uint32(523),
	63:  uint32(517),
	64:  uint32(512),
	65:  uint32(506),
	66:  uint32(500),
	67:  uint32(495),
	68:  uint32(489),
	69:  uint32(484),
	70:  uint32(478),
	71:  uint32(473),
	72:  uint32(468),
	73:  uint32(463),
	74:  uint32(458),
	75:  uint32(453),
	76:  uint32(448),
	77:  uint32(443),
	78:  uint32(438),
	79:  uint32(434),
	80:  uint32(429),
	81:  uint32(424),
	82:  uint32(420),
	83:  uint32(415),
	84:  uint32(411),
	85:  uint32(407),
	86:  uint32(402),
	87:  uint32(398),
	88:  uint32(394),
	89:  uint32(390),
	90:  uint32(386),
	91:  uint32(382),
	92:  uint32(377),
	93:  uint32(373),
	94:  uint32(370),
	95:  uint32(366),
	96:  uint32(362),
	97:  uint32(358),
	98:  uint32(354),
	99:  uint32(350),
	100: uint32(347),
	101: uint32(343),
	102: uint32(339),
	103: uint32(336),
	104: uint32(332),
	105: uint32(329),
	106: uint32(325),
	107: uint32(322),
	108: uint32(318),
	109: uint32(315),
	110: uint32(311),
	111: uint32(308),
	112: uint32(305),
	113: uint32(302),
	114: uint32(298),
	115: uint32(295),
	116: uint32(292),
	117: uint32(289),
	118: uint32(286),
	119: uint32(282),
	120: uint32(279),
	121: uint32(276),
	122: uint32(273),
	123: uint32(270),
	124: uint32(267),
	125: uint32(264),
	126: uint32(261),
	127: uint32(258),
	128: uint32(256),
	129: uint32(253),
	130: uint32(250),
	131: uint32(247),
	132: uint32(244),
	133: uint32(241),
	134: uint32(239),
	135: uint32(236),
	136: uint32(233),
	137: uint32(230),
	138: uint32(228),
	139: uint32(225),
	140: uint32(222),
	141: uint32(220),
	142: uint32(217),
	143: uint32(215),
	144: uint32(212),
	145: uint32(209),
	146: uint32(207),
	147: uint32(204),
	148: uint32(202),
	149: uint32(199),
	150: uint32(197),
	151: uint32(194),
	152: uint32(192),
	153: uint32(190),
	154: uint32(187),
	155: uint32(185),
	156: uint32(182),
	157: uint32(180),
	158: uint32(178),
	159: uint32(175),
	160: uint32(173),
	161: uint32(171),
	162: uint32(168),
	163: uint32(166),
	164: uint32(164),
	165: uint32(162),
	166: uint32(159),
	167: uint32(157),
	168: uint32(155),
	169: uint32(153),
	170: uint32(151),
	171: uint32(149),
	172: uint32(146),
	173: uint32(144),
	174: uint32(142),
	175: uint32(140),
	176: uint32(138),
	177: uint32(136),
	178: uint32(134),
	179: uint32(132),
	180: uint32(130),
	181: uint32(128),
	182: uint32(126),
	183: uint32(123),
	184: uint32(121),
	185: uint32(119),
	186: uint32(117),
	187: uint32(115),
	188: uint32(114),
	189: uint32(112),
	190: uint32(110),
	191: uint32(108),
	192: uint32(106),
	193: uint32(104),
	194: uint32(102),
	195: uint32(100),
	196: uint32(98),
	197: uint32(96),
	198: uint32(94),
	199: uint32(93),
	200: uint32(91),
	201: uint32(89),
	202: uint32(87),
	203: uint32(85),
	204: uint32(83),
	205: uint32(82),
	206: uint32(80),
	207: uint32(78),
	208: uint32(76),
	209: uint32(74),
	210: uint32(73),
	211: uint32(71),
	212: uint32(69),
	213: uint32(67),
	214: uint32(66),
	215: uint32(64),
	216: uint32(62),
	217: uint32(61),
	218: uint32(59),
	219: uint32(57),
	220: uint32(55),
	221: uint32(54),
	222: uint32(52),
	223: uint32(50),
	224: uint32(49),
	225: uint32(47),
	226: uint32(46),
	227: uint32(44),
	228: uint32(42),
	229: uint32(41),
	230: uint32(39),
	231: uint32(37),
	232: uint32(36),
	233: uint32(34),
	234: uint32(33),
	235: uint32(31),
	236: uint32(30),
	237: uint32(28),
	238: uint32(26),
	239: uint32(25),
	240: uint32(23),
	241: uint32(22),
	242: uint32(20),
	243: uint32(19),
	244: uint32(17),
	245: uint32(16),
	246: uint32(14),
	247: uint32(13),
	248: uint32(11),
	249: uint32(10),
	250: uint32(8),
	251: uint32(7),
	252: uint32(5),
	253: uint32(4),
	254: uint32(2),
	255: uint32(1),
}

func ZSTD_getFSEMaxSymbolValue(tls *libc.TLS, ctable uintptr) (r uint32) {
	var maxSymbolValue U32
	var ptr, u16ptr uintptr
	_, _, _ = maxSymbolValue, ptr, u16ptr
	ptr = ctable
	u16ptr = ptr
	maxSymbolValue = uint32(MEM_read16(tls, u16ptr+uintptr(1)*2))
	return maxSymbolValue
}

// C documentation
//
//	/**
//	 * Returns true if we should use ncount=-1 else we should
//	 * use ncount=1 for low probability symbols instead.
//	 */
func ZSTD_useLowProbCount(tls *libc.TLS, nbSeq size_t) (r uint32) {
	/* Heuristic: This should cover most blocks <= 16K and
	 * start to fade out after 16K to about 32K depending on
	 * compressibility.
	 */
	return libc.BoolUint32(nbSeq >= uint64(2048))
}

// C documentation
//
//	/**
//	 * Returns the cost in bytes of encoding the normalized count header.
//	 * Returns an error if any of the helper functions return an error.
//	 */
func ZSTD_NCountCost(tls *libc.TLS, count uintptr, max uint32, nbSeq size_t, FSELog uint32) (r size_t) {
	bp := tls.Alloc(624)
	defer tls.Free(624)
	var err_code size_t
	var tableLog U32
	var _ /* norm at bp+512 */ [53]S16
	var _ /* wksp at bp+0 */ [512]BYTE
	_, _ = err_code, tableLog
	tableLog = FSE_optimalTableLog(tls, FSELog, nbSeq, max)
	err_code = FSE_normalizeCount(tls, bp+512, tableLog, count, nbSeq, max, ZSTD_useLowProbCount(tls, nbSeq))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return FSE_writeNCount(tls, bp, uint64(512), bp+512, max, tableLog)
}

// C documentation
//
//	/**
//	 * Returns the cost in bits of encoding the distribution described by count
//	 * using the entropy bound.
//	 */
func ZSTD_entropyCost(tls *libc.TLS, count uintptr, max uint32, total size_t) (r size_t) {
	var cost, norm, s uint32
	_, _, _ = cost, norm, s
	cost = uint32(0)
	s = uint32(0)
	for {
		if !(s <= max) {
			break
		}
		norm = uint32(uint64(libc.Uint32FromInt32(256)**(*uint32)(unsafe.Pointer(count + uintptr(s)*4))) / total)
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) != uint32(0) && norm == uint32(0) {
			norm = uint32(1)
		}
		cost = cost + *(*uint32)(unsafe.Pointer(count + uintptr(s)*4))*kInverseProbabilityLog256[norm]
		goto _1
	_1:
		;
		s = s + 1
	}
	return uint64(cost >> int32(8))
}

// C documentation
//
//	/**
//	 * Returns the cost in bits of encoding the distribution in count using ctable.
//	 * Returns an error if ctable cannot represent all the symbols in count.
//	 */
func ZSTD_fseBitCost(tls *libc.TLS, ctable uintptr, count uintptr, max uint32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var badCost, bitCost, kAccuracyLog, s, tableLog uint32
	var cost size_t
	var _ /* cstate at bp+0 */ FSE_CState_t
	_, _, _, _, _, _ = badCost, bitCost, cost, kAccuracyLog, s, tableLog
	kAccuracyLog = uint32(8)
	cost = uint64(0)
	FSE_initCState(tls, bp, ctable)
	if ZSTD_getFSEMaxSymbolValue(tls, ctable) < max {
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	s = uint32(0)
	for {
		if !(s <= max) {
			break
		}
		tableLog = (*(*FSE_CState_t)(unsafe.Pointer(bp))).FstateLog
		badCost = (tableLog + uint32(1)) << kAccuracyLog
		bitCost = FSE_bitCost(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp))).FsymbolTT, tableLog, s, kAccuracyLog)
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) == uint32(0) {
			goto _1
		}
		if bitCost >= badCost {
			return uint64(-int32(ZSTD_error_GENERIC))
		}
		cost = cost + uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))*uint64(bitCost)
		goto _1
	_1:
		;
		s = s + 1
	}
	return cost >> kAccuracyLog
}

// C documentation
//
//	/**
//	 * Returns the cost in bits of encoding the distribution in count using the
//	 * table described by norm. The max symbol support by norm is assumed >= max.
//	 * norm must be valid for every symbol with non-zero probability in count.
//	 */
func ZSTD_crossEntropyCost(tls *libc.TLS, norm uintptr, accuracyLog uint32, count uintptr, max uint32) (r size_t) {
	var cost size_t
	var norm256, normAcc, s, shift, v2 uint32
	_, _, _, _, _, _ = cost, norm256, normAcc, s, shift, v2
	shift = uint32(8) - accuracyLog
	cost = uint64(0)
	s = uint32(0)
	for {
		if !(s <= max) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) != -int32(1) {
			v2 = uint32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)))
		} else {
			v2 = uint32(1)
		}
		normAcc = v2
		norm256 = normAcc << shift
		cost = cost + uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4))*kInverseProbabilityLog256[norm256])
		goto _1
	_1:
		;
		s = s + 1
	}
	return cost >> int32(8)
}

func ZSTD_selectEncodingType(tls *libc.TLS, repeatMode uintptr, count uintptr, max uint32, mostFrequent size_t, nbSeq size_t, FSELog uint32, prevCTable uintptr, defaultNorm uintptr, defaultNormLog U32, isDefaultAllowed ZSTD_DefaultPolicy_e, strategy ZSTD_strategy) (r SymbolEncodingType_e) {
	var NCountCost, baseLog, basicCost, compressedCost, dynamicFse_nbSeq_min, mult, repeatCost, staticFse_nbSeq_max size_t
	var v1, v2 uint64
	_, _, _, _, _, _, _, _, _, _ = NCountCost, baseLog, basicCost, compressedCost, dynamicFse_nbSeq_min, mult, repeatCost, staticFse_nbSeq_max, v1, v2
	_ = libc.Uint64FromInt64(1)
	if mostFrequent == nbSeq {
		*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_none)
		if isDefaultAllowed != 0 && nbSeq <= uint64(2) {
			/* Prefer set_basic over set_rle when there are 2 or fewer symbols,
			 * since RLE uses 1 byte, but set_basic uses 5-6 bits per symbol.
			 * If basic encoding isn't possible, always choose RLE.
			 */
			return int32(set_basic)
		}
		return int32(set_rle)
	}
	if strategy < int32(ZSTD_lazy) {
		if isDefaultAllowed != 0 {
			staticFse_nbSeq_max = uint64(1000)
			mult = uint64(int32(10) - strategy)
			baseLog = uint64(3)
			dynamicFse_nbSeq_min = libc.Uint64FromInt32(1) << defaultNormLog * mult >> baseLog /* 28-36 for offset, 56-72 for lengths */
			/* xx_DEFAULTNORMLOG */
			if *(*FSE_repeat)(unsafe.Pointer(repeatMode)) == int32(FSE_repeat_valid) && nbSeq < staticFse_nbSeq_max {
				return int32(set_repeat)
			}
			if nbSeq < dynamicFse_nbSeq_min || mostFrequent < nbSeq>>(defaultNormLog-libc.Uint32FromInt32(1)) {
				/* The format allows default tables to be repeated, but it isn't useful.
				 * When using simple heuristics to select encoding type, we don't want
				 * to confuse these tables with dictionaries. When running more careful
				 * analysis, we don't need to waste time checking both repeating tables
				 * and default tables.
				 */
				*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_none)
				return int32(set_basic)
			}
		}
	} else {
		if isDefaultAllowed != 0 {
			v1 = ZSTD_crossEntropyCost(tls, defaultNorm, defaultNormLog, count, max)
		} else {
			v1 = uint64(-int32(ZSTD_error_GENERIC))
		}
		basicCost = v1
		if *(*FSE_repeat)(unsafe.Pointer(repeatMode)) != int32(FSE_repeat_none) {
			v2 = ZSTD_fseBitCost(tls, prevCTable, count, max)
		} else {
			v2 = uint64(-int32(ZSTD_error_GENERIC))
		}
		repeatCost = v2
		NCountCost = ZSTD_NCountCost(tls, count, max, nbSeq, FSELog)
		compressedCost = NCountCost<<libc.Int32FromInt32(3) + ZSTD_entropyCost(tls, count, max, nbSeq)
		if isDefaultAllowed != 0 {
		}
		if basicCost <= repeatCost && basicCost <= compressedCost {
			*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_none)
			return int32(set_basic)
		}
		if repeatCost <= compressedCost {
			return int32(set_repeat)
		}
	}
	*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_check)
	return int32(set_compressed)
}

type ZSTD_BuildCTableWksp = struct {
	Fnorm [53]S16
	Fwksp [285]U32
}

func ZSTD_buildCTable(tls *libc.TLS, dst uintptr, dstCapacity size_t, nextCTable uintptr, FSELog U32, type1 SymbolEncodingType_e, count uintptr, max U32, codeTable uintptr, nbSeq size_t, defaultNorm uintptr, defaultNormLog U32, defaultMax U32, prevCTable uintptr, prevCTableSize size_t, entropyWorkspace uintptr, entropyWorkspaceSize size_t) (r size_t) {
	var NCountSize, err_code, err_code1, err_code2, err_code3, err_code4, nbSeq_1 size_t
	var oend, op, wksp uintptr
	var tableLog U32
	_, _, _, _, _, _, _, _, _, _, _ = NCountSize, err_code, err_code1, err_code2, err_code3, err_code4, nbSeq_1, oend, op, tableLog, wksp
	op = dst
	oend = op + uintptr(dstCapacity)
	switch type1 {
	case int32(set_rle):
		goto _1
	case int32(set_repeat):
		goto _2
	case int32(set_basic):
		goto _3
	case int32(set_compressed):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
_9:
	;
	err_code = FSE_buildCTable_rle(tls, nextCTable, uint8(max))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	goto _8
_8:
	;
	if 0 != 0 {
		goto _9
	}
	goto _7
_7:
	;
	if dstCapacity == uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1467, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	*(*BYTE)(unsafe.Pointer(op)) = *(*BYTE)(unsafe.Pointer(codeTable))
	return uint64(1)
_2:
	;
	libc.Xmemcpy(tls, nextCTable, prevCTable, prevCTableSize)
	return uint64(0)
_3:
	;
	err_code1 = FSE_buildCTable_wksp(tls, nextCTable, defaultNorm, defaultMax, defaultNormLog, entropyWorkspace, entropyWorkspaceSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	} /* note : could be pre-calculated */
	return uint64(0)
_4:
	;
	wksp = entropyWorkspace
	nbSeq_1 = nbSeq
	tableLog = FSE_optimalTableLog(tls, FSELog, nbSeq, max)
	if *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(codeTable + uintptr(nbSeq-uint64(1)))))*4)) > uint32(1) {
		*(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(codeTable + uintptr(nbSeq-uint64(1)))))*4)) = *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(codeTable + uintptr(nbSeq-uint64(1)))))*4)) - 1
		nbSeq_1 = nbSeq_1 - 1
	}
	_ = entropyWorkspaceSize
	err_code2 = FSE_normalizeCount(tls, wksp, tableLog, count, nbSeq_1, max, ZSTD_useLowProbCount(tls, nbSeq_1))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1484, 0)
		}
		return err_code2
	}
	NCountSize = FSE_writeNCount(tls, op, uint64(int64(oend)-int64(op)), wksp, max, tableLog) /* overflow protected */
	err_code3 = NCountSize
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1510, 0)
		}
		return err_code3
	}
	err_code4 = FSE_buildCTable_wksp(tls, nextCTable, wksp, max, tableLog, wksp+108, uint64(1140))
	if ERR_isError(tls, err_code4) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1533, 0)
		}
		return err_code4
	}
	return NCountSize
_5:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1561, 0)
	}
	return uint64(-int32(ZSTD_error_GENERIC))
_6:
	;
	return r
}

func ZSTD_encodeSequences_body(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	var extraBits, extraBits1, v1 uint32
	var llBits, mlBits, ofBits, ofBits1 U32
	var llCode, mlCode, ofCode BYTE
	var n, streamSize size_t
	var v2, v3 int32
	var _ /* blockStream at bp+0 */ BIT_CStream_t
	var _ /* stateLitLength at bp+104 */ FSE_CState_t
	var _ /* stateMatchLength at bp+40 */ FSE_CState_t
	var _ /* stateOffsetBits at bp+72 */ FSE_CState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = extraBits, extraBits1, llBits, llCode, mlBits, mlCode, n, ofBits, ofBits1, ofCode, streamSize, v1, v2, v3
	if ERR_isError(tls, BIT_initCStream(tls, bp, dst, dstCapacity)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1581, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* first symbols */
	FSE_initCState2(tls, bp+40, CTable_MatchLength, uint32(*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(nbSeq-uint64(1))))))
	FSE_initCState2(tls, bp+72, CTable_OffsetBits, uint32(*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(nbSeq-uint64(1))))))
	FSE_initCState2(tls, bp+104, CTable_LitLength, uint32(*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(nbSeq-uint64(1))))))
	BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FlitLength), uint32(LL_bits[*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(nbSeq-uint64(1))))]))
	if MEM_32bits(tls) != 0 {
		BIT_flushBits(tls, bp)
	}
	BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FmlBase), uint32(ML_bits[*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(nbSeq-uint64(1))))]))
	if MEM_32bits(tls) != 0 {
		BIT_flushBits(tls, bp)
	}
	if longOffsets != 0 {
		ofBits = uint32(*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(nbSeq-uint64(1)))))
		if MEM_32bits(tls) != 0 {
			v2 = int32(STREAM_ACCUMULATOR_MIN_32)
		} else {
			v2 = int32(STREAM_ACCUMULATOR_MIN_64)
		}
		if ofBits < uint32(v2)-uint32(1) {
			v1 = ofBits
		} else {
			if MEM_32bits(tls) != 0 {
				v3 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v3 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
			v1 = uint32(v3) - uint32(1)
		}
		extraBits = ofBits - v1
		if extraBits != 0 {
			BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FoffBase), extraBits)
			BIT_flushBits(tls, bp)
		}
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FoffBase>>extraBits), ofBits-extraBits)
	} else {
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FoffBase), uint32(*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(nbSeq-uint64(1))))))
	}
	BIT_flushBits(tls, bp)
	n = nbSeq - uint64(2)
	for {
		if !(n < nbSeq) {
			break
		} /* intentional underflow */
		llCode = *(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(n)))
		ofCode = *(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(n)))
		mlCode = *(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(n)))
		llBits = uint32(LL_bits[llCode])
		ofBits1 = uint32(ofCode)
		mlBits = uint32(ML_bits[mlCode])
		/* 32b*/ /* 64b*/
		/* (7)*/                                         /* (7)*/
		FSE_encodeSymbol(tls, bp, bp+72, uint32(ofCode)) /* 15 */ /* 15 */
		FSE_encodeSymbol(tls, bp, bp+40, uint32(mlCode)) /* 24 */ /* 24 */
		if MEM_32bits(tls) != 0 {
			BIT_flushBits(tls, bp)
		} /* (7)*/
		FSE_encodeSymbol(tls, bp, bp+104, uint32(llCode)) /* 16 */ /* 33 */
		if MEM_32bits(tls) != 0 || ofBits1+mlBits+llBits >= uint32(libc.Int32FromInt32(64)-libc.Int32FromInt32(7)-(libc.Int32FromInt32(LLFSELog)+libc.Int32FromInt32(MLFSELog)+libc.Int32FromInt32(OffFSELog))) {
			BIT_flushBits(tls, bp)
		} /* (7)*/
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FlitLength), llBits)
		if MEM_32bits(tls) != 0 && llBits+mlBits > uint32(24) {
			BIT_flushBits(tls, bp)
		}
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FmlBase), mlBits)
		if MEM_32bits(tls) != 0 || ofBits1+mlBits+llBits > uint32(56) {
			BIT_flushBits(tls, bp)
		}
		if longOffsets != 0 {
			if MEM_32bits(tls) != 0 {
				v2 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v2 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
			if ofBits1 < uint32(v2)-uint32(1) {
				v1 = ofBits1
			} else {
				if MEM_32bits(tls) != 0 {
					v3 = int32(STREAM_ACCUMULATOR_MIN_32)
				} else {
					v3 = int32(STREAM_ACCUMULATOR_MIN_64)
				}
				v1 = uint32(v3) - uint32(1)
			}
			extraBits1 = ofBits1 - v1
			if extraBits1 != 0 {
				BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FoffBase), extraBits1)
				BIT_flushBits(tls, bp) /* (7)*/
			}
			BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FoffBase>>extraBits1), ofBits1-extraBits1) /* 31 */
		} else {
			BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FoffBase), ofBits1) /* 31 */
		}
		BIT_flushBits(tls, bp) /* (7)*/
		goto _4
	_4:
		;
		n = n - 1
	}
	FSE_flushCState(tls, bp, bp+40)
	FSE_flushCState(tls, bp, bp+72)
	FSE_flushCState(tls, bp, bp+104)
	streamSize = BIT_closeCStream(tls, bp)
	if streamSize == uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1467, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	return streamSize
	return r
}

func ZSTD_encodeSequences_default(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32) (r size_t) {
	return ZSTD_encodeSequences_body(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
}

func ZSTD_encodeSequences_bmi2(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32) (r size_t) {
	return ZSTD_encodeSequences_body(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
}

func ZSTD_encodeSequences(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32, bmi2 int32) (r size_t) {
	if bmi2 != 0 {
		return ZSTD_encodeSequences_bmi2(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
	}
	_ = bmi2
	return ZSTD_encodeSequences_default(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
}

/**** ended inlining zstd_compress_superblock.h ****/

/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_compress_sequences.h ****/
/**** skipping file: zstd_compress_literals.h ****/

// C documentation
//
//	/** ZSTD_compressSubBlock_literal() :
//	 *  Compresses literals section for a sub-block.
//	 *  When we have to write the Huffman table we will sometimes choose a header
//	 *  size larger than necessary. This is because we have to pick the header size
//	 *  before we know the table size + compressed size, so we have a bound on the
//	 *  table size. If we guessed incorrectly, we fall back to uncompressed literals.
//	 *
//	 *  We write the header when writeEntropy=1 and set entropyWritten=1 when we succeeded
//	 *  in writing the header, otherwise it is set to 0.
//	 *
//	 *  hufMetadata->hType has literals block type info.
//	 *      If it is set_basic, all sub-blocks literals section will be Raw_Literals_Block.
//	 *      If it is set_rle, all sub-blocks literals section will be RLE_Literals_Block.
//	 *      If it is set_compressed, first sub-block's literals section will be Compressed_Literals_Block
//	 *      If it is set_compressed, first sub-block's literals section will be Treeless_Literals_Block
//	 *      and the following sub-blocks' literals sections will be Treeless_Literals_Block.
//	 *  @return : compressed size of literals section of a sub-block
//	 *            Or 0 if unable to compress.
//	 *            Or error code */
func ZSTD_compressSubBlock_literal(tls *libc.TLS, hufTable uintptr, hufMetadata uintptr, literals uintptr, litSize size_t, dst uintptr, dstSize size_t, bmi2 int32, writeEntropy int32, entropyWritten uintptr) (r size_t) {
	var cLitSize, cSize, header, lhSize size_t
	var flags, v1, v2 int32
	var hType SymbolEncodingType_e
	var lhc, lhc1, lhc2, singleStream U32
	var oend, op, ostart uintptr
	var v4 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cLitSize, cSize, flags, hType, header, lhSize, lhc, lhc1, lhc2, oend, op, ostart, singleStream, v1, v2, v4
	if writeEntropy != 0 {
		v1 = int32(200)
	} else {
		v1 = 0
	}
	header = uint64(v1)
	lhSize = uint64(int32(3) + libc.BoolInt32(litSize >= uint64(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))-header) + libc.BoolInt32(litSize >= uint64(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))-header))
	ostart = dst
	oend = ostart + uintptr(dstSize)
	op = ostart + uintptr(lhSize)
	singleStream = libc.BoolUint32(lhSize == uint64(3))
	if writeEntropy != 0 {
		v2 = (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType
	} else {
		v2 = int32(set_repeat)
	}
	hType = v2
	cLitSize = uint64(0)
	*(*int32)(unsafe.Pointer(entropyWritten)) = 0
	if litSize == uint64(0) || (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_basic) {
		return ZSTD_noCompressLiterals(tls, dst, dstSize, literals, litSize)
	} else {
		if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_rle) {
			return ZSTD_compressRleLiteralsBlock(tls, dst, dstSize, literals, litSize)
		}
	}
	if writeEntropy != 0 && (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_compressed) {
		libc.Xmemcpy(tls, op, hufMetadata+4, (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize)
		op = op + uintptr((*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize)
		cLitSize = cLitSize + (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize
	}
	if bmi2 != 0 {
		v1 = int32(HUF_flags_bmi2)
	} else {
		v1 = 0
	}
	flags = v1
	if singleStream != 0 {
		v4 = HUF_compress1X_usingCTable(tls, op, uint64(int64(oend)-int64(op)), literals, litSize, hufTable, flags)
	} else {
		v4 = HUF_compress4X_usingCTable(tls, op, uint64(int64(oend)-int64(op)), literals, litSize, hufTable, flags)
	}
	cSize = v4
	op = op + uintptr(cSize)
	cLitSize = cLitSize + cSize
	if cSize == uint64(0) || ERR_isError(tls, cSize) != 0 {
		return uint64(0)
	}
	/* If we expand and we aren't writing a header then emit uncompressed */
	if !(writeEntropy != 0) && cLitSize >= litSize {
		return ZSTD_noCompressLiterals(tls, dst, dstSize, literals, litSize)
	}
	/* If we are writing headers then allow expansion that doesn't change our header size. */
	if lhSize < uint64(libc.Int32FromInt32(3)+libc.BoolInt32(cLitSize >= uint64(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))))+libc.BoolInt32(cLitSize >= uint64(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))))) {
		return ZSTD_noCompressLiterals(tls, dst, dstSize, literals, litSize)
	}
	/* Build header */
	switch lhSize {
	case uint64(3): /* 2 - 2 - 10 - 10 */
		lhc = uint32(hType) + libc.BoolUint32(!(singleStream != 0))<<libc.Int32FromInt32(2) + uint32(litSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(14)
		MEM_writeLE24(tls, ostart, lhc)
	case uint64(4): /* 2 - 2 - 14 - 14 */
		lhc1 = uint32(hType+libc.Int32FromInt32(2)<<libc.Int32FromInt32(2)) + uint32(litSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(18)
		MEM_writeLE32(tls, ostart, lhc1)
	case uint64(5): /* 2 - 2 - 18 - 18 */
		lhc2 = uint32(hType+libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)) + uint32(litSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(22)
		MEM_writeLE32(tls, ostart, lhc2)
		*(*BYTE)(unsafe.Pointer(ostart + 4)) = uint8(cLitSize >> libc.Int32FromInt32(10))
	default: /* not possible : lhSize is {3,4,5} */
	}
	*(*int32)(unsafe.Pointer(entropyWritten)) = int32(1)
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_seqDecompressedSize(tls *libc.TLS, seqStore uintptr, sequences uintptr, nbSeqs size_t, litSize size_t, lastSubBlock int32) (r size_t) {
	var litLengthSum, matchLengthSum, n size_t
	var seqLen ZSTD_SequenceLength
	_, _, _, _ = litLengthSum, matchLengthSum, n, seqLen
	matchLengthSum = uint64(0)
	litLengthSum = uint64(0)
	n = uint64(0)
	for {
		if !(n < nbSeqs) {
			break
		}
		seqLen = ZSTD_getSequenceLength(tls, seqStore, sequences+uintptr(n)*8)
		litLengthSum = litLengthSum + uint64(seqLen.FlitLength)
		matchLengthSum = matchLengthSum + uint64(seqLen.FmatchLength)
		goto _1
	_1:
		;
		n = n + 1
	}
	if !(lastSubBlock != 0) {
	} else {
	}
	_ = litLengthSum
	return matchLengthSum + litSize
}

// C documentation
//
//	/** ZSTD_compressSubBlock_sequences() :
//	 *  Compresses sequences section for a sub-block.
//	 *  fseMetadata->llType, fseMetadata->ofType, and fseMetadata->mlType have
//	 *  symbol compression modes for the super-block.
//	 *  The first successfully compressed block will have these in its header.
//	 *  We set entropyWritten=1 when we succeed in compressing the sequences.
//	 *  The following sub-blocks will always have repeat mode.
//	 *  @return : compressed size of sequences section of a sub-block
//	 *            Or 0 if it is unable to compress
//	 *            Or error code. */
func ZSTD_compressSubBlock_sequences(tls *libc.TLS, fseTables uintptr, fseMetadata uintptr, sequences uintptr, nbSeq size_t, llCode uintptr, mlCode uintptr, ofCode uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, bmi2 int32, writeEntropy int32, entropyWritten uintptr) (r size_t) {
	var LLtype, MLtype, Offtype, repeat U32
	var bitstreamSize, err_code size_t
	var longOffsets, v1 int32
	var oend, op, ostart, seqHead, v2 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _ = LLtype, MLtype, Offtype, bitstreamSize, err_code, longOffsets, oend, op, ostart, repeat, seqHead, v1, v2
	if MEM_32bits(tls) != 0 {
		v1 = int32(STREAM_ACCUMULATOR_MIN_32)
	} else {
		v1 = int32(STREAM_ACCUMULATOR_MIN_64)
	}
	longOffsets = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.FwindowLog > uint32(v1))
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart
	*(*int32)(unsafe.Pointer(entropyWritten)) = 0
	/* Sequences Header */
	if int64(oend)-int64(op) < int64(libc.Int32FromInt32(3)+libc.Int32FromInt32(1)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbSeq < uint64(128) {
		v2 = op
		op = op + 1
		*(*BYTE)(unsafe.Pointer(v2)) = uint8(nbSeq)
	} else {
		if nbSeq < uint64(LONGNBSEQ) {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(nbSeq>>libc.Int32FromInt32(8) + libc.Uint64FromInt32(0x80))
			*(*BYTE)(unsafe.Pointer(op + 1)) = uint8(nbSeq)
			op = op + uintptr(2)
		} else {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(0xFF)
			MEM_writeLE16(tls, op+uintptr(1), uint16(nbSeq-libc.Uint64FromInt32(LONGNBSEQ)))
			op = op + uintptr(3)
		}
	}
	if nbSeq == uint64(0) {
		return uint64(int64(op) - int64(ostart))
	}
	/* seqHead : flags for FSE encoding type */
	v2 = op
	op = op + 1
	seqHead = v2
	if writeEntropy != 0 {
		LLtype = uint32((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType)
		Offtype = uint32((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType)
		MLtype = uint32((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType)
		*(*BYTE)(unsafe.Pointer(seqHead)) = uint8(LLtype<<libc.Int32FromInt32(6) + Offtype<<libc.Int32FromInt32(4) + MLtype<<libc.Int32FromInt32(2))
		libc.Xmemcpy(tls, op, fseMetadata+12, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize)
		op = op + uintptr((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize)
	} else {
		repeat = uint32(set_repeat)
		*(*BYTE)(unsafe.Pointer(seqHead)) = uint8(repeat<<libc.Int32FromInt32(6) + repeat<<libc.Int32FromInt32(4) + repeat<<libc.Int32FromInt32(2))
	}
	bitstreamSize = ZSTD_encodeSequences(tls, op, uint64(int64(oend)-int64(op)), fseTables+772, mlCode, fseTables, ofCode, fseTables+2224, llCode, sequences, nbSeq, longOffsets, bmi2)
	err_code = bitstreamSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1608, 0)
		}
		return err_code
	}
	op = op + uintptr(bitstreamSize)
	/* zstd versions <= 1.3.4 mistakenly report corruption when
	 * FSE_readNCount() receives a buffer < 4 bytes.
	 * Fixed by https://github.com/facebook/zstd/pull/1146.
	 * This can happen when the last set_compressed table present is 2
	 * bytes and the bitstream is only one byte.
	 * In this exceedingly rare case, we will simply emit an uncompressed
	 * block, since it isn't worth optimizing.
	 */
	if writeEntropy != 0 && (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FlastCountSize != 0 && (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FlastCountSize+bitstreamSize < uint64(4) {
		/* NCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */
		return uint64(0)
	}
	/* zstd versions <= 1.4.0 mistakenly report error when
	 * sequences section body size is less than 3 bytes.
	 * Fixed by https://github.com/facebook/zstd/pull/1664.
	 * This can happen when the previous sequences section block is compressed
	 * with rle mode and the current block's sequences section is compressed
	 * with repeat mode where sequences section body size can be 1 byte.
	 */
	if int64(op)-int64(seqHead) < int64(4) {
		return uint64(0)
	}
	*(*int32)(unsafe.Pointer(entropyWritten)) = int32(1)
	return uint64(int64(op) - int64(ostart))
}

// C documentation
//
//	/** ZSTD_compressSubBlock() :
//	 *  Compresses a single sub-block.
//	 *  @return : compressed size of the sub-block
//	 *            Or 0 if it failed to compress. */
func ZSTD_compressSubBlock(tls *libc.TLS, entropy uintptr, entropyMetadata uintptr, sequences uintptr, nbSeq size_t, literals uintptr, litSize size_t, llCode uintptr, mlCode uintptr, ofCode uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, bmi2 int32, writeLitEntropy int32, writeSeqEntropy int32, litEntropyWritten uintptr, seqEntropyWritten uintptr, lastBlock U32) (r size_t) {
	var cBlockHeader24 U32
	var cLitSize, cSeqSize, cSize, err_code, err_code1 size_t
	var oend, op, ostart uintptr
	_, _, _, _, _, _, _, _, _ = cBlockHeader24, cLitSize, cSeqSize, cSize, err_code, err_code1, oend, op, ostart
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart + uintptr(ZSTD_blockHeaderSize)
	cLitSize = ZSTD_compressSubBlock_literal(tls, entropy, entropyMetadata, literals, litSize, op, uint64(int64(oend)-int64(op)), bmi2, writeLitEntropy, litEntropyWritten)
	err_code = cLitSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1636, 0)
		}
		return err_code
	}
	if cLitSize == uint64(0) {
		return uint64(0)
	}
	op = op + uintptr(cLitSize)
	cSeqSize = ZSTD_compressSubBlock_sequences(tls, entropy+2064, entropyMetadata+144, sequences, nbSeq, llCode, mlCode, ofCode, cctxParams, op, uint64(int64(oend)-int64(op)), bmi2, writeSeqEntropy, seqEntropyWritten)
	err_code1 = cSeqSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1673, 0)
		}
		return err_code1
	}
	if cSeqSize == uint64(0) {
		return uint64(0)
	}
	op = op + uintptr(cSeqSize)
	/* Write block header */
	cSize = uint64(int64(op)-int64(ostart)) - ZSTD_blockHeaderSize
	cBlockHeader24 = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(cSize<<libc.Int32FromInt32(3))
	MEM_writeLE24(tls, ostart, cBlockHeader24)
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_estimateSubBlockSize_literal(tls *libc.TLS, literals uintptr, litSize size_t, huf uintptr, hufMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cLitSizeEstimate, largest, literalSectionHeaderSize size_t
	var countWksp uintptr
	var _ /* maxSymbolValue at bp+0 */ uint32
	_, _, _, _ = cLitSizeEstimate, countWksp, largest, literalSectionHeaderSize
	countWksp = workspace
	*(*uint32)(unsafe.Pointer(bp)) = uint32(255)
	literalSectionHeaderSize = uint64(3) /* Use hard coded size of 3 bytes */
	if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_basic) {
		return litSize
	} else {
		if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_rle) {
			return uint64(1)
		} else {
			if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_compressed) || (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_repeat) {
				largest = HIST_count_wksp(tls, countWksp, bp, literals, litSize, workspace, wkspSize)
				if ZSTD_isError(tls, largest) != 0 {
					return litSize
				}
				cLitSizeEstimate = HUF_estimateCompressedSize(tls, huf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
				if writeEntropy != 0 {
					cLitSizeEstimate = cLitSizeEstimate + (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize
				}
				return cLitSizeEstimate + literalSectionHeaderSize
			}
		}
	}
	/* impossible */
	return uint64(0)
}

func ZSTD_estimateSubBlockSize_symbolType(tls *libc.TLS, type1 SymbolEncodingType_e, codeTable uintptr, maxCode uint32, nbSeq size_t, fseCTable uintptr, additionalBits uintptr, defaultNorm uintptr, defaultNormLog U32, defaultMax U32, workspace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cSymbolTypeSizeEstimateInBits size_t
	var countWksp, ctEnd, ctStart, ctp uintptr
	var v1 uint64
	var _ /* max at bp+0 */ uint32
	_, _, _, _, _, _ = cSymbolTypeSizeEstimateInBits, countWksp, ctEnd, ctStart, ctp, v1
	countWksp = workspace
	ctp = codeTable
	ctStart = ctp
	ctEnd = ctStart + uintptr(nbSeq)
	cSymbolTypeSizeEstimateInBits = uint64(0)
	*(*uint32)(unsafe.Pointer(bp)) = maxCode
	HIST_countFast_wksp(tls, countWksp, bp, codeTable, nbSeq, workspace, wkspSize) /* can't fail */
	if type1 == int32(set_basic) {
		/* We selected this encoding type, so it must be valid. */
		if *(*uint32)(unsafe.Pointer(bp)) <= defaultMax {
			v1 = ZSTD_crossEntropyCost(tls, defaultNorm, defaultNormLog, countWksp, *(*uint32)(unsafe.Pointer(bp)))
		} else {
			v1 = uint64(-int32(ZSTD_error_GENERIC))
		}
		cSymbolTypeSizeEstimateInBits = v1
	} else {
		if type1 == int32(set_rle) {
			cSymbolTypeSizeEstimateInBits = uint64(0)
		} else {
			if type1 == int32(set_compressed) || type1 == int32(set_repeat) {
				cSymbolTypeSizeEstimateInBits = ZSTD_fseBitCost(tls, fseCTable, countWksp, *(*uint32)(unsafe.Pointer(bp)))
			}
		}
	}
	if ZSTD_isError(tls, cSymbolTypeSizeEstimateInBits) != 0 {
		return nbSeq * uint64(10)
	}
	for ctp < ctEnd {
		if additionalBits != 0 {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*U8)(unsafe.Pointer(additionalBits + uintptr(*(*BYTE)(unsafe.Pointer(ctp))))))
		} else {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*BYTE)(unsafe.Pointer(ctp)))
		} /* for offset, offset code is also the number of additional bits */
		ctp = ctp + 1
	}
	return cSymbolTypeSizeEstimateInBits / uint64(8)
}

func ZSTD_estimateSubBlockSize_sequences(tls *libc.TLS, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, fseTables uintptr, fseMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	var cSeqSizeEstimate, sequencesSectionHeaderSize size_t
	_, _ = cSeqSizeEstimate, sequencesSectionHeaderSize
	sequencesSectionHeaderSize = uint64(3) /* Use hard coded size of 3 bytes */
	cSeqSizeEstimate = uint64(0)
	if nbSeq == uint64(0) {
		return sequencesSectionHeaderSize
	}
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateSubBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType, ofCodeTable, uint32(MaxOff), nbSeq, fseTables, libc.UintptrFromInt32(0), uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, uint32(DefaultMaxOff), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateSubBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType, llCodeTable, uint32(MaxLL), nbSeq, fseTables+2224, uintptr(unsafe.Pointer(&LL_bits)), uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, uint32(MaxLL), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateSubBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType, mlCodeTable, uint32(MaxML), nbSeq, fseTables+772, uintptr(unsafe.Pointer(&ML_bits)), uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, uint32(MaxML), workspace, wkspSize)
	if writeEntropy != 0 {
		cSeqSizeEstimate = cSeqSizeEstimate + (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize
	}
	return cSeqSizeEstimate + sequencesSectionHeaderSize
}

type EstimatedBlockSize = struct {
	FestLitSize   size_t
	FestBlockSize size_t
}

func ZSTD_estimateSubBlockSize(tls *libc.TLS, literals uintptr, litSize size_t, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, entropy uintptr, entropyMetadata uintptr, workspace uintptr, wkspSize size_t, writeLitEntropy int32, writeSeqEntropy int32) (r EstimatedBlockSize) {
	var ebs EstimatedBlockSize
	_ = ebs
	ebs.FestLitSize = ZSTD_estimateSubBlockSize_literal(tls, literals, litSize, entropy, entropyMetadata, workspace, wkspSize, writeLitEntropy)
	ebs.FestBlockSize = ZSTD_estimateSubBlockSize_sequences(tls, ofCodeTable, llCodeTable, mlCodeTable, nbSeq, entropy+2064, entropyMetadata+144, workspace, wkspSize, writeSeqEntropy)
	ebs.FestBlockSize += ebs.FestLitSize + ZSTD_blockHeaderSize
	return ebs
}

func ZSTD_needSequenceEntropyTables(tls *libc.TLS, fseMetadata uintptr) (r int32) {
	if (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType == int32(set_compressed) || (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType == int32(set_rle) {
		return int32(1)
	}
	if (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType == int32(set_compressed) || (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType == int32(set_rle) {
		return int32(1)
	}
	if (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType == int32(set_compressed) || (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType == int32(set_rle) {
		return int32(1)
	}
	return 0
}

func countLiterals(tls *libc.TLS, seqStore uintptr, sp uintptr, seqCount size_t) (r size_t) {
	var n, total size_t
	_, _ = n, total
	total = uint64(0)
	n = uint64(0)
	for {
		if !(n < seqCount) {
			break
		}
		total = total + uint64(ZSTD_getSequenceLength(tls, seqStore, sp+uintptr(n)*8).FlitLength)
		goto _1
	_1:
		;
		n = n + 1
	}
	return total
}

func sizeBlockSequences(tls *libc.TLS, sp uintptr, nbSeqs size_t, targetBudget size_t, avgLitCost size_t, avgSeqCost size_t, firstSubBlock int32) (r size_t) {
	var budget, currentCost, headerSize, inSize, n size_t
	_, _, _, _, _ = budget, currentCost, headerSize, inSize, n
	budget = uint64(0)
	inSize = uint64(0)
	/* entropy headers */
	headerSize = uint64(firstSubBlock) * uint64(120) * uint64(BYTESCALE) /* generous estimate */
	budget = budget + headerSize
	/* first sequence => at least one sequence*/
	budget = budget + (uint64((*(*SeqDef)(unsafe.Pointer(sp))).FlitLength)*avgLitCost + avgSeqCost)
	if budget > targetBudget {
		return uint64(1)
	}
	inSize = uint64(int32((*(*SeqDef)(unsafe.Pointer(sp))).FlitLength) + (int32((*(*SeqDef)(unsafe.Pointer(sp))).FmlBase) + int32(MINMATCH)))
	/* loop over sequences */
	n = uint64(1)
	for {
		if !(n < nbSeqs) {
			break
		}
		currentCost = uint64((*(*SeqDef)(unsafe.Pointer(sp + uintptr(n)*8))).FlitLength)*avgLitCost + avgSeqCost
		budget = budget + currentCost
		inSize = inSize + uint64(int32((*(*SeqDef)(unsafe.Pointer(sp + uintptr(n)*8))).FlitLength)+(int32((*(*SeqDef)(unsafe.Pointer(sp + uintptr(n)*8))).FmlBase)+int32(MINMATCH)))
		/* stop when sub-block budget is reached */
		if budget > targetBudget && budget < inSize*uint64(BYTESCALE) {
			break
		}
		goto _1
	_1:
		;
		n = n + 1
	}
	return n
}

// C documentation
//
//	/** ZSTD_compressSubBlock_multi() :
//	 *  Breaks super-block into multiple sub-blocks and compresses them.
//	 *  Entropy will be written into the first block.
//	 *  The following blocks use repeat_mode to compress.
//	 *  Sub-blocks are all compressed, except the last one when beneficial.
//	 *  @return : compressed size of the super block (which features multiple ZSTD blocks)
//	 *            or 0 if it failed to compress. */
func ZSTD_compressSubBlock_multi(tls *libc.TLS, seqStorePtr uintptr, prevCBlock uintptr, nextCBlock uintptr, entropyMetadata uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, bmi2 int32, lastBlock U32, workspace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var avgBlockBudget, avgLitCost, avgSeqCost, blockBudgetSupp, cSize, cSize1, cSize2, decompressedSize, decompressedSize1, err_code, err_code1, err_code2, litSize, litSize1, minTarget, n, nbLiterals, nbSeqs, nbSubBlocks, rSize, seqCount, seqCount1, targetCBlockSize size_t
	var ebs EstimatedBlockSize
	var iend, ip, lend, llCodePtr, lp, lstart, mlCodePtr, oend, ofCodePtr, op, ostart, send, seq, sp, sstart uintptr
	var writeLitEntropy, writeSeqEntropy int32
	var v1, v2 uint64
	var _ /* litEntropyWritten at bp+0 */ int32
	var _ /* litEntropyWritten at bp+8 */ int32
	var _ /* rep at bp+16 */ Repcodes_t
	var _ /* seqEntropyWritten at bp+12 */ int32
	var _ /* seqEntropyWritten at bp+4 */ int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = avgBlockBudget, avgLitCost, avgSeqCost, blockBudgetSupp, cSize, cSize1, cSize2, decompressedSize, decompressedSize1, ebs, err_code, err_code1, err_code2, iend, ip, lend, litSize, litSize1, llCodePtr, lp, lstart, minTarget, mlCodePtr, n, nbLiterals, nbSeqs, nbSubBlocks, oend, ofCodePtr, op, ostart, rSize, send, seq, seqCount, seqCount1, sp, sstart, targetCBlockSize, writeLitEntropy, writeSeqEntropy, v1, v2
	sstart = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
	send = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences
	sp = sstart /* tracks progresses within seqStorePtr->sequences */
	nbSeqs = uint64((int64(send) - int64(sstart)) / 8)
	lstart = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart
	lend = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit
	lp = lstart
	nbLiterals = uint64(int64(lend) - int64(lstart))
	ip = src
	iend = ip + uintptr(srcSize)
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart
	llCodePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	mlCodePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	ofCodePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	minTarget = uint64(ZSTD_TARGETCBLOCKSIZE_MIN)
	if minTarget > (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FtargetCBlockSize {
		v1 = minTarget
	} else {
		v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FtargetCBlockSize
	} /* enforce minimum size, to reduce undesirable side effects */
	targetCBlockSize = v1
	writeLitEntropy = libc.BoolInt32((*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhType == int32(set_compressed))
	writeSeqEntropy = int32(1)
	/* let's start by a general estimation for the full block */
	if nbSeqs > uint64(0) {
		ebs = ZSTD_estimateSubBlockSize(tls, lp, nbLiterals, ofCodePtr, llCodePtr, mlCodePtr, nbSeqs, nextCBlock, entropyMetadata, workspace, wkspSize, writeLitEntropy, writeSeqEntropy)
		if nbLiterals != 0 {
			v1 = ebs.FestLitSize * uint64(BYTESCALE) / nbLiterals
		} else {
			v1 = uint64(BYTESCALE)
		}
		/* quick estimation */
		avgLitCost = v1
		avgSeqCost = (ebs.FestBlockSize - ebs.FestLitSize) * uint64(BYTESCALE) / nbSeqs
		if (ebs.FestBlockSize+targetCBlockSize/uint64(2))/targetCBlockSize > uint64(libc.Int32FromInt32(1)) {
			v2 = (ebs.FestBlockSize + targetCBlockSize/uint64(2)) / targetCBlockSize
		} else {
			v2 = uint64(libc.Int32FromInt32(1))
		}
		nbSubBlocks = v2
		blockBudgetSupp = uint64(0)
		avgBlockBudget = ebs.FestBlockSize * uint64(BYTESCALE) / nbSubBlocks
		/* simplification: if estimates states that the full superblock doesn't compress, just bail out immediately
		 * this will result in the production of a single uncompressed block covering @srcSize.*/
		if ebs.FestBlockSize > srcSize {
			return uint64(0)
		}
		/* compress and write sub-blocks */
		n = uint64(0)
		for {
			if !(n < nbSubBlocks-uint64(1)) {
				break
			}
			/* determine nb of sequences for current sub-block + nbLiterals from next sequence */
			seqCount = sizeBlockSequences(tls, sp, uint64((int64(send)-int64(sp))/8), avgBlockBudget+blockBudgetSupp, avgLitCost, avgSeqCost, libc.BoolInt32(n == uint64(0)))
			/* if reached last sequence : break to last sub-block (simplification) */
			if sp+uintptr(seqCount)*8 == send {
				break
			}
			/* compress sub-block */
			*(*int32)(unsafe.Pointer(bp)) = 0
			*(*int32)(unsafe.Pointer(bp + 4)) = 0
			litSize = countLiterals(tls, seqStorePtr, sp, seqCount)
			decompressedSize = ZSTD_seqDecompressedSize(tls, seqStorePtr, sp, seqCount, litSize, 0)
			cSize = ZSTD_compressSubBlock(tls, nextCBlock, entropyMetadata, sp, seqCount, lp, litSize, llCodePtr, mlCodePtr, ofCodePtr, cctxParams, op, uint64(int64(oend)-int64(op)), bmi2, writeLitEntropy, writeSeqEntropy, bp, bp+4, uint32(0))
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1712, 0)
				}
				return err_code
			}
			/* check compressibility, update state components */
			if cSize > uint64(0) && cSize < decompressedSize {
				ip = ip + uintptr(decompressedSize)
				lp = lp + uintptr(litSize)
				op = op + uintptr(cSize)
				llCodePtr = llCodePtr + uintptr(seqCount)
				mlCodePtr = mlCodePtr + uintptr(seqCount)
				ofCodePtr = ofCodePtr + uintptr(seqCount)
				/* Entropy only needs to be written once */
				if *(*int32)(unsafe.Pointer(bp)) != 0 {
					writeLitEntropy = 0
				}
				if *(*int32)(unsafe.Pointer(bp + 4)) != 0 {
					writeSeqEntropy = 0
				}
				sp = sp + uintptr(seqCount)*8
				blockBudgetSupp = uint64(0)
			}
			/* otherwise : do not compress yet, coalesce current sub-block with following one */
			goto _4
		_4:
			;
			n = n + 1
		}
	} /* if (nbSeqs > 0) */
	/* write last block */
	*(*int32)(unsafe.Pointer(bp + 8)) = 0
	*(*int32)(unsafe.Pointer(bp + 12)) = 0
	litSize1 = uint64(int64(lend) - int64(lp))
	seqCount1 = uint64((int64(send) - int64(sp)) / 8)
	decompressedSize1 = ZSTD_seqDecompressedSize(tls, seqStorePtr, sp, seqCount1, litSize1, int32(1))
	cSize1 = ZSTD_compressSubBlock(tls, nextCBlock, entropyMetadata, sp, seqCount1, lp, litSize1, llCodePtr, mlCodePtr, ofCodePtr, cctxParams, op, uint64(int64(oend)-int64(op)), bmi2, writeLitEntropy, writeSeqEntropy, bp+8, bp+12, lastBlock)
	err_code1 = cSize1
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1712, 0)
		}
		return err_code1
	}
	/* update pointers, the nb of literals borrowed from next sequence must be preserved */
	if cSize1 > uint64(0) && cSize1 < decompressedSize1 {
		ip = ip + uintptr(decompressedSize1)
		lp = lp + uintptr(litSize1)
		op = op + uintptr(cSize1)
		llCodePtr = llCodePtr + uintptr(seqCount1)
		mlCodePtr = mlCodePtr + uintptr(seqCount1)
		ofCodePtr = ofCodePtr + uintptr(seqCount1)
		/* Entropy only needs to be written once */
		if *(*int32)(unsafe.Pointer(bp + 8)) != 0 {
			writeLitEntropy = 0
		}
		if *(*int32)(unsafe.Pointer(bp + 12)) != 0 {
			writeSeqEntropy = 0
		}
		sp = sp + uintptr(seqCount1)*8
	}
	if writeLitEntropy != 0 {
		libc.Xmemcpy(tls, nextCBlock, prevCBlock, libc.Uint64FromInt64(2064))
	}
	if writeSeqEntropy != 0 && ZSTD_needSequenceEntropyTables(tls, entropyMetadata+144) != 0 {
		/* If we haven't written our entropy tables, then we've violated our contract and
		 * must emit an uncompressed block.
		 */
		return uint64(0)
	}
	if ip < iend {
		/* some data left : last part of the block sent uncompressed */
		rSize = uint64(int64(iend) - int64(ip))
		cSize2 = ZSTD_noCompressBlock(tls, op, uint64(int64(oend)-int64(op)), ip, rSize, lastBlock)
		err_code2 = cSize2
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1741, 0)
			}
			return err_code2
		}
		op = op + uintptr(cSize2)
		/* We have to regenerate the repcodes because we've skipped some sequences */
		if sp < send {
			libc.Xmemcpy(tls, bp+16, prevCBlock+5616, libc.Uint64FromInt64(12))
			seq = sstart
			for {
				if !(seq < sp) {
					break
				}
				ZSTD_updateRep(tls, bp+16, (*SeqDef)(unsafe.Pointer(seq)).FoffBase, libc.BoolUint32(ZSTD_getSequenceLength(tls, seqStorePtr, seq).FlitLength == uint32(0)))
				goto _5
			_5:
				;
				seq += 8
			}
			libc.Xmemcpy(tls, nextCBlock+5616, bp+16, libc.Uint64FromInt64(12))
		}
	}
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_compressSuperBlock(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock uint32) (r size_t) {
	bp := tls.Alloc(320)
	defer tls.Free(320)
	var err_code size_t
	var _ /* entropyMetadata at bp+0 */ ZSTD_entropyCTablesMetadata_t
	_ = err_code
	err_code = ZSTD_buildBlockEntropyStats(tls, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, bp, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_compressSubBlock_multi(tls, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, bp, zc+240, dst, dstCapacity, src, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).Fbmi2, lastBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize)
}

/**** ended inlining compress/zstd_compress_superblock.c ****/
/**** start inlining compress/zstd_preSplit.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: zstd_preSplit.h ****/

// C documentation
//
//	/* for hashLog > 8, hash 2 bytes.
//	 * for hashLog == 8, just take the byte, no hashing.
//	 * The speed of this method relies on compile-time constant propagation */
func hash2(tls *libc.TLS, p uintptr, hashLog uint32) (r uint32) {
	if hashLog == uint32(8) {
		return uint32(*(*BYTE)(unsafe.Pointer(p)))
	}
	return uint32(MEM_read16(tls, p)) * uint32(KNUTH) >> (uint32(32) - hashLog)
}

type Fingerprint = struct {
	Fevents   [1024]uint32
	FnbEvents size_t
}

type FPStats = struct {
	FpastEvents Fingerprint
	FnewEvents  Fingerprint
}

func initStats(tls *libc.TLS, fpstats uintptr) {
	libc.Xmemset(tls, fpstats, 0, libc.Uint64FromInt64(8208))
}

func addEvents_generic(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t, samplingRate size_t, hashLog uint32) {
	var limit, n size_t
	var p uintptr
	_, _, _ = limit, n, p
	p = src
	limit = srcSize - uint64(HASHLENGTH) + uint64(1)
	n = uint64(0)
	for {
		if !(n < limit) {
			break
		}
		*(*uint32)(unsafe.Pointer(fp + uintptr(hash2(tls, p+uintptr(n), hashLog))*4)) = *(*uint32)(unsafe.Pointer(fp + uintptr(hash2(tls, p+uintptr(n), hashLog))*4)) + 1
		goto _1
	_1:
		;
		n = n + samplingRate
	}
	*(*size_t)(unsafe.Pointer(fp + 4096)) += limit / samplingRate
}

func recordFingerprint_generic(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t, samplingRate size_t, hashLog uint32) {
	libc.Xmemset(tls, fp, 0, libc.Uint64FromInt64(4)*(libc.Uint64FromInt32(1)<<hashLog))
	(*Fingerprint)(unsafe.Pointer(fp)).FnbEvents = uint64(0)
	addEvents_generic(tls, fp, src, srcSize, samplingRate, hashLog)
}

type RecordEvents_f = uintptr

func ZSTD_recordFingerprint_1(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(1), uint32(10))
}

func ZSTD_recordFingerprint_5(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(5), uint32(10))
}

func ZSTD_recordFingerprint_11(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(11), uint32(9))
}

func ZSTD_recordFingerprint_43(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(43), uint32(8))
}

func abs64(tls *libc.TLS, s64 S64) (r U64) {
	var v1 int64
	_ = v1
	if s64 < 0 {
		v1 = -s64
	} else {
		v1 = s64
	}
	return uint64(v1)
}

func fpDistance(tls *libc.TLS, fp1 uintptr, fp2 uintptr, hashLog uint32) (r U64) {
	var distance U64
	var n size_t
	_, _ = distance, n
	distance = uint64(0)
	n = uint64(0)
	for {
		if !(n < libc.Uint64FromInt32(1)<<hashLog) {
			break
		}
		distance = distance + abs64(tls, int64(*(*uint32)(unsafe.Pointer(fp1 + uintptr(n)*4)))*int64((*Fingerprint)(unsafe.Pointer(fp2)).FnbEvents)-int64(*(*uint32)(unsafe.Pointer(fp2 + uintptr(n)*4)))*int64((*Fingerprint)(unsafe.Pointer(fp1)).FnbEvents))
		goto _1
	_1:
		;
		n = n + 1
	}
	return distance
}

// C documentation
//
//	/* Compare newEvents with pastEvents
//	 * return 1 when considered "too different"
//	 */
func compareFingerprints(tls *libc.TLS, ref uintptr, newfp uintptr, penalty int32, hashLog uint32) (r int32) {
	var deviation, p50, threshold U64
	_, _, _ = deviation, p50, threshold
	p50 = (*Fingerprint)(unsafe.Pointer(ref)).FnbEvents * (*Fingerprint)(unsafe.Pointer(newfp)).FnbEvents
	deviation = fpDistance(tls, ref, newfp, hashLog)
	threshold = p50 * uint64(libc.Int32FromInt32(THRESHOLD_PENALTY_RATE)-libc.Int32FromInt32(2)+penalty) / uint64(THRESHOLD_PENALTY_RATE)
	return libc.BoolInt32(deviation >= threshold)
	return r
}

func mergeEvents(tls *libc.TLS, acc uintptr, newfp uintptr) {
	var n size_t
	_ = n
	n = uint64(0)
	for {
		if !(n < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(HASHLOG_MAX))) {
			break
		}
		*(*uint32)(unsafe.Pointer(acc + uintptr(n)*4)) += *(*uint32)(unsafe.Pointer(newfp + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	*(*size_t)(unsafe.Pointer(acc + 4096)) += (*Fingerprint)(unsafe.Pointer(newfp)).FnbEvents
}

func flushEvents(tls *libc.TLS, fpstats uintptr) {
	var n size_t
	_ = n
	n = uint64(0)
	for {
		if !(n < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(HASHLOG_MAX))) {
			break
		}
		*(*uint32)(unsafe.Pointer(fpstats + uintptr(n)*4)) = *(*uint32)(unsafe.Pointer(fpstats + 4104 + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	(*FPStats)(unsafe.Pointer(fpstats)).FpastEvents.FnbEvents = (*FPStats)(unsafe.Pointer(fpstats)).FnewEvents.FnbEvents
	libc.Xmemset(tls, fpstats+4104, 0, libc.Uint64FromInt64(4104))
}

func removeEvents(tls *libc.TLS, acc uintptr, slice uintptr) {
	var n size_t
	_ = n
	n = uint64(0)
	for {
		if !(n < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(HASHLOG_MAX))) {
			break
		}
		*(*uint32)(unsafe.Pointer(acc + uintptr(n)*4)) -= *(*uint32)(unsafe.Pointer(slice + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	*(*size_t)(unsafe.Pointer(acc + 4096)) -= (*Fingerprint)(unsafe.Pointer(slice)).FnbEvents
}

func ZSTD_splitBlock_byChunks(tls *libc.TLS, blockStart uintptr, blockSize size_t, level int32, workspace uintptr, wkspSize size_t) (r size_t) {
	var fpstats, p uintptr
	var penalty int32
	var pos size_t
	var record_f RecordEvents_f
	_, _, _, _, _ = fpstats, p, penalty, pos, record_f
	record_f = records_fs[level]
	fpstats = workspace
	p = blockStart
	penalty = int32(THRESHOLD_PENALTY)
	pos = uint64(0)
	_ = libc.Uint64FromInt64(1)
	_ = wkspSize
	initStats(tls, fpstats)
	(*(*func(*libc.TLS, uintptr, uintptr, size_t))(unsafe.Pointer(&struct{ uintptr }{record_f})))(tls, fpstats, p, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)))
	pos = uint64(libc.Int32FromInt32(8) << libc.Int32FromInt32(10))
	for {
		if !(pos <= blockSize-uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10))) {
			break
		}
		(*(*func(*libc.TLS, uintptr, uintptr, size_t))(unsafe.Pointer(&struct{ uintptr }{record_f})))(tls, fpstats+4104, p+uintptr(pos), uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)))
		if compareFingerprints(tls, fpstats, fpstats+4104, penalty, hashParams[level]) != 0 {
			return pos
		} else {
			mergeEvents(tls, fpstats, fpstats+4104)
			if penalty > 0 {
				penalty = penalty - 1
			}
		}
		goto _1
	_1:
		;
		pos = pos + uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10))
	}
	return blockSize
	_ = __ccgo_fp(flushEvents)
	_ = __ccgo_fp(removeEvents)
	return r
}

var records_fs = [4]RecordEvents_f{}

func init() {
	p := unsafe.Pointer(&records_fs)
	*(*uintptr)(unsafe.Add(p, 0)) = __ccgo_fp(ZSTD_recordFingerprint_43)
	*(*uintptr)(unsafe.Add(p, 8)) = __ccgo_fp(ZSTD_recordFingerprint_11)
	*(*uintptr)(unsafe.Add(p, 16)) = __ccgo_fp(ZSTD_recordFingerprint_5)
	*(*uintptr)(unsafe.Add(p, 24)) = __ccgo_fp(ZSTD_recordFingerprint_1)
}

var hashParams = [4]uint32{
	0: uint32(8),
	1: uint32(9),
	2: uint32(10),
	3: uint32(10),
}

// C documentation
//
//	/* ZSTD_splitBlock_fromBorders(): very fast strategy :
//	 * compare fingerprint from beginning and end of the block,
//	 * derive from their difference if it's preferable to split in the middle,
//	 * repeat the process a second time, for finer grained decision.
//	 * 3 times did not brought improvements, so I stopped at 2.
//	 * Benefits are good enough for a cheap heuristic.
//	 * More accurate splitting saves more, but speed impact is also more perceptible.
//	 * For better accuracy, use more elaborate variant *_byChunks.
//	 */
func ZSTD_splitBlock_fromBorders(tls *libc.TLS, blockStart uintptr, blockSize size_t, workspace uintptr, wkspSize size_t) (r size_t) {
	var distFromBegin, distFromEnd, minDistance U64
	var fpstats, middleEvents uintptr
	var v1 size_t
	var v2 int32
	_, _, _, _, _, _, _ = distFromBegin, distFromEnd, fpstats, middleEvents, minDistance, v1, v2
	fpstats = workspace
	middleEvents = workspace + uintptr(libc.Uint64FromInt32(512)*libc.Uint64FromInt64(4))
	_ = libc.Uint64FromInt64(1)
	_ = wkspSize
	initStats(tls, fpstats)
	HIST_add(tls, fpstats, blockStart, uint64(SEGMENT_SIZE))
	HIST_add(tls, fpstats+4104, blockStart+uintptr(blockSize)-uintptr(SEGMENT_SIZE), uint64(SEGMENT_SIZE))
	v1 = libc.Uint64FromInt32(SEGMENT_SIZE)
	(*FPStats)(unsafe.Pointer(fpstats)).FnewEvents.FnbEvents = v1
	(*FPStats)(unsafe.Pointer(fpstats)).FpastEvents.FnbEvents = v1
	if !(compareFingerprints(tls, fpstats, fpstats+4104, 0, uint32(8)) != 0) {
		return blockSize
	}
	HIST_add(tls, middleEvents, blockStart+uintptr(blockSize/uint64(2))-uintptr(libc.Int32FromInt32(SEGMENT_SIZE)/libc.Int32FromInt32(2)), uint64(SEGMENT_SIZE))
	(*Fingerprint)(unsafe.Pointer(middleEvents)).FnbEvents = uint64(SEGMENT_SIZE)
	distFromBegin = fpDistance(tls, fpstats, middleEvents, uint32(8))
	distFromEnd = fpDistance(tls, fpstats+4104, middleEvents, uint32(8))
	minDistance = uint64(libc.Int32FromInt32(SEGMENT_SIZE) * libc.Int32FromInt32(SEGMENT_SIZE) / libc.Int32FromInt32(3))
	if abs64(tls, int64(distFromBegin)-int64(distFromEnd)) < minDistance {
		return uint64(libc.Int32FromInt32(64) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	if distFromBegin > distFromEnd {
		v2 = libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))
	} else {
		v2 = libc.Int32FromInt32(96) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))
	}
	return uint64(v2)
	return r
}

func ZSTD_splitBlock(tls *libc.TLS, blockStart uintptr, blockSize size_t, level int32, workspace uintptr, wkspSize size_t) (r size_t) {
	if level == 0 {
		return ZSTD_splitBlock_fromBorders(tls, blockStart, blockSize, workspace, wkspSize)
	}
	/* level >= 1*/
	return ZSTD_splitBlock_byChunks(tls, blockStart, blockSize, level-int32(1), workspace, wkspSize)
}

/**** ended inlining zstd_ldm.h ****/
/**** skipping file: zstd_compress_superblock.h ****/
/**** skipping file: ../common/bits.h ****/

/* ***************************************************************
*  Tuning parameters
*****************************************************************/
/*!
 * COMPRESS_HEAPMODE :
 * Select how default decompression function ZSTD_compress() allocates its context,
 * on stack (0, default), or into heap (1).
 * Note that functions with explicit context such as ZSTD_compressCCtx() are unaffected.
 */

/*!
 * ZSTD_HASHLOG3_MAX :
 * Maximum size of the hash table dedicated to find 3-bytes matches,
 * in log format, aka 17 => 1 << 17 == 128Ki positions.
 * This structure is only used in zstd_opt.
 * Since allocation is centralized for all strategies, it has to be known here.
 * The actual (selected) size of the hash table is then stored in ZSTD_MatchState_t.hashLog3,
 * so that zstd_opt.c doesn't need to know about this constant.
 */

// C documentation
//
//	/*-*************************************
//	*  Helper functions
//	***************************************/
//	/* ZSTD_compressBound()
//	 * Note that the result from this function is only valid for
//	 * the one-pass compression functions.
//	 * When employing the streaming mode,
//	 * if flushes are frequently altering the size of blocks,
//	 * the overhead from block headers can make the compressed data larger
//	 * than the return value of ZSTD_compressBound().
//	 */
func ZSTD_compressBound(tls *libc.TLS, srcSize size_t) (r1 size_t) {
	var r size_t
	var v1, v2 uint64
	_, _, _ = r, v1, v2
	if srcSize >= uint64(0xFF00FF00FF00FF00) {
		v1 = uint64(0)
	} else {
		if srcSize < uint64(libc.Int32FromInt32(128)<<libc.Int32FromInt32(10)) {
			v2 = (uint64(libc.Int32FromInt32(128)<<libc.Int32FromInt32(10)) - srcSize) >> int32(11)
		} else {
			v2 = uint64(0)
		}
		v1 = srcSize + srcSize>>libc.Int32FromInt32(8) + v2
	}
	r = v1
	if r == uint64(0) {
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	return r
}

/* typedef'd to ZSTD_CDict within "zstd.h" */

func ZSTD_createCCtx(tls *libc.TLS) (r uintptr) {
	return ZSTD_createCCtx_advanced(tls, ZSTD_defaultCMem)
}

func ZSTD_initCCtx(tls *libc.TLS, cctx uintptr, memManager ZSTD_customMem) {
	var err size_t
	_ = err
	libc.Xmemset(tls, cctx, 0, libc.Uint64FromInt64(5280))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem = memManager
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2 = ZSTD_cpuSupportsBmi2(tls)
	err = ZSTD_CCtx_reset(tls, cctx, int32(ZSTD_reset_parameters))
	_ = err
}

func ZSTD_createCCtx_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var cctx uintptr
	_ = cctx
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	cctx = ZSTD_customMalloc(tls, uint64(5280), customMem)
	if !(cctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_initCCtx(tls, cctx, customMem)
	return cctx
	return r
}

func ZSTD_initStaticCCtx(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	bp := tls.Alloc(80)
	defer tls.Free(80)
	var cctx uintptr
	var _ /* ws at bp+0 */ ZSTD_cwksp
	_ = cctx
	if workspaceSize <= uint64(5280) {
		return libc.UintptrFromInt32(0)
	} /* minimum size */
	if uint64(workspace)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* must be 8-aligned */
	ZSTD_cwksp_init(tls, bp, workspace, workspaceSize, int32(ZSTD_cwksp_static_alloc))
	cctx = ZSTD_cwksp_reserve_object(tls, bp, uint64(5280))
	if cctx == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	libc.Xmemset(tls, cctx, 0, libc.Uint64FromInt64(5280))
	ZSTD_cwksp_move(tls, cctx+704, bp)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize = workspaceSize
	/* statically sized space. tmpWorkspace never moves (but prev/next block swap places) */
	if !(ZSTD_cwksp_check_available(tls, cctx+704, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2))+libc.Uint64FromInt32(2)*libc.Uint64FromInt64(5632)) != 0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock = ZSTD_cwksp_reserve_object(tls, cctx+704, uint64(5632))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock = ZSTD_cwksp_reserve_object(tls, cctx+704, uint64(5632))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace = ZSTD_cwksp_reserve_object(tls, cctx+704, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2)))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize = uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)) + libc.Uint64FromInt64(4)*uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2 = ZSTD_cpuid_bmi2(tls, ZSTD_cpuid(tls))
	return cctx
}

// C documentation
//
//	/**
//	 * Clears and frees all of the dictionaries in the CCtx.
//	 */
func ZSTD_clearAllDicts(tls *libc.TLS, cctx uintptr) {
	ZSTD_customFree(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictBuffer, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
	ZSTD_freeCDict(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fcdict)
	libc.Xmemset(tls, cctx+3688, 0, libc.Uint64FromInt64(40))
	libc.Xmemset(tls, cctx+3736, 0, libc.Uint64FromInt64(24))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict = libc.UintptrFromInt32(0)
}

func ZSTD_sizeof_localDict(tls *libc.TLS, dict ZSTD_localDict) (r size_t) {
	var bufferSize, cdictSize size_t
	var v1 uint64
	_, _, _ = bufferSize, cdictSize, v1
	if dict.FdictBuffer != libc.UintptrFromInt32(0) {
		v1 = dict.FdictSize
	} else {
		v1 = uint64(0)
	}
	bufferSize = v1
	cdictSize = ZSTD_sizeof_CDict(tls, dict.Fcdict)
	return bufferSize + cdictSize
}

func ZSTD_freeCCtxContent(tls *libc.TLS, cctx uintptr) {
	ZSTD_clearAllDicts(tls, cctx)
	ZSTDMT_freeCCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx = libc.UintptrFromInt32(0)
	ZSTD_cwksp_free(tls, cctx+704, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
}

func ZSTD_freeCCtx(tls *libc.TLS, cctx uintptr) (r size_t) {
	var cctxInWorkspace int32
	_ = cctxInWorkspace
	if cctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1769, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	cctxInWorkspace = ZSTD_cwksp_owns_buffer(tls, cctx+704, cctx)
	ZSTD_freeCCtxContent(tls, cctx)
	if !(cctxInWorkspace != 0) {
		ZSTD_customFree(tls, cctx, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
	}
	return uint64(0)
}

func ZSTD_sizeof_mtctx(tls *libc.TLS, cctx uintptr) (r size_t) {
	return ZSTDMT_sizeof_CCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
}

func ZSTD_sizeof_CCtx(tls *libc.TLS, cctx uintptr) (r size_t) {
	var v1 uint64
	_ = v1
	if cctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof on NULL */
	/* cctx may be in the workspace */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fworkspace.Fworkspace == cctx {
		v1 = uint64(0)
	} else {
		v1 = uint64(5280)
	}
	return v1 + ZSTD_cwksp_sizeof(tls, cctx+704) + ZSTD_sizeof_localDict(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict) + ZSTD_sizeof_mtctx(tls, cctx)
}

func ZSTD_sizeof_CStream(tls *libc.TLS, zcs uintptr) (r size_t) {
	return ZSTD_sizeof_CCtx(tls, zcs) /* same object */
}

// C documentation
//
//	/* private API call, for dictBuilder only */
func ZSTD_getSeqStore(tls *libc.TLS, ctx uintptr) (r uintptr) {
	return ctx + 976
}

// C documentation
//
//	/* Returns true if the strategy supports using a row based matchfinder */
func ZSTD_rowMatchFinderSupported(tls *libc.TLS, strategy ZSTD_strategy) (r int32) {
	return libc.BoolInt32(strategy >= int32(ZSTD_greedy) && strategy <= int32(ZSTD_lazy2))
}

// C documentation
//
//	/* Returns true if the strategy and useRowMatchFinder mode indicate that we will use the row based matchfinder
//	 * for this compression.
//	 */
func ZSTD_rowMatchFinderUsed(tls *libc.TLS, strategy ZSTD_strategy, mode ZSTD_ParamSwitch_e) (r int32) {
	return libc.BoolInt32(ZSTD_rowMatchFinderSupported(tls, strategy) != 0 && mode == int32(ZSTD_ps_enable))
}

// C documentation
//
//	/* Returns row matchfinder usage given an initial mode and cParams */
func ZSTD_resolveRowMatchFinderMode(tls *libc.TLS, mode ZSTD_ParamSwitch_e, cParams uintptr) (r ZSTD_ParamSwitch_e) {
	if mode != int32(ZSTD_ps_auto) {
		return mode
	} /* if requested enabled, but no SIMD, we still will use row matchfinder */
	mode = int32(ZSTD_ps_disable)
	if !(ZSTD_rowMatchFinderSupported(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy) != 0) {
		return mode
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog > uint32(14) {
		mode = int32(ZSTD_ps_enable)
	}
	return mode
}

// C documentation
//
//	/* Returns block splitter usage (generally speaking, when using slower/stronger compression modes) */
func ZSTD_resolveBlockSplitterMode(tls *libc.TLS, mode ZSTD_ParamSwitch_e, cParams uintptr) (r ZSTD_ParamSwitch_e) {
	var v1 int32
	_ = v1
	if mode != int32(ZSTD_ps_auto) {
		return mode
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog >= uint32(17) {
		v1 = int32(ZSTD_ps_enable)
	} else {
		v1 = int32(ZSTD_ps_disable)
	}
	return v1
}

// C documentation
//
//	/* Returns 1 if the arguments indicate that we should allocate a chainTable, 0 otherwise */
func ZSTD_allocateChainTable(tls *libc.TLS, strategy ZSTD_strategy, useRowMatchFinder ZSTD_ParamSwitch_e, forDDSDict U32) (r int32) {
	/* We always should allocate a chaintable if we are allocating a matchstate for a DDS dictionary matchstate.
	 * We do not allocate a chaintable if we are using ZSTD_fast, or are using the row-based matchfinder.
	 */
	return libc.BoolInt32(forDDSDict != 0 || strategy != int32(ZSTD_fast) && !(ZSTD_rowMatchFinderUsed(tls, strategy, useRowMatchFinder) != 0))
}

// C documentation
//
//	/* Returns ZSTD_ps_enable if compression parameters are such that we should
//	 * enable long distance matching (wlog >= 27, strategy >= btopt).
//	 * Returns ZSTD_ps_disable otherwise.
//	 */
func ZSTD_resolveEnableLdm(tls *libc.TLS, mode ZSTD_ParamSwitch_e, cParams uintptr) (r ZSTD_ParamSwitch_e) {
	var v1 int32
	_ = v1
	if mode != int32(ZSTD_ps_auto) {
		return mode
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog >= uint32(27) {
		v1 = int32(ZSTD_ps_enable)
	} else {
		v1 = int32(ZSTD_ps_disable)
	}
	return v1
}

func ZSTD_resolveExternalSequenceValidation(tls *libc.TLS, mode int32) (r int32) {
	return mode
}

// C documentation
//
//	/* Resolves maxBlockSize to the default if no value is present. */
func ZSTD_resolveMaxBlockSize(tls *libc.TLS, maxBlockSize size_t) (r size_t) {
	if maxBlockSize == uint64(0) {
		return uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	} else {
		return maxBlockSize
	}
	return r
}

func ZSTD_resolveExternalRepcodeSearch(tls *libc.TLS, value ZSTD_ParamSwitch_e, cLevel int32) (r ZSTD_ParamSwitch_e) {
	if value != int32(ZSTD_ps_auto) {
		return value
	}
	if cLevel < int32(10) {
		return int32(ZSTD_ps_disable)
	} else {
		return int32(ZSTD_ps_enable)
	}
	return r
}

// C documentation
//
//	/* Returns 1 if compression parameters are such that CDict hashtable and chaintable indices are tagged.
//	 * If so, the tags need to be removed in ZSTD_resetCCtx_byCopyingCDict. */
func ZSTD_CDictIndicesAreTagged(tls *libc.TLS, cParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy == int32(ZSTD_fast) || (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy == int32(ZSTD_dfast))
}

func ZSTD_makeCCtxParamsFromCParams(tls *libc.TLS, _cParams ZSTD_compressionParameters) (r ZSTD_CCtx_params) {
	bp := tls.Alloc(256)
	defer tls.Free(256)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var _ /* cctxParams at bp+32 */ ZSTD_CCtx_params
	/* should not matter, as all cParams are presumed properly defined */
	ZSTD_CCtxParams_init(tls, bp+32, int32(ZSTD_CLEVEL_DEFAULT))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(bp))
	/* Adjust advanced params according to cParams */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FldmParams.FenableLdm = ZSTD_resolveEnableLdm(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FldmParams.FenableLdm, bp)
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		ZSTD_ldm_adjustParameters(tls, bp+32+96, bp)
	}
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FpostBlockSplitter = ZSTD_resolveBlockSplitterMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FpostBlockSplitter, bp)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FuseRowMatchFinder, bp)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FvalidateSequences = ZSTD_resolveExternalSequenceValidation(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FvalidateSequences)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FmaxBlockSize = ZSTD_resolveMaxBlockSize(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FmaxBlockSize)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FsearchForExternalRepcodes = ZSTD_resolveExternalRepcodeSearch(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FsearchForExternalRepcodes, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FcompressionLevel)
	return *(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))
}

func ZSTD_createCCtxParams_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var params uintptr
	_ = params
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	params = ZSTD_customCalloc(tls, uint64(224), customMem)
	if !(params != 0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_CCtxParams_init(tls, params, int32(ZSTD_CLEVEL_DEFAULT))
	(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcustomMem = customMem
	return params
}

func ZSTD_createCCtxParams(tls *libc.TLS) (r uintptr) {
	return ZSTD_createCCtxParams_advanced(tls, ZSTD_defaultCMem)
}

func ZSTD_freeCCtxParams(tls *libc.TLS, params uintptr) (r size_t) {
	if params == libc.UintptrFromInt32(0) {
		return uint64(0)
	}
	ZSTD_customFree(tls, params, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcustomMem)
	return uint64(0)
}

func ZSTD_CCtxParams_reset(tls *libc.TLS, params uintptr) (r size_t) {
	return ZSTD_CCtxParams_init(tls, params, int32(ZSTD_CLEVEL_DEFAULT))
}

func ZSTD_CCtxParams_init(tls *libc.TLS, cctxParams uintptr, compressionLevel int32) (r size_t) {
	if !(cctxParams != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	libc.Xmemset(tls, cctxParams, 0, libc.Uint64FromInt64(224))
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel = compressionLevel
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FfParams.FcontentSizeFlag = int32(1)
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Initializes `cctxParams` from `params` and `compressionLevel`.
//	 * @param compressionLevel If params are derived from a compression level then that compression level, otherwise ZSTD_NO_CLEVEL.
//	 */
func ZSTD_CCtxParams_init_internal(tls *libc.TLS, cctxParams uintptr, params uintptr, compressionLevel int32) {
	libc.Xmemset(tls, cctxParams, 0, libc.Uint64FromInt64(224))
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FcParams
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FfParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FfParams
	/* Should not matter, as all cParams are presumed properly defined.
	 * But, set it for tracing anyway.
	 */
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel = compressionLevel
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FuseRowMatchFinder, params)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FpostBlockSplitter = ZSTD_resolveBlockSplitterMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FpostBlockSplitter, params)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FldmParams.FenableLdm = ZSTD_resolveEnableLdm(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FldmParams.FenableLdm, params)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FvalidateSequences = ZSTD_resolveExternalSequenceValidation(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FvalidateSequences)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FmaxBlockSize = ZSTD_resolveMaxBlockSize(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FmaxBlockSize)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FsearchForExternalRepcodes = ZSTD_resolveExternalRepcodeSearch(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FsearchForExternalRepcodes, compressionLevel)
}

func ZSTD_CCtxParams_init_advanced(tls *libc.TLS, cctxParams uintptr, _params ZSTD_parameters) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var err_code size_t
	_ = err_code
	if !(cctxParams != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	err_code = ZSTD_checkCParams(tls, (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ZSTD_CCtxParams_init_internal(tls, cctxParams, bp, ZSTD_NO_CLEVEL)
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Sets cctxParams' cParams and fParams from params, but otherwise leaves them alone.
//	 * @param params Validated zstd parameters.
//	 */
func ZSTD_CCtxParams_setZstdParams(tls *libc.TLS, cctxParams uintptr, params uintptr) {
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FcParams
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FfParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FfParams
	/* Should not matter, as all cParams are presumed properly defined.
	 * But, set it for tracing anyway.
	 */
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel = ZSTD_NO_CLEVEL
}

func ZSTD_cParam_getBounds(tls *libc.TLS, param ZSTD_cParameter) (r ZSTD_bounds) {
	var bounds ZSTD_bounds
	var v1 int32
	_, _ = bounds, v1
	bounds = ZSTD_bounds{}
	switch param {
	case int32(ZSTD_c_compressionLevel):
		bounds.FlowerBound = ZSTD_minCLevel(tls)
		bounds.FupperBound = ZSTD_maxCLevel(tls)
		return bounds
	case int32(ZSTD_c_windowLog):
		bounds.FlowerBound = int32(ZSTD_WINDOWLOG_MIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)
		return bounds
	case int32(ZSTD_c_hashLog):
		bounds.FlowerBound = int32(ZSTD_HASHLOG_MIN)
		bounds.FupperBound = int32(30)
		return bounds
	case int32(ZSTD_c_chainLog):
		bounds.FlowerBound = int32(ZSTD_HASHLOG_MIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_CHAINLOG_MAX_64)
		return bounds
	case int32(ZSTD_c_searchLog):
		bounds.FlowerBound = int32(ZSTD_SEARCHLOG_MIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64) - libc.Int32FromInt32(1)
		return bounds
	case int32(ZSTD_c_minMatch):
		bounds.FlowerBound = int32(ZSTD_MINMATCH_MIN)
		bounds.FupperBound = int32(ZSTD_MINMATCH_MAX)
		return bounds
	case int32(ZSTD_c_targetLength):
		bounds.FlowerBound = ZSTD_TARGETLENGTH_MIN
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_strategy):
		bounds.FlowerBound = int32(ZSTD_fast)
		bounds.FupperBound = int32(ZSTD_btultra2)
		return bounds
	case int32(ZSTD_c_contentSizeFlag):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_checksumFlag):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_dictIDFlag):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_nbWorkers):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(256)
		return bounds
	case int32(ZSTD_c_jobSize):
		bounds.FlowerBound = 0
		if MEM_32bits(tls) != 0 {
			v1 = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		} else {
			v1 = libc.Int32FromInt32(1024) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		}
		bounds.FupperBound = v1
		return bounds
	case int32(ZSTD_c_overlapLog):
		bounds.FlowerBound = ZSTD_OVERLAPLOG_MIN
		bounds.FupperBound = int32(ZSTD_OVERLAPLOG_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam8):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_enableLongDistanceMatching):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_ldmHashLog):
		bounds.FlowerBound = int32(ZSTD_HASHLOG_MIN)
		bounds.FupperBound = int32(30)
		return bounds
	case int32(ZSTD_c_ldmMinMatch):
		bounds.FlowerBound = int32(ZSTD_LDM_MINMATCH_MIN)
		bounds.FupperBound = int32(ZSTD_LDM_MINMATCH_MAX)
		return bounds
	case int32(ZSTD_c_ldmBucketSizeLog):
		bounds.FlowerBound = int32(ZSTD_LDM_BUCKETSIZELOG_MIN)
		bounds.FupperBound = int32(ZSTD_LDM_BUCKETSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_ldmHashRateLog):
		bounds.FlowerBound = ZSTD_LDM_HASHRATELOG_MIN
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64) - libc.Int32FromInt32(ZSTD_HASHLOG_MIN)
		return bounds
		/* experimental parameters */
		fallthrough
	case int32(ZSTD_c_experimentalParam1):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam3):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam2):
		_ = libc.Uint64FromInt64(1)
		bounds.FlowerBound = int32(ZSTD_f_zstd1)
		bounds.FupperBound = int32(ZSTD_f_zstd1_magicless) /* note : how to ensure at compile time that this is the highest value enum ? */
		return bounds
	case int32(ZSTD_c_experimentalParam4):
		_ = libc.Uint64FromInt64(1)
		bounds.FlowerBound = int32(ZSTD_dictDefaultAttach)
		bounds.FupperBound = int32(ZSTD_dictForceLoad) /* note : how to ensure at compile time that this is the highest value enum ? */
		return bounds
	case int32(ZSTD_c_experimentalParam5):
		_ = libc.Uint64FromInt64(1)
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_targetCBlockSize):
		bounds.FlowerBound = int32(ZSTD_TARGETCBLOCKSIZE_MIN)
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam7):
		bounds.FlowerBound = ZSTD_SRCSIZEHINT_MIN
		bounds.FupperBound = int32(__INT_MAX__)
		return bounds
	case int32(ZSTD_c_experimentalParam9):
		fallthrough
	case int32(ZSTD_c_experimentalParam10):
		bounds.FlowerBound = int32(ZSTD_bm_buffered)
		bounds.FupperBound = int32(ZSTD_bm_stable)
		return bounds
	case int32(ZSTD_c_experimentalParam11):
		bounds.FlowerBound = int32(ZSTD_sf_noBlockDelimiters)
		bounds.FupperBound = int32(ZSTD_sf_explicitBlockDelimiters)
		return bounds
	case int32(ZSTD_c_experimentalParam12):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam13):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_experimentalParam20):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(ZSTD_BLOCKSPLITTER_LEVEL_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam14):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_experimentalParam15):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam16):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_experimentalParam17):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam18):
		bounds.FlowerBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(10)
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam19):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	default:
		bounds.Ferror1 = uint64(-int32(ZSTD_error_parameter_unsupported))
		return bounds
	}
	return r
}

// C documentation
//
//	/* ZSTD_cParam_clampBounds:
//	 * Clamps the value into the bounded range.
//	 */
func ZSTD_cParam_clampBounds(tls *libc.TLS, cParam ZSTD_cParameter, value uintptr) (r size_t) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_cParam_getBounds(tls, cParam)
	if ZSTD_isError(tls, bounds.Ferror1) != 0 {
		return bounds.Ferror1
	}
	if *(*int32)(unsafe.Pointer(value)) < bounds.FlowerBound {
		*(*int32)(unsafe.Pointer(value)) = bounds.FlowerBound
	}
	if *(*int32)(unsafe.Pointer(value)) > bounds.FupperBound {
		*(*int32)(unsafe.Pointer(value)) = bounds.FupperBound
	}
	return uint64(0)
}

func ZSTD_isUpdateAuthorized(tls *libc.TLS, param ZSTD_cParameter) (r int32) {
	switch param {
	case int32(ZSTD_c_compressionLevel):
		fallthrough
	case int32(ZSTD_c_hashLog):
		fallthrough
	case int32(ZSTD_c_chainLog):
		fallthrough
	case int32(ZSTD_c_searchLog):
		fallthrough
	case int32(ZSTD_c_minMatch):
		fallthrough
	case int32(ZSTD_c_targetLength):
		fallthrough
	case int32(ZSTD_c_strategy):
		fallthrough
	case int32(ZSTD_c_experimentalParam20):
		return int32(1)
	case int32(ZSTD_c_experimentalParam2):
		fallthrough
	case int32(ZSTD_c_windowLog):
		fallthrough
	case int32(ZSTD_c_contentSizeFlag):
		fallthrough
	case int32(ZSTD_c_checksumFlag):
		fallthrough
	case int32(ZSTD_c_dictIDFlag):
		fallthrough
	case int32(ZSTD_c_experimentalParam3):
		fallthrough
	case int32(ZSTD_c_nbWorkers):
		fallthrough
	case int32(ZSTD_c_jobSize):
		fallthrough
	case int32(ZSTD_c_overlapLog):
		fallthrough
	case int32(ZSTD_c_experimentalParam1):
		fallthrough
	case int32(ZSTD_c_experimentalParam8):
		fallthrough
	case int32(ZSTD_c_enableLongDistanceMatching):
		fallthrough
	case int32(ZSTD_c_ldmHashLog):
		fallthrough
	case int32(ZSTD_c_ldmMinMatch):
		fallthrough
	case int32(ZSTD_c_ldmBucketSizeLog):
		fallthrough
	case int32(ZSTD_c_ldmHashRateLog):
		fallthrough
	case int32(ZSTD_c_experimentalParam4):
		fallthrough
	case int32(ZSTD_c_experimentalParam5):
		fallthrough
	case int32(ZSTD_c_targetCBlockSize):
		fallthrough
	case int32(ZSTD_c_experimentalParam7):
		fallthrough
	case int32(ZSTD_c_experimentalParam9):
		fallthrough
	case int32(ZSTD_c_experimentalParam10):
		fallthrough
	case int32(ZSTD_c_experimentalParam11):
		fallthrough
	case int32(ZSTD_c_experimentalParam12):
		fallthrough
	case int32(ZSTD_c_experimentalParam13):
		fallthrough
	case int32(ZSTD_c_experimentalParam14):
		fallthrough
	case int32(ZSTD_c_experimentalParam15):
		fallthrough
	case int32(ZSTD_c_experimentalParam16):
		fallthrough
	case int32(ZSTD_c_experimentalParam17):
		fallthrough
	case int32(ZSTD_c_experimentalParam18):
		fallthrough
	case int32(ZSTD_c_experimentalParam19):
		fallthrough
	default:
		return 0
	}
	return r
}

func ZSTD_CCtx_setParameter(tls *libc.TLS, cctx uintptr, param ZSTD_cParameter, value int32) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if ZSTD_isUpdateAuthorized(tls, param) != 0 {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcParamsChanged = int32(1)
		} else {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1801, 0)
			}
			return uint64(-int32(ZSTD_error_stage_wrong))
		}
	}
	switch param {
	case int32(ZSTD_c_nbWorkers):
		goto _1
	case int32(ZSTD_c_experimentalParam19):
		goto _2
	case int32(ZSTD_c_experimentalParam18):
		goto _3
	case int32(ZSTD_c_experimentalParam17):
		goto _4
	case int32(ZSTD_c_experimentalParam16):
		goto _5
	case int32(ZSTD_c_experimentalParam15):
		goto _6
	case int32(ZSTD_c_experimentalParam14):
		goto _7
	case int32(ZSTD_c_experimentalParam20):
		goto _8
	case int32(ZSTD_c_experimentalParam13):
		goto _9
	case int32(ZSTD_c_experimentalParam12):
		goto _10
	case int32(ZSTD_c_experimentalParam11):
		goto _11
	case int32(ZSTD_c_experimentalParam10):
		goto _12
	case int32(ZSTD_c_experimentalParam9):
		goto _13
	case int32(ZSTD_c_experimentalParam7):
		goto _14
	case int32(ZSTD_c_targetCBlockSize):
		goto _15
	case int32(ZSTD_c_ldmBucketSizeLog):
		goto _16
	case int32(ZSTD_c_ldmMinMatch):
		goto _17
	case int32(ZSTD_c_ldmHashLog):
		goto _18
	case int32(ZSTD_c_enableLongDistanceMatching):
		goto _19
	case int32(ZSTD_c_experimentalParam8):
		goto _20
	case int32(ZSTD_c_experimentalParam1):
		goto _21
	case int32(ZSTD_c_overlapLog):
		goto _22
	case int32(ZSTD_c_jobSize):
		goto _23
	case int32(ZSTD_c_experimentalParam5):
		goto _24
	case int32(ZSTD_c_experimentalParam4):
		goto _25
	case int32(ZSTD_c_experimentalParam3):
		goto _26
	case int32(ZSTD_c_dictIDFlag):
		goto _27
	case int32(ZSTD_c_checksumFlag):
		goto _28
	case int32(ZSTD_c_contentSizeFlag):
		goto _29
	case int32(ZSTD_c_experimentalParam2):
		goto _30
	case int32(ZSTD_c_ldmHashRateLog):
		goto _31
	case int32(ZSTD_c_strategy):
		goto _32
	case int32(ZSTD_c_targetLength):
		goto _33
	case int32(ZSTD_c_minMatch):
		goto _34
	case int32(ZSTD_c_searchLog):
		goto _35
	case int32(ZSTD_c_chainLog):
		goto _36
	case int32(ZSTD_c_hashLog):
		goto _37
	case int32(ZSTD_c_windowLog):
		goto _38
	case int32(ZSTD_c_compressionLevel):
		goto _39
	default:
		goto _40
	}
	goto _41
_1:
	;
_44:
	;
	if value != 0 && (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1840, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_unsupported))
	}
	goto _43
_43:
	;
	if 0 != 0 {
		goto _44
	}
	goto _42
_42:
	;
	goto _41
_39:
	;
_38:
	;
_37:
	;
_36:
	;
_35:
	;
_34:
	;
_33:
	;
_32:
	;
_31:
	;
_30:
	;
_29:
	;
_28:
	;
_27:
	;
_26:
	;
_25:
	;
_24:
	;
_23:
	;
_22:
	;
_21:
	;
_20:
	;
_19:
	;
_18:
	;
_17:
	;
_16:
	;
_15:
	;
_14:
	;
_13:
	;
_12:
	;
_11:
	;
_10:
	;
_9:
	;
_8:
	;
_7:
	;
_6:
	;
_5:
	;
_4:
	;
_3:
	;
_2:
	;
	goto _41
_40:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1876, 0)
	}
	return uint64(-int32(ZSTD_error_parameter_unsupported))
_41:
	;
	return ZSTD_CCtxParams_setParameter(tls, cctx+16, param, value)
}

func ZSTD_CCtxParams_setParameter(tls *libc.TLS, CCtxParams uintptr, param ZSTD_cParameter, _value int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*int32)(unsafe.Pointer(bp)) = _value
	var err_code, err_code1, err_code2, err_code3, err_code4 size_t
	var lcm ZSTD_ParamSwitch_e
	var pref ZSTD_dictAttachPref_e
	var v45 int32
	_, _, _, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, err_code4, lcm, pref, v45
	switch param {
	case int32(ZSTD_c_experimentalParam2):
		goto _1
	case int32(ZSTD_c_compressionLevel):
		goto _2
	case int32(ZSTD_c_windowLog):
		goto _3
	case int32(ZSTD_c_hashLog):
		goto _4
	case int32(ZSTD_c_chainLog):
		goto _5
	case int32(ZSTD_c_searchLog):
		goto _6
	case int32(ZSTD_c_minMatch):
		goto _7
	case int32(ZSTD_c_targetLength):
		goto _8
	case int32(ZSTD_c_strategy):
		goto _9
	case int32(ZSTD_c_contentSizeFlag):
		goto _10
	case int32(ZSTD_c_checksumFlag):
		goto _11
	case int32(ZSTD_c_dictIDFlag):
		goto _12
	case int32(ZSTD_c_experimentalParam3):
		goto _13
	case int32(ZSTD_c_experimentalParam4):
		goto _14
	case int32(ZSTD_c_experimentalParam5):
		goto _15
	case int32(ZSTD_c_nbWorkers):
		goto _16
	case int32(ZSTD_c_jobSize):
		goto _17
	case int32(ZSTD_c_overlapLog):
		goto _18
	case int32(ZSTD_c_experimentalParam1):
		goto _19
	case int32(ZSTD_c_experimentalParam8):
		goto _20
	case int32(ZSTD_c_enableLongDistanceMatching):
		goto _21
	case int32(ZSTD_c_ldmHashLog):
		goto _22
	case int32(ZSTD_c_ldmMinMatch):
		goto _23
	case int32(ZSTD_c_ldmBucketSizeLog):
		goto _24
	case int32(ZSTD_c_ldmHashRateLog):
		goto _25
	case int32(ZSTD_c_targetCBlockSize):
		goto _26
	case int32(ZSTD_c_experimentalParam7):
		goto _27
	case int32(ZSTD_c_experimentalParam9):
		goto _28
	case int32(ZSTD_c_experimentalParam10):
		goto _29
	case int32(ZSTD_c_experimentalParam11):
		goto _30
	case int32(ZSTD_c_experimentalParam12):
		goto _31
	case int32(ZSTD_c_experimentalParam13):
		goto _32
	case int32(ZSTD_c_experimentalParam20):
		goto _33
	case int32(ZSTD_c_experimentalParam14):
		goto _34
	case int32(ZSTD_c_experimentalParam15):
		goto _35
	case int32(ZSTD_c_experimentalParam16):
		goto _36
	case int32(ZSTD_c_experimentalParam17):
		goto _37
	case int32(ZSTD_c_experimentalParam18):
		goto _38
	case int32(ZSTD_c_experimentalParam19):
		goto _39
	default:
		goto _40
	}
	goto _41
_1:
	;
_44:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam2), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	goto _43
_43:
	;
	if 0 != 0 {
		goto _44
	}
	goto _42
_42:
	;
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Fformat = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Fformat)
_2:
	;
	err_code = ZSTD_cParam_clampBounds(tls, param, bp)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if *(*int32)(unsafe.Pointer(bp)) == 0 {
		(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel = *(*int32)(unsafe.Pointer(bp))
	}
	if (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel >= 0 {
		return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel)
	}
	return uint64(0) /* return type (size_t) cannot represent negative values */
_3:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_windowLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FwindowLog = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FwindowLog)
_4:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_hashLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FhashLog = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FhashLog)
_5:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_chainLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FchainLog = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FchainLog)
_6:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_searchLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FsearchLog = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64(*(*int32)(unsafe.Pointer(bp)))
_7:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_minMatch), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FminMatch = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FminMatch)
_8:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_targetLength), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FtargetLength = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FtargetLength)
_9:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_strategy), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.Fstrategy = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.Fstrategy)
_10:
	;
	/* Content size written in frame header _when known_ (default:1) */
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FcontentSizeFlag = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FcontentSizeFlag)
_11:
	;
	/* A 32-bits content checksum will be calculated and written at end of frame (default:0) */
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FchecksumFlag = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FchecksumFlag)
_12:
	; /* When applicable, dictionary's dictID is provided in frame header (default:1) */
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FnoDictIDFlag = libc.BoolInt32(!(*(*int32)(unsafe.Pointer(bp)) != 0))
	return libc.BoolUint64(!((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FnoDictIDFlag != 0))
_13:
	;
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FforceWindow = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FforceWindow)
_14:
	;
	pref = *(*int32)(unsafe.Pointer(bp))
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam4), pref) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FattachDictPref = pref
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FattachDictPref)
_15:
	;
	lcm = *(*int32)(unsafe.Pointer(bp))
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam5), lcm) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FliteralCompressionMode = lcm
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FliteralCompressionMode)
_16:
	;
	err_code1 = ZSTD_cParam_clampBounds(tls, param, bp)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FnbWorkers = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FnbWorkers)
_17:
	;
	/* Adjust to the minimum non-default value. */
	if *(*int32)(unsafe.Pointer(bp)) != 0 && *(*int32)(unsafe.Pointer(bp)) < libc.Int32FromInt32(512)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)) {
		*(*int32)(unsafe.Pointer(bp)) = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))
	}
	err_code2 = ZSTD_cParam_clampBounds(tls, param, bp)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FjobSize = uint64(*(*int32)(unsafe.Pointer(bp)))
	return (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FjobSize
_18:
	;
	err_code3 = ZSTD_cParam_clampBounds(tls, int32(ZSTD_c_overlapLog), bp)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoverlapLog = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoverlapLog)
_19:
	;
	err_code4 = ZSTD_cParam_clampBounds(tls, int32(ZSTD_c_overlapLog), bp)
	if ERR_isError(tls, err_code4) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code4
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Frsyncable = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Frsyncable)
_20:
	;
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableDedicatedDictSearch = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableDedicatedDictSearch)
_21:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_enableLongDistanceMatching), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm)
_22:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> auto */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmHashLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashLog = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashLog)
_23:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmMinMatch), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FminMatchLength = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FminMatchLength)
_24:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmBucketSizeLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FbucketSizeLog = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FbucketSizeLog)
_25:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmHashRateLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashRateLog = uint32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashRateLog)
_26:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if *(*int32)(unsafe.Pointer(bp)) > int32(ZSTD_TARGETCBLOCKSIZE_MIN) {
			v45 = *(*int32)(unsafe.Pointer(bp))
		} else {
			v45 = int32(ZSTD_TARGETCBLOCKSIZE_MIN)
		}
		*(*int32)(unsafe.Pointer(bp)) = v45
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_targetCBlockSize), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FtargetCBlockSize = uint64(uint32(*(*int32)(unsafe.Pointer(bp))))
	return (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FtargetCBlockSize
_27:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam7), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint)
_28:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam9), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FinBufferMode = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FinBufferMode)
_29:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam10), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoutBufferMode = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoutBufferMode)
_30:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam11), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FblockDelimiters = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FblockDelimiters)
_31:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam12), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FvalidateSequences = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FvalidateSequences)
_32:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam13), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpostBlockSplitter = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpostBlockSplitter)
_33:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam20), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpreBlockSplitter_level = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpreBlockSplitter_level)
_34:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam14), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder)
_35:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam15), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FdeterministicRefPrefix = libc.BoolInt32(!!(*(*int32)(unsafe.Pointer(bp)) != 0))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FdeterministicRefPrefix)
_36:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam16), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FprefetchCDictTables = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FprefetchCDictTables)
_37:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam17), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableMatchFinderFallback = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableMatchFinderFallback)
_38:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam18), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FmaxBlockSize = uint64(*(*int32)(unsafe.Pointer(bp)))
	return (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FmaxBlockSize
_39:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam19), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsearchForExternalRepcodes = *(*int32)(unsafe.Pointer(bp))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsearchForExternalRepcodes)
_40:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1876, 0)
	}
	return uint64(-int32(ZSTD_error_parameter_unsupported))
_41:
	;
	return r
}

func ZSTD_CCtx_getParameter(tls *libc.TLS, cctx uintptr, param ZSTD_cParameter, value uintptr) (r size_t) {
	return ZSTD_CCtxParams_getParameter(tls, cctx+16, param, value)
}

func ZSTD_CCtxParams_getParameter(tls *libc.TLS, CCtxParams uintptr, param ZSTD_cParameter, value uintptr) (r size_t) {
	switch param {
	case int32(ZSTD_c_experimentalParam2):
		goto _1
	case int32(ZSTD_c_compressionLevel):
		goto _2
	case int32(ZSTD_c_windowLog):
		goto _3
	case int32(ZSTD_c_hashLog):
		goto _4
	case int32(ZSTD_c_chainLog):
		goto _5
	case int32(ZSTD_c_searchLog):
		goto _6
	case int32(ZSTD_c_minMatch):
		goto _7
	case int32(ZSTD_c_targetLength):
		goto _8
	case int32(ZSTD_c_strategy):
		goto _9
	case int32(ZSTD_c_contentSizeFlag):
		goto _10
	case int32(ZSTD_c_checksumFlag):
		goto _11
	case int32(ZSTD_c_dictIDFlag):
		goto _12
	case int32(ZSTD_c_experimentalParam3):
		goto _13
	case int32(ZSTD_c_experimentalParam4):
		goto _14
	case int32(ZSTD_c_experimentalParam5):
		goto _15
	case int32(ZSTD_c_nbWorkers):
		goto _16
	case int32(ZSTD_c_jobSize):
		goto _17
	case int32(ZSTD_c_overlapLog):
		goto _18
	case int32(ZSTD_c_experimentalParam1):
		goto _19
	case int32(ZSTD_c_experimentalParam8):
		goto _20
	case int32(ZSTD_c_enableLongDistanceMatching):
		goto _21
	case int32(ZSTD_c_ldmHashLog):
		goto _22
	case int32(ZSTD_c_ldmMinMatch):
		goto _23
	case int32(ZSTD_c_ldmBucketSizeLog):
		goto _24
	case int32(ZSTD_c_ldmHashRateLog):
		goto _25
	case int32(ZSTD_c_targetCBlockSize):
		goto _26
	case int32(ZSTD_c_experimentalParam7):
		goto _27
	case int32(ZSTD_c_experimentalParam9):
		goto _28
	case int32(ZSTD_c_experimentalParam10):
		goto _29
	case int32(ZSTD_c_experimentalParam11):
		goto _30
	case int32(ZSTD_c_experimentalParam12):
		goto _31
	case int32(ZSTD_c_experimentalParam13):
		goto _32
	case int32(ZSTD_c_experimentalParam20):
		goto _33
	case int32(ZSTD_c_experimentalParam14):
		goto _34
	case int32(ZSTD_c_experimentalParam15):
		goto _35
	case int32(ZSTD_c_experimentalParam16):
		goto _36
	case int32(ZSTD_c_experimentalParam17):
		goto _37
	case int32(ZSTD_c_experimentalParam18):
		goto _38
	case int32(ZSTD_c_experimentalParam19):
		goto _39
	default:
		goto _40
	}
	goto _41
_1:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Fformat
	goto _41
_2:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel
	goto _41
_3:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FwindowLog)
	goto _41
_4:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FhashLog)
	goto _41
_5:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FchainLog)
	goto _41
_6:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FsearchLog)
	goto _41
_7:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FminMatch)
	goto _41
_8:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FtargetLength)
	goto _41
_9:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.Fstrategy
	goto _41
_10:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FcontentSizeFlag
	goto _41
_11:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FchecksumFlag
	goto _41
_12:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.BoolInt32(!((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FnoDictIDFlag != 0))
	goto _41
_13:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FforceWindow
	goto _41
_14:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FattachDictPref
	goto _41
_15:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FliteralCompressionMode
	goto _41
_16:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FnbWorkers
	goto _41
_17:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FjobSize)
	goto _41
_18:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoverlapLog
	goto _41
_19:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Frsyncable
	goto _41
_20:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableDedicatedDictSearch
	goto _41
_21:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm
	goto _41
_22:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashLog)
	goto _41
_23:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FminMatchLength)
	goto _41
_24:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FbucketSizeLog)
	goto _41
_25:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashRateLog)
	goto _41
_26:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FtargetCBlockSize)
	goto _41
_27:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint
	goto _41
_28:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FinBufferMode
	goto _41
_29:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoutBufferMode
	goto _41
_30:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FblockDelimiters
	goto _41
_31:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FvalidateSequences
	goto _41
_32:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpostBlockSplitter
	goto _41
_33:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpreBlockSplitter_level
	goto _41
_34:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder
	goto _41
_35:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FdeterministicRefPrefix
	goto _41
_36:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FprefetchCDictTables
	goto _41
_37:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableMatchFinderFallback
	goto _41
_38:
	;
	*(*int32)(unsafe.Pointer(value)) = int32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FmaxBlockSize)
	goto _41
_39:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsearchForExternalRepcodes
	goto _41
_40:
	;
_44:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1876, 0)
	}
	return uint64(-int32(ZSTD_error_parameter_unsupported))
	goto _43
_43:
	;
	if 0 != 0 {
		goto _44
	}
	goto _42
_42:
	;
_41:
	;
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_CCtx_setParametersUsingCCtxParams() :
//	 *  just applies `params` into `cctx`
//	 *  no action is performed, parameters are merely stored.
//	 *  If ZSTDMT is enabled, parameters are pushed to cctx->mtctx.
//	 *    This is possible even if a compression is ongoing.
//	 *    In which case, new parameters will be applied on the fly, starting with next compression job.
//	 */
func ZSTD_CCtx_setParametersUsingCCtxParams(tls *libc.TLS, cctx uintptr, params uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1914, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1949, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(params))
	return uint64(0)
}

func ZSTD_CCtx_setCParams(tls *libc.TLS, cctx uintptr, cparams ZSTD_compressionParameters) (r size_t) {
	var err_code, err_code1, err_code2, err_code3, err_code4, err_code5, err_code6, err_code7 size_t
	_, _, _, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, err_code4, err_code5, err_code6, err_code7
	_ = libc.Uint64FromInt64(1)
	/* only update if all parameters are valid */
	err_code = ZSTD_checkCParams(tls, cparams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_windowLog), int32(cparams.FwindowLog))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_chainLog), int32(cparams.FchainLog))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	err_code3 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_hashLog), int32(cparams.FhashLog))
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	err_code4 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_searchLog), int32(cparams.FsearchLog))
	if ERR_isError(tls, err_code4) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code4
	}
	err_code5 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_minMatch), int32(cparams.FminMatch))
	if ERR_isError(tls, err_code5) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code5
	}
	err_code6 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_targetLength), int32(cparams.FtargetLength))
	if ERR_isError(tls, err_code6) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code6
	}
	err_code7 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_strategy), cparams.Fstrategy)
	if ERR_isError(tls, err_code7) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code7
	}
	return uint64(0)
}

func ZSTD_CCtx_setFParams(tls *libc.TLS, cctx uintptr, fparams ZSTD_frameParameters) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	_ = libc.Uint64FromInt64(1)
	err_code = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_contentSizeFlag), libc.BoolInt32(fparams.FcontentSizeFlag != 0))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_checksumFlag), libc.BoolInt32(fparams.FchecksumFlag != 0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_dictIDFlag), libc.BoolInt32(fparams.FnoDictIDFlag == 0))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

func ZSTD_CCtx_setParams(tls *libc.TLS, cctx uintptr, params ZSTD_parameters) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	/* First check cParams, because we want to update all or none. */
	err_code = ZSTD_checkCParams(tls, params.FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	/* Next set fParams, because this could fail if the cctx isn't in init stage. */
	err_code1 = ZSTD_CCtx_setFParams(tls, cctx, params.FfParams)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	/* Finally set cParams, which should succeed. */
	err_code2 = ZSTD_CCtx_setCParams(tls, cctx, params.FcParams)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

func ZSTD_CCtx_setPledgedSrcSize(tls *libc.TLS, cctx uintptr, pledgedSrcSize uint64) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2036, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne = pledgedSrcSize + uint64(1)
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Initializes the local dictionary using requested parameters.
//	 * NOTE: Initialization does not employ the pledged src size,
//	 * because the dictionary may be used for multiple compressions.
//	 */
func ZSTD_initLocalDict(tls *libc.TLS, cctx uintptr) (r size_t) {
	var dl uintptr
	_ = dl
	dl = cctx + 3688
	if (*ZSTD_localDict)(unsafe.Pointer(dl)).Fdict == libc.UintptrFromInt32(0) {
		/* No local dictionary. */
		return uint64(0)
	}
	if (*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict != libc.UintptrFromInt32(0) {
		/* Local dictionary already initialized. */
		return uint64(0)
	}
	(*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict = ZSTD_createCDict_advanced2(tls, (*ZSTD_localDict)(unsafe.Pointer(dl)).Fdict, (*ZSTD_localDict)(unsafe.Pointer(dl)).FdictSize, int32(ZSTD_dlm_byRef), (*ZSTD_localDict)(unsafe.Pointer(dl)).FdictContentType, cctx+16, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
	if !((*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2085, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict = (*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict
	return uint64(0)
}

func ZSTD_CCtx_loadDictionary_advanced(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var dictBuffer uintptr
	_ = dictBuffer
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2118, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearAllDicts(tls, cctx)                                  /* erase any previously set dictionary */
	if dict == libc.UintptrFromInt32(0) || dictSize == uint64(0) { /* no dictionary */
		return uint64(0)
	}
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fdict = dict
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2174, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		dictBuffer = ZSTD_customMalloc(tls, dictSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
		if dictBuffer == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2236, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		libc.Xmemcpy(tls, dictBuffer, dict, dictSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictBuffer = dictBuffer /* owned ptr to free */
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fdict = dictBuffer       /* read-only reference */
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictSize = dictSize
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictContentType = dictContentType
	return uint64(0)
}

func ZSTD_CCtx_loadDictionary_byReference(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_CCtx_loadDictionary_advanced(tls, cctx, dict, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto))
}

func ZSTD_CCtx_loadDictionary(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_CCtx_loadDictionary_advanced(tls, cctx, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto))
}

func ZSTD_CCtx_refCDict(tls *libc.TLS, cctx uintptr, cdict uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2277, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	/* Free the existing local cdict (if any) to save memory. */
	ZSTD_clearAllDicts(tls, cctx)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict = cdict
	return uint64(0)
}

func ZSTD_CCtx_refThreadPool(tls *libc.TLS, cctx uintptr, pool uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2322, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fpool = pool
	return uint64(0)
}

func ZSTD_CCtx_refPrefix(tls *libc.TLS, cctx uintptr, prefix uintptr, prefixSize size_t) (r size_t) {
	return ZSTD_CCtx_refPrefix_advanced(tls, cctx, prefix, prefixSize, int32(ZSTD_dct_rawContent))
}

func ZSTD_CCtx_refPrefix_advanced(tls *libc.TLS, cctx uintptr, prefix uintptr, prefixSize size_t, dictContentType ZSTD_dictContentType_e) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2367, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearAllDicts(tls, cctx)
	if prefix != libc.UintptrFromInt32(0) && prefixSize > uint64(0) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.Fdict = prefix
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictSize = prefixSize
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictContentType = dictContentType
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_CCtx_reset() :
//	 *  Also dumps dictionary */
func ZSTD_CCtx_reset(tls *libc.TLS, cctx uintptr, reset ZSTD_ResetDirective) (r size_t) {
	if reset == int32(ZSTD_reset_session_only) || reset == int32(ZSTD_reset_session_and_parameters) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage = int32(zcss_init)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne = uint64(0)
	}
	if reset == int32(ZSTD_reset_parameters) || reset == int32(ZSTD_reset_session_and_parameters) {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2414, 0)
			}
			return uint64(-int32(ZSTD_error_stage_wrong))
		}
		ZSTD_clearAllDicts(tls, cctx)
		return ZSTD_CCtxParams_reset(tls, cctx+16)
	}
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_checkCParams() :
//	    control CParam values remain within authorized range.
//	    @return : 0, or an error code if one value is beyond authorized range */
func ZSTD_checkCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r size_t) {
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_windowLog), int32(cParams.FwindowLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_chainLog), int32(cParams.FchainLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_hashLog), int32(cParams.FhashLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_searchLog), int32(cParams.FsearchLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_minMatch), int32(cParams.FminMatch)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_targetLength), int32(cParams.FtargetLength)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_strategy), cParams.Fstrategy) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_clampCParams() :
//	 *  make CParam values within valid range.
//	 *  @return : valid CParams */
func ZSTD_clampCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r ZSTD_compressionParameters) {
	var bounds, bounds1, bounds2, bounds3, bounds4, bounds5, bounds6 ZSTD_bounds
	_, _, _, _, _, _, _ = bounds, bounds1, bounds2, bounds3, bounds4, bounds5, bounds6
	bounds = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_windowLog))
	if int32(cParams.FwindowLog) < bounds.FlowerBound {
		cParams.FwindowLog = uint32(bounds.FlowerBound)
	} else {
		if int32(cParams.FwindowLog) > bounds.FupperBound {
			cParams.FwindowLog = uint32(bounds.FupperBound)
		}
	}
	bounds1 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_chainLog))
	if int32(cParams.FchainLog) < bounds1.FlowerBound {
		cParams.FchainLog = uint32(bounds1.FlowerBound)
	} else {
		if int32(cParams.FchainLog) > bounds1.FupperBound {
			cParams.FchainLog = uint32(bounds1.FupperBound)
		}
	}
	bounds2 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_hashLog))
	if int32(cParams.FhashLog) < bounds2.FlowerBound {
		cParams.FhashLog = uint32(bounds2.FlowerBound)
	} else {
		if int32(cParams.FhashLog) > bounds2.FupperBound {
			cParams.FhashLog = uint32(bounds2.FupperBound)
		}
	}
	bounds3 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_searchLog))
	if int32(cParams.FsearchLog) < bounds3.FlowerBound {
		cParams.FsearchLog = uint32(bounds3.FlowerBound)
	} else {
		if int32(cParams.FsearchLog) > bounds3.FupperBound {
			cParams.FsearchLog = uint32(bounds3.FupperBound)
		}
	}
	bounds4 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_minMatch))
	if int32(cParams.FminMatch) < bounds4.FlowerBound {
		cParams.FminMatch = uint32(bounds4.FlowerBound)
	} else {
		if int32(cParams.FminMatch) > bounds4.FupperBound {
			cParams.FminMatch = uint32(bounds4.FupperBound)
		}
	}
	bounds5 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_targetLength))
	if int32(cParams.FtargetLength) < bounds5.FlowerBound {
		cParams.FtargetLength = uint32(bounds5.FlowerBound)
	} else {
		if int32(cParams.FtargetLength) > bounds5.FupperBound {
			cParams.FtargetLength = uint32(bounds5.FupperBound)
		}
	}
	bounds6 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_strategy))
	if cParams.Fstrategy < bounds6.FlowerBound {
		cParams.Fstrategy = bounds6.FlowerBound
	} else {
		if cParams.Fstrategy > bounds6.FupperBound {
			cParams.Fstrategy = bounds6.FupperBound
		}
	}
	return cParams
}

// C documentation
//
//	/** ZSTD_cycleLog() :
//	 *  condition for correct operation : hashLog > 1 */
func ZSTD_cycleLog(tls *libc.TLS, hashLog U32, strat ZSTD_strategy) (r U32) {
	var btScale U32
	_ = btScale
	btScale = libc.BoolUint32(uint32(strat) >= uint32(ZSTD_btlazy2))
	return hashLog - btScale
}

// C documentation
//
//	/** ZSTD_dictAndWindowLog() :
//	 * Returns an adjusted window log that is large enough to fit the source and the dictionary.
//	 * The zstd format says that the entire dictionary is valid if one byte of the dictionary
//	 * is within the window. So the hashLog and chainLog should be large enough to reference both
//	 * the dictionary and the window. So we must use this adjusted dictAndWindowLog when downsizing
//	 * the hashLog and windowLog.
//	 * NOTE: srcSize must not be ZSTD_CONTENTSIZE_UNKNOWN.
//	 */
func ZSTD_dictAndWindowLog(tls *libc.TLS, windowLog U32, srcSize U64, dictSize U64) (r U32) {
	var dictAndWindowSize, maxWindowSize, windowSize U64
	_, _, _ = dictAndWindowSize, maxWindowSize, windowSize
	maxWindowSize = libc.Uint64FromUint64(1) << libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)
	/* No dictionary ==> No change */
	if dictSize == uint64(0) {
		return windowLog
	}
	/* Handled in ZSTD_adjustCParams_internal() */
	windowSize = uint64(1) << windowLog
	dictAndWindowSize = dictSize + windowSize
	/* If the window size is already large enough to fit both the source and the dictionary
	 * then just use the window size. Otherwise adjust so that it fits the dictionary and
	 * the window.
	 */
	if windowSize >= dictSize+srcSize {
		return windowLog /* Window size large enough already */
	} else {
		if dictAndWindowSize >= maxWindowSize {
			return uint32(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)) /* Larger than max window log */
		} else {
			return ZSTD_highbit32(tls, uint32(dictAndWindowSize)-uint32(1)) + uint32(1)
		}
	}
	return r
}

// C documentation
//
//	/** ZSTD_adjustCParams_internal() :
//	 *  optimize `cPar` for a specified input (`srcSize` and `dictSize`).
//	 *  mostly downsize to reduce memory consumption and initialization latency.
//	 * `srcSize` can be ZSTD_CONTENTSIZE_UNKNOWN when not known.
//	 * `mode` is the mode for parameter adjustment. See docs for `ZSTD_CParamMode_e`.
//	 *  note : `srcSize==0` means 0!
//	 *  condition : cPar is presumed validated (can be checked using ZSTD_checkCParams()). */
func ZSTD_adjustCParams_internal(tls *libc.TLS, _cPar ZSTD_compressionParameters, srcSize uint64, dictSize size_t, mode ZSTD_CParamMode_e, useRowMatchFinder ZSTD_ParamSwitch_e) (r ZSTD_compressionParameters) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cPar
	var cycleLog, dictAndWindowLog, maxHashLog, maxRowHashLog, maxShortCacheHashLog, rowLog, srcLog, tSize U32
	var maxWindowResize, minSrcSize U64
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = cycleLog, dictAndWindowLog, maxHashLog, maxRowHashLog, maxShortCacheHashLog, maxWindowResize, minSrcSize, rowLog, srcLog, tSize, v1, v2, v3
	minSrcSize = uint64(513) /* (1<<9) + 1 */
	maxWindowResize = libc.Uint64FromUint64(1) << (libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64) - libc.Int32FromInt32(1))
	/* Cascade the selected strategy down to the next-highest one built into
	 * this binary. */
	switch mode {
	case int32(ZSTD_cpm_unknown):
		fallthrough
	case int32(ZSTD_cpm_noAttachDict):
		/* If we don't know the source size, don't make any
		 * assumptions about it. We will already have selected
		 * smaller parameters if a dictionary is in use.
		 */
	case int32(ZSTD_cpm_createCDict):
		/* Assume a small source size when creating a dictionary
		 * with an unknown source size.
		 */
		if dictSize != 0 && srcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
			srcSize = minSrcSize
		}
	case int32(ZSTD_cpm_attachDict):
		/* Dictionary has its own dedicated parameters which have
		 * already been selected. We are selecting parameters
		 * for only the source.
		 */
		dictSize = uint64(0)
	default:
		break
	}
	/* resize windowLog if input is small enough, to use less memory */
	if srcSize <= maxWindowResize && dictSize <= maxWindowResize {
		tSize = uint32(srcSize + dictSize)
		if tSize < hashSizeMin {
			v1 = uint32(ZSTD_HASHLOG_MIN)
		} else {
			v1 = ZSTD_highbit32(tls, tSize-uint32(1)) + uint32(1)
		}
		srcLog = v1
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog > srcLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog = srcLog
		}
	}
	if srcSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		dictAndWindowLog = ZSTD_dictAndWindowLog(tls, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog, srcSize, dictSize)
		cycleLog = ZSTD_cycleLog(tls, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).Fstrategy)
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog > dictAndWindowLog+uint32(1) {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog = dictAndWindowLog + uint32(1)
		}
		if cycleLog > dictAndWindowLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog -= cycleLog - dictAndWindowLog
		}
	}
	if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog < uint32(ZSTD_WINDOWLOG_ABSOLUTEMIN) {
		(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog = uint32(ZSTD_WINDOWLOG_ABSOLUTEMIN)
	} /* minimum wlog required for valid frame header */
	/* We can't use more than 32 bits of hash in total, so that means that we require:
	 * (hashLog + 8) <= 32 && (chainLog + 8) <= 32
	 */
	if mode == int32(ZSTD_cpm_createCDict) && ZSTD_CDictIndicesAreTagged(tls, bp) != 0 {
		maxShortCacheHashLog = uint32(libc.Int32FromInt32(32) - libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS))
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog > maxShortCacheHashLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog = maxShortCacheHashLog
		}
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog > maxShortCacheHashLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog = maxShortCacheHashLog
		}
	}
	/* At this point, we aren't 100% sure if we are using the row match finder.
	 * Unless it is explicitly disabled, conservatively assume that it is enabled.
	 * In this case it will only be disabled for small sources, so shrinking the
	 * hash log a little bit shouldn't result in any ratio loss.
	 */
	if useRowMatchFinder == int32(ZSTD_ps_auto) {
		useRowMatchFinder = int32(ZSTD_ps_enable)
	}
	/* We can't hash more than 32-bits in total. So that means that we require:
	 * (hashLog - rowLog + 8) <= 32
	 */
	if ZSTD_rowMatchFinderUsed(tls, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).Fstrategy, useRowMatchFinder) != 0 {
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog < uint32(libc.Int32FromInt32(6)) {
			v2 = (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog
		} else {
			v2 = uint32(libc.Int32FromInt32(6))
		}
		if uint32(libc.Int32FromInt32(4)) > v2 {
			v1 = uint32(libc.Int32FromInt32(4))
		} else {
			if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog < uint32(libc.Int32FromInt32(6)) {
				v3 = (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog
			} else {
				v3 = uint32(libc.Int32FromInt32(6))
			}
			v1 = v3
		}
		/* Switch to 32-entry rows if searchLog is 5 (or more) */
		rowLog = v1
		maxRowHashLog = uint32(libc.Int32FromInt32(32) - libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS))
		maxHashLog = maxRowHashLog + rowLog
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog > maxHashLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog = maxHashLog
		}
	}
	return *(*ZSTD_compressionParameters)(unsafe.Pointer(bp))
}

var hashSizeMin = uint32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_HASHLOG_MIN))

func ZSTD_adjustCParams(tls *libc.TLS, cPar ZSTD_compressionParameters, srcSize uint64, dictSize size_t) (r ZSTD_compressionParameters) {
	cPar = ZSTD_clampCParams(tls, cPar) /* resulting cPar is necessarily valid (all parameters within range) */
	if srcSize == uint64(0) {
		srcSize = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	return ZSTD_adjustCParams_internal(tls, cPar, srcSize, dictSize, int32(ZSTD_cpm_unknown), int32(ZSTD_ps_auto))
}

func ZSTD_overrideCParams(tls *libc.TLS, cParams uintptr, overrides uintptr) {
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FwindowLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FwindowLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FhashLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FhashLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FchainLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FchainLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FsearchLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FsearchLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FminMatch != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FminMatch
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FtargetLength != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FtargetLength
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).Fstrategy != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).Fstrategy
	}
}

func ZSTD_getCParamsFromCCtxParams(tls *libc.TLS, CCtxParams uintptr, srcSizeHint U64, dictSize size_t, mode ZSTD_CParamMode_e) (r ZSTD_compressionParameters) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _ /* cParams at bp+0 */ ZSTD_compressionParameters
	if srcSizeHint == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint > 0 {
		srcSizeHint = uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint)
	}
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = ZSTD_getCParams_internal(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel, srcSizeHint, dictSize, mode)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog = uint32(ZSTD_WINDOWLOG_LIMIT_DEFAULT)
	}
	ZSTD_overrideCParams(tls, bp, CCtxParams+4)
	/* srcSizeHint == 0 means 0 */
	return ZSTD_adjustCParams_internal(tls, *(*ZSTD_compressionParameters)(unsafe.Pointer(bp)), srcSizeHint, dictSize, mode, (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder)
}

func ZSTD_sizeof_matchState(tls *libc.TLS, cParams uintptr, useRowMatchFinder ZSTD_ParamSwitch_e, enableDedicatedDictSearch int32, forCCtx U32) (r size_t) {
	var chainSize, h3Size, hSize, lazyAdditionalSpace, optPotentialSpace, optSpace, slackSpace, tableSpace size_t
	var hashLog3 U32
	var v1, v4, v5, v6 uint64
	var v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = chainSize, h3Size, hSize, hashLog3, lazyAdditionalSpace, optPotentialSpace, optSpace, slackSpace, tableSpace, v1, v2, v3, v4, v5, v6
	if ZSTD_allocateChainTable(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder, libc.BoolUint32(enableDedicatedDictSearch != 0 && !(forCCtx != 0))) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	} else {
		v1 = uint64(0)
	}
	/* chain table size should be 0 for fast or row-hash strategies */
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	if forCCtx != 0 && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch == uint32(3) {
		if uint32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX)) < (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog {
			v3 = uint32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX))
		} else {
			v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
		}
		v2 = v3
	} else {
		v2 = uint32(0)
	}
	hashLog3 = v2
	if hashLog3 != 0 {
		v4 = libc.Uint64FromInt32(1) << hashLog3
	} else {
		v4 = uint64(0)
	}
	h3Size = v4
	/* We don't use ZSTD_cwksp_alloc_size() here because the tables aren't
	 * surrounded by redzones in ASAN. */
	tableSpace = chainSize*uint64(4) + hSize*uint64(4) + h3Size*uint64(4)
	optPotentialSpace = ZSTD_cwksp_aligned64_alloc_size(tls, uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, uint64(libc.Int32FromInt32(MaxLL)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, uint64(libc.Int32FromInt32(MaxOff)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(8)) + ZSTD_cwksp_aligned64_alloc_size(tls, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(28))
	if ZSTD_rowMatchFinderUsed(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder) != 0 {
		v5 = ZSTD_cwksp_aligned64_alloc_size(tls, hSize)
	} else {
		v5 = uint64(0)
	}
	lazyAdditionalSpace = v5
	if forCCtx != 0 && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) {
		v6 = optPotentialSpace
	} else {
		v6 = uint64(0)
	}
	optSpace = v6
	slackSpace = ZSTD_cwksp_slack_space_required(tls)
	/* tables are guaranteed to be sized in multiples of 64 bytes (or 16 uint32_t) */
	_ = libc.Uint64FromInt64(1)
	return tableSpace + optSpace + slackSpace + lazyAdditionalSpace
}

// C documentation
//
//	/* Helper function for calculating memory requirements.
//	 * Gives a tighter bound than ZSTD_sequenceBound() by taking minMatch into account. */
func ZSTD_maxNbSeq(tls *libc.TLS, blockSize size_t, minMatch uint32, useSequenceProducer int32) (r size_t) {
	var divider U32
	var v1 int32
	_, _ = divider, v1
	if minMatch == uint32(3) || useSequenceProducer != 0 {
		v1 = int32(3)
	} else {
		v1 = int32(4)
	}
	divider = uint32(v1)
	return blockSize / uint64(divider)
}

func ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls *libc.TLS, cParams uintptr, ldmParams uintptr, isStatic int32, useRowMatchFinder ZSTD_ParamSwitch_e, buffInSize size_t, buffOutSize size_t, pledgedSrcSize U64, useSequenceProducer int32, maxBlockSize size_t) (r size_t) {
	var blockSize, blockStateSpace, bufferSpace, cctxSpace, externalSeqSpace, ldmSeqSpace, ldmSpace, matchStateSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, tmpWorkSpace, tokenSpace, windowSize size_t
	var v1, v2, v3, v4, v5, v6, v7 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, blockStateSpace, bufferSpace, cctxSpace, externalSeqSpace, ldmSeqSpace, ldmSpace, matchStateSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, tmpWorkSpace, tokenSpace, windowSize, v1, v2, v3, v4, v5, v6, v7
	if uint64(1)<<(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog < pledgedSrcSize {
		v2 = uint64(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	} else {
		v2 = pledgedSrcSize
	}
	if uint64(1) > v2 {
		v1 = uint64(1)
	} else {
		if uint64(1)<<(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog < pledgedSrcSize {
			v3 = uint64(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
		} else {
			v3 = pledgedSrcSize
		}
		v1 = v3
	}
	windowSize = v1
	if ZSTD_resolveMaxBlockSize(tls, maxBlockSize) < windowSize {
		v4 = ZSTD_resolveMaxBlockSize(tls, maxBlockSize)
	} else {
		v4 = windowSize
	}
	blockSize = v4
	maxNbSeq = ZSTD_maxNbSeq(tls, blockSize, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch, useSequenceProducer)
	tokenSpace = ZSTD_cwksp_alloc_size(tls, uint64(WILDCOPY_OVERLENGTH)+blockSize) + ZSTD_cwksp_aligned64_alloc_size(tls, maxNbSeq*uint64(8)) + uint64(3)*ZSTD_cwksp_alloc_size(tls, maxNbSeq*uint64(1))
	tmpWorkSpace = ZSTD_cwksp_alloc_size(tls, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2)))
	blockStateSpace = uint64(2) * ZSTD_cwksp_alloc_size(tls, uint64(5632))
	matchStateSize = ZSTD_sizeof_matchState(tls, cParams, useRowMatchFinder, 0, uint32(1))
	ldmSpace = ZSTD_ldm_getTableSize(tls, *(*ldmParams_t)(unsafe.Pointer(ldmParams)))
	maxNbLdmSeq = ZSTD_ldm_getMaxNbSeq(tls, *(*ldmParams_t)(unsafe.Pointer(ldmParams)), blockSize)
	if (*ldmParams_t)(unsafe.Pointer(ldmParams)).FenableLdm == int32(ZSTD_ps_enable) {
		v5 = ZSTD_cwksp_aligned64_alloc_size(tls, maxNbLdmSeq*uint64(12))
	} else {
		v5 = uint64(0)
	}
	ldmSeqSpace = v5
	bufferSpace = ZSTD_cwksp_alloc_size(tls, buffInSize) + ZSTD_cwksp_alloc_size(tls, buffOutSize)
	if isStatic != 0 {
		v6 = ZSTD_cwksp_alloc_size(tls, uint64(5280))
	} else {
		v6 = uint64(0)
	}
	cctxSpace = v6
	maxNbExternalSeq = ZSTD_sequenceBound(tls, blockSize)
	if useSequenceProducer != 0 {
		v7 = ZSTD_cwksp_aligned64_alloc_size(tls, maxNbExternalSeq*uint64(16))
	} else {
		v7 = uint64(0)
	}
	externalSeqSpace = v7
	neededSpace = cctxSpace + tmpWorkSpace + blockStateSpace + ldmSpace + ldmSeqSpace + matchStateSize + tokenSpace + bufferSpace + externalSeqSpace
	return neededSpace
}

func ZSTD_estimateCCtxSize_usingCCtxParams(tls *libc.TLS, params uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var useRowMatchFinder ZSTD_ParamSwitch_e
	var _ /* cParams at bp+0 */ ZSTD_compressionParameters
	_ = useRowMatchFinder
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = ZSTD_getCParamsFromCCtxParams(tls, params, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), uint64(0), int32(ZSTD_cpm_noAttachDict))
	useRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, bp)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FnbWorkers > 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2467, 0)
		}
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	/* estimateCCtxSize is for one-shot compression. So no buffers should
	 * be needed. However, we still allocate two 0-sized buffers, which can
	 * take space under ASAN. */
	return ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls, bp, params+96, int32(1), useRowMatchFinder, uint64(0), uint64(0), libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), ZSTD_hasExtSeqProd(tls, params), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
}

func ZSTD_estimateCCtxSize_usingCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var noRowCCtxSize, rowCCtxSize size_t
	var v1 uint64
	var _ /* initialParams at bp+0 */ ZSTD_CCtx_params
	_, _, _ = noRowCCtxSize, rowCCtxSize, v1
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = ZSTD_makeCCtxParamsFromCParams(tls, cParams)
	if ZSTD_rowMatchFinderSupported(tls, cParams.Fstrategy) != 0 {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_disable)
		noRowCCtxSize = ZSTD_estimateCCtxSize_usingCCtxParams(tls, bp)
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_enable)
		rowCCtxSize = ZSTD_estimateCCtxSize_usingCCtxParams(tls, bp)
		if noRowCCtxSize > rowCCtxSize {
			v1 = noRowCCtxSize
		} else {
			v1 = rowCCtxSize
		}
		return v1
	} else {
		return ZSTD_estimateCCtxSize_usingCCtxParams(tls, bp)
	}
	return r
}

func ZSTD_estimateCCtxSize_internal(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var cParams ZSTD_compressionParameters
	var largestSize size_t
	var tier int32
	var v2 uint64
	_, _, _, _ = cParams, largestSize, tier, v2
	tier = 0
	largestSize = uint64(0)
	for {
		if !(tier < int32(4)) {
			break
		}
		/* Choose the set of cParams for a given level across all srcSizes that give the largest cctxSize */
		cParams = ZSTD_getCParams_internal(tls, compressionLevel, srcSizeTiers[tier], uint64(0), int32(ZSTD_cpm_noAttachDict))
		if ZSTD_estimateCCtxSize_usingCParams(tls, cParams) > largestSize {
			v2 = ZSTD_estimateCCtxSize_usingCParams(tls, cParams)
		} else {
			v2 = largestSize
		}
		largestSize = v2
		goto _1
	_1:
		;
		tier = tier + 1
	}
	return largestSize
}

var srcSizeTiers = [4]uint64{
	0: uint64(libc.Int32FromInt32(16) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	1: uint64(libc.Int32FromInt32(128) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	2: uint64(libc.Int32FromInt32(256) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	3: libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1),
}

func ZSTD_estimateCCtxSize(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var level, v2 int32
	var memBudget, newMB size_t
	_, _, _, _ = level, memBudget, newMB, v2
	memBudget = uint64(0)
	if compressionLevel < int32(1) {
		v2 = compressionLevel
	} else {
		v2 = int32(1)
	}
	level = v2
	for {
		if !(level <= compressionLevel) {
			break
		}
		/* Ensure monotonically increasing memory usage as compression level increases */
		newMB = ZSTD_estimateCCtxSize_internal(tls, level)
		if newMB > memBudget {
			memBudget = newMB
		}
		goto _1
	_1:
		;
		level = level + 1
	}
	return memBudget
}

func ZSTD_estimateCStreamSize_usingCCtxParams(tls *libc.TLS, params uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var blockSize, inBuffSize, outBuffSize size_t
	var useRowMatchFinder ZSTD_ParamSwitch_e
	var v1, v2, v3 uint64
	var _ /* cParams at bp+0 */ ZSTD_compressionParameters
	_, _, _, _, _, _, _ = blockSize, inBuffSize, outBuffSize, useRowMatchFinder, v1, v2, v3
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FnbWorkers > 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2467, 0)
		}
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = ZSTD_getCParamsFromCCtxParams(tls, params, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), uint64(0), int32(ZSTD_cpm_noAttachDict))
	if ZSTD_resolveMaxBlockSize(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize) < libc.Uint64FromInt32(1)<<(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog {
		v1 = ZSTD_resolveMaxBlockSize(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
	} else {
		v1 = libc.Uint64FromInt32(1) << (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog
	}
	blockSize = v1
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FinBufferMode == int32(ZSTD_bm_buffered) {
		v2 = libc.Uint64FromInt32(1)<<(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog + blockSize
	} else {
		v2 = uint64(0)
	}
	inBuffSize = v2
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FoutBufferMode == int32(ZSTD_bm_buffered) {
		v3 = ZSTD_compressBound(tls, blockSize) + uint64(1)
	} else {
		v3 = uint64(0)
	}
	outBuffSize = v3
	useRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, params+4)
	return ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls, bp, params+96, int32(1), useRowMatchFinder, inBuffSize, outBuffSize, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), ZSTD_hasExtSeqProd(tls, params), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
	return r
}

func ZSTD_estimateCStreamSize_usingCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var noRowCCtxSize, rowCCtxSize size_t
	var v1 uint64
	var _ /* initialParams at bp+0 */ ZSTD_CCtx_params
	_, _, _ = noRowCCtxSize, rowCCtxSize, v1
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = ZSTD_makeCCtxParamsFromCParams(tls, cParams)
	if ZSTD_rowMatchFinderSupported(tls, cParams.Fstrategy) != 0 {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_disable)
		noRowCCtxSize = ZSTD_estimateCStreamSize_usingCCtxParams(tls, bp)
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_enable)
		rowCCtxSize = ZSTD_estimateCStreamSize_usingCCtxParams(tls, bp)
		if noRowCCtxSize > rowCCtxSize {
			v1 = noRowCCtxSize
		} else {
			v1 = rowCCtxSize
		}
		return v1
	} else {
		return ZSTD_estimateCStreamSize_usingCCtxParams(tls, bp)
	}
	return r
}

func ZSTD_estimateCStreamSize_internal(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var cParams ZSTD_compressionParameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), uint64(0), int32(ZSTD_cpm_noAttachDict))
	return ZSTD_estimateCStreamSize_usingCParams(tls, cParams)
}

func ZSTD_estimateCStreamSize(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var level, v2 int32
	var memBudget, newMB size_t
	_, _, _, _ = level, memBudget, newMB, v2
	memBudget = uint64(0)
	if compressionLevel < int32(1) {
		v2 = compressionLevel
	} else {
		v2 = int32(1)
	}
	level = v2
	for {
		if !(level <= compressionLevel) {
			break
		}
		newMB = ZSTD_estimateCStreamSize_internal(tls, level)
		if newMB > memBudget {
			memBudget = newMB
		}
		goto _1
	_1:
		;
		level = level + 1
	}
	return memBudget
}

// C documentation
//
//	/* ZSTD_getFrameProgression():
//	 * tells how much data has been consumed (input) and produced (output) for current frame.
//	 * able to count progression inside worker threads (non-blocking mode).
//	 */
func ZSTD_getFrameProgression(tls *libc.TLS, cctx uintptr) (r ZSTD_frameProgression) {
	var buffered size_t
	var fp ZSTD_frameProgression
	var v1 uint64
	_, _, _ = buffered, fp, v1
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers > 0 {
		return ZSTDMT_getFrameProgression(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuff == libc.UintptrFromInt32(0) {
		v1 = uint64(0)
	} else {
		v1 = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffPos - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinToCompress
	}
	buffered = v1
	if buffered != 0 {
	}
	fp.Fingested = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize + buffered
	fp.Fconsumed = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize
	fp.Fproduced = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize
	fp.Fflushed = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize /* simplified; some data might still be left within streaming output buffer */
	fp.FcurrentJobID = uint32(0)
	fp.FnbActiveWorkers = uint32(0)
	return fp
	return r
}

// C documentation
//
//	/*! ZSTD_toFlushNow()
//	 *  Only useful for multithreading scenarios currently (nbWorkers >= 1).
//	 */
func ZSTD_toFlushNow(tls *libc.TLS, cctx uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers > 0 {
		return ZSTDMT_toFlushNow(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	}
	_ = cctx
	return uint64(0) /* over-simplification; could also check if context is currently running in streaming mode, and in which case, report how many bytes are left to be flushed within output buffer */
}

func ZSTD_assertEqualCParams(tls *libc.TLS, cParams1 ZSTD_compressionParameters, cParams2 ZSTD_compressionParameters) {
	_ = cParams1
	_ = cParams2
}

func ZSTD_reset_compressedBlockState(tls *libc.TLS, bs uintptr) {
	var i int32
	_ = i
	i = 0
	for {
		if !(i < int32(ZSTD_REP_NUM)) {
			break
		}
		*(*U32)(unsafe.Pointer(bs + 5616 + uintptr(i)*4)) = repStartValue[i]
		goto _1
	_1:
		;
		i = i + 1
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Fhuf.FrepeatMode = int32(HUF_repeat_none)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Fmatchlength_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Flitlength_repeatMode = int32(FSE_repeat_none)
}

// C documentation
//
//	/*! ZSTD_invalidateMatchState()
//	 *  Invalidate all the matches in the match finder tables.
//	 *  Requires nextSrc and base to be set (can be NULL).
//	 */
func ZSTD_invalidateMatchState(tls *libc.TLS, ms uintptr) {
	ZSTD_window_clear(tls, ms)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd = uint32(0)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitLengthSum = uint32(0) /* force reset of btopt stats */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState = libc.UintptrFromInt32(0)
}

// C documentation
//
//	/**
//	 * Controls, for this matchState reset, whether the tables need to be cleared /
//	 * prepared for the coming compression (ZSTDcrp_makeClean), or whether the
//	 * tables can be left unclean (ZSTDcrp_leaveDirty), because we know that a
//	 * subsequent operation will overwrite the table space anyways (e.g., copying
//	 * the matchState contents in from a CDict).
//	 */
type ZSTD_compResetPolicy_e = int32

const ZSTDcrp_makeClean = 0
const ZSTDcrp_leaveDirty = 1

// C documentation
//
//	/**
//	 * Controls, for this matchState reset, whether indexing can continue where it
//	 * left off (ZSTDirp_continue), or whether it needs to be restarted from zero
//	 * (ZSTDirp_reset).
//	 */
type ZSTD_indexResetPolicy_e = int32

const ZSTDirp_continue = 0
const ZSTDirp_reset = 1

type ZSTD_resetTarget_e = int32

const ZSTD_resetTarget_CDict = 0
const ZSTD_resetTarget_CCtx = 1

// C documentation
//
//	/* Mixes bits in a 64 bits in a value, based on XXH3_rrmxmx */
func ZSTD_bitmix(tls *libc.TLS, val U64, len1 U64) (r U64) {
	val = val ^ (ZSTD_rotateRight_U64(tls, val, uint32(49)) ^ ZSTD_rotateRight_U64(tls, val, uint32(24)))
	val = val * uint64(0x9FB21C651E98DF25)
	val = val ^ (val>>libc.Int32FromInt32(35) + len1)
	val = val * uint64(0x9FB21C651E98DF25)
	return val ^ val>>libc.Int32FromInt32(28)
}

// C documentation
//
//	/* Mixes in the hashSalt and hashSaltEntropy to create a new hashSalt */
func ZSTD_advanceHashSalt(tls *libc.TLS, ms uintptr) {
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt = ZSTD_bitmix(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt, uint64(8)) ^ ZSTD_bitmix(tls, uint64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSaltEntropy), uint64(4))
}

func ZSTD_reset_matchState(tls *libc.TLS, ms uintptr, ws uintptr, cParams uintptr, useRowMatchFinder ZSTD_ParamSwitch_e, crp ZSTD_compResetPolicy_e, forceResetIndex ZSTD_indexResetPolicy_e, forWho ZSTD_resetTarget_e) (r size_t) {
	var chainSize, h3Size, hSize, tagTableSize size_t
	var hashLog3, rowLog U32
	var v1, v4 uint64
	var v2, v3, v5 uint32
	_, _, _, _, _, _, _, _, _, _, _ = chainSize, h3Size, hSize, hashLog3, rowLog, tagTableSize, v1, v2, v3, v4, v5
	if ZSTD_allocateChainTable(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder, libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdedicatedDictSearch != 0 && forWho == int32(ZSTD_resetTarget_CDict))) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	} else {
		v1 = uint64(0)
	}
	/* disable chain table allocation for fast or row-based strategies */
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	if forWho == int32(ZSTD_resetTarget_CCtx) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch == uint32(3) {
		if uint32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX)) < (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog {
			v3 = uint32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX))
		} else {
			v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
		}
		v2 = v3
	} else {
		v2 = uint32(0)
	}
	hashLog3 = v2
	if hashLog3 != 0 {
		v4 = libc.Uint64FromInt32(1) << hashLog3
	} else {
		v4 = uint64(0)
	}
	h3Size = v4
	if forceResetIndex == int32(ZSTDirp_reset) {
		ZSTD_window_init(tls, ms)
		ZSTD_cwksp_mark_tables_dirty(tls, ws)
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3 = hashLog3
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
	ZSTD_invalidateMatchState(tls, ms)
	/* check that allocation hasn't already failed */
	ZSTD_cwksp_clear_tables(tls, ws)
	/* table Space */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable = ZSTD_cwksp_reserve_table(tls, ws, hSize*uint64(4))
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable = ZSTD_cwksp_reserve_table(tls, ws, chainSize*uint64(4))
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable3 = ZSTD_cwksp_reserve_table(tls, ws, h3Size*uint64(4))
	if ZSTD_cwksp_reserve_failed(tls, ws) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2537, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	if crp != int32(ZSTDcrp_leaveDirty) {
		/* reset tables only */
		ZSTD_cwksp_clean_tables(tls, ws)
	}
	if ZSTD_rowMatchFinderUsed(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder) != 0 {
		/* Row match finder needs an additional table of hashes ("tags") */
		tagTableSize = hSize
		/* We want to generate a new salt in case we reset a Cctx, but we always want to use
		 * 0 when we reset a Cdict */
		if forWho == int32(ZSTD_resetTarget_CCtx) {
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable = ZSTD_cwksp_reserve_aligned_init_once(tls, ws, tagTableSize)
			ZSTD_advanceHashSalt(tls, ms)
		} else {
			/* When we are not salting we want to always memset the memory */
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable = ZSTD_cwksp_reserve_aligned64(tls, ws, tagTableSize)
			libc.Xmemset(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable, 0, tagTableSize)
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt = uint64(0)
		}
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog < uint32(libc.Int32FromInt32(6)) {
			v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
		} else {
			v3 = uint32(libc.Int32FromInt32(6))
		}
		if uint32(libc.Int32FromInt32(4)) > v3 {
			v2 = uint32(libc.Int32FromInt32(4))
		} else {
			if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog < uint32(libc.Int32FromInt32(6)) {
				v5 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
			} else {
				v5 = uint32(libc.Int32FromInt32(6))
			}
			v2 = v5
		} /* Switch to 32-entry rows if searchLog is 5 (or more) */
		rowLog = v2
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog - rowLog
	}
	/* opt parser space */
	if forWho == int32(ZSTD_resetTarget_CCtx) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) {
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitLengthFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, uint64(libc.Int32FromInt32(MaxLL)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FmatchLengthFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FoffCodeFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, uint64(libc.Int32FromInt32(MaxOff)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FmatchTable = ZSTD_cwksp_reserve_aligned64(tls, ws, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(8))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FpriceTable = ZSTD_cwksp_reserve_aligned64(tls, ws, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(28))
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(cParams))
	if ZSTD_cwksp_reserve_failed(tls, ws) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2537, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_indexTooCloseToMax() :
//	 * minor optimization : prefer memset() rather than reduceIndex()
//	 * which is measurably slow in some circumstances (reported for Visual Studio).
//	 * Works when re-using a context for a lot of smallish inputs :
//	 * if all inputs are smaller than ZSTD_INDEXOVERFLOW_MARGIN,
//	 * memset() will be triggered before reduceIndex().
//	 */
func ZSTD_indexTooCloseToMax(tls *libc.TLS, w ZSTD_window_t) (r int32) {
	var v1 uint32
	_ = v1
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	return libc.BoolInt32(uint64(int64(w.FnextSrc)-int64(w.Fbase)) > uint64(v1-uint32(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20)))))
}

// C documentation
//
//	/** ZSTD_dictTooBig():
//	 * When dictionaries are larger than ZSTD_CHUNKSIZE_MAX they can't be loaded in
//	 * one go generically. So we ensure that in that case we reset the tables to zero,
//	 * so that we can load as much of the dictionary as possible.
//	 */
func ZSTD_dictTooBig(tls *libc.TLS, loadedDictSize size_t) (r int32) {
	var v1 uint32
	_ = v1
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	return libc.BoolInt32(loadedDictSize > uint64(uint32(-libc.Int32FromInt32(1))-v1))
}

// C documentation
//
//	/*! ZSTD_resetCCtx_internal() :
//	 * @param loadedDictSize The size of the dictionary to be loaded
//	 * into the context, if any. If no dictionary is used, or the
//	 * dictionary is being attached / copied, then pass 0.
//	 * note : `params` are assumed fully validated at this stage.
//	 */
func ZSTD_resetCCtx_internal(tls *libc.TLS, zc uintptr, params uintptr, pledgedSrcSize U64, loadedDictSize size_t, crp ZSTD_compResetPolicy_e, zbuff ZSTD_buffered_policy_e) (r size_t) {
	var blockSize, buffInSize, buffOutSize, err_code, err_code1, err_code2, ldmHSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, numBuckets, windowSize size_t
	var dictTooBig, indexTooClose, resizeWorkspace, workspaceTooSmall, workspaceWasteful, v7 int32
	var needsIndexReset ZSTD_indexResetPolicy_e
	var ws uintptr
	var v1, v2, v3, v4, v5, v6 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, buffInSize, buffOutSize, dictTooBig, err_code, err_code1, err_code2, indexTooClose, ldmHSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, needsIndexReset, numBuckets, resizeWorkspace, windowSize, workspaceTooSmall, workspaceWasteful, ws, v1, v2, v3, v4, v5, v6, v7
	ws = zc + 704
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock = int32(1)
	/* Set applied params early so we can modify them for LDM,
	 * and point params at the applied params.
	 */
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(params))
	params = zc + 240
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* Adjust long distance matching parameters */
		ZSTD_ldm_adjustParameters(tls, zc+240+96, params+4)
	}
	if libc.Uint64FromInt32(1)<<(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog < pledgedSrcSize {
		v2 = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
	} else {
		v2 = pledgedSrcSize
	}
	if uint64(libc.Int32FromInt32(1)) > v2 {
		v1 = uint64(libc.Int32FromInt32(1))
	} else {
		if libc.Uint64FromInt32(1)<<(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog < pledgedSrcSize {
			v3 = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
		} else {
			v3 = pledgedSrcSize
		}
		v1 = v3
	}
	windowSize = v1
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize < windowSize {
		v4 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize
	} else {
		v4 = windowSize
	}
	blockSize = v4
	maxNbSeq = ZSTD_maxNbSeq(tls, blockSize, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FminMatch, ZSTD_hasExtSeqProd(tls, params))
	if zbuff == int32(ZSTDb_buffered) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FoutBufferMode == int32(ZSTD_bm_buffered) {
		v5 = ZSTD_compressBound(tls, blockSize) + uint64(1)
	} else {
		v5 = uint64(0)
	}
	buffOutSize = v5
	if zbuff == int32(ZSTDb_buffered) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FinBufferMode == int32(ZSTD_bm_buffered) {
		v6 = windowSize + blockSize
	} else {
		v6 = uint64(0)
	}
	buffInSize = v6
	maxNbLdmSeq = ZSTD_ldm_getMaxNbSeq(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams, blockSize)
	indexTooClose = ZSTD_indexTooCloseToMax(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FmatchState.Fwindow)
	dictTooBig = ZSTD_dictTooBig(tls, loadedDictSize)
	if indexTooClose != 0 || dictTooBig != 0 || !((*ZSTD_CCtx)(unsafe.Pointer(zc)).Finitialized != 0) {
		v7 = int32(ZSTDirp_reset)
	} else {
		v7 = int32(ZSTDirp_continue)
	}
	needsIndexReset = v7
	neededSpace = ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls, params+4, params+96, libc.BoolInt32((*ZSTD_CCtx)(unsafe.Pointer(zc)).FstaticSize != uint64(0)), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, buffInSize, buffOutSize, pledgedSrcSize, ZSTD_hasExtSeqProd(tls, params), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
	err_code = neededSpace
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2592, 0)
		}
		return err_code
	}
	if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FstaticSize != 0) {
		ZSTD_cwksp_bump_oversized_duration(tls, ws, uint64(0))
	}
	/* Check if workspace is large enough, alloc a new one if needed */
	workspaceTooSmall = libc.BoolInt32(ZSTD_cwksp_sizeof(tls, ws) < neededSpace)
	workspaceWasteful = ZSTD_cwksp_check_wasteful(tls, ws, neededSpace)
	resizeWorkspace = libc.BoolInt32(workspaceTooSmall != 0 || workspaceWasteful != 0)
	if resizeWorkspace != 0 {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FstaticSize != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2619, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		needsIndexReset = int32(ZSTDirp_reset)
		ZSTD_cwksp_free(tls, ws, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FcustomMem)
		err_code1 = ZSTD_cwksp_create(tls, ws, neededSpace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FcustomMem)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code1
		}
		/* Statically sized space.
		 * tmpWorkspace never moves,
		 * though prev/next block swap places */
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock = ZSTD_cwksp_reserve_object(tls, ws, uint64(5632))
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2643, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock = ZSTD_cwksp_reserve_object(tls, ws, uint64(5632))
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2672, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace = ZSTD_cwksp_reserve_object(tls, ws, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2)))
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2701, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize = uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)) + libc.Uint64FromInt64(4)*uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2))
	}
	ZSTD_cwksp_clear(tls, ws)
	/* init params */
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FmatchState.FcParams = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FmatchState.FprefetchCDictTables = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FprefetchCDictTables == int32(ZSTD_ps_enable))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FpledgedSrcSizePlusOne = pledgedSrcSize + uint64(1)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FconsumedSrcSize = uint64(0)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FproducedCSize = uint64(0)
	if pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FfParams.FcontentSizeFlag = 0
	}
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockSizeMax = blockSize
	XXH_INLINE_XXH64_reset(tls, zc+808, uint64(0))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).Fstage = int32(ZSTDcs_init)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FdictID = uint32(0)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FdictContentSize = uint64(0)
	ZSTD_reset_compressedBlockState(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)
	err_code2 = ZSTD_reset_matchState(tls, zc+3224+16, ws, params+4, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, crp, needsIndexReset, int32(ZSTD_resetTarget_CCtx))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FsequencesStart = ZSTD_cwksp_reserve_aligned64(tls, ws, maxNbSeq*uint64(8))
	/* ldm hash table */
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* TODO: avoid memset? */
		ldmHSize = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FhashLog
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FhashTable = ZSTD_cwksp_reserve_aligned64(tls, ws, ldmHSize*uint64(8))
		libc.Xmemset(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FhashTable, 0, ldmHSize*libc.Uint64FromInt64(8))
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmSequences = ZSTD_cwksp_reserve_aligned64(tls, ws, maxNbLdmSeq*uint64(12))
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FmaxNbLdmSequences = maxNbLdmSeq
		ZSTD_window_init(tls, zc+1056)
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FloadedDictEnd = uint32(0)
	}
	/* reserve space for block-level external sequences */
	if ZSTD_hasExtSeqProd(tls, params) != 0 {
		maxNbExternalSeq = ZSTD_sequenceBound(tls, blockSize)
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBufCapacity = maxNbExternalSeq
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf = ZSTD_cwksp_reserve_aligned64(tls, ws, maxNbExternalSeq*uint64(16))
	}
	/* buffers */
	/* ZSTD_wildcopy() is used to copy into the literals buffer,
	 * so we have to oversize the buffer by WILDCOPY_OVERLENGTH bytes.
	 */
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FlitStart = ZSTD_cwksp_reserve_buffer(tls, ws, blockSize+uint64(WILDCOPY_OVERLENGTH))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FmaxNbLit = blockSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FbufferedPolicy = zbuff
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FinBuffSize = buffInSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FinBuff = ZSTD_cwksp_reserve_buffer(tls, ws, buffInSize)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FoutBuffSize = buffOutSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FoutBuff = ZSTD_cwksp_reserve_buffer(tls, ws, buffOutSize)
	/* ldm bucketOffsets table */
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* TODO: avoid memset? */
		numBuckets = libc.Uint64FromInt32(1) << ((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FhashLog - (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FbucketSizeLog)
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FbucketOffsets = ZSTD_cwksp_reserve_buffer(tls, ws, numBuckets)
		libc.Xmemset(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FbucketOffsets, 0, numBuckets)
	}
	/* sequences storage */
	ZSTD_referenceExternalSequences(tls, zc, libc.UintptrFromInt32(0), uint64(0))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FmaxNbSeq = maxNbSeq
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FllCode = ZSTD_cwksp_reserve_buffer(tls, ws, maxNbSeq*uint64(1))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FmlCode = ZSTD_cwksp_reserve_buffer(tls, ws, maxNbSeq*uint64(1))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FofCode = ZSTD_cwksp_reserve_buffer(tls, ws, maxNbSeq*uint64(1))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).Finitialized = int32(1)
	return uint64(0)
	return r
}

// C documentation
//
//	/* ZSTD_invalidateRepCodes() :
//	 * ensures next compression will not use repcodes from previous block.
//	 * Note : only works with regular variant;
//	 *        do not use with extDict variant ! */
func ZSTD_invalidateRepCodes(tls *libc.TLS, cctx uintptr) {
	var i int32
	_ = i
	i = 0
	for {
		if !(i < int32(ZSTD_REP_NUM)) {
			break
		}
		*(*U32)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock + 5616 + uintptr(i)*4)) = uint32(0)
		goto _1
	_1:
		;
		i = i + 1
	}
}

// C documentation
//
//	/* These are the approximate sizes for each strategy past which copying the
//	 * dictionary tables into the working context is faster than using them
//	 * in-place.
//	 */
var attachDictSizeCutoffs = [10]size_t{
	0: uint64(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	1: uint64(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	2: uint64(libc.Int32FromInt32(16) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	3: uint64(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	4: uint64(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	5: uint64(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	6: uint64(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	7: uint64(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	8: uint64(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	9: uint64(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
}

func ZSTD_shouldAttachDict(tls *libc.TLS, cdict uintptr, params uintptr, pledgedSrcSize U64) (r int32) {
	var cutoff size_t
	var dedicatedDictSearch int32
	_, _ = cutoff, dedicatedDictSearch
	cutoff = attachDictSizeCutoffs[(*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams.Fstrategy]
	dedicatedDictSearch = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FdedicatedDictSearch
	return libc.BoolInt32(dedicatedDictSearch != 0 || (pledgedSrcSize <= cutoff || pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) || (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FattachDictPref == int32(ZSTD_dictForceAttach)) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FattachDictPref != int32(ZSTD_dictForceCopy) && !((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FforceWindow != 0)) /* dictMatchState isn't correctly
	 * handled in _enforceMaxDist */
}

func ZSTD_resetCCtx_byAttachingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr, _params ZSTD_CCtx_params, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	bp := tls.Alloc(256)
	defer tls.Free(256)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var cdictEnd, cdictLen U32
	var err_code size_t
	var windowLog uint32
	var _ /* adjusted_cdict_cParams at bp+224 */ ZSTD_compressionParameters
	_, _, _, _ = cdictEnd, cdictLen, err_code, windowLog
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams
	windowLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog
	/* Resize working context table params for input only, since the dict
	 * has its own tables. */
	/* pledgedSrcSize == 0 means 0! */
	if (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FdedicatedDictSearch != 0 {
		ZSTD_dedicatedDictSearch_revertCParams(tls, bp+224)
	}
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = ZSTD_adjustCParams_internal(tls, *(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)), pledgedSrcSize, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize, int32(ZSTD_cpm_attachDict), (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog = windowLog
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder /* cdict overrides */
	err_code = ZSTD_resetCCtx_internal(tls, cctx, bp, pledgedSrcSize, uint64(0), int32(ZSTDcrp_makeClean), zbuff)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	cdictEnd = uint32(int64((*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.Fwindow.FnextSrc) - int64((*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.Fwindow.Fbase))
	cdictLen = cdictEnd - (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.Fwindow.FdictLimit
	if cdictLen == uint32(0) {
		/* don't even attach dictionaries with no contents */
	} else {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FdictMatchState = cdict + 104
		/* prep working match state so dict matches never have negative indices
		 * when they are translated to the working context's index space. */
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.FdictLimit < cdictEnd {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.FnextSrc = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.Fbase + uintptr(cdictEnd)
			ZSTD_window_clear(tls, cctx+3224+16)
		}
		/* loadedDictEnd is expressed within the referential of the active context */
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FloadedDictEnd = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.FdictLimit
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize
	/* copy block state */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cdict+408, libc.Uint64FromInt64(5632))
	return uint64(0)
}

func ZSTD_copyCDictTableIntoCCtx(tls *libc.TLS, dst uintptr, src uintptr, tableSize size_t, cParams uintptr) {
	var i size_t
	var index, taggedIndex U32
	_, _, _ = i, index, taggedIndex
	if ZSTD_CDictIndicesAreTagged(tls, cParams) != 0 {
		i = uint64(0)
		for {
			if !(i < tableSize) {
				break
			}
			taggedIndex = *(*U32)(unsafe.Pointer(src + uintptr(i)*4))
			index = taggedIndex >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
			*(*U32)(unsafe.Pointer(dst + uintptr(i)*4)) = index
			goto _1
		_1:
			;
			i = i + 1
		}
	} else {
		libc.Xmemcpy(tls, dst, src, tableSize*libc.Uint64FromInt64(4))
	}
}

func ZSTD_resetCCtx_byCopyingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr, _params ZSTD_CCtx_params, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var cdict_cParams, dstMatchState, srcMatchState uintptr
	var chainSize, err_code, h3Size, hSize, tagTableSize size_t
	var h3log U32
	var windowLog uint32
	var v1 uint64
	_, _, _, _, _, _, _, _, _, _, _ = cdict_cParams, chainSize, dstMatchState, err_code, h3Size, h3log, hSize, srcMatchState, tagTableSize, windowLog, v1
	cdict_cParams = cdict + 104 + 256
	windowLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog
	/* Copy only compression parameters related to tables. */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog = windowLog
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder
	err_code = ZSTD_resetCCtx_internal(tls, cctx, bp, pledgedSrcSize, uint64(0), int32(ZSTDcrp_leaveDirty), zbuff)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ZSTD_cwksp_mark_tables_dirty(tls, cctx+704)
	/* copy tables */
	if ZSTD_allocateChainTable(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).Fstrategy, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder, uint32(0)) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).FchainLog
	} else {
		v1 = uint64(0)
	}
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).FhashLog
	ZSTD_copyCDictTableIntoCCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashTable, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FhashTable, hSize, cdict_cParams)
	/* Do not copy cdict's chainTable if cctx has parameters such that it would not use chainTable */
	if ZSTD_allocateChainTable(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FuseRowMatchFinder, uint32(0)) != 0 {
		ZSTD_copyCDictTableIntoCCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FchainTable, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FchainTable, chainSize, cdict_cParams)
	}
	/* copy tag table */
	if ZSTD_rowMatchFinderUsed(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).Fstrategy, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder) != 0 {
		tagTableSize = hSize
		libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FtagTable, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FtagTable, tagTableSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashSalt = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FhashSalt
	}
	/* Zero the hashTable3, since the cdict never fills it */
	h3log = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashLog3
	if h3log != 0 {
		v1 = libc.Uint64FromInt32(1) << h3log
	} else {
		v1 = uint64(0)
	}
	h3Size = v1
	libc.Xmemset(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashTable3, 0, h3Size*libc.Uint64FromInt64(4))
	ZSTD_cwksp_mark_tables_clean(tls, cctx+704)
	/* copy dictionary offsets */
	srcMatchState = cdict + 104
	dstMatchState = cctx + 3224 + 16
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).Fwindow = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).Fwindow
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FnextToUpdate
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FloadedDictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FloadedDictEnd
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize
	/* copy block state */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cdict+408, libc.Uint64FromInt64(5632))
	return uint64(0)
}

// C documentation
//
//	/* We have a choice between copying the dictionary context into the working
//	 * context, or referencing the dictionary context from the working context
//	 * in-place. We decide here which strategy to use. */
func ZSTD_resetCCtx_usingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr, params uintptr, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	if ZSTD_shouldAttachDict(tls, cdict, params, pledgedSrcSize) != 0 {
		return ZSTD_resetCCtx_byAttachingCDict(tls, cctx, cdict, *(*ZSTD_CCtx_params)(unsafe.Pointer(params)), pledgedSrcSize, zbuff)
	} else {
		return ZSTD_resetCCtx_byCopyingCDict(tls, cctx, cdict, *(*ZSTD_CCtx_params)(unsafe.Pointer(params)), pledgedSrcSize, zbuff)
	}
	return r
}

// C documentation
//
//	/*! ZSTD_copyCCtx_internal() :
//	 *  Duplicate an existing context `srcCCtx` into another one `dstCCtx`.
//	 *  Only works during stage ZSTDcs_init (i.e. after creation, but before first call to ZSTD_compressContinue()).
//	 *  The "context", in this case, refers to the hash and chain tables,
//	 *  entropy tables, and dictionary references.
//	 * `windowLog` value is enforced if != 0, otherwise value is copied from srcCCtx.
//	 * @return : 0, or an error code */
func ZSTD_copyCCtx_internal(tls *libc.TLS, dstCCtx uintptr, srcCCtx uintptr, fParams ZSTD_frameParameters, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var chainSize, h3Size, hSize size_t
	var dstMatchState, srcMatchState uintptr
	var h3log U32
	var v1, v2 uint64
	var _ /* params at bp+0 */ ZSTD_CCtx_params
	_, _, _, _, _, _, _, _ = chainSize, dstMatchState, h3Size, h3log, hSize, srcMatchState, v1, v2
	if (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).Fstage != int32(ZSTDcs_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2732, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	libc.Xmemcpy(tls, dstCCtx+896, srcCCtx+896, libc.Uint64FromInt64(24))
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FrequestedParams
	/* Copy only compression parameters related to tables. */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FuseRowMatchFinder
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FpostBlockSplitter = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FpostBlockSplitter
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FldmParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams = fParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FmaxBlockSize = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FmaxBlockSize
	ZSTD_resetCCtx_internal(tls, dstCCtx, bp, pledgedSrcSize, uint64(0), int32(ZSTDcrp_leaveDirty), zbuff)
	ZSTD_cwksp_mark_tables_dirty(tls, dstCCtx+704)
	/* copy tables */
	if ZSTD_allocateChainTable(tls, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FuseRowMatchFinder, uint32(0)) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams.FchainLog
	} else {
		v1 = uint64(0)
	}
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams.FhashLog
	h3log = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FhashLog3
	if h3log != 0 {
		v2 = libc.Uint64FromInt32(1) << h3log
	} else {
		v2 = uint64(0)
	}
	h3Size = v2
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FmatchState.FhashTable, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FhashTable, hSize*libc.Uint64FromInt64(4))
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FmatchState.FchainTable, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FchainTable, chainSize*libc.Uint64FromInt64(4))
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FmatchState.FhashTable3, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FhashTable3, h3Size*libc.Uint64FromInt64(4))
	ZSTD_cwksp_mark_tables_clean(tls, dstCCtx+704)
	/* copy dictionary offsets */
	srcMatchState = srcCCtx + 3224 + 16
	dstMatchState = dstCCtx + 3224 + 16
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).Fwindow = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).Fwindow
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FnextToUpdate
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FloadedDictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FloadedDictEnd
	(*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FdictID = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FdictID
	(*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FdictContentSize = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FdictContentSize
	/* copy block state */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FprevCBlock, libc.Uint64FromInt64(5632))
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_copyCCtx() :
//	 *  Duplicate an existing context `srcCCtx` into another one `dstCCtx`.
//	 *  Only works during stage ZSTDcs_init (i.e. after creation, but before first call to ZSTD_compressContinue()).
//	 *  pledgedSrcSize==0 means "unknown".
//	*   @return : 0, or an error code */
func ZSTD_copyCCtx(tls *libc.TLS, dstCCtx uintptr, srcCCtx uintptr, pledgedSrcSize uint64) (r size_t) {
	var fParams ZSTD_frameParameters
	var zbuff ZSTD_buffered_policy_e
	_, _ = fParams, zbuff
	fParams = ZSTD_frameParameters{
		FcontentSizeFlag: int32(1),
	}
	zbuff = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FbufferedPolicy
	_ = libc.Uint64FromInt64(1)
	if pledgedSrcSize == uint64(0) {
		pledgedSrcSize = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	fParams.FcontentSizeFlag = libc.BoolInt32(pledgedSrcSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
	return ZSTD_copyCCtx_internal(tls, dstCCtx, srcCCtx, fParams, pledgedSrcSize, zbuff)
}

// C documentation
//
//	/*! ZSTD_reduceTable() :
//	 *  reduce table indexes by `reducerValue`, or squash to zero.
//	 *  PreserveMark preserves "unsorted mark" for btlazy2 strategy.
//	 *  It must be set to a clear 0/1 value, to remove branch during inlining.
//	 *  Presume table size is a multiple of ZSTD_ROWSIZE
//	 *  to help auto-vectorization */
func ZSTD_reduceTable_internal(tls *libc.TLS, table uintptr, size U32, reducerValue U32, preserveMark int32) {
	var cellNb, column, nbRows, rowNb int32
	var newVal, reducerThreshold U32
	_, _, _, _, _, _ = cellNb, column, nbRows, newVal, reducerThreshold, rowNb
	nbRows = int32(size) / int32(ZSTD_ROWSIZE)
	cellNb = 0
	/* Protect special index values < ZSTD_WINDOW_START_INDEX. */
	reducerThreshold = reducerValue + uint32(ZSTD_WINDOW_START_INDEX)
	/* multiple of ZSTD_ROWSIZE */
	/* can be cast to int */
	rowNb = 0
	for {
		if !(rowNb < nbRows) {
			break
		}
		column = 0
		for {
			if !(column < int32(ZSTD_ROWSIZE)) {
				break
			}
			if preserveMark != 0 && *(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) == uint32(ZSTD_DUBT_UNSORTED_MARK) {
				/* This write is pointless, but is required(?) for the compiler
				 * to auto-vectorize the loop. */
				newVal = uint32(ZSTD_DUBT_UNSORTED_MARK)
			} else {
				if *(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) < reducerThreshold {
					newVal = uint32(0)
				} else {
					newVal = *(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) - reducerValue
				}
			}
			*(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) = newVal
			cellNb = cellNb + 1
			goto _2
		_2:
			;
			column = column + 1
		}
		goto _1
	_1:
		;
		rowNb = rowNb + 1
	}
}

func ZSTD_reduceTable(tls *libc.TLS, table uintptr, size U32, reducerValue U32) {
	ZSTD_reduceTable_internal(tls, table, size, reducerValue, 0)
}

func ZSTD_reduceTable_btlazy2(tls *libc.TLS, table uintptr, size U32, reducerValue U32) {
	ZSTD_reduceTable_internal(tls, table, size, reducerValue, int32(1))
}

// C documentation
//
//	/*! ZSTD_reduceIndex() :
//	*   rescale all indexes to avoid future overflow (indexes are U32) */
func ZSTD_reduceIndex(tls *libc.TLS, ms uintptr, params uintptr, reducerValue U32) {
	var chainSize, h3Size, hSize U32
	_, _, _ = chainSize, h3Size, hSize
	hSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog
	ZSTD_reduceTable(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable, hSize, reducerValue)
	if ZSTD_allocateChainTable(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, uint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdedicatedDictSearch)) != 0 {
		chainSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy == int32(ZSTD_btlazy2) {
			ZSTD_reduceTable_btlazy2(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable, chainSize, reducerValue)
		} else {
			ZSTD_reduceTable(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable, chainSize, reducerValue)
		}
	}
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3 != 0 {
		h3Size = libc.Uint32FromInt32(1) << (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3
		ZSTD_reduceTable(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable3, h3Size, reducerValue)
	}
}

/*-*******************************************************
*  Block entropic compression
*********************************************************/

/* See doc/zstd_compression_format.md for detailed format description */

func ZSTD_seqToCodes(tls *libc.TLS, seqStorePtr uintptr) (r int32) {
	var llCodeTable, mlCodeTable, ofCodeTable, sequences uintptr
	var llv, mlv, nbSeq, ofCode, u U32
	var longOffsets, v2 int32
	var v3 bool
	_, _, _, _, _, _, _, _, _, _, _, _ = llCodeTable, llv, longOffsets, mlCodeTable, mlv, nbSeq, ofCode, ofCodeTable, sequences, u, v2, v3
	sequences = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
	llCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	ofCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	mlCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	nbSeq = uint32((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	longOffsets = 0
	u = uint32(0)
	for {
		if !(u < nbSeq) {
			break
		}
		llv = uint32((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(u)*8))).FlitLength)
		ofCode = ZSTD_highbit32(tls, (*(*SeqDef)(unsafe.Pointer(sequences + uintptr(u)*8))).FoffBase)
		mlv = uint32((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(u)*8))).FmlBase)
		*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(u))) = uint8(ZSTD_LLcode(tls, llv))
		*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(u))) = uint8(ofCode)
		*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(u))) = uint8(ZSTD_MLcode(tls, mlv))
		if v3 = MEM_32bits(tls) != 0; v3 {
			if MEM_32bits(tls) != 0 {
				v2 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v2 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
		}
		if v3 && ofCode >= uint32(v2) {
			longOffsets = int32(1)
		}
		goto _1
	_1:
		;
		u = u + 1
	}
	if (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType == int32(ZSTD_llt_literalLength) {
		*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos))) = uint8(MaxLL)
	}
	if (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType == int32(ZSTD_llt_matchLength) {
		*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos))) = uint8(MaxML)
	}
	return longOffsets
}

// C documentation
//
//	/* ZSTD_useTargetCBlockSize():
//	 * Returns if target compressed block size param is being used.
//	 * If used, compression will do best effort to make a compressed block size to be around targetCBlockSize.
//	 * Returns 1 if true, 0 otherwise. */
func ZSTD_useTargetCBlockSize(tls *libc.TLS, cctxParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FtargetCBlockSize != uint64(0))
}

// C documentation
//
//	/* ZSTD_blockSplitterEnabled():
//	 * Returns if block splitting param is being used
//	 * If used, compression will do best effort to split a block in order to improve compression ratio.
//	 * At the time this function is called, the parameter must be finalized.
//	 * Returns 1 if true, 0 otherwise. */
func ZSTD_blockSplitterEnabled(tls *libc.TLS, cctxParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FpostBlockSplitter == int32(ZSTD_ps_enable))
}

// C documentation
//
//	/* Type returned by ZSTD_buildSequencesStatistics containing finalized symbol encoding types
//	 * and size of the sequences statistics
//	 */
type ZSTD_symbolEncodingTypeStats_t = struct {
	FLLtype        U32
	FOfftype       U32
	FMLtype        U32
	Fsize          size_t
	FlastCountSize size_t
	FlongOffsets   int32
}

// C documentation
//
//	/* ZSTD_buildSequencesStatistics():
//	 * Returns a ZSTD_symbolEncodingTypeStats_t, or a zstd error code in the `size` field.
//	 * Modifies `nextEntropy` to have the appropriate values as a side effect.
//	 * nbSeq must be greater than 0.
//	 *
//	 * entropyWkspSize must be of size at least ENTROPY_WORKSPACE_SIZE - (MaxSeq + 1)*sizeof(U32)
//	 */
func ZSTD_buildSequencesStatistics(tls *libc.TLS, seqStorePtr uintptr, nbSeq size_t, prevEntropy uintptr, nextEntropy uintptr, dst uintptr, dstEnd uintptr, strategy ZSTD_strategy, countWorkspace uintptr, entropyWorkspace uintptr, entropyWkspSize size_t) (r ZSTD_symbolEncodingTypeStats_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, llCodeTable, mlCodeTable, oend, ofCodeTable, op, ostart uintptr
	var countSize, countSize1, countSize2, mostFrequent, mostFrequent1, mostFrequent2 size_t
	var defaultPolicy ZSTD_DefaultPolicy_e
	var stats ZSTD_symbolEncodingTypeStats_t
	var v1 int32
	var _ /* max at bp+0 */ uint32
	var _ /* max at bp+4 */ uint32
	var _ /* max at bp+8 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, countSize, countSize1, countSize2, defaultPolicy, llCodeTable, mlCodeTable, mostFrequent, mostFrequent1, mostFrequent2, oend, ofCodeTable, op, ostart, stats, v1
	ostart = dst
	oend = dstEnd
	op = ostart
	CTable_LitLength = nextEntropy + 2224
	CTable_OffsetBits = nextEntropy
	CTable_MatchLength = nextEntropy + 772
	ofCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	llCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	mlCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	stats.FlastCountSize = uint64(0)
	/* convert length/distances into codes */
	stats.FlongOffsets = ZSTD_seqToCodes(tls, seqStorePtr)
	/* ZSTD_selectEncodingType() divides by nbSeq */
	/* build CTable for Literal Lengths */
	*(*uint32)(unsafe.Pointer(bp)) = uint32(MaxLL)
	mostFrequent = HIST_countFast_wksp(tls, countWorkspace, bp, llCodeTable, nbSeq, entropyWorkspace, entropyWkspSize) /* can't fail */
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Flitlength_repeatMode = (*ZSTD_fseCTables_t)(unsafe.Pointer(prevEntropy)).Flitlength_repeatMode
	stats.FLLtype = uint32(ZSTD_selectEncodingType(tls, nextEntropy+3548, countWorkspace, *(*uint32)(unsafe.Pointer(bp)), mostFrequent, nbSeq, uint32(LLFSELog), prevEntropy+2224, uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, int32(ZSTD_defaultAllowed), strategy))
	/* We don't copy tables */
	countSize = ZSTD_buildCTable(tls, op, uint64(int64(oend)-int64(op)), CTable_LitLength, uint32(LLFSELog), int32(stats.FLLtype), countWorkspace, *(*uint32)(unsafe.Pointer(bp)), llCodeTable, nbSeq, uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, uint32(MaxLL), prevEntropy+2224, uint64(1316), entropyWorkspace, entropyWkspSize)
	if ZSTD_isError(tls, countSize) != 0 {
		stats.Fsize = countSize
		return stats
	}
	if stats.FLLtype == uint32(set_compressed) {
		stats.FlastCountSize = countSize
	}
	op = op + uintptr(countSize)
	/* build CTable for Offsets */
	*(*uint32)(unsafe.Pointer(bp + 4)) = uint32(MaxOff)
	mostFrequent1 = HIST_countFast_wksp(tls, countWorkspace, bp+4, ofCodeTable, nbSeq, entropyWorkspace, entropyWkspSize)
	if *(*uint32)(unsafe.Pointer(bp + 4)) <= uint32(DefaultMaxOff) {
		v1 = int32(ZSTD_defaultAllowed)
	} else {
		v1 = int32(ZSTD_defaultDisallowed)
	} /* can't fail */
	/* We can only use the basic table if max <= DefaultMaxOff, otherwise the offsets are too large */
	defaultPolicy = v1
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Foffcode_repeatMode = (*ZSTD_fseCTables_t)(unsafe.Pointer(prevEntropy)).Foffcode_repeatMode
	stats.FOfftype = uint32(ZSTD_selectEncodingType(tls, nextEntropy+3540, countWorkspace, *(*uint32)(unsafe.Pointer(bp + 4)), mostFrequent1, nbSeq, uint32(OffFSELog), prevEntropy, uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, defaultPolicy, strategy))
	/* We don't copy tables */
	countSize1 = ZSTD_buildCTable(tls, op, uint64(int64(oend)-int64(op)), CTable_OffsetBits, uint32(OffFSELog), int32(stats.FOfftype), countWorkspace, *(*uint32)(unsafe.Pointer(bp + 4)), ofCodeTable, nbSeq, uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, uint32(DefaultMaxOff), prevEntropy, uint64(772), entropyWorkspace, entropyWkspSize)
	if ZSTD_isError(tls, countSize1) != 0 {
		stats.Fsize = countSize1
		return stats
	}
	if stats.FOfftype == uint32(set_compressed) {
		stats.FlastCountSize = countSize1
	}
	op = op + uintptr(countSize1)
	/* build CTable for MatchLengths */
	*(*uint32)(unsafe.Pointer(bp + 8)) = uint32(MaxML)
	mostFrequent2 = HIST_countFast_wksp(tls, countWorkspace, bp+8, mlCodeTable, nbSeq, entropyWorkspace, entropyWkspSize) /* can't fail */
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Fmatchlength_repeatMode = (*ZSTD_fseCTables_t)(unsafe.Pointer(prevEntropy)).Fmatchlength_repeatMode
	stats.FMLtype = uint32(ZSTD_selectEncodingType(tls, nextEntropy+3544, countWorkspace, *(*uint32)(unsafe.Pointer(bp + 8)), mostFrequent2, nbSeq, uint32(MLFSELog), prevEntropy+772, uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, int32(ZSTD_defaultAllowed), strategy))
	/* We don't copy tables */
	countSize2 = ZSTD_buildCTable(tls, op, uint64(int64(oend)-int64(op)), CTable_MatchLength, uint32(MLFSELog), int32(stats.FMLtype), countWorkspace, *(*uint32)(unsafe.Pointer(bp + 8)), mlCodeTable, nbSeq, uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, uint32(MaxML), prevEntropy+772, uint64(1452), entropyWorkspace, entropyWkspSize)
	if ZSTD_isError(tls, countSize2) != 0 {
		stats.Fsize = countSize2
		return stats
	}
	if stats.FMLtype == uint32(set_compressed) {
		stats.FlastCountSize = countSize2
	}
	op = op + uintptr(countSize2)
	stats.Fsize = uint64(int64(op) - int64(ostart))
	return stats
}

// C documentation
//
//	/* ZSTD_entropyCompressSeqStore_internal():
//	 * compresses both literals and sequences
//	 * Returns compressed size of block, or a zstd error.
//	 */
func ZSTD_entropyCompressSeqStore_internal(tls *libc.TLS, dst uintptr, dstCapacity size_t, literals uintptr, litSize size_t, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, entropyWorkspace uintptr, entropyWkspSize size_t, bmi2 int32) (r size_t) {
	var CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, count, llCodeTable, mlCodeTable, oend, ofCodeTable, op, ostart, seqHead, sequences, v1 uintptr
	var bitstreamSize, cSize, err_code, err_code1, err_code2, lastCountSize, nbSeq, numSequences size_t
	var longOffsets, suspectUncompressible int32
	var stats ZSTD_symbolEncodingTypeStats_t
	var strategy ZSTD_strategy
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, bitstreamSize, cSize, count, err_code, err_code1, err_code2, lastCountSize, llCodeTable, longOffsets, mlCodeTable, nbSeq, numSequences, oend, ofCodeTable, op, ostart, seqHead, sequences, stats, strategy, suspectUncompressible, v1
	strategy = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy
	count = entropyWorkspace
	CTable_LitLength = nextEntropy + 2064 + 2224
	CTable_OffsetBits = nextEntropy + 2064
	CTable_MatchLength = nextEntropy + 2064 + 772
	sequences = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
	nbSeq = uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	ofCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	llCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	mlCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart
	longOffsets = 0
	entropyWorkspace = count + uintptr(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*4
	entropyWkspSize = entropyWkspSize - uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)
	_ = libc.Uint64FromInt64(1)
	/* Compress literals */
	numSequences = uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	/* Base suspicion of uncompressibility on ratio of literals to sequences */
	suspectUncompressible = libc.BoolInt32(numSequences == uint64(0) || litSize/numSequences >= uint64(SUSPECT_UNCOMPRESSIBLE_LITERAL_RATIO))
	cSize = ZSTD_compressLiterals(tls, op, dstCapacity, literals, litSize, entropyWorkspace, entropyWkspSize, prevEntropy, nextEntropy, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy, ZSTD_literalsCompressionIsDisabled(tls, cctxParams), suspectUncompressible, bmi2)
	err_code = cSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2775, 0)
		}
		return err_code
	}
	op = op + uintptr(cSize)
	/* Sequences Header */
	if int64(oend)-int64(op) < int64(libc.Int32FromInt32(3)+libc.Int32FromInt32(1)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2804, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbSeq < uint64(128) {
		v1 = op
		op = op + 1
		*(*BYTE)(unsafe.Pointer(v1)) = uint8(nbSeq)
	} else {
		if nbSeq < uint64(LONGNBSEQ) {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(nbSeq>>libc.Int32FromInt32(8) + libc.Uint64FromInt32(0x80))
			*(*BYTE)(unsafe.Pointer(op + 1)) = uint8(nbSeq)
			op = op + uintptr(2)
		} else {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(0xFF)
			MEM_writeLE16(tls, op+uintptr(1), uint16(nbSeq-libc.Uint64FromInt32(LONGNBSEQ)))
			op = op + uintptr(3)
		}
	}
	if nbSeq == uint64(0) {
		/* Copy the old tables over as if we repeated them */
		libc.Xmemcpy(tls, nextEntropy+2064, prevEntropy+2064, libc.Uint64FromInt64(3552))
		return uint64(int64(op) - int64(ostart))
	}
	v1 = op
	op = op + 1
	seqHead = v1
	/* build stats for sequences */
	stats = ZSTD_buildSequencesStatistics(tls, seqStorePtr, nbSeq, prevEntropy+2064, nextEntropy+2064, op, oend, strategy, count, entropyWorkspace, entropyWkspSize)
	err_code1 = stats.Fsize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2837, 0)
		}
		return err_code1
	}
	*(*BYTE)(unsafe.Pointer(seqHead)) = uint8(stats.FLLtype<<libc.Int32FromInt32(6) + stats.FOfftype<<libc.Int32FromInt32(4) + stats.FMLtype<<libc.Int32FromInt32(2))
	lastCountSize = stats.FlastCountSize
	op = op + uintptr(stats.Fsize)
	longOffsets = stats.FlongOffsets
	bitstreamSize = ZSTD_encodeSequences(tls, op, uint64(int64(oend)-int64(op)), CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets, bmi2)
	err_code2 = bitstreamSize
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1608, 0)
		}
		return err_code2
	}
	op = op + uintptr(bitstreamSize)
	/* zstd versions <= 1.3.4 mistakenly report corruption when
	 * FSE_readNCount() receives a buffer < 4 bytes.
	 * Fixed by https://github.com/facebook/zstd/pull/1146.
	 * This can happen when the last set_compressed table present is 2
	 * bytes and the bitstream is only one byte.
	 * In this exceedingly rare case, we will simply emit an uncompressed
	 * block, since it isn't worth optimizing.
	 */
	if lastCountSize != 0 && lastCountSize+bitstreamSize < uint64(4) {
		/* lastCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */
		return uint64(0)
	}
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_entropyCompressSeqStore_wExtLitBuffer(tls *libc.TLS, dst uintptr, dstCapacity size_t, literals uintptr, litSize size_t, blockSize size_t, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, entropyWorkspace uintptr, entropyWkspSize size_t, bmi2 int32) (r size_t) {
	var cSize, err_code, maxCSize size_t
	_, _, _ = cSize, err_code, maxCSize
	cSize = ZSTD_entropyCompressSeqStore_internal(tls, dst, dstCapacity, literals, litSize, seqStorePtr, prevEntropy, nextEntropy, cctxParams, entropyWorkspace, entropyWkspSize, bmi2)
	if cSize == uint64(0) {
		return uint64(0)
	}
	/* When srcSize <= dstCapacity, there is enough space to write a raw uncompressed block.
	 * Since we ran out of space, block must be not compressible, so fall back to raw uncompressed block.
	 */
	if libc.BoolInt32(cSize == uint64(-int32(ZSTD_error_dstSize_tooSmall)))&libc.BoolInt32(blockSize <= dstCapacity) != 0 {
		return uint64(0) /* block not compressed */
	}
	err_code = cSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2875, 0)
		}
		return err_code
	}
	/* Check compressibility */
	maxCSize = blockSize - ZSTD_minGain(tls, blockSize, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy)
	if cSize >= maxCSize {
		return uint64(0)
	} /* block not compressed */
	/* libzstd decoder before  > v1.5.4 is not compatible with compressed blocks of size ZSTD_BLOCKSIZE_MAX exactly.
	 * This restriction is indirectly already fulfilled by respecting ZSTD_minGain() condition above.
	 */
	return cSize
}

func ZSTD_entropyCompressSeqStore(tls *libc.TLS, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, srcSize size_t, entropyWorkspace uintptr, entropyWkspSize size_t, bmi2 int32) (r size_t) {
	return ZSTD_entropyCompressSeqStore_wExtLitBuffer(tls, dst, dstCapacity, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart, uint64(int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit)-int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart)), srcSize, seqStorePtr, prevEntropy, nextEntropy, cctxParams, entropyWorkspace, entropyWkspSize, bmi2)
}

// C documentation
//
//	/* ZSTD_selectBlockCompressor() :
//	 * Not static, but internal use only (used by long distance matcher)
//	 * assumption : strat is a valid strategy */
func ZSTD_selectBlockCompressor(tls *libc.TLS, strat ZSTD_strategy, useRowMatchFinder ZSTD_ParamSwitch_e, dictMode ZSTD_dictMode_e) (r ZSTD_BlockCompressor_f) {
	var selectedCompressor ZSTD_BlockCompressor_f
	_ = selectedCompressor
	_ = libc.Uint64FromInt64(1)
	if ZSTD_rowMatchFinderUsed(tls, strat, useRowMatchFinder) != 0 {
		selectedCompressor = *(*ZSTD_BlockCompressor_f)(unsafe.Pointer(uintptr(unsafe.Pointer(&rowBasedBlockCompressors)) + uintptr(dictMode)*24 + uintptr(strat-int32(ZSTD_greedy))*8))
	} else {
		selectedCompressor = *(*ZSTD_BlockCompressor_f)(unsafe.Pointer(uintptr(unsafe.Pointer(&blockCompressor)) + uintptr(dictMode)*80 + uintptr(strat)*8))
	}
	return selectedCompressor
}

var blockCompressor = [4][10]ZSTD_BlockCompressor_f{
	0: {},
	1: {},
	2: {},
	3: {},
}

func init() {
	p := unsafe.Pointer(&blockCompressor)
	*(*uintptr)(unsafe.Add(p, 0)) = __ccgo_fp(ZSTD_compressBlock_fast)
	*(*uintptr)(unsafe.Add(p, 8)) = __ccgo_fp(ZSTD_compressBlock_fast)
	*(*uintptr)(unsafe.Add(p, 16)) = __ccgo_fp(ZSTD_compressBlock_doubleFast)
	*(*uintptr)(unsafe.Add(p, 24)) = __ccgo_fp(ZSTD_compressBlock_greedy)
	*(*uintptr)(unsafe.Add(p, 32)) = __ccgo_fp(ZSTD_compressBlock_lazy)
	*(*uintptr)(unsafe.Add(p, 40)) = __ccgo_fp(ZSTD_compressBlock_lazy2)
	*(*uintptr)(unsafe.Add(p, 48)) = __ccgo_fp(ZSTD_compressBlock_btlazy2)
	*(*uintptr)(unsafe.Add(p, 56)) = __ccgo_fp(ZSTD_compressBlock_btopt)
	*(*uintptr)(unsafe.Add(p, 64)) = __ccgo_fp(ZSTD_compressBlock_btultra)
	*(*uintptr)(unsafe.Add(p, 72)) = __ccgo_fp(ZSTD_compressBlock_btultra2)
	*(*uintptr)(unsafe.Add(p, 80)) = __ccgo_fp(ZSTD_compressBlock_fast_extDict)
	*(*uintptr)(unsafe.Add(p, 88)) = __ccgo_fp(ZSTD_compressBlock_fast_extDict)
	*(*uintptr)(unsafe.Add(p, 96)) = __ccgo_fp(ZSTD_compressBlock_doubleFast_extDict)
	*(*uintptr)(unsafe.Add(p, 104)) = __ccgo_fp(ZSTD_compressBlock_greedy_extDict)
	*(*uintptr)(unsafe.Add(p, 112)) = __ccgo_fp(ZSTD_compressBlock_lazy_extDict)
	*(*uintptr)(unsafe.Add(p, 120)) = __ccgo_fp(ZSTD_compressBlock_lazy2_extDict)
	*(*uintptr)(unsafe.Add(p, 128)) = __ccgo_fp(ZSTD_compressBlock_btlazy2_extDict)
	*(*uintptr)(unsafe.Add(p, 136)) = __ccgo_fp(ZSTD_compressBlock_btopt_extDict)
	*(*uintptr)(unsafe.Add(p, 144)) = __ccgo_fp(ZSTD_compressBlock_btultra_extDict)
	*(*uintptr)(unsafe.Add(p, 152)) = __ccgo_fp(ZSTD_compressBlock_btultra_extDict)
	*(*uintptr)(unsafe.Add(p, 160)) = __ccgo_fp(ZSTD_compressBlock_fast_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 168)) = __ccgo_fp(ZSTD_compressBlock_fast_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 176)) = __ccgo_fp(ZSTD_compressBlock_doubleFast_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 184)) = __ccgo_fp(ZSTD_compressBlock_greedy_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 192)) = __ccgo_fp(ZSTD_compressBlock_lazy_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 200)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 208)) = __ccgo_fp(ZSTD_compressBlock_btlazy2_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 216)) = __ccgo_fp(ZSTD_compressBlock_btopt_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 224)) = __ccgo_fp(ZSTD_compressBlock_btultra_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 232)) = __ccgo_fp(ZSTD_compressBlock_btultra_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 240)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 248)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 256)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 264)) = __ccgo_fp(ZSTD_compressBlock_greedy_dedicatedDictSearch)
	*(*uintptr)(unsafe.Add(p, 272)) = __ccgo_fp(ZSTD_compressBlock_lazy_dedicatedDictSearch)
	*(*uintptr)(unsafe.Add(p, 280)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dedicatedDictSearch)
	*(*uintptr)(unsafe.Add(p, 288)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 296)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 304)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 312)) = libc.UintptrFromInt32(0)
}

var rowBasedBlockCompressors = [4][3]ZSTD_BlockCompressor_f{
	0: {},
	1: {},
	2: {},
	3: {},
}

func init() {
	p := unsafe.Pointer(&rowBasedBlockCompressors)
	*(*uintptr)(unsafe.Add(p, 0)) = __ccgo_fp(ZSTD_compressBlock_greedy_row)
	*(*uintptr)(unsafe.Add(p, 8)) = __ccgo_fp(ZSTD_compressBlock_lazy_row)
	*(*uintptr)(unsafe.Add(p, 16)) = __ccgo_fp(ZSTD_compressBlock_lazy2_row)
	*(*uintptr)(unsafe.Add(p, 24)) = __ccgo_fp(ZSTD_compressBlock_greedy_extDict_row)
	*(*uintptr)(unsafe.Add(p, 32)) = __ccgo_fp(ZSTD_compressBlock_lazy_extDict_row)
	*(*uintptr)(unsafe.Add(p, 40)) = __ccgo_fp(ZSTD_compressBlock_lazy2_extDict_row)
	*(*uintptr)(unsafe.Add(p, 48)) = __ccgo_fp(ZSTD_compressBlock_greedy_dictMatchState_row)
	*(*uintptr)(unsafe.Add(p, 56)) = __ccgo_fp(ZSTD_compressBlock_lazy_dictMatchState_row)
	*(*uintptr)(unsafe.Add(p, 64)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dictMatchState_row)
	*(*uintptr)(unsafe.Add(p, 72)) = __ccgo_fp(ZSTD_compressBlock_greedy_dedicatedDictSearch_row)
	*(*uintptr)(unsafe.Add(p, 80)) = __ccgo_fp(ZSTD_compressBlock_lazy_dedicatedDictSearch_row)
	*(*uintptr)(unsafe.Add(p, 88)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dedicatedDictSearch_row)
}

func ZSTD_storeLastLiterals(tls *libc.TLS, seqStorePtr uintptr, anchor uintptr, lastLLSize size_t) {
	libc.Xmemcpy(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit, anchor, lastLLSize)
	*(*uintptr)(unsafe.Pointer(seqStorePtr + 24)) += uintptr(lastLLSize)
}

func ZSTD_resetSeqStore(tls *libc.TLS, ssPtr uintptr) {
	(*SeqStore_t)(unsafe.Pointer(ssPtr)).Flit = (*SeqStore_t)(unsafe.Pointer(ssPtr)).FlitStart
	(*SeqStore_t)(unsafe.Pointer(ssPtr)).Fsequences = (*SeqStore_t)(unsafe.Pointer(ssPtr)).FsequencesStart
	(*SeqStore_t)(unsafe.Pointer(ssPtr)).FlongLengthType = int32(ZSTD_llt_none)
}

// C documentation
//
//	/* ZSTD_postProcessSequenceProducerResult() :
//	 * Validates and post-processes sequences obtained through the external matchfinder API:
//	 *   - Checks whether nbExternalSeqs represents an error condition.
//	 *   - Appends a block delimiter to outSeqs if one is not already present.
//	 *     See zstd.h for context regarding block delimiters.
//	 * Returns the number of sequences after post-processing, or an error code. */
func ZSTD_postProcessSequenceProducerResult(tls *libc.TLS, outSeqs uintptr, nbExternalSeqs size_t, outSeqsCapacity size_t, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var lastSeq ZSTD_Sequence
	_ = lastSeq
	if nbExternalSeqs > outSeqsCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2920, libc.VaList(bp+8, uint32(nbExternalSeqs)))
		}
		return uint64(-int32(ZSTD_error_sequenceProducer_failed))
	}
	if nbExternalSeqs == uint64(0) && srcSize > uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2971, 0)
		}
		return uint64(-int32(ZSTD_error_sequenceProducer_failed))
	}
	if srcSize == uint64(0) {
		libc.Xmemset(tls, outSeqs, 0, libc.Uint64FromInt64(16))
		return uint64(1)
	}
	lastSeq = *(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbExternalSeqs-uint64(1))*16))
	/* We can return early if lastSeq is already a block delimiter. */
	if lastSeq.Foffset == uint32(0) && lastSeq.FmatchLength == uint32(0) {
		return nbExternalSeqs
	}
	/* This error condition is only possible if the external matchfinder
	 * produced an invalid parse, by definition of ZSTD_sequenceBound(). */
	if nbExternalSeqs == outSeqsCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3050, 0)
		}
		return uint64(-int32(ZSTD_error_sequenceProducer_failed))
	}
	/* lastSeq is not a block delimiter, so we need to append one. */
	libc.Xmemset(tls, outSeqs+uintptr(nbExternalSeqs)*16, 0, libc.Uint64FromInt64(16))
	return nbExternalSeqs + uint64(1)
	return r
}

// C documentation
//
//	/* ZSTD_fastSequenceLengthSum() :
//	 * Returns sum(litLen) + sum(matchLen) + lastLits for *seqBuf*.
//	 * Similar to another function in zstd_compress.c (determine_blockSize),
//	 * except it doesn't check for a block delimiter to end summation.
//	 * Removing the early exit allows the compiler to auto-vectorize (https://godbolt.org/z/cY1cajz9P).
//	 * This function can be deleted and replaced by determine_blockSize after we resolve issue #3456. */
func ZSTD_fastSequenceLengthSum(tls *libc.TLS, seqBuf uintptr, seqBufSize size_t) (r size_t) {
	var i, litLenSum, matchLenSum size_t
	_, _, _ = i, litLenSum, matchLenSum
	matchLenSum = uint64(0)
	litLenSum = uint64(0)
	i = uint64(0)
	for {
		if !(i < seqBufSize) {
			break
		}
		litLenSum = litLenSum + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqBuf + uintptr(i)*16))).FlitLength)
		matchLenSum = matchLenSum + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqBuf + uintptr(i)*16))).FmatchLength)
		goto _1
	_1:
		;
		i = i + 1
	}
	return litLenSum + matchLenSum
}

// C documentation
//
//	/**
//	 * Function to validate sequences produced by a block compressor.
//	 */
func ZSTD_validateSeqStore(tls *libc.TLS, seqStore uintptr, cParams uintptr) {
	_ = seqStore
	_ = cParams
}

type ZSTD_BuildSeqStore_e = int32

const ZSTDbss_compress = 0
const ZSTDbss_noCompress = 1

func ZSTD_buildSeqStore(tls *libc.TLS, zc uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var base, istart, lastLiterals, ms uintptr
	var blockCompressor, blockCompressor1 ZSTD_BlockCompressor_f
	var curr, windowSize U32
	var dictMode ZSTD_dictMode_e
	var err_code, err_code1, lastLLSize, nbExternalSeqs, nbPostProcessedSeqs, seqLenSum size_t
	var i int32
	var v1 uint32
	var _ /* ldmSeqStore at bp+0 */ RawSeqStore_t
	var _ /* seqPos at bp+40 */ ZSTD_SequencePosition
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, blockCompressor, blockCompressor1, curr, dictMode, err_code, err_code1, i, istart, lastLLSize, lastLiterals, ms, nbExternalSeqs, nbPostProcessedSeqs, seqLenSum, windowSize, v1
	ms = zc + 3224 + 16
	/* Assert that we have correctly flushed the ctx params into the ms's copy */
	ZSTD_assertEqualCParams(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams)
	/* TODO: See 3090. We reduced MIN_CBLOCK_SIZE from 3 to 2 so to compensate we are adding
	 * additional 1. We need to revisit and change this logic to be more consistent */
	if srcSize < uint64(libc.Int32FromInt32(1)+libc.Int32FromInt32(1))+ZSTD_blockHeaderSize+uint64(1)+uint64(1) {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy >= int32(ZSTD_btopt) {
			ZSTD_ldm_skipRawSeqStoreBytes(tls, zc+3184, srcSize)
		} else {
			ZSTD_ldm_skipSequences(tls, zc+3184, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.FminMatch)
		}
		return uint64(ZSTDbss_noCompress) /* don't even attempt compression below a certain srcSize */
	}
	ZSTD_resetSeqStore(tls, zc+976)
	/* required for optimal parser to read stats from dictionary */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FsymbolCosts = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock
	/* tell the optimal parser how we expect to compress literals */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FliteralCompressionMode = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FliteralCompressionMode
	/* a gap between an attached dict and the current window is not safe,
	 * they must remain adjacent,
	 * and when that stops being the case, the dict must be unset */
	/* limited update after a very long match */
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	curr = uint32(int64(istart) - int64(base))
	if uint64(8) == uint64(8) {
	} /* ensure no overflow */
	if curr > (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate+uint32(384) {
		if uint32(libc.Int32FromInt32(192)) < curr-(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate-libc.Uint32FromInt32(384) {
			v1 = uint32(libc.Int32FromInt32(192))
		} else {
			v1 = curr - (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate - libc.Uint32FromInt32(384)
		}
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr - v1
	}
	/* select and store sequences */
	dictMode = ZSTD_matchState_dictMode(tls, ms)
	i = 0
	for {
		if !(i < int32(ZSTD_REP_NUM)) {
			break
		}
		*(*U32)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock + 5616 + uintptr(i)*4)) = *(*U32)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock + 5616 + uintptr(i)*4))
		goto _2
	_2:
		;
		i = i + 1
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FexternSeqStore.Fpos < (*ZSTD_CCtx)(unsafe.Pointer(zc)).FexternSeqStore.Fsize {
		/* External matchfinder + LDM is technically possible, just not implemented yet.
		 * We need to revisit soon and implement it. */
		if ZSTD_hasExtSeqProd(tls, zc+240) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3122, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_combination_unsupported))
		}
		/* Updates ldmSeqStore.pos */
		lastLLSize = ZSTD_ldm_blockCompress(tls, zc+3184, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, src, srcSize)
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
			*(*RawSeqStore_t)(unsafe.Pointer(bp)) = kNullRawSeqStore
			/* External matchfinder + LDM is technically possible, just not implemented yet.
			 * We need to revisit soon and implement it. */
			if ZSTD_hasExtSeqProd(tls, zc+240) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3122, 0)
				}
				return uint64(-int32(ZSTD_error_parameter_combination_unsupported))
			}
			(*(*RawSeqStore_t)(unsafe.Pointer(bp))).Fseq = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmSequences
			(*(*RawSeqStore_t)(unsafe.Pointer(bp))).Fcapacity = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FmaxNbLdmSequences
			/* Updates ldmSeqStore.size */
			err_code = ZSTD_ldm_generateSequences(tls, zc+1056, bp, zc+240+96, src, srcSize)
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code
			}
			/* Updates ldmSeqStore.pos */
			lastLLSize = ZSTD_ldm_blockCompress(tls, bp, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, src, srcSize)
		} else {
			if ZSTD_hasExtSeqProd(tls, zc+240) != 0 {
				windowSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.FwindowLog
				nbExternalSeqs = (*(*func(*libc.TLS, uintptr, uintptr, size_t, uintptr, size_t, uintptr, size_t, int32, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{(*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FextSeqProdFunc})))(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FextSeqProdState, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBufCapacity, src, srcSize, libc.UintptrFromInt32(0), uint64(0), (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcompressionLevel, uint64(windowSize))
				nbPostProcessedSeqs = ZSTD_postProcessSequenceProducerResult(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, nbExternalSeqs, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBufCapacity, srcSize)
				/* Return early if there is no error, since we don't need to worry about last literals */
				if !(ZSTD_isError(tls, nbPostProcessedSeqs) != 0) {
					*(*ZSTD_SequencePosition)(unsafe.Pointer(bp + 40)) = ZSTD_SequencePosition{}
					seqLenSum = ZSTD_fastSequenceLengthSum(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, nbPostProcessedSeqs)
					if seqLenSum > srcSize {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+3213, 0)
						}
						return uint64(-int32(ZSTD_error_externalSequences_invalid))
					}
					err_code1 = ZSTD_transferSequences_wBlockDelim(tls, zc, bp+40, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, nbPostProcessedSeqs, src, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FsearchForExternalRepcodes)
					if ERR_isError(tls, err_code1) != 0 {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+3257, 0)
						}
						return err_code1
					}
					(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = libc.UintptrFromInt32(0)
					return uint64(ZSTDbss_compress)
				}
				/* Propagate the error if fallback is disabled */
				if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FenableMatchFinderFallback != 0) {
					return nbPostProcessedSeqs
				}
				/* Fallback to software matchfinder */
				blockCompressor = ZSTD_selectBlockCompressor(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, dictMode)
				(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = libc.UintptrFromInt32(0)
				lastLLSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, src, srcSize)
			} else { /* not long range mode and no external matchfinder */
				blockCompressor1 = ZSTD_selectBlockCompressor(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, dictMode)
				(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = libc.UintptrFromInt32(0)
				lastLLSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor1})))(tls, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, src, srcSize)
			}
		}
	}
	lastLiterals = src + uintptr(srcSize) - uintptr(lastLLSize)
	ZSTD_storeLastLiterals(tls, zc+976, lastLiterals, lastLLSize)
	ZSTD_validateSeqStore(tls, zc+976, zc+240+4)
	return uint64(ZSTDbss_compress)
}

func ZSTD_copyBlockSequences(tls *libc.TLS, seqCollector uintptr, seqStore uintptr, prevRepcodes uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var i, lastLLSize, nbInLiterals, nbInSequences, nbOutLiterals, nbOutSequences size_t
	var inSeqs, outSeqs, v1 uintptr
	var rawOffset, repcode U32
	var _ /* repcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _ = i, inSeqs, lastLLSize, nbInLiterals, nbInSequences, nbOutLiterals, nbOutSequences, outSeqs, rawOffset, repcode, v1
	inSeqs = (*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart
	nbInSequences = uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64(inSeqs)) / 8)
	nbInLiterals = uint64(int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Flit) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart))
	if (*SeqCollector)(unsafe.Pointer(seqCollector)).FseqIndex == uint64(0) {
		v1 = (*SeqCollector)(unsafe.Pointer(seqCollector)).FseqStart
	} else {
		v1 = (*SeqCollector)(unsafe.Pointer(seqCollector)).FseqStart + uintptr((*SeqCollector)(unsafe.Pointer(seqCollector)).FseqIndex)*16
	}
	outSeqs = v1
	nbOutSequences = nbInSequences + uint64(1)
	nbOutLiterals = uint64(0)
	/* Bounds check that we have enough space for every input sequence
	 * and the block delimiter
	 */
	if nbOutSequences > (*SeqCollector)(unsafe.Pointer(seqCollector)).FmaxSequences-(*SeqCollector)(unsafe.Pointer(seqCollector)).FseqIndex {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3304, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	libc.Xmemcpy(tls, bp, prevRepcodes, libc.Uint64FromInt64(12))
	i = uint64(0)
	for {
		if !(i < nbInSequences) {
			break
		}
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength = uint32((*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FlitLength)
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FmatchLength = uint32(int32((*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FmlBase) + int32(MINMATCH))
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).Frep = uint32(0)
		/* Handle the possible single length >= 64K
		 * There can only be one because we add MINMATCH to every match length,
		 * and blocks are at most 128K.
		 */
		if i == uint64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos) {
			if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
				(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength += uint32(0x10000)
			} else {
				if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_matchLength) {
					(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FmatchLength += uint32(0x10000)
				}
			}
		}
		/* Determine the raw offset given the offBase, which may be a repcode. */
		if uint32(1) <= (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase && (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase <= uint32(ZSTD_REP_NUM) {
			repcode = (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase
			(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).Frep = repcode
			if (*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength != uint32(0) {
				rawOffset = *(*U32)(unsafe.Pointer(bp + uintptr(repcode-uint32(1))*4))
			} else {
				if repcode == uint32(3) {
					rawOffset = *(*U32)(unsafe.Pointer(bp)) - uint32(1)
				} else {
					rawOffset = *(*U32)(unsafe.Pointer(bp + uintptr(repcode)*4))
				}
			}
		} else {
			rawOffset = (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase - libc.Uint32FromInt32(ZSTD_REP_NUM)
		}
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).Foffset = rawOffset
		/* Update repcode history for the sequence */
		ZSTD_updateRep(tls, bp, (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase, libc.BoolUint32(int32((*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FlitLength) == 0))
		nbOutLiterals = nbOutLiterals + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength)
		goto _2
	_2:
		;
		i = i + 1
	}
	/* Insert last literals (if any exist) in the block as a sequence with ml == off == 0.
	 * If there are no last literals, then we'll emit (of: 0, ml: 0, ll: 0), which is a marker
	 * for the block boundary, according to the API.
	 */
	lastLLSize = nbInLiterals - nbOutLiterals
	(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbInSequences)*16))).FlitLength = uint32(lastLLSize)
	(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbInSequences)*16))).FmatchLength = uint32(0)
	(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbInSequences)*16))).Foffset = uint32(0)
	*(*size_t)(unsafe.Pointer(seqCollector + 16)) += nbOutSequences
	return uint64(0)
}

func ZSTD_sequenceBound(tls *libc.TLS, srcSize size_t) (r size_t) {
	var maxNbDelims, maxNbSeq size_t
	_, _ = maxNbDelims, maxNbSeq
	maxNbSeq = srcSize/uint64(ZSTD_MINMATCH_MIN) + uint64(1)
	maxNbDelims = srcSize/uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)) + uint64(1)
	return maxNbSeq + maxNbDelims
}

func ZSTD_generateSequences(tls *libc.TLS, zc uintptr, outSeqs uintptr, outSeqsSize size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var dst uintptr
	var dstCapacity, err_code, err_code1, err_code2, ret size_t
	var seqCollector SeqCollector
	var _ /* nbWorkers at bp+4 */ int32
	var _ /* targetCBlockSize at bp+0 */ int32
	_, _, _, _, _, _, _ = dst, dstCapacity, err_code, err_code1, err_code2, ret, seqCollector
	dstCapacity = ZSTD_compressBound(tls, srcSize)
	err_code = ZSTD_CCtx_getParameter(tls, zc, int32(ZSTD_c_targetCBlockSize), bp)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if *(*int32)(unsafe.Pointer(bp)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3339, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_unsupported))
	}
	err_code1 = ZSTD_CCtx_getParameter(tls, zc, int32(ZSTD_c_nbWorkers), bp+4)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	if *(*int32)(unsafe.Pointer(bp + 4)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3361, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_unsupported))
	}
	dst = ZSTD_customMalloc(tls, dstCapacity, ZSTD_defaultCMem)
	if dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	seqCollector.FcollectSequences = int32(1)
	seqCollector.FseqStart = outSeqs
	seqCollector.FseqIndex = uint64(0)
	seqCollector.FmaxSequences = outSeqsSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector = seqCollector
	ret = ZSTD_compress2(tls, zc, dst, dstCapacity, src, srcSize)
	ZSTD_customFree(tls, dst, ZSTD_defaultCMem)
	err_code2 = ret
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3376, 0)
		}
		return err_code2
	}
	return (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FseqIndex
}

func ZSTD_mergeBlockDelimiters(tls *libc.TLS, sequences uintptr, seqsSize size_t) (r size_t) {
	var in, out size_t
	_, _ = in, out
	in = uint64(0)
	out = uint64(0)
	for {
		if !(in < seqsSize) {
			break
		}
		if (*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))).Foffset == uint32(0) && (*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))).FmatchLength == uint32(0) {
			if in != seqsSize-uint64(1) {
				(*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in+uint64(1))*16))).FlitLength += (*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))).FlitLength
			}
		} else {
			*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(out)*16)) = *(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))
			out = out + 1
		}
		goto _1
	_1:
		;
		in = in + 1
	}
	return out
}

// C documentation
//
//	/* Unrolled loop to read four size_ts of input at a time. Returns 1 if is RLE, 0 if not. */
func ZSTD_isRLE(tls *libc.TLS, src uintptr, length size_t) (r int32) {
	var i, prefixLength, u, unrollMask, unrollSize, valueST size_t
	var ip uintptr
	var value BYTE
	_, _, _, _, _, _, _, _ = i, ip, prefixLength, u, unrollMask, unrollSize, value, valueST
	ip = src
	value = *(*BYTE)(unsafe.Pointer(ip))
	valueST = uint64(value) * libc.Uint64FromUint64(0x0101010101010101)
	unrollSize = libc.Uint64FromInt64(8) * libc.Uint64FromInt32(4)
	unrollMask = unrollSize - uint64(1)
	prefixLength = length & unrollMask
	if length == uint64(1) {
		return int32(1)
	}
	/* Check if prefix is RLE first before using unrolled loop */
	if prefixLength != 0 && ZSTD_count(tls, ip+uintptr(1), ip, ip+uintptr(prefixLength)) != prefixLength-uint64(1) {
		return 0
	}
	i = prefixLength
	for {
		if !(i != length) {
			break
		}
		u = uint64(0)
		for {
			if !(u < unrollSize) {
				break
			}
			if MEM_readST(tls, ip+uintptr(i)+uintptr(u)) != valueST {
				return 0
			}
			goto _2
		_2:
			;
			u = u + uint64(8)
		}
		goto _1
	_1:
		;
		i = i + unrollSize
	}
	return int32(1)
}

// C documentation
//
//	/* Returns true if the given block may be RLE.
//	 * This is just a heuristic based on the compressibility.
//	 * It may return both false positives and false negatives.
//	 */
func ZSTD_maybeRLE(tls *libc.TLS, seqStore uintptr) (r int32) {
	var nbLits, nbSeqs size_t
	_, _ = nbLits, nbSeqs
	nbSeqs = uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart)) / 8)
	nbLits = uint64(int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Flit) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart))
	return libc.BoolInt32(nbSeqs < uint64(4) && nbLits < uint64(10))
}

func ZSTD_blockState_confirmRepcodesAndEntropyTables(tls *libc.TLS, bs uintptr) {
	var tmp uintptr
	_ = tmp
	tmp = (*ZSTD_blockState_t)(unsafe.Pointer(bs)).FprevCBlock
	(*ZSTD_blockState_t)(unsafe.Pointer(bs)).FprevCBlock = (*ZSTD_blockState_t)(unsafe.Pointer(bs)).FnextCBlock
	(*ZSTD_blockState_t)(unsafe.Pointer(bs)).FnextCBlock = tmp
}

// C documentation
//
//	/* Writes the block header */
func writeBlockHeader(tls *libc.TLS, op uintptr, cSize size_t, blockSize size_t, lastBlock U32) {
	var cBlockHeader U32
	var v1 uint32
	_, _ = cBlockHeader, v1
	if cSize == uint64(1) {
		v1 = lastBlock + uint32(bt_rle)<<libc.Int32FromInt32(1) + uint32(blockSize<<libc.Int32FromInt32(3))
	} else {
		v1 = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(cSize<<libc.Int32FromInt32(3))
	}
	cBlockHeader = v1
	MEM_writeLE24(tls, op, cBlockHeader)
}

// C documentation
//
//	/** ZSTD_buildBlockEntropyStats_literals() :
//	 *  Builds entropy for the literals.
//	 *  Stores literals block type (raw, rle, compressed, repeat) and
//	 *  huffman description table to hufMetadata.
//	 *  Requires ENTROPY_WORKSPACE_SIZE workspace
//	 * @return : size of huffman description table, or an error code
//	 */
func ZSTD_buildBlockEntropyStats_literals(tls *libc.TLS, src uintptr, srcSize size_t, prevHuf uintptr, nextHuf uintptr, hufMetadata uintptr, literalsCompressionIsDisabled int32, workspace uintptr, wkspSize size_t, hufFlags int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var countWksp, countWkspStart, nodeWksp, wkspEnd, wkspStart uintptr
	var countWkspSize, err_code, err_code1, hSize, largest, maxBits, minLitSize, newCSize, nodeWkspSize, oldCSize size_t
	var huffLog uint32
	var repeat HUF_repeat
	var v1 int32
	var _ /* maxSymbolValue at bp+0 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = countWksp, countWkspSize, countWkspStart, err_code, err_code1, hSize, huffLog, largest, maxBits, minLitSize, newCSize, nodeWksp, nodeWkspSize, oldCSize, repeat, wkspEnd, wkspStart, v1
	wkspStart = workspace
	wkspEnd = wkspStart + uintptr(wkspSize)
	countWkspStart = wkspStart
	countWksp = workspace
	countWkspSize = uint64(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)) * libc.Uint64FromInt64(4)
	nodeWksp = countWkspStart + uintptr(countWkspSize)
	nodeWkspSize = uint64(int64(wkspEnd) - int64(nodeWksp))
	*(*uint32)(unsafe.Pointer(bp)) = uint32(HUF_SYMBOLVALUE_MAX)
	huffLog = uint32(LitHufLog)
	repeat = (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode
	/* Prepare nextEntropy assuming reusing the existing table */
	libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
	if literalsCompressionIsDisabled != 0 {
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	/* small ? don't even attempt compression (speed opt) */
	if (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode == int32(HUF_repeat_valid) {
		v1 = int32(6)
	} else {
		v1 = int32(COMPRESS_LITERALS_SIZE_MIN)
	}
	minLitSize = uint64(v1)
	if srcSize <= minLitSize {
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	/* Scan input and build symbol stats */
	largest = HIST_count_wksp(tls, countWksp, bp, src, srcSize, workspace, wkspSize)
	err_code = largest
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3398, 0)
		}
		return err_code
	}
	if largest == srcSize {
		/* only one literal symbol */
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_rle)
		return uint64(0)
	}
	if largest <= srcSize>>libc.Int32FromInt32(7)+uint64(4) {
		/* heuristic: likely not compressible */
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	/* Validate the previous Huffman table */
	if repeat == int32(HUF_repeat_check) && !(HUF_validateCTable(tls, prevHuf, countWksp, *(*uint32)(unsafe.Pointer(bp))) != 0) {
		repeat = int32(HUF_repeat_none)
	}
	/* Build Huffman Tree */
	libc.Xmemset(tls, nextHuf, 0, libc.Uint64FromInt64(2056))
	huffLog = HUF_optimalTableLog(tls, huffLog, srcSize, *(*uint32)(unsafe.Pointer(bp)), nodeWksp, nodeWkspSize, nextHuf, countWksp, hufFlags)
	maxBits = HUF_buildCTable_wksp(tls, nextHuf, countWksp, *(*uint32)(unsafe.Pointer(bp)), huffLog, nodeWksp, nodeWkspSize)
	err_code1 = maxBits
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3421, 0)
		}
		return err_code1
	}
	huffLog = uint32(maxBits)
	/* Build and write the CTable */
	newCSize = HUF_estimateCompressedSize(tls, nextHuf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
	hSize = HUF_writeCTable_wksp(tls, hufMetadata+4, uint64(128), nextHuf, *(*uint32)(unsafe.Pointer(bp)), huffLog, nodeWksp, nodeWkspSize)
	/* Check against repeating the previous CTable */
	if repeat != int32(HUF_repeat_none) {
		oldCSize = HUF_estimateCompressedSize(tls, prevHuf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
		if oldCSize < srcSize && (oldCSize <= hSize+newCSize || hSize+uint64(12) >= srcSize) {
			libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
			(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_repeat)
			return uint64(0)
		}
	}
	if newCSize+hSize >= srcSize {
		libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_compressed)
	(*ZSTD_hufCTables_t)(unsafe.Pointer(nextHuf)).FrepeatMode = int32(HUF_repeat_check)
	return hSize
	return r
}

// C documentation
//
//	/* ZSTD_buildDummySequencesStatistics():
//	 * Returns a ZSTD_symbolEncodingTypeStats_t with all encoding types as set_basic,
//	 * and updates nextEntropy to the appropriate repeatMode.
//	 */
func ZSTD_buildDummySequencesStatistics(tls *libc.TLS, nextEntropy uintptr) (r ZSTD_symbolEncodingTypeStats_t) {
	var stats ZSTD_symbolEncodingTypeStats_t
	_ = stats
	stats = ZSTD_symbolEncodingTypeStats_t{}
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Flitlength_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Foffcode_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Fmatchlength_repeatMode = int32(FSE_repeat_none)
	return stats
}

// C documentation
//
//	/** ZSTD_buildBlockEntropyStats_sequences() :
//	 *  Builds entropy for the sequences.
//	 *  Stores symbol compression modes and fse table to fseMetadata.
//	 *  Requires ENTROPY_WORKSPACE_SIZE wksp.
//	 * @return : size of fse tables or error code */
func ZSTD_buildBlockEntropyStats_sequences(tls *libc.TLS, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, fseMetadata uintptr, workspace uintptr, wkspSize size_t) (r size_t) {
	var countWorkspace, entropyWorkspace, oend, op, ostart uintptr
	var entropyWorkspaceSize, err_code, nbSeq size_t
	var stats, v1 ZSTD_symbolEncodingTypeStats_t
	var strategy ZSTD_strategy
	_, _, _, _, _, _, _, _, _, _, _ = countWorkspace, entropyWorkspace, entropyWorkspaceSize, err_code, nbSeq, oend, op, ostart, stats, strategy, v1
	strategy = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy
	nbSeq = uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	ostart = fseMetadata + 12
	oend = ostart + uintptr(133)
	op = ostart
	countWorkspace = workspace
	entropyWorkspace = countWorkspace + uintptr(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*4
	entropyWorkspaceSize = wkspSize - uint64(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)
	if nbSeq != uint64(0) {
		v1 = ZSTD_buildSequencesStatistics(tls, seqStorePtr, nbSeq, prevEntropy, nextEntropy, op, oend, strategy, countWorkspace, entropyWorkspace, entropyWorkspaceSize)
	} else {
		v1 = ZSTD_buildDummySequencesStatistics(tls, nextEntropy)
	}
	stats = v1
	err_code = stats.Fsize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2837, 0)
		}
		return err_code
	}
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType = int32(stats.FLLtype)
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType = int32(stats.FOfftype)
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType = int32(stats.FMLtype)
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FlastCountSize = stats.FlastCountSize
	return stats.Fsize
}

// C documentation
//
//	/** ZSTD_buildBlockEntropyStats() :
//	 *  Builds entropy for the block.
//	 *  Requires workspace size ENTROPY_WORKSPACE_SIZE
//	 * @return : 0 on success, or an error code
//	 *  Note : also employed in superblock
//	 */
func ZSTD_buildBlockEntropyStats(tls *libc.TLS, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, entropyMetadata uintptr, workspace uintptr, wkspSize size_t) (r size_t) {
	var err_code, err_code1, litSize size_t
	var hufFlags, huf_useOptDepth, v1 int32
	_, _, _, _, _, _ = err_code, err_code1, hufFlags, huf_useOptDepth, litSize, v1
	litSize = uint64(int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart))
	huf_useOptDepth = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy >= int32(ZSTD_btultra))
	if huf_useOptDepth != 0 {
		v1 = int32(HUF_flags_optimalDepth)
	} else {
		v1 = 0
	}
	hufFlags = v1
	(*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhufDesSize = ZSTD_buildBlockEntropyStats_literals(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart, litSize, prevEntropy, nextEntropy, entropyMetadata, ZSTD_literalsCompressionIsDisabled(tls, cctxParams), workspace, wkspSize, hufFlags)
	err_code = (*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhufDesSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3442, 0)
		}
		return err_code
	}
	(*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FfseMetadata.FfseTablesSize = ZSTD_buildBlockEntropyStats_sequences(tls, seqStorePtr, prevEntropy+2064, nextEntropy+2064, cctxParams, entropyMetadata+144, workspace, wkspSize)
	err_code1 = (*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FfseMetadata.FfseTablesSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3486, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/* Returns the size estimate for the literals section (header + content) of a block */
func ZSTD_estimateBlockSize_literal(tls *libc.TLS, literals uintptr, litSize size_t, huf uintptr, hufMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cLitSizeEstimate, largest, literalSectionHeaderSize size_t
	var countWksp uintptr
	var singleStream U32
	var _ /* maxSymbolValue at bp+0 */ uint32
	_, _, _, _, _ = cLitSizeEstimate, countWksp, largest, literalSectionHeaderSize, singleStream
	countWksp = workspace
	*(*uint32)(unsafe.Pointer(bp)) = uint32(HUF_SYMBOLVALUE_MAX)
	literalSectionHeaderSize = uint64(int32(3) + libc.BoolInt32(litSize >= uint64(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(litSize >= uint64(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	singleStream = libc.BoolUint32(litSize < uint64(256))
	if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_basic) {
		return litSize
	} else {
		if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_rle) {
			return uint64(1)
		} else {
			if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_compressed) || (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_repeat) {
				largest = HIST_count_wksp(tls, countWksp, bp, literals, litSize, workspace, wkspSize)
				if ZSTD_isError(tls, largest) != 0 {
					return litSize
				}
				cLitSizeEstimate = HUF_estimateCompressedSize(tls, huf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
				if writeEntropy != 0 {
					cLitSizeEstimate = cLitSizeEstimate + (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize
				}
				if !(singleStream != 0) {
					cLitSizeEstimate = cLitSizeEstimate + uint64(6)
				} /* multi-stream huffman uses 6-byte jump table */
				return cLitSizeEstimate + literalSectionHeaderSize
			}
		}
	}
	/* impossible */
	return uint64(0)
}

// C documentation
//
//	/* Returns the size estimate for the FSE-compressed symbols (of, ml, ll) of a block */
func ZSTD_estimateBlockSize_symbolType(tls *libc.TLS, type1 SymbolEncodingType_e, codeTable uintptr, nbSeq size_t, maxCode uint32, fseCTable uintptr, additionalBits uintptr, defaultNorm uintptr, defaultNormLog U32, defaultMax U32, workspace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cSymbolTypeSizeEstimateInBits size_t
	var countWksp, ctEnd, ctStart, ctp uintptr
	var _ /* max at bp+0 */ uint32
	_, _, _, _, _ = cSymbolTypeSizeEstimateInBits, countWksp, ctEnd, ctStart, ctp
	countWksp = workspace
	ctp = codeTable
	ctStart = ctp
	ctEnd = ctStart + uintptr(nbSeq)
	cSymbolTypeSizeEstimateInBits = uint64(0)
	*(*uint32)(unsafe.Pointer(bp)) = maxCode
	HIST_countFast_wksp(tls, countWksp, bp, codeTable, nbSeq, workspace, wkspSize) /* can't fail */
	if type1 == int32(set_basic) {
		/* We selected this encoding type, so it must be valid. */
		_ = defaultMax
		cSymbolTypeSizeEstimateInBits = ZSTD_crossEntropyCost(tls, defaultNorm, defaultNormLog, countWksp, *(*uint32)(unsafe.Pointer(bp)))
	} else {
		if type1 == int32(set_rle) {
			cSymbolTypeSizeEstimateInBits = uint64(0)
		} else {
			if type1 == int32(set_compressed) || type1 == int32(set_repeat) {
				cSymbolTypeSizeEstimateInBits = ZSTD_fseBitCost(tls, fseCTable, countWksp, *(*uint32)(unsafe.Pointer(bp)))
			}
		}
	}
	if ZSTD_isError(tls, cSymbolTypeSizeEstimateInBits) != 0 {
		return nbSeq * uint64(10)
	}
	for ctp < ctEnd {
		if additionalBits != 0 {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*U8)(unsafe.Pointer(additionalBits + uintptr(*(*BYTE)(unsafe.Pointer(ctp))))))
		} else {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*BYTE)(unsafe.Pointer(ctp)))
		} /* for offset, offset code is also the number of additional bits */
		ctp = ctp + 1
	}
	return cSymbolTypeSizeEstimateInBits >> int32(3)
}

// C documentation
//
//	/* Returns the size estimate for the sequences section (header + content) of a block */
func ZSTD_estimateBlockSize_sequences(tls *libc.TLS, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, fseTables uintptr, fseMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	var cSeqSizeEstimate, sequencesSectionHeaderSize size_t
	_, _ = cSeqSizeEstimate, sequencesSectionHeaderSize
	sequencesSectionHeaderSize = uint64(libc.Int32FromInt32(1) + libc.Int32FromInt32(1) + libc.BoolInt32(nbSeq >= uint64(128)) + libc.BoolInt32(nbSeq >= uint64(LONGNBSEQ)))
	cSeqSizeEstimate = uint64(0)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType, ofCodeTable, nbSeq, uint32(MaxOff), fseTables, libc.UintptrFromInt32(0), uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, uint32(DefaultMaxOff), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType, llCodeTable, nbSeq, uint32(MaxLL), fseTables+2224, uintptr(unsafe.Pointer(&LL_bits)), uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, uint32(MaxLL), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType, mlCodeTable, nbSeq, uint32(MaxML), fseTables+772, uintptr(unsafe.Pointer(&ML_bits)), uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, uint32(MaxML), workspace, wkspSize)
	if writeEntropy != 0 {
		cSeqSizeEstimate = cSeqSizeEstimate + (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize
	}
	return cSeqSizeEstimate + sequencesSectionHeaderSize
}

// C documentation
//
//	/* Returns the size estimate for a given stream of literals, of, ll, ml */
func ZSTD_estimateBlockSize(tls *libc.TLS, literals uintptr, litSize size_t, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, entropy uintptr, entropyMetadata uintptr, workspace uintptr, wkspSize size_t, writeLitEntropy int32, writeSeqEntropy int32) (r size_t) {
	var literalsSize, seqSize size_t
	_, _ = literalsSize, seqSize
	literalsSize = ZSTD_estimateBlockSize_literal(tls, literals, litSize, entropy, entropyMetadata, workspace, wkspSize, writeLitEntropy)
	seqSize = ZSTD_estimateBlockSize_sequences(tls, ofCodeTable, llCodeTable, mlCodeTable, nbSeq, entropy+2064, entropyMetadata+144, workspace, wkspSize, writeSeqEntropy)
	return seqSize + literalsSize + ZSTD_blockHeaderSize
}

// C documentation
//
//	/* Builds entropy statistics and uses them for blocksize estimation.
//	 *
//	 * @return: estimated compressed size of the seqStore, or a zstd error.
//	 */
func ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls *libc.TLS, seqStore uintptr, zc uintptr) (r size_t) {
	var entropyMetadata uintptr
	var err_code size_t
	_, _ = entropyMetadata, err_code
	entropyMetadata = zc + 3768 + 1184
	err_code = ZSTD_buildBlockEntropyStats(tls, seqStore, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, entropyMetadata, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_estimateBlockSize(tls, (*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart, uint64(int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Flit)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart)), (*SeqStore_t)(unsafe.Pointer(seqStore)).FofCode, (*SeqStore_t)(unsafe.Pointer(seqStore)).FllCode, (*SeqStore_t)(unsafe.Pointer(seqStore)).FmlCode, uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart))/8), (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, entropyMetadata, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize, libc.BoolInt32((*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhType == int32(set_compressed)), int32(1))
}

// C documentation
//
//	/* Returns literals bytes represented in a seqStore */
func ZSTD_countSeqStoreLiteralsBytes(tls *libc.TLS, seqStore uintptr) (r size_t) {
	var i, literalsBytes, nbSeqs size_t
	var seq SeqDef
	_, _, _, _ = i, literalsBytes, nbSeqs, seq
	literalsBytes = uint64(0)
	nbSeqs = uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart)) / 8)
	i = uint64(0)
	for {
		if !(i < nbSeqs) {
			break
		}
		seq = *(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart + uintptr(i)*8))
		literalsBytes = literalsBytes + uint64(seq.FlitLength)
		if i == uint64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos) && (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
			literalsBytes = literalsBytes + uint64(0x10000)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return literalsBytes
}

// C documentation
//
//	/* Returns match bytes represented in a seqStore */
func ZSTD_countSeqStoreMatchBytes(tls *libc.TLS, seqStore uintptr) (r size_t) {
	var i, matchBytes, nbSeqs size_t
	var seq SeqDef
	_, _, _, _ = i, matchBytes, nbSeqs, seq
	matchBytes = uint64(0)
	nbSeqs = uint64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart)) / 8)
	i = uint64(0)
	for {
		if !(i < nbSeqs) {
			break
		}
		seq = *(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart + uintptr(i)*8))
		matchBytes = matchBytes + uint64(int32(seq.FmlBase)+int32(MINMATCH))
		if i == uint64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos) && (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_matchLength) {
			matchBytes = matchBytes + uint64(0x10000)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return matchBytes
}

// C documentation
//
//	/* Derives the seqStore that is a chunk of the originalSeqStore from [startIdx, endIdx).
//	 * Stores the result in resultSeqStore.
//	 */
func ZSTD_deriveSeqStoreChunk(tls *libc.TLS, resultSeqStore uintptr, originalSeqStore uintptr, startIdx size_t, endIdx size_t) {
	var literalsBytes size_t
	_ = literalsBytes
	*(*SeqStore_t)(unsafe.Pointer(resultSeqStore)) = *(*SeqStore_t)(unsafe.Pointer(originalSeqStore))
	if startIdx > uint64(0) {
		(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).Fsequences = (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart + uintptr(startIdx)*8
		*(*uintptr)(unsafe.Pointer(resultSeqStore + 16)) += uintptr(ZSTD_countSeqStoreLiteralsBytes(tls, resultSeqStore))
	}
	/* Move longLengthPos into the correct position if necessary */
	if (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FlongLengthType != int32(ZSTD_llt_none) {
		if uint64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FlongLengthPos) < startIdx || uint64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FlongLengthPos) > endIdx {
			(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).FlongLengthType = int32(ZSTD_llt_none)
		} else {
			*(*U32)(unsafe.Pointer(resultSeqStore + 76)) -= uint32(startIdx)
		}
	}
	(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).FsequencesStart = (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart + uintptr(startIdx)*8
	(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).Fsequences = (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart + uintptr(endIdx)*8
	if endIdx == uint64((int64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).Fsequences)-int64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart))/8) {
		/* This accounts for possible last literals if the derived chunk reaches the end of the block */
	} else {
		literalsBytes = ZSTD_countSeqStoreLiteralsBytes(tls, resultSeqStore)
		(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).Flit = (*SeqStore_t)(unsafe.Pointer(resultSeqStore)).FlitStart + uintptr(literalsBytes)
	}
	*(*uintptr)(unsafe.Pointer(resultSeqStore + 32)) += uintptr(startIdx)
	*(*uintptr)(unsafe.Pointer(resultSeqStore + 40)) += uintptr(startIdx)
	*(*uintptr)(unsafe.Pointer(resultSeqStore + 48)) += uintptr(startIdx)
}

// C documentation
//
//	/**
//	 * Returns the raw offset represented by the combination of offBase, ll0, and repcode history.
//	 * offBase must represent a repcode in the numeric representation of ZSTD_storeSeq().
//	 */
func ZSTD_resolveRepcodeToRawOffset(tls *libc.TLS, rep uintptr, offBase U32, ll0 U32) (r U32) {
	var adjustedRepCode U32
	_ = adjustedRepCode
	adjustedRepCode = offBase - uint32(1) + ll0 /* [ 0 - 3 ] */
	if adjustedRepCode == uint32(ZSTD_REP_NUM) {
		/* litlength == 0 and offCode == 2 implies selection of first repcode - 1
		 * This is only valid if it results in a valid offset value, aka > 0.
		 * Note : it may happen that `rep[0]==1` in exceptional circumstances.
		 * In which case this function will return 0, which is an invalid offset.
		 * It's not an issue though, since this value will be
		 * compared and discarded within ZSTD_seqStore_resolveOffCodes().
		 */
		return *(*U32)(unsafe.Pointer(rep)) - uint32(1)
	}
	return *(*U32)(unsafe.Pointer(rep + uintptr(adjustedRepCode)*4))
}

// C documentation
//
//	/**
//	 * ZSTD_seqStore_resolveOffCodes() reconciles any possible divergences in offset history that may arise
//	 * due to emission of RLE/raw blocks that disturb the offset history,
//	 * and replaces any repcodes within the seqStore that may be invalid.
//	 *
//	 * dRepcodes are updated as would be on the decompression side.
//	 * cRepcodes are updated exactly in accordance with the seqStore.
//	 *
//	 * Note : this function assumes seq->offBase respects the following numbering scheme :
//	 *        0 : invalid
//	 *        1-3 : repcode 1-3
//	 *        4+ : real_offset+3
//	 */
func ZSTD_seqStore_resolveOffCodes(tls *libc.TLS, dRepcodes uintptr, cRepcodes uintptr, seqStore uintptr, nbSeq U32) {
	var cRawOffset, dRawOffset, idx, ll0, longLitLenIdx, offBase U32
	var seq uintptr
	var v1 uint32
	_, _, _, _, _, _, _, _ = cRawOffset, dRawOffset, idx, ll0, longLitLenIdx, offBase, seq, v1
	idx = uint32(0)
	if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
		v1 = (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos
	} else {
		v1 = nbSeq
	}
	longLitLenIdx = v1
	for {
		if !(idx < nbSeq) {
			break
		}
		seq = (*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart + uintptr(idx)*8
		ll0 = libc.BoolUint32(int32((*SeqDef)(unsafe.Pointer(seq)).FlitLength) == 0 && idx != longLitLenIdx)
		offBase = (*SeqDef)(unsafe.Pointer(seq)).FoffBase
		if uint32(1) <= offBase && offBase <= uint32(ZSTD_REP_NUM) {
			dRawOffset = ZSTD_resolveRepcodeToRawOffset(tls, dRepcodes, offBase, ll0)
			cRawOffset = ZSTD_resolveRepcodeToRawOffset(tls, cRepcodes, offBase, ll0)
			/* Adjust simulated decompression repcode history if we come across a mismatch. Replace
			 * the repcode with the offset it actually references, determined by the compression
			 * repcode history.
			 */
			if dRawOffset != cRawOffset {
				(*SeqDef)(unsafe.Pointer(seq)).FoffBase = cRawOffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
			}
		}
		/* Compression repcode history is always updated with values directly from the unmodified seqStore.
		 * Decompression repcode history may use modified seq->offset value taken from compression repcode history.
		 */
		ZSTD_updateRep(tls, dRepcodes, (*SeqDef)(unsafe.Pointer(seq)).FoffBase, ll0)
		ZSTD_updateRep(tls, cRepcodes, offBase, ll0)
		goto _2
	_2:
		;
		idx = idx + 1
	}
}

// C documentation
//
//	/* ZSTD_compressSeqStore_singleBlock():
//	 * Compresses a seqStore into a block with a block header, into the buffer dst.
//	 *
//	 * Returns the total size of that block (including header) or a ZSTD error code.
//	 */
func ZSTD_compressSeqStore_singleBlock(tls *libc.TLS, zc uintptr, seqStore uintptr, dRep uintptr, cRep uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32, isPartition U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cSeqsSize, cSize, err_code, err_code1, err_code2, err_code3 size_t
	var ip, op uintptr
	var rleMaxLength U32
	var _ /* dRepOriginal at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _ = cSeqsSize, cSize, err_code, err_code1, err_code2, err_code3, ip, op, rleMaxLength
	rleMaxLength = uint32(25)
	op = dst
	ip = src
	/* In case of an RLE or raw block, the simulated decompression repcode history must be reset */
	*(*Repcodes_t)(unsafe.Pointer(bp)) = *(*Repcodes_t)(unsafe.Pointer(dRep))
	if isPartition != 0 {
		ZSTD_seqStore_resolveOffCodes(tls, dRep, cRep, seqStore, uint32((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart))/8))
	}
	if dstCapacity < ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3531, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	cSeqsSize = ZSTD_entropyCompressSeqStore(tls, seqStore, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).Fbmi2)
	err_code = cSeqsSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3556, 0)
		}
		return err_code
	}
	if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock != 0) && cSeqsSize < uint64(rleMaxLength) && ZSTD_isRLE(tls, src, srcSize) != 0 {
		/* We don't want to emit our first block as a RLE even if it qualifies because
		 * doing so will cause the decoder (cli only) to throw a "should consume all input error."
		 * This is only an issue for zstd <= v1.4.3
		 */
		cSeqsSize = uint64(1)
	}
	/* Sequence collection not supported when block splitting */
	if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
		err_code1 = ZSTD_copyBlockSequences(tls, zc+936, seqStore, bp)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3593, 0)
			}
			return err_code1
		}
		ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
		return uint64(0)
	}
	if cSeqsSize == uint64(0) {
		cSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, srcSize, lastBlock)
		err_code2 = cSize
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3619, 0)
			}
			return err_code2
		}
		*(*Repcodes_t)(unsafe.Pointer(dRep)) = *(*Repcodes_t)(unsafe.Pointer(bp)) /* reset simulated decompression repcode history */
	} else {
		if cSeqsSize == uint64(1) {
			cSize = ZSTD_rleCompressBlock(tls, op, dstCapacity, *(*BYTE)(unsafe.Pointer(ip)), srcSize, lastBlock)
			err_code3 = cSize
			if ERR_isError(tls, err_code3) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3643, 0)
				}
				return err_code3
			}
			*(*Repcodes_t)(unsafe.Pointer(dRep)) = *(*Repcodes_t)(unsafe.Pointer(bp)) /* reset simulated decompression repcode history */
		} else {
			ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
			writeBlockHeader(tls, op, cSeqsSize, srcSize, lastBlock)
			cSize = ZSTD_blockHeaderSize + cSeqsSize
		}
	}
	if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
	}
	return cSize
}

// C documentation
//
//	/* Struct to keep track of where we are in our recursive calls. */
type seqStoreSplits = struct {
	FsplitLocations uintptr
	Fidx            size_t
}

// C documentation
//
//	/* Helper function to perform the recursive search for block splits.
//	 * Estimates the cost of seqStore prior to split, and estimates the cost of splitting the sequences in half.
//	 * If advantageous to split, then we recurse down the two sub-blocks.
//	 * If not, or if an error occurred in estimation, then we do not recurse.
//	 *
//	 * Note: The recursion depth is capped by a heuristic minimum number of sequences,
//	 * defined by MIN_SEQUENCES_BLOCK_SPLITTING.
//	 * In theory, this means the absolute largest recursion depth is 10 == log2(maxNbSeqInBlock/MIN_SEQUENCES_BLOCK_SPLITTING).
//	 * In practice, recursion depth usually doesn't go beyond 4.
//	 *
//	 * Furthermore, the number of splits is capped by ZSTD_MAX_NB_BLOCK_SPLITS.
//	 * At ZSTD_MAX_NB_BLOCK_SPLITS == 196 with the current existing blockSize
//	 * maximum of 128 KB, this value is actually impossible to reach.
//	 */
func ZSTD_deriveBlockSplitsHelper(tls *libc.TLS, splits uintptr, startIdx size_t, endIdx size_t, zc uintptr, origSeqStore uintptr) {
	var estimatedFirstHalfSize, estimatedOriginalSize, estimatedSecondHalfSize, midIdx size_t
	var firstHalfSeqStore, fullSeqStoreChunk, secondHalfSeqStore uintptr
	_, _, _, _, _, _, _ = estimatedFirstHalfSize, estimatedOriginalSize, estimatedSecondHalfSize, firstHalfSeqStore, fullSeqStoreChunk, midIdx, secondHalfSeqStore
	fullSeqStoreChunk = zc + 3768
	firstHalfSeqStore = zc + 3768 + 80
	secondHalfSeqStore = zc + 3768 + 160
	midIdx = (startIdx + endIdx) / uint64(2)
	if endIdx-startIdx < uint64(MIN_SEQUENCES_BLOCK_SPLITTING) || (*seqStoreSplits)(unsafe.Pointer(splits)).Fidx >= uint64(ZSTD_MAX_NB_BLOCK_SPLITS) {
		return
	}
	ZSTD_deriveSeqStoreChunk(tls, fullSeqStoreChunk, origSeqStore, startIdx, endIdx)
	ZSTD_deriveSeqStoreChunk(tls, firstHalfSeqStore, origSeqStore, startIdx, midIdx)
	ZSTD_deriveSeqStoreChunk(tls, secondHalfSeqStore, origSeqStore, midIdx, endIdx)
	estimatedOriginalSize = ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls, fullSeqStoreChunk, zc)
	estimatedFirstHalfSize = ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls, firstHalfSeqStore, zc)
	estimatedSecondHalfSize = ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls, secondHalfSeqStore, zc)
	if ZSTD_isError(tls, estimatedOriginalSize) != 0 || ZSTD_isError(tls, estimatedFirstHalfSize) != 0 || ZSTD_isError(tls, estimatedSecondHalfSize) != 0 {
		return
	}
	if estimatedFirstHalfSize+estimatedSecondHalfSize < estimatedOriginalSize {
		ZSTD_deriveBlockSplitsHelper(tls, splits, startIdx, midIdx, zc, origSeqStore)
		*(*U32)(unsafe.Pointer((*seqStoreSplits)(unsafe.Pointer(splits)).FsplitLocations + uintptr((*seqStoreSplits)(unsafe.Pointer(splits)).Fidx)*4)) = uint32(midIdx)
		(*seqStoreSplits)(unsafe.Pointer(splits)).Fidx = (*seqStoreSplits)(unsafe.Pointer(splits)).Fidx + 1
		ZSTD_deriveBlockSplitsHelper(tls, splits, midIdx, endIdx, zc, origSeqStore)
	}
}

// C documentation
//
//	/* Base recursive function.
//	 * Populates a table with intra-block partition indices that can improve compression ratio.
//	 *
//	 * @return: number of splits made (which equals the size of the partition table - 1).
//	 */
func ZSTD_deriveBlockSplits(tls *libc.TLS, zc uintptr, partitions uintptr, nbSeq U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* splits at bp+0 */ seqStoreSplits
	(*(*seqStoreSplits)(unsafe.Pointer(bp))).FsplitLocations = partitions
	(*(*seqStoreSplits)(unsafe.Pointer(bp))).Fidx = uint64(0)
	if nbSeq <= uint32(4) {
		/* Refuse to try and split anything with less than 4 sequences */
		return uint64(0)
	}
	ZSTD_deriveBlockSplitsHelper(tls, bp, uint64(0), uint64(nbSeq), zc, zc+976)
	*(*U32)(unsafe.Pointer((*(*seqStoreSplits)(unsafe.Pointer(bp))).FsplitLocations + uintptr((*(*seqStoreSplits)(unsafe.Pointer(bp))).Fidx)*4)) = nbSeq
	return (*(*seqStoreSplits)(unsafe.Pointer(bp))).Fidx
}

// C documentation
//
//	/* ZSTD_compressBlock_splitBlock():
//	 * Attempts to split a given block into multiple blocks to improve compression ratio.
//	 *
//	 * Returns combined size of all blocks (which includes headers), or a ZSTD error code.
//	 */
func ZSTD_compressBlock_splitBlock_internal(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, blockSize size_t, lastBlock U32, nbSeq U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cSize, cSizeChunk, cSizeSingleBlock, err_code, err_code1, i, numSplits, srcBytes, srcBytesTotal size_t
	var currSeqStore, ip, nextSeqStore, op, partitions uintptr
	var lastBlockEntireSrc, lastPartition U32
	var _ /* cRep at bp+12 */ Repcodes_t
	var _ /* dRep at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cSize, cSizeChunk, cSizeSingleBlock, currSeqStore, err_code, err_code1, i, ip, lastBlockEntireSrc, lastPartition, nextSeqStore, numSplits, op, partitions, srcBytes, srcBytesTotal
	cSize = uint64(0)
	ip = src
	op = dst
	i = uint64(0)
	srcBytesTotal = uint64(0)
	partitions = zc + 3768 + 400 /* size == ZSTD_MAX_NB_BLOCK_SPLITS */
	nextSeqStore = zc + 3768 + 320
	currSeqStore = zc + 3768 + 240
	numSplits = ZSTD_deriveBlockSplits(tls, zc, partitions, nbSeq)
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	libc.Xmemcpy(tls, bp+12, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	libc.Xmemset(tls, nextSeqStore, 0, libc.Uint64FromInt64(80))
	if numSplits == uint64(0) {
		cSizeSingleBlock = ZSTD_compressSeqStore_singleBlock(tls, zc, zc+976, bp, bp+12, op, dstCapacity, ip, blockSize, lastBlock, uint32(0))
		err_code = cSizeSingleBlock
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3669, 0)
			}
			return err_code
		}
		return cSizeSingleBlock
	}
	ZSTD_deriveSeqStoreChunk(tls, currSeqStore, zc+976, uint64(0), uint64(*(*U32)(unsafe.Pointer(partitions))))
	i = uint64(0)
	for {
		if !(i <= numSplits) {
			break
		}
		lastPartition = libc.BoolUint32(i == numSplits)
		lastBlockEntireSrc = uint32(0)
		srcBytes = ZSTD_countSeqStoreLiteralsBytes(tls, currSeqStore) + ZSTD_countSeqStoreMatchBytes(tls, currSeqStore)
		srcBytesTotal = srcBytesTotal + srcBytes
		if lastPartition != 0 {
			/* This is the final partition, need to account for possible last literals */
			srcBytes = srcBytes + (blockSize - srcBytesTotal)
			lastBlockEntireSrc = lastBlock
		} else {
			ZSTD_deriveSeqStoreChunk(tls, nextSeqStore, zc+976, uint64(*(*U32)(unsafe.Pointer(partitions + uintptr(i)*4))), uint64(*(*U32)(unsafe.Pointer(partitions + uintptr(i+uint64(1))*4))))
		}
		cSizeChunk = ZSTD_compressSeqStore_singleBlock(tls, zc, currSeqStore, bp, bp+12, op, dstCapacity, ip, srcBytes, lastBlockEntireSrc, uint32(1))
		err_code1 = cSizeChunk
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3729, 0)
			}
			return err_code1
		}
		ip = ip + uintptr(srcBytes)
		op = op + uintptr(cSizeChunk)
		dstCapacity = dstCapacity - cSizeChunk
		cSize = cSize + cSizeChunk
		*(*SeqStore_t)(unsafe.Pointer(currSeqStore)) = *(*SeqStore_t)(unsafe.Pointer(nextSeqStore))
		goto _1
	_1:
		;
		i = i + 1
	}
	/* cRep and dRep may have diverged during the compression.
	 * If so, we use the dRep repcodes for the next block.
	 */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616, bp, libc.Uint64FromInt64(12))
	return cSize
}

func ZSTD_compressBlock_splitBlock(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32) (r size_t) {
	var bss, cSize, err_code, err_code1, err_code2 size_t
	var nbSeq U32
	_, _, _, _, _, _ = bss, cSize, err_code, err_code1, err_code2, nbSeq
	bss = ZSTD_buildSeqStore(tls, zc, src, srcSize)
	err_code = bss
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3755, 0)
		}
		return err_code
	}
	if bss == uint64(ZSTDbss_noCompress) {
		if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
			(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3781, 0)
			}
			return uint64(-int32(ZSTD_error_sequenceProducer_failed))
		}
		cSize = ZSTD_noCompressBlock(tls, dst, dstCapacity, src, srcSize, lastBlock)
		err_code1 = cSize
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1741, 0)
			}
			return err_code1
		}
		return cSize
	}
	nbSeq = uint32((int64((*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.Fsequences) - int64((*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FsequencesStart)) / 8)
	cSize = ZSTD_compressBlock_splitBlock_internal(tls, zc, dst, dstCapacity, src, srcSize, lastBlock, nbSeq)
	err_code2 = cSize
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3802, 0)
		}
		return err_code2
	}
	return cSize
}

func ZSTD_compressBlock_internal(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, frame U32) (r size_t) {
	var bss, cSize, err_code, err_code1 size_t
	var ip, op uintptr
	var rleMaxLength U32
	_, _, _, _, _, _, _ = bss, cSize, err_code, err_code1, ip, op, rleMaxLength
	/* This is an estimated upper bound for the length of an rle block.
	 * This isn't the actual upper bound.
	 * Finding the real threshold needs further investigation.
	 */
	rleMaxLength = uint32(25)
	ip = src
	op = dst
	bss = ZSTD_buildSeqStore(tls, zc, src, srcSize)
	err_code = bss
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3755, 0)
		}
		return err_code
	}
	if bss == uint64(ZSTDbss_noCompress) {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3781, 0)
			}
			return uint64(-int32(ZSTD_error_sequenceProducer_failed))
		}
		cSize = uint64(0)
		goto out
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
		err_code1 = ZSTD_copyBlockSequences(tls, zc+936, ZSTD_getSeqStore(tls, zc), (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3593, 0)
			}
			return err_code1
		}
		ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
		return uint64(0)
	}
	/* encode sequences and literals */
	cSize = ZSTD_entropyCompressSeqStore(tls, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, dst, dstCapacity, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).Fbmi2)
	if frame != 0 && !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock != 0) && cSize < uint64(rleMaxLength) && ZSTD_isRLE(tls, ip, srcSize) != 0 {
		cSize = uint64(1)
		*(*BYTE)(unsafe.Pointer(op)) = *(*BYTE)(unsafe.Pointer(ip))
	}
	goto out
out:
	;
	if !(ZSTD_isError(tls, cSize) != 0) && cSize > uint64(1) {
		ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
	}
	/* We check that dictionaries have offset codes available for the first
	 * block. After the first block, the offcode table might not have large
	 * enough codes to represent the offsets in the data.
	 */
	if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
	}
	return cSize
}

func ZSTD_compressBlock_targetCBlockSize_body(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, bss size_t, lastBlock U32) (r size_t) {
	var cSize, err_code, maxCSize size_t
	_, _, _ = cSize, err_code, maxCSize
	if bss == uint64(ZSTDbss_compress) {
		if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock != 0) && ZSTD_maybeRLE(tls, zc+976) != 0 && ZSTD_isRLE(tls, src, srcSize) != 0 {
			return ZSTD_rleCompressBlock(tls, dst, dstCapacity, *(*BYTE)(unsafe.Pointer(src)), srcSize, lastBlock)
		}
		/* Attempt superblock compression.
		 *
		 * Note that compressed size of ZSTD_compressSuperBlock() is not bound by the
		 * standard ZSTD_compressBound(). This is a problem, because even if we have
		 * space now, taking an extra byte now could cause us to run out of space later
		 * and violate ZSTD_compressBound().
		 *
		 * Define blockBound(blockSize) = blockSize + ZSTD_blockHeaderSize.
		 *
		 * In order to respect ZSTD_compressBound() we must attempt to emit a raw
		 * uncompressed block in these cases:
		 *   * cSize == 0: Return code for an uncompressed block.
		 *   * cSize == dstSize_tooSmall: We may have expanded beyond blockBound(srcSize).
		 *     ZSTD_noCompressBlock() will return dstSize_tooSmall if we are really out of
		 *     output space.
		 *   * cSize >= blockBound(srcSize): We have expanded the block too much so
		 *     emit an uncompressed block.
		 */
		cSize = ZSTD_compressSuperBlock(tls, zc, dst, dstCapacity, src, srcSize, lastBlock)
		if cSize != uint64(-int32(ZSTD_error_dstSize_tooSmall)) {
			maxCSize = srcSize - ZSTD_minGain(tls, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy)
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3827, 0)
				}
				return err_code
			}
			if cSize != uint64(0) && cSize < maxCSize+ZSTD_blockHeaderSize {
				ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
				return cSize
			}
		}
	} /* if (bss == ZSTDbss_compress)*/
	/* Superblock compression failed, attempt to emit a single no compress block.
	 * The decoder will be able to stream this block since it is uncompressed.
	 */
	return ZSTD_noCompressBlock(tls, dst, dstCapacity, src, srcSize, lastBlock)
}

func ZSTD_compressBlock_targetCBlockSize(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32) (r size_t) {
	var bss, cSize, err_code, err_code1 size_t
	_, _, _, _ = bss, cSize, err_code, err_code1
	cSize = uint64(0)
	bss = ZSTD_buildSeqStore(tls, zc, src, srcSize)
	err_code = bss
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3755, 0)
		}
		return err_code
	}
	cSize = ZSTD_compressBlock_targetCBlockSize_body(tls, zc, dst, dstCapacity, src, srcSize, bss, lastBlock)
	err_code1 = cSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3858, 0)
		}
		return err_code1
	}
	if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
	}
	return cSize
}

func ZSTD_overflowCorrectIfNeeded(tls *libc.TLS, ms uintptr, ws uintptr, params uintptr, ip uintptr, iend uintptr) {
	var correction, cycleLog, maxDist U32
	_, _, _ = correction, cycleLog, maxDist
	cycleLog = ZSTD_cycleLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy)
	maxDist = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
	if ZSTD_window_needOverflowCorrection(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow, cycleLog, maxDist, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd, ip, iend) != 0 {
		correction = ZSTD_window_correctOverflow(tls, ms, cycleLog, maxDist, ip)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		ZSTD_cwksp_mark_tables_dirty(tls, ws)
		ZSTD_reduceIndex(tls, ms, params, correction)
		ZSTD_cwksp_mark_tables_clean(tls, ws)
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate < correction {
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = uint32(0)
		} else {
			*(*U32)(unsafe.Pointer(ms + 44)) -= correction
		}
		/* invalidate dictionaries on overflow correction */
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd = uint32(0)
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState = libc.UintptrFromInt32(0)
	}
}

/**** skipping file: zstd_preSplit.h ****/

func ZSTD_optimalBlockSize(tls *libc.TLS, cctx uintptr, src uintptr, srcSize size_t, blockSizeMax size_t, splitLevel int32, strat ZSTD_strategy, savings S64) (r size_t) {
	var v1 uint64
	_ = v1
	/* note: conservatively only split full blocks (128 KB) currently.
	 * While it's possible to go lower, let's keep it simple for a first implementation.
	 * Besides, benefits of splitting are reduced when blocks are already small.
	 */
	if srcSize < uint64(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) || blockSizeMax < uint64(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) {
		if srcSize < blockSizeMax {
			v1 = srcSize
		} else {
			v1 = blockSizeMax
		}
		return v1
	}
	/* do not split incompressible data though:
	 * require verified savings to allow pre-splitting.
	 * Note: as a consequence, the first full block is not split.
	 */
	if savings < int64(3) {
		return uint64(libc.Int32FromInt32(128) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	/* apply @splitLevel, or use default value (which depends on @strat).
	 * note that splitting heuristic is still conditioned by @savings >= 3,
	 * so the first block will not reach this code path */
	if splitLevel == int32(1) {
		return uint64(libc.Int32FromInt32(128) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	if splitLevel == 0 {
		splitLevel = splitLevels[strat]
	} else {
		splitLevel = splitLevel - int32(2)
	}
	return ZSTD_splitBlock(tls, src, blockSizeMax, splitLevel, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize)
}

/* split level based on compression strategy, from `fast` to `btultra2` */
var splitLevels = [10]int32{
	2: int32(1),
	3: int32(2),
	4: int32(2),
	5: int32(3),
	6: int32(3),
	7: int32(4),
	8: int32(4),
	9: int32(4),
}

// C documentation
//
//	/*! ZSTD_compress_frameChunk() :
//	*   Compress a chunk of data into one or multiple blocks.
//	*   All blocks will be terminated, all input will be consumed.
//	*   Function will issue an error if there is not enough `dstCapacity` to hold the compressed content.
//	*   Frame is supposed already started (header already produced)
//	*  @return : compressed size, or an error code
//	*/
func ZSTD_compress_frameChunk(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastFrameChunk U32) (r size_t) {
	var blockSize, blockSizeMax, cSize, err_code, err_code1, err_code2, err_code3, remaining size_t
	var cBlockHeader, lastBlock, maxDist U32
	var ip, ms, op, ostart uintptr
	var savings S64
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, blockSizeMax, cBlockHeader, cSize, err_code, err_code1, err_code2, err_code3, ip, lastBlock, maxDist, ms, op, ostart, remaining, savings, v1
	blockSizeMax = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax
	remaining = srcSize
	ip = src
	ostart = dst
	op = ostart
	maxDist = libc.Uint32FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FwindowLog
	savings = int64((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize) - int64((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize)
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 && srcSize != 0 {
		XXH_INLINE_XXH64_update(tls, cctx+808, src, srcSize)
	}
	for remaining != 0 {
		ms = cctx + 3224 + 16
		blockSize = ZSTD_optimalBlockSize(tls, cctx, ip, remaining, blockSizeMax, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FpreBlockSplitter_level, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.Fstrategy, savings)
		lastBlock = lastFrameChunk & libc.BoolUint32(blockSize == remaining)
		/* TODO: See 3090. We reduced MIN_CBLOCK_SIZE from 3 to 2 so to compensate we are adding
		 * additional 1. We need to revisit and change this logic to be more consistent */
		if dstCapacity < ZSTD_blockHeaderSize+uint64(libc.Int32FromInt32(1)+libc.Int32FromInt32(1))+uint64(1) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3906, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		ZSTD_overflowCorrectIfNeeded(tls, ms, cctx+704, cctx+240, ip, ip+uintptr(blockSize))
		ZSTD_checkDictValidity(tls, ms, ip+uintptr(blockSize), maxDist, ms+40, ms+248)
		ZSTD_window_enforceMaxDist(tls, ms, ip, maxDist, ms+40, ms+248)
		/* Ensure hash/chain table insertion resumes no sooner than lowlimit */
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate < (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit {
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
		}
		if ZSTD_useTargetCBlockSize(tls, cctx+240) != 0 {
			cSize = ZSTD_compressBlock_targetCBlockSize(tls, cctx, op, dstCapacity, ip, blockSize, lastBlock)
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3949, 0)
				}
				return err_code
			}
		} else {
			if ZSTD_blockSplitterEnabled(tls, cctx+240) != 0 {
				cSize = ZSTD_compressBlock_splitBlock(tls, cctx, op, dstCapacity, ip, blockSize, lastBlock)
				err_code1 = cSize
				if ERR_isError(tls, err_code1) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+3992, 0)
					}
					return err_code1
				}
			} else {
				cSize = ZSTD_compressBlock_internal(tls, cctx, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, ip, blockSize, uint32(1))
				err_code2 = cSize
				if ERR_isError(tls, err_code2) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+4029, 0)
					}
					return err_code2
				}
				if cSize == uint64(0) { /* block is not compressible */
					cSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, blockSize, lastBlock)
					err_code3 = cSize
					if ERR_isError(tls, err_code3) != 0 {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+1741, 0)
						}
						return err_code3
					}
				} else {
					if cSize == uint64(1) {
						v1 = lastBlock + uint32(bt_rle)<<libc.Int32FromInt32(1) + uint32(blockSize<<libc.Int32FromInt32(3))
					} else {
						v1 = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(cSize<<libc.Int32FromInt32(3))
					}
					cBlockHeader = v1
					MEM_writeLE24(tls, op, cBlockHeader)
					cSize = cSize + ZSTD_blockHeaderSize
				}
			}
		} /* if (ZSTD_useTargetCBlockSize(&cctx->appliedParams))*/
		/* @savings is employed to ensure that splitting doesn't worsen expansion of incompressible data.
		 * Without splitting, the maximum expansion is 3 bytes per full block.
		 * An adversarial input could attempt to fudge the split detector,
		 * and make it split incompressible data, resulting in more block headers.
		 * Note that, since ZSTD_COMPRESSBOUND() assumes a worst case scenario of 1KB per block,
		 * and the splitter never creates blocks that small (current lower limit is 8 KB),
		 * there is already no risk to expand beyond ZSTD_COMPRESSBOUND() limit.
		 * But if the goal is to not expand by more than 3-bytes per 128 KB full block,
		 * then yes, it becomes possible to make the block splitter oversplit incompressible data.
		 * Using @savings, we enforce an even more conservative condition,
		 * requiring the presence of enough savings (at least 3 bytes) to authorize splitting,
		 * otherwise only full blocks are used.
		 * But being conservative is fine,
		 * since splitting barely compressible blocks is not fruitful anyway */
		savings = savings + (int64(blockSize) - int64(cSize))
		ip = ip + uintptr(blockSize)
		remaining = remaining - blockSize
		op = op + uintptr(cSize)
		dstCapacity = dstCapacity - cSize
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock = 0
	}
	if lastFrameChunk != 0 && op > ostart {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_ending)
	}
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_writeFrameHeader(tls *libc.TLS, dst uintptr, dstCapacity size_t, params uintptr, pledgedSrcSize U64, dictID U32) (r size_t) {
	var checksumFlag, dictIDSizeCode, dictIDSizeCodeLength, fcsCode, singleSegment, windowSize U32
	var frameHeaderDescriptionByte, windowLogByte BYTE
	var op uintptr
	var pos, v3 size_t
	var v1 uint32
	var v2 int32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = checksumFlag, dictIDSizeCode, dictIDSizeCodeLength, fcsCode, frameHeaderDescriptionByte, op, pos, singleSegment, windowLogByte, windowSize, v1, v2, v3
	op = dst
	dictIDSizeCodeLength = uint32(libc.BoolInt32(dictID > uint32(0)) + libc.BoolInt32(dictID >= uint32(256)) + libc.BoolInt32(dictID >= uint32(65536)))
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FnoDictIDFlag != 0 {
		v1 = uint32(0)
	} else {
		v1 = dictIDSizeCodeLength
	} /* 0-3 */
	dictIDSizeCode = v1 /* 0-3 */
	checksumFlag = libc.BoolUint32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FchecksumFlag > 0)
	windowSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
	singleSegment = libc.BoolUint32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FcontentSizeFlag != 0 && uint64(windowSize) >= pledgedSrcSize)
	windowLogByte = uint8(((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog - libc.Uint32FromInt32(ZSTD_WINDOWLOG_ABSOLUTEMIN)) << libc.Int32FromInt32(3))
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FcontentSizeFlag != 0 {
		v2 = libc.BoolInt32(pledgedSrcSize >= uint64(256)) + libc.BoolInt32(pledgedSrcSize >= uint64(libc.Int32FromInt32(65536)+libc.Int32FromInt32(256))) + libc.BoolInt32(pledgedSrcSize >= uint64(0xFFFFFFFF))
	} else {
		v2 = 0
	}
	fcsCode = uint32(v2) /* 0-3 */
	frameHeaderDescriptionByte = uint8(dictIDSizeCode + checksumFlag<<libc.Int32FromInt32(2) + singleSegment<<libc.Int32FromInt32(5) + fcsCode<<libc.Int32FromInt32(6))
	pos = uint64(0)
	if dstCapacity < uint64(ZSTD_FRAMEHEADERSIZE_MAX) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4064, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).Fformat == int32(ZSTD_f_zstd1) {
		MEM_writeLE32(tls, dst, uint32(ZSTD_MAGICNUMBER))
		pos = uint64(4)
	}
	v3 = pos
	pos = pos + 1
	*(*BYTE)(unsafe.Pointer(op + uintptr(v3))) = frameHeaderDescriptionByte
	if !(singleSegment != 0) {
		v3 = pos
		pos = pos + 1
		*(*BYTE)(unsafe.Pointer(op + uintptr(v3))) = windowLogByte
	}
	switch dictIDSizeCode {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
	case uint32(1):
		*(*BYTE)(unsafe.Pointer(op + uintptr(pos))) = uint8(dictID)
		pos = pos + 1
	case uint32(2):
		MEM_writeLE16(tls, op+uintptr(pos), uint16(dictID))
		pos = pos + uint64(2)
	case uint32(3):
		MEM_writeLE32(tls, op+uintptr(pos), dictID)
		pos = pos + uint64(4)
		break
	}
	switch fcsCode {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
		if singleSegment != 0 {
			v3 = pos
			pos = pos + 1
			*(*BYTE)(unsafe.Pointer(op + uintptr(v3))) = uint8(pledgedSrcSize)
		}
	case uint32(1):
		MEM_writeLE16(tls, op+uintptr(pos), uint16(pledgedSrcSize-libc.Uint64FromInt32(256)))
		pos = pos + uint64(2)
	case uint32(2):
		MEM_writeLE32(tls, op+uintptr(pos), uint32(pledgedSrcSize))
		pos = pos + uint64(4)
	case uint32(3):
		MEM_writeLE64(tls, op+uintptr(pos), pledgedSrcSize)
		pos = pos + uint64(8)
		break
	}
	return pos
}

// C documentation
//
//	/* ZSTD_writeSkippableFrame_advanced() :
//	 * Writes out a skippable frame with the specified magic number variant (16 are supported),
//	 * from ZSTD_MAGIC_SKIPPABLE_START to ZSTD_MAGIC_SKIPPABLE_START+15, and the desired source data.
//	 *
//	 * Returns the total number of bytes written, or a ZSTD error code.
//	 */
func ZSTD_writeSkippableFrame(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, magicVariant uint32) (r size_t) {
	var op uintptr
	_ = op
	op = dst
	if dstCapacity < srcSize+uint64(ZSTD_SKIPPABLEHEADERSIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4122, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if srcSize > uint64(libc.Uint32FromUint32(0xFFFFFFFF)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4158, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if magicVariant > uint32(15) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4197, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	MEM_writeLE32(tls, op, libc.Uint32FromInt32(ZSTD_MAGIC_SKIPPABLE_START)+magicVariant)
	MEM_writeLE32(tls, op+uintptr(4), uint32(srcSize))
	libc.Xmemcpy(tls, op+libc.UintptrFromInt32(8), src, srcSize)
	return srcSize + uint64(ZSTD_SKIPPABLEHEADERSIZE)
}

// C documentation
//
//	/* ZSTD_writeLastEmptyBlock() :
//	 * output an empty Block with end-of-frame mark to complete a frame
//	 * @return : size of data written into `dst` (== ZSTD_blockHeaderSize (defined in zstd_internal.h))
//	 *           or an error code if `dstCapacity` is too small (<ZSTD_blockHeaderSize)
//	 */
func ZSTD_writeLastEmptyBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t) (r size_t) {
	var cBlockHeader24 U32
	_ = cBlockHeader24
	if dstCapacity < ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4248, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1) /* 0 size */
	MEM_writeLE24(tls, dst, cBlockHeader24)
	return ZSTD_blockHeaderSize
	return r
}

func ZSTD_referenceExternalSequences(tls *libc.TLS, cctx uintptr, seq uintptr, nbSeq size_t) {
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fseq = seq
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fsize = nbSeq
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fcapacity = nbSeq
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fpos = uint64(0)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.FposInSequence = uint64(0)
}

func ZSTD_compressContinue_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, frame U32, lastFrameChunk U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cSize, err_code, err_code1, fhSize size_t
	var ms, v2 uintptr
	var v1 uint64
	_, _, _, _, _, _, _ = cSize, err_code, err_code1, fhSize, ms, v1, v2
	ms = cctx + 3224 + 16
	fhSize = uint64(0)
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_created) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4305, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	if frame != 0 && (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_init) {
		fhSize = ZSTD_writeFrameHeader(tls, dst, dstCapacity, cctx+240, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID)
		err_code = fhSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4339, 0)
			}
			return err_code
		}
		dstCapacity = dstCapacity - fhSize
		dst = dst + uintptr(fhSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_ongoing)
	}
	if !(srcSize != 0) {
		return fhSize
	} /* do not generate an empty block if no input */
	if !(ZSTD_window_update(tls, ms, src, srcSize, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FforceNonContiguous) != 0) {
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FforceNonContiguous = 0
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		ZSTD_window_update(tls, cctx+1056, src, srcSize, 0)
	}
	if !(frame != 0) {
		/* overflow check and correction for block mode */
		ZSTD_overflowCorrectIfNeeded(tls, ms, cctx+704, cctx+240, src, src+uintptr(srcSize))
	}
	if frame != 0 {
		v1 = ZSTD_compress_frameChunk(tls, cctx, dst, dstCapacity, src, srcSize, lastFrameChunk)
	} else {
		v1 = ZSTD_compressBlock_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(0))
	}
	cSize = v1
	err_code1 = cSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			if frame != 0 {
				v2 = __ccgo_ts + 4368
			} else {
				v2 = __ccgo_ts + 4029
			}
			_force_has_format_string(tls, __ccgo_ts+4400, libc.VaList(bp+8, v2))
		}
		return err_code1
	}
	*(*uint64)(unsafe.Pointer(cctx + 792)) += srcSize
	*(*uint64)(unsafe.Pointer(cctx + 800)) += cSize + fhSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne != uint64(0) { /* control src size */
		_ = libc.Uint64FromInt64(1)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize+uint64(1) > (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4403, libc.VaList(bp+8, uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne)-uint32(1), uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize)))
			}
			return uint64(-int32(ZSTD_error_srcSize_wrong))
		}
	}
	return cSize + fhSize
	return r
}

func ZSTD_compressContinue_public(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressContinue_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(1), uint32(0))
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressContinue_public() */
func ZSTD_compressContinue(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressContinue_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

func ZSTD_getBlockSize_deprecated(tls *libc.TLS, cctx uintptr) (r size_t) {
	var cParams ZSTD_compressionParameters
	var v1 uint64
	_, _ = cParams, v1
	cParams = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FmaxBlockSize < libc.Uint64FromInt32(1)<<cParams.FwindowLog {
		v1 = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FmaxBlockSize
	} else {
		v1 = libc.Uint64FromInt32(1) << cParams.FwindowLog
	}
	return v1
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_getBlockSize_deprecated() */
func ZSTD_getBlockSize(tls *libc.TLS, cctx uintptr) (r size_t) {
	return ZSTD_getBlockSize_deprecated(tls, cctx)
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressBlock_deprecated() */
func ZSTD_compressBlock_deprecated(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var blockSizeMax size_t
	_ = blockSizeMax
	blockSizeMax = ZSTD_getBlockSize_deprecated(tls, cctx)
	if srcSize > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4456, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	return ZSTD_compressContinue_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(0), uint32(0))
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressBlock_deprecated() */
func ZSTD_compressBlock(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_deprecated(tls, cctx, dst, dstCapacity, src, srcSize)
}

// C documentation
//
//	/*! ZSTD_loadDictionaryContent() :
//	 *  @return : 0, or an error code
//	 */
func ZSTD_loadDictionaryContent(tls *libc.TLS, ms uintptr, ls uintptr, ws uintptr, params uintptr, src uintptr, srcSize size_t, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e) (r size_t) {
	var CDictTaggedIndices, loadLdmDict int32
	var iend, ip uintptr
	var maxDictSize, maxDictSize1, shortCacheMaxDictSize U32
	var tagTableSize size_t
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _ = CDictTaggedIndices, iend, ip, loadLdmDict, maxDictSize, maxDictSize1, shortCacheMaxDictSize, tagTableSize, v1, v2, v3
	ip = src
	iend = ip + uintptr(srcSize)
	loadLdmDict = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) && ls != libc.UintptrFromInt32(0))
	/* Assert that the ms params match the params we're being given */
	ZSTD_assertEqualCParams(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams)
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} /* Ensure large dictionaries can't cause index overflow */
	/* Allow the dictionary to set indices up to exactly ZSTD_CURRENT_MAX.
	 * Dictionaries right at the edge will immediately trigger overflow
	 * correction, but I don't want to insert extra constraints here.
	 */
	maxDictSize = v1 - uint32(ZSTD_WINDOW_START_INDEX)
	CDictTaggedIndices = ZSTD_CDictIndicesAreTagged(tls, params+4)
	if CDictTaggedIndices != 0 && tfp == int32(ZSTD_tfp_forCDict) {
		/* Some dictionary matchfinders in zstd use "short cache",
		 * which treats the lower ZSTD_SHORT_CACHE_TAG_BITS of each
		 * CDict hashtable entry as a tag rather than as part of an index.
		 * When short cache is used, we need to truncate the dictionary
		 * so that its indices don't overlap with the tag. */
		shortCacheMaxDictSize = libc.Uint32FromUint32(1)<<(libc.Int32FromInt32(32)-libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)) - libc.Uint32FromInt32(ZSTD_WINDOW_START_INDEX)
		if maxDictSize < shortCacheMaxDictSize {
			v2 = maxDictSize
		} else {
			v2 = shortCacheMaxDictSize
		}
		maxDictSize = v2
	}
	/* If the dictionary is too large, only load the suffix of the dictionary. */
	if srcSize > uint64(maxDictSize) {
		ip = iend - uintptr(maxDictSize)
		src = ip
		srcSize = uint64(maxDictSize)
	}
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	if srcSize > uint64(uint32(-libc.Int32FromInt32(1))-v1) {
		/* We must have cleared our windows when our source is this large. */
		if loadLdmDict != 0 {
		}
	}
	ZSTD_window_update(tls, ms, src, srcSize, 0)
	if loadLdmDict != 0 { /* Load the entire dict into LDM matchfinders. */
		ZSTD_window_update(tls, ls, src, srcSize, 0)
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FforceWindow != 0 {
			v1 = uint32(0)
		} else {
			v1 = uint32(int64(iend) - int64((*ldmState_t)(unsafe.Pointer(ls)).Fwindow.Fbase))
		}
		(*ldmState_t)(unsafe.Pointer(ls)).FloadedDictEnd = v1
		ZSTD_ldm_fillHashTable(tls, ls, ip, iend, params+96)
	}
	/* If the dict is larger than we can reasonably index in our tables, only load the suffix. */
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog+uint32(3) > (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog+uint32(1) {
		v2 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog + uint32(3)
	} else {
		v2 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog + uint32(1)
	}
	if v2 < uint32(libc.Int32FromInt32(31)) {
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog+uint32(3) > (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog+uint32(1) {
			v3 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog + uint32(3)
		} else {
			v3 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog + uint32(1)
		}
		v1 = v3
	} else {
		v1 = uint32(libc.Int32FromInt32(31))
	}
	maxDictSize1 = uint32(1) << v1
	if srcSize > uint64(maxDictSize1) {
		ip = iend - uintptr(maxDictSize1)
		src = ip
		srcSize = uint64(maxDictSize1)
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = uint32(int64(ip) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FforceWindow != 0 {
		v1 = uint32(0)
	} else {
		v1 = uint32(int64(iend) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd = v1
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FforceNonContiguous = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FdeterministicRefPrefix
	if srcSize <= uint64(HASH_READ_SIZE) {
		return uint64(0)
	}
	ZSTD_overflowCorrectIfNeeded(tls, ms, ws, params, ip, iend)
	switch (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy {
	case int32(ZSTD_fast):
		ZSTD_fillHashTable(tls, ms, iend, dtlm, tfp)
	case int32(ZSTD_dfast):
		ZSTD_fillDoubleHashTable(tls, ms, iend, dtlm, tfp)
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdedicatedDictSearch != 0 {
			ZSTD_dedicatedDictSearch_lazy_loadDictionary(tls, ms, iend-uintptr(HASH_READ_SIZE))
		} else {
			if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder == int32(ZSTD_ps_enable) {
				tagTableSize = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog
				libc.Xmemset(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable, 0, tagTableSize)
				ZSTD_row_update(tls, ms, iend-uintptr(HASH_READ_SIZE))
			} else {
				ZSTD_insertAndFindFirstIndex(tls, ms, iend-uintptr(HASH_READ_SIZE))
			}
		}
	case int32(ZSTD_btlazy2): /* we want the dictionary table fully sorted */
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
		ZSTD_updateTree(tls, ms, iend-uintptr(HASH_READ_SIZE), iend)
	default:
		/* not possible : not a valid strategy id */
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = uint32(int64(iend) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	return uint64(0)
}

// C documentation
//
//	/* Dictionaries that assign zero probability to symbols that show up causes problems
//	 * when FSE encoding. Mark dictionaries with zero probability symbols as FSE_repeat_check
//	 * and only dictionaries with 100% valid symbols can be assumed valid.
//	 */
func ZSTD_dictNCountRepeat(tls *libc.TLS, normalizedCounter uintptr, dictMaxSymbolValue uint32, maxSymbolValue uint32) (r FSE_repeat) {
	var s U32
	_ = s
	if dictMaxSymbolValue < maxSymbolValue {
		return int32(FSE_repeat_check)
	}
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) == 0 {
			return int32(FSE_repeat_check)
		}
		goto _1
	_1:
		;
		s = s + 1
	}
	return int32(FSE_repeat_valid)
}

func ZSTD_loadCEntropy(tls *libc.TLS, bs uintptr, workspace uintptr, dict uintptr, dictSize size_t) (r size_t) {
	bp := tls.Alloc(288)
	defer tls.Free(288)
	var dictContentSize, hufHeaderSize, litlengthHeaderSize, matchlengthHeaderSize, offcodeHeaderSize size_t
	var dictEnd, dictPtr uintptr
	var maxOffset, offcodeMax, u U32
	var v1 uint32
	var _ /* hasZeroWeights at bp+72 */ uint32
	var _ /* litlengthLog at bp+272 */ uint32
	var _ /* litlengthMaxValue at bp+268 */ uint32
	var _ /* litlengthNCount at bp+196 */ [36]int16
	var _ /* matchlengthLog at bp+192 */ uint32
	var _ /* matchlengthMaxValue at bp+188 */ uint32
	var _ /* matchlengthNCount at bp+80 */ [53]int16
	var _ /* maxSymbolValue at bp+68 */ uint32
	var _ /* offcodeLog at bp+76 */ uint32
	var _ /* offcodeMaxValue at bp+64 */ uint32
	var _ /* offcodeNCount at bp+0 */ [32]int16
	_, _, _, _, _, _, _, _, _, _, _ = dictContentSize, dictEnd, dictPtr, hufHeaderSize, litlengthHeaderSize, matchlengthHeaderSize, maxOffset, offcodeHeaderSize, offcodeMax, u, v1
	*(*uint32)(unsafe.Pointer(bp + 64)) = uint32(MaxOff)
	dictPtr = dict /* skip magic num and dict ID */
	dictEnd = dictPtr + uintptr(dictSize)
	dictPtr = dictPtr + uintptr(8)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Fhuf.FrepeatMode = int32(HUF_repeat_check)
	*(*uint32)(unsafe.Pointer(bp + 68)) = uint32(255)
	*(*uint32)(unsafe.Pointer(bp + 72)) = uint32(1)
	hufHeaderSize = HUF_readCTable(tls, bs, bp+68, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)), bp+72)
	/* We only set the loaded table as valid if it contains all non-zero
	 * weights. Otherwise, we set it to check */
	if !(*(*uint32)(unsafe.Pointer(bp + 72)) != 0) && *(*uint32)(unsafe.Pointer(bp + 68)) == uint32(255) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Fhuf.FrepeatMode = int32(HUF_repeat_valid)
	}
	if ERR_isError(tls, hufHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	dictPtr = dictPtr + uintptr(hufHeaderSize)
	offcodeHeaderSize = FSE_readNCount(tls, bp, bp+64, bp+76, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, offcodeHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 76)) > uint32(OffFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	/* fill all offset symbols to avoid garbage at end of table */
	if ERR_isError(tls, FSE_buildCTable_wksp(tls, bs+2064, bp, uint32(MaxOff), *(*uint32)(unsafe.Pointer(bp + 76)), workspace, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	/* Defer checking offcodeMaxValue because we need to know the size of the dictionary content */
	dictPtr = dictPtr + uintptr(offcodeHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 188)) = uint32(MaxML)
	matchlengthHeaderSize = FSE_readNCount(tls, bp+80, bp+188, bp+192, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, matchlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 192)) > uint32(MLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if ERR_isError(tls, FSE_buildCTable_wksp(tls, bs+2064+772, bp+80, *(*uint32)(unsafe.Pointer(bp + 188)), *(*uint32)(unsafe.Pointer(bp + 192)), workspace, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Fmatchlength_repeatMode = ZSTD_dictNCountRepeat(tls, bp+80, *(*uint32)(unsafe.Pointer(bp + 188)), uint32(MaxML))
	dictPtr = dictPtr + uintptr(matchlengthHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 268)) = uint32(MaxLL)
	litlengthHeaderSize = FSE_readNCount(tls, bp+196, bp+268, bp+272, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, litlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 272)) > uint32(LLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if ERR_isError(tls, FSE_buildCTable_wksp(tls, bs+2064+2224, bp+196, *(*uint32)(unsafe.Pointer(bp + 268)), *(*uint32)(unsafe.Pointer(bp + 272)), workspace, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Flitlength_repeatMode = ZSTD_dictNCountRepeat(tls, bp+196, *(*uint32)(unsafe.Pointer(bp + 268)), uint32(MaxLL))
	dictPtr = dictPtr + uintptr(litlengthHeaderSize)
	if dictPtr+uintptr(12) > dictEnd {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	*(*U32)(unsafe.Pointer(bs + 5616)) = MEM_readLE32(tls, dictPtr+uintptr(0))
	*(*U32)(unsafe.Pointer(bs + 5616 + 1*4)) = MEM_readLE32(tls, dictPtr+uintptr(4))
	*(*U32)(unsafe.Pointer(bs + 5616 + 2*4)) = MEM_readLE32(tls, dictPtr+uintptr(8))
	dictPtr = dictPtr + uintptr(12)
	dictContentSize = uint64(int64(dictEnd) - int64(dictPtr))
	offcodeMax = uint32(MaxOff)
	if dictContentSize <= uint64(uint32(-libc.Int32FromInt32(1))-uint32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) {
		maxOffset = uint32(dictContentSize) + uint32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) /* The maximum offset that must be supported */
		offcodeMax = ZSTD_highbit32(tls, maxOffset)                                                                              /* Calculate minimum offset code required to represent maxOffset */
	}
	/* All offset values <= dictContentSize + 128 KB must be representable for a valid table */
	if offcodeMax < uint32(libc.Int32FromInt32(MaxOff)) {
		v1 = offcodeMax
	} else {
		v1 = uint32(libc.Int32FromInt32(MaxOff))
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Foffcode_repeatMode = ZSTD_dictNCountRepeat(tls, bp, *(*uint32)(unsafe.Pointer(bp + 64)), v1)
	/* All repCodes must be <= dictContentSize and != 0 */
	u = uint32(0)
	for {
		if !(u < uint32(3)) {
			break
		}
		if *(*U32)(unsafe.Pointer(bs + 5616 + uintptr(u)*4)) == uint32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dictionary_corrupted))
		}
		if uint64(*(*U32)(unsafe.Pointer(bs + 5616 + uintptr(u)*4))) > dictContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dictionary_corrupted))
		}
		goto _2
	_2:
		;
		u = u + 1
	}
	return uint64(int64(dictPtr) - int64(dict))
}

// C documentation
//
//	/* Dictionary format :
//	 * See :
//	 * https://github.com/facebook/zstd/blob/release/doc/zstd_compression_format.md#dictionary-format
//	 */
//	/*! ZSTD_loadZstdDictionary() :
//	 * @return : dictID, or an error code
//	 *  assumptions : magic number supposed already checked
//	 *                dictSize supposed >= 8
//	 */
func ZSTD_loadZstdDictionary(tls *libc.TLS, bs uintptr, ms uintptr, ws uintptr, params uintptr, dict uintptr, dictSize size_t, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e, workspace uintptr) (r size_t) {
	var dictContentSize, dictID, eSize, err_code, err_code1 size_t
	var dictEnd, dictPtr uintptr
	var v1 uint32
	_, _, _, _, _, _, _, _ = dictContentSize, dictEnd, dictID, dictPtr, eSize, err_code, err_code1, v1
	dictPtr = dict
	dictEnd = dictPtr + uintptr(dictSize)
	_ = libc.Uint64FromInt64(1)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FnoDictIDFlag != 0 {
		v1 = uint32(0)
	} else {
		v1 = MEM_readLE32(tls, dictPtr+uintptr(4))
	}
	dictID = uint64(v1)
	eSize = ZSTD_loadCEntropy(tls, bs, workspace, dict, dictSize)
	err_code = eSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4485, 0)
		}
		return err_code
	}
	dictPtr = dictPtr + uintptr(eSize)
	dictContentSize = uint64(int64(dictEnd) - int64(dictPtr))
	err_code1 = ZSTD_loadDictionaryContent(tls, ms, libc.UintptrFromInt32(0), ws, params, dictPtr, dictContentSize, dtlm, tfp)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return dictID
}

// C documentation
//
//	/** ZSTD_compress_insertDictionary() :
//	*   @return : dictID, or an error code */
func ZSTD_compress_insertDictionary(tls *libc.TLS, bs uintptr, ms uintptr, ls uintptr, ws uintptr, params uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e, workspace uintptr) (r size_t) {
	if dict == libc.UintptrFromInt32(0) || dictSize < uint64(8) {
		if dictContentType == int32(ZSTD_dct_fullDict) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dictionary_wrong))
		}
		return uint64(0)
	}
	ZSTD_reset_compressedBlockState(tls, bs)
	/* dict restricted modes */
	if dictContentType == int32(ZSTD_dct_rawContent) {
		return ZSTD_loadDictionaryContent(tls, ms, ls, ws, params, dict, dictSize, dtlm, tfp)
	}
	if MEM_readLE32(tls, dict) != uint32(ZSTD_MAGIC_DICTIONARY) {
		if dictContentType == int32(ZSTD_dct_auto) {
			return ZSTD_loadDictionaryContent(tls, ms, ls, ws, params, dict, dictSize, dtlm, tfp)
		}
		if dictContentType == int32(ZSTD_dct_fullDict) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dictionary_wrong))
		}
		/* impossible */
	}
	/* dict as full zstd dictionary */
	return ZSTD_loadZstdDictionary(tls, bs, ms, ws, params, dict, dictSize, dtlm, tfp, workspace)
}

// C documentation
//
//	/*! ZSTD_compressBegin_internal() :
//	 * Assumption : either @dict OR @cdict (or none) is non-NULL, never both
//	 * @return : 0, or an error code */
func ZSTD_compressBegin_internal(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, dtlm ZSTD_dictTableLoadMethod_e, cdict uintptr, params uintptr, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	var dictContentSize, dictID, err_code, err_code1 size_t
	var v1 uint64
	_, _, _, _, _ = dictContentSize, dictID, err_code, err_code1, v1
	if cdict != 0 {
		v1 = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize
	} else {
		v1 = dictSize
	}
	dictContentSize = v1
	/* params are supposed to be fully validated at this point */
	/* either dict or cdict, not both */
	if cdict != 0 && (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize > uint64(0) && (pledgedSrcSize < uint64(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) || pledgedSrcSize < (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize*uint64(6) || pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) || (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel == 0) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FattachDictPref != int32(ZSTD_dictForceLoad) {
		return ZSTD_resetCCtx_usingCDict(tls, cctx, cdict, params, pledgedSrcSize, zbuff)
	}
	err_code = ZSTD_resetCCtx_internal(tls, cctx, params, pledgedSrcSize, dictContentSize, int32(ZSTDcrp_makeClean), zbuff)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if cdict != 0 {
		v1 = ZSTD_compress_insertDictionary(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cctx+3224+16, cctx+1056, cctx+704, cctx+240, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentType, dtlm, int32(ZSTD_tfp_forCCtx), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace)
	} else {
		v1 = ZSTD_compress_insertDictionary(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cctx+3224+16, cctx+1056, cctx+704, cctx+240, dict, dictSize, dictContentType, dtlm, int32(ZSTD_tfp_forCCtx), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace)
	}
	dictID = v1
	err_code1 = dictID
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4510, 0)
		}
		return err_code1
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = uint32(dictID)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = dictContentSize
	return uint64(0)
}

func ZSTD_compressBegin_advanced_internal(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, dtlm ZSTD_dictTableLoadMethod_e, cdict uintptr, params uintptr, pledgedSrcSize uint64) (r size_t) {
	var err_code size_t
	_ = err_code
	/* compression parameters verification and optimization */
	err_code = ZSTD_checkCParams(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_compressBegin_internal(tls, cctx, dict, dictSize, dictContentType, dtlm, cdict, params, pledgedSrcSize, int32(ZSTDb_not_buffered))
}

// C documentation
//
//	/*! ZSTD_compressBegin_advanced() :
//	*   @return : 0, or an error code */
func ZSTD_compressBegin_advanced(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, _params ZSTD_parameters, pledgedSrcSize uint64) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var _ /* cctxParams at bp+40 */ ZSTD_CCtx_params
	ZSTD_CCtxParams_init_internal(tls, bp+40, bp, ZSTD_NO_CLEVEL)
	return ZSTD_compressBegin_advanced_internal(tls, cctx, dict, dictSize, int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), bp+40, pledgedSrcSize)
}

func ZSTD_compressBegin_usingDict_deprecated(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var v1 int32
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	var _ /* params at bp+224 */ ZSTD_parameters
	_ = v1
	*(*ZSTD_parameters)(unsafe.Pointer(bp + 224)) = ZSTD_getParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_noAttachDict))
	if compressionLevel == 0 {
		v1 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v1 = compressionLevel
	}
	ZSTD_CCtxParams_init_internal(tls, bp, bp+224, v1)
	return ZSTD_compressBegin_internal(tls, cctx, dict, dictSize, int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), bp, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), int32(ZSTDb_not_buffered))
}

func ZSTD_compressBegin_usingDict(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	return ZSTD_compressBegin_usingDict_deprecated(tls, cctx, dict, dictSize, compressionLevel)
}

func ZSTD_compressBegin(tls *libc.TLS, cctx uintptr, compressionLevel int32) (r size_t) {
	return ZSTD_compressBegin_usingDict_deprecated(tls, cctx, libc.UintptrFromInt32(0), uint64(0), compressionLevel)
}

// C documentation
//
//	/*! ZSTD_writeEpilogue() :
//	*   Ends a frame.
//	*   @return : nb of bytes written into dst (or an error code) */
func ZSTD_writeEpilogue(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t) (r size_t) {
	var cBlockHeader24, checksum U32
	var err_code, fhSize size_t
	var op, ostart uintptr
	_, _, _, _, _, _ = cBlockHeader24, checksum, err_code, fhSize, op, ostart
	ostart = dst
	op = ostart
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_created) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4548, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	/* special case : empty frame */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_init) {
		fhSize = ZSTD_writeFrameHeader(tls, dst, dstCapacity, cctx+240, uint64(0), uint32(0))
		err_code = fhSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4339, 0)
			}
			return err_code
		}
		dstCapacity = dstCapacity - fhSize
		op = op + uintptr(fhSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_ongoing)
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage != int32(ZSTDcs_ending) {
		/* write one last empty block, make it the "last" block */
		cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1) + libc.Uint32FromInt32(0)
		_ = libc.Uint64FromInt64(1)
		if dstCapacity < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4561, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE24(tls, op, cBlockHeader24)
		op = op + uintptr(ZSTD_blockHeaderSize)
		dstCapacity = dstCapacity - ZSTD_blockHeaderSize
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 {
		checksum = uint32(XXH_INLINE_XXH64_digest(tls, cctx+808))
		if dstCapacity < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4582, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE32(tls, op, checksum)
		op = op + uintptr(4)
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_created) /* return to "created but no init" status */
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_CCtx_trace(tls *libc.TLS, cctx uintptr, extraCSize size_t) {
	_ = cctx
	_ = extraCSize
}

func ZSTD_compressEnd_public(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cSize, endResult, err_code, err_code1 size_t
	_, _, _, _ = cSize, endResult, err_code, err_code1
	cSize = ZSTD_compressContinue_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(1), uint32(1))
	err_code = cSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4603, 0)
		}
		return err_code
	}
	endResult = ZSTD_writeEpilogue(tls, cctx, dst+uintptr(cSize), dstCapacity-cSize)
	err_code1 = endResult
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4641, 0)
		}
		return err_code1
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne != uint64(0) { /* control src size */
		_ = libc.Uint64FromInt64(1)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne != (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize+uint64(1) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4667, libc.VaList(bp+8, uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne)-uint32(1), uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize)))
			}
			return uint64(-int32(ZSTD_error_srcSize_wrong))
		}
	}
	ZSTD_CCtx_trace(tls, cctx, endResult)
	return cSize + endResult
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressEnd_public() */
func ZSTD_compressEnd(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressEnd_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

func ZSTD_compress_advanced(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t, _params ZSTD_parameters) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var err_code size_t
	_ = err_code
	err_code = ZSTD_checkCParams(tls, (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ZSTD_CCtxParams_init_internal(tls, cctx+464, bp, ZSTD_NO_CLEVEL)
	return ZSTD_compress_advanced_internal(tls, cctx, dst, dstCapacity, src, srcSize, dict, dictSize, cctx+464)
}

// C documentation
//
//	/* Internal */
func ZSTD_compress_advanced_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t, params uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_compressBegin_internal(tls, cctx, dict, dictSize, int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), params, srcSize, int32(ZSTDb_not_buffered))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_compressEnd_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

func ZSTD_compress_usingDict(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var v1 uint64
	var v2 int32
	var _ /* params at bp+0 */ ZSTD_parameters
	_, _ = v1, v2
	if dict != 0 {
		v1 = dictSize
	} else {
		v1 = uint64(0)
	}
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = ZSTD_getParams_internal(tls, compressionLevel, srcSize, v1, int32(ZSTD_cpm_noAttachDict))
	if compressionLevel == 0 {
		v2 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v2 = compressionLevel
	}
	ZSTD_CCtxParams_init_internal(tls, cctx+464, bp, v2)
	return ZSTD_compress_advanced_internal(tls, cctx, dst, dstCapacity, src, srcSize, dict, dictSize, cctx+464)
}

func ZSTD_compressCCtx(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, compressionLevel int32) (r size_t) {
	return ZSTD_compress_usingDict(tls, cctx, dst, dstCapacity, src, srcSize, libc.UintptrFromInt32(0), uint64(0), compressionLevel)
}

func ZSTD_compress(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, compressionLevel int32) (r size_t) {
	bp := tls.Alloc(5280)
	defer tls.Free(5280)
	var result size_t
	var _ /* ctxBody at bp+0 */ ZSTD_CCtx
	_ = result
	ZSTD_initCCtx(tls, bp, ZSTD_defaultCMem)
	result = ZSTD_compressCCtx(tls, bp, dst, dstCapacity, src, srcSize, compressionLevel)
	ZSTD_freeCCtxContent(tls, bp) /* can't free ctxBody itself, as it's on stack; free only heap content */
	return result
}

/* =====  Dictionary API  ===== */

// C documentation
//
//	/*! ZSTD_estimateCDictSize_advanced() :
//	 *  Estimate amount of memory that will be needed to create a dictionary with following arguments */
func ZSTD_estimateCDictSize_advanced(tls *libc.TLS, dictSize size_t, _cParams ZSTD_compressionParameters, dictLoadMethod ZSTD_dictLoadMethod_e) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var v1 uint64
	_ = v1
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
	}
	return ZSTD_cwksp_alloc_size(tls, uint64(6080)) + ZSTD_cwksp_alloc_size(tls, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))) + ZSTD_sizeof_matchState(tls, bp, ZSTD_resolveRowMatchFinderMode(tls, int32(ZSTD_ps_auto), bp), int32(1), uint32(0)) + v1
}

func ZSTD_estimateCDictSize(tls *libc.TLS, dictSize size_t, compressionLevel int32) (r size_t) {
	var cParams ZSTD_compressionParameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	return ZSTD_estimateCDictSize_advanced(tls, dictSize, cParams, int32(ZSTD_dlm_byCopy))
}

func ZSTD_sizeof_CDict(tls *libc.TLS, cdict uintptr) (r size_t) {
	var v1 uint64
	_ = v1
	if cdict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof on NULL */
	/* cdict may be in the workspace */
	if (*ZSTD_CDict)(unsafe.Pointer(cdict)).Fworkspace.Fworkspace == cdict {
		v1 = uint64(0)
	} else {
		v1 = uint64(6080)
	}
	return v1 + ZSTD_cwksp_sizeof(tls, cdict+32)
}

func ZSTD_initCDict_internal(tls *libc.TLS, cdict uintptr, dictBuffer uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, _params ZSTD_CCtx_params) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var dictID, err_code, err_code1 size_t
	var internalBuffer uintptr
	_, _, _, _ = dictID, err_code, err_code1, internalBuffer
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FdedicatedDictSearch = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch
	if dictLoadMethod == int32(ZSTD_dlm_byRef) || !(dictBuffer != 0) || !(dictSize != 0) {
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent = dictBuffer
	} else {
		internalBuffer = ZSTD_cwksp_reserve_object(tls, cdict+32, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
		if !(internalBuffer != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1377, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent = internalBuffer
		libc.Xmemcpy(tls, internalBuffer, dictBuffer, dictSize)
	}
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize = dictSize
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentType = dictContentType
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FentropyWorkspace = ZSTD_cwksp_reserve_object(tls, cdict+32, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))
	/* Reset the state to no dictionary */
	ZSTD_reset_compressedBlockState(tls, cdict+408)
	err_code = ZSTD_reset_matchState(tls, cdict+104, cdict+32, bp+4, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, int32(ZSTDcrp_makeClean), int32(ZSTDirp_reset), int32(ZSTD_resetTarget_CDict))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	/* (Maybe) load the dictionary
	 * Skips loading the dictionary if it is < 8 bytes.
	 */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams.FcontentSizeFlag = int32(1)
	dictID = ZSTD_compress_insertDictionary(tls, cdict+408, cdict+104, libc.UintptrFromInt32(0), cdict+32, bp, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize, dictContentType, int32(ZSTD_dtlm_full), int32(ZSTD_tfp_forCDict), (*ZSTD_CDict)(unsafe.Pointer(cdict)).FentropyWorkspace)
	err_code1 = dictID
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4510, 0)
		}
		return err_code1
	}
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID = uint32(dictID)
	return uint64(0)
}

func ZSTD_createCDict_advanced_internal(tls *libc.TLS, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, _cParams ZSTD_compressionParameters, useRowMatchFinder ZSTD_ParamSwitch_e, enableDedicatedDictSearch int32, customMem ZSTD_customMem) (r uintptr) {
	bp := tls.Alloc(112)
	defer tls.Free(112)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var cdict, workspace uintptr
	var workspaceSize size_t
	var v1 uint64
	var _ /* ws at bp+32 */ ZSTD_cwksp
	_, _, _, _ = cdict, workspace, workspaceSize, v1
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
	}
	workspaceSize = ZSTD_cwksp_alloc_size(tls, uint64(6080)) + ZSTD_cwksp_alloc_size(tls, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))) + ZSTD_sizeof_matchState(tls, bp, useRowMatchFinder, enableDedicatedDictSearch, uint32(0)) + v1
	workspace = ZSTD_customMalloc(tls, workspaceSize, customMem)
	if !(workspace != 0) {
		ZSTD_customFree(tls, workspace, customMem)
		return libc.UintptrFromInt32(0)
	}
	ZSTD_cwksp_init(tls, bp+32, workspace, workspaceSize, int32(ZSTD_cwksp_dynamic_alloc))
	cdict = ZSTD_cwksp_reserve_object(tls, bp+32, uint64(6080))
	ZSTD_cwksp_move(tls, cdict+32, bp+32)
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcustomMem = customMem
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = ZSTD_NO_CLEVEL /* signals advanced API usage */
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder = useRowMatchFinder
	return cdict
	return r
}

func ZSTD_createCDict_advanced(tls *libc.TLS, dictBuffer uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, cParams ZSTD_compressionParameters, customMem ZSTD_customMem) (r uintptr) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(224))
	ZSTD_CCtxParams_init(tls, bp, 0)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = cParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcustomMem = customMem
	return ZSTD_createCDict_advanced2(tls, dictBuffer, dictSize, dictLoadMethod, dictContentType, bp, customMem)
}

func ZSTD_createCDict_advanced2(tls *libc.TLS, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, originalCctxParams uintptr, customMem ZSTD_customMem) (r uintptr) {
	bp := tls.Alloc(256)
	defer tls.Free(256)
	var cdict uintptr
	var _ /* cParams at bp+224 */ ZSTD_compressionParameters
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	_ = cdict
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = *(*ZSTD_CCtx_params)(unsafe.Pointer(originalCctxParams))
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch != 0 {
		*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = ZSTD_dedicatedDictSearch_getCParams(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel, dictSize)
		ZSTD_overrideCParams(tls, bp+224, bp+4)
	} else {
		*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = ZSTD_getCParamsFromCCtxParams(tls, bp, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	}
	if !(ZSTD_dedicatedDictSearch_isSupported(tls, bp+224) != 0) {
		/* Fall back to non-DDSS params */
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch = 0
		*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = ZSTD_getCParamsFromCCtxParams(tls, bp, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	}
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, bp+224)
	cdict = ZSTD_createCDict_advanced_internal(tls, dictSize, dictLoadMethod, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch, customMem)
	if !(cdict != 0) || ZSTD_isError(tls, ZSTD_initCDict_internal(tls, cdict, dict, dictSize, dictLoadMethod, dictContentType, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp)))) != 0 {
		ZSTD_freeCDict(tls, cdict)
		return libc.UintptrFromInt32(0)
	}
	return cdict
}

func ZSTD_createCDict(tls *libc.TLS, dict uintptr, dictSize size_t, compressionLevel int32) (r uintptr) {
	var cParams ZSTD_compressionParameters
	var cdict uintptr
	var v1 int32
	_, _, _ = cParams, cdict, v1
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	cdict = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto), cParams, ZSTD_defaultCMem)
	if cdict != 0 {
		if compressionLevel == 0 {
			v1 = int32(ZSTD_CLEVEL_DEFAULT)
		} else {
			v1 = compressionLevel
		}
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = v1
	}
	return cdict
}

func ZSTD_createCDict_byReference(tls *libc.TLS, dict uintptr, dictSize size_t, compressionLevel int32) (r uintptr) {
	var cParams ZSTD_compressionParameters
	var cdict uintptr
	var v1 int32
	_, _, _ = cParams, cdict, v1
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	cdict = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto), cParams, ZSTD_defaultCMem)
	if cdict != 0 {
		if compressionLevel == 0 {
			v1 = int32(ZSTD_CLEVEL_DEFAULT)
		} else {
			v1 = compressionLevel
		}
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = v1
	}
	return cdict
}

func ZSTD_freeCDict(tls *libc.TLS, cdict uintptr) (r size_t) {
	var cMem ZSTD_customMem
	var cdictInWorkspace int32
	_, _ = cMem, cdictInWorkspace
	if cdict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	cMem = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcustomMem
	cdictInWorkspace = ZSTD_cwksp_owns_buffer(tls, cdict+32, cdict)
	ZSTD_cwksp_free(tls, cdict+32, cMem)
	if !(cdictInWorkspace != 0) {
		ZSTD_customFree(tls, cdict, cMem)
	}
	return uint64(0)
	return r
}

// C documentation
//
//	/*! ZSTD_initStaticCDict_advanced() :
//	 *  Generate a digested dictionary in provided memory area.
//	 *  workspace: The memory area to emplace the dictionary into.
//	 *             Provided pointer must 8-bytes aligned.
//	 *             It must outlive dictionary usage.
//	 *  workspaceSize: Use ZSTD_estimateCDictSize()
//	 *                 to determine how large workspace must be.
//	 *  cParams : use ZSTD_getCParams() to transform a compression level
//	 *            into its relevant cParams.
//	 * @return : pointer to ZSTD_CDict*, or NULL if error (size too small)
//	 *  Note : there is no corresponding "free" function.
//	 *         Since workspace was allocated externally, it must be freed externally.
//	 */
func ZSTD_initStaticCDict(tls *libc.TLS, workspace uintptr, workspaceSize size_t, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, _cParams ZSTD_compressionParameters) (r uintptr) {
	bp := tls.Alloc(336)
	defer tls.Free(336)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var cdict uintptr
	var matchStateSize, neededSize size_t
	var useRowMatchFinder ZSTD_ParamSwitch_e
	var v1 uint64
	var _ /* params at bp+32 */ ZSTD_CCtx_params
	var _ /* ws at bp+256 */ ZSTD_cwksp
	_, _, _, _, _ = cdict, matchStateSize, neededSize, useRowMatchFinder, v1
	useRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, int32(ZSTD_ps_auto), bp)
	/* enableDedicatedDictSearch == 1 ensures matchstate is not too small in case this CDict will be used for DDS + row hash */
	matchStateSize = ZSTD_sizeof_matchState(tls, bp, useRowMatchFinder, int32(1), uint32(0))
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
	}
	neededSize = ZSTD_cwksp_alloc_size(tls, uint64(6080)) + v1 + ZSTD_cwksp_alloc_size(tls, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))) + matchStateSize
	if uint64(workspace)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* 8-aligned */
	ZSTD_cwksp_init(tls, bp+256, workspace, workspaceSize, int32(ZSTD_cwksp_static_alloc))
	cdict = ZSTD_cwksp_reserve_object(tls, bp+256, uint64(6080))
	if cdict == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_cwksp_move(tls, cdict+32, bp+256)
	if workspaceSize < neededSize {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_CCtxParams_init(tls, bp+32, 0)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(bp))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FuseRowMatchFinder = useRowMatchFinder
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder = useRowMatchFinder
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = ZSTD_NO_CLEVEL
	if ZSTD_isError(tls, ZSTD_initCDict_internal(tls, cdict, dict, dictSize, dictLoadMethod, dictContentType, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32)))) != 0 {
		return libc.UintptrFromInt32(0)
	}
	return cdict
}

func ZSTD_getCParamsFromCDict(tls *libc.TLS, cdict uintptr) (r ZSTD_compressionParameters) {
	return (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams
}

// C documentation
//
//	/*! ZSTD_getDictID_fromCDict() :
//	 *  Provides the dictID of the dictionary loaded into `cdict`.
//	 *  If @return == 0, the dictionary is not conformant to Zstandard specification, or empty.
//	 *  Non-conformant dictionaries can still be loaded, but as content-only dictionaries. */
func ZSTD_getDictID_fromCDict(tls *libc.TLS, cdict uintptr) (r uint32) {
	if cdict == libc.UintptrFromInt32(0) {
		return uint32(0)
	}
	return (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID
}

// C documentation
//
//	/* ZSTD_compressBegin_usingCDict_internal() :
//	 * Implementation of various ZSTD_compressBegin_usingCDict* functions.
//	 */
func ZSTD_compressBegin_usingCDict_internal(tls *libc.TLS, cctx uintptr, cdict uintptr, fParams ZSTD_frameParameters, pledgedSrcSize uint64) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var limitedSrcLog, limitedSrcSize U32
	var v1 ZSTD_compressionParameters
	var v2 uint64
	var v3, v4 uint32
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	var _ /* params at bp+224 */ ZSTD_parameters
	_, _, _, _, _, _ = limitedSrcLog, limitedSrcSize, v1, v2, v3, v4
	if cdict == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_wrong))
	}
	/* Initialize the cctxParams from the cdict */
	(*(*ZSTD_parameters)(unsafe.Pointer(bp + 224))).FfParams = fParams
	if pledgedSrcSize < uint64(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) || pledgedSrcSize < (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize*uint64(6) || pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) || (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel == 0 {
		v1 = ZSTD_getCParamsFromCDict(tls, cdict)
	} else {
		v1 = ZSTD_getCParams(tls, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel, pledgedSrcSize, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize)
	}
	(*(*ZSTD_parameters)(unsafe.Pointer(bp + 224))).FcParams = v1
	ZSTD_CCtxParams_init_internal(tls, bp, bp+224, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel)
	/* Increase window log to fit the entire dictionary and source if the
	 * source size is known. Limit the increase to 19, which is the
	 * window log for compression level 1 with the largest source size.
	 */
	if pledgedSrcSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		if pledgedSrcSize < uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) {
			v2 = pledgedSrcSize
		} else {
			v2 = uint64(libc.Uint32FromUint32(1) << libc.Int32FromInt32(19))
		}
		limitedSrcSize = uint32(v2)
		if limitedSrcSize > uint32(1) {
			v3 = ZSTD_highbit32(tls, limitedSrcSize-uint32(1)) + uint32(1)
		} else {
			v3 = uint32(1)
		}
		limitedSrcLog = v3
		if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog > limitedSrcLog {
			v4 = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog
		} else {
			v4 = limitedSrcLog
		}
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog = v4
	}
	return ZSTD_compressBegin_internal(tls, cctx, libc.UintptrFromInt32(0), uint64(0), int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), cdict, bp, pledgedSrcSize, int32(ZSTDb_not_buffered))
}

// C documentation
//
//	/* ZSTD_compressBegin_usingCDict_advanced() :
//	 * This function is DEPRECATED.
//	 * cdict must be != NULL */
func ZSTD_compressBegin_usingCDict_advanced(tls *libc.TLS, cctx uintptr, cdict uintptr, fParams ZSTD_frameParameters, pledgedSrcSize uint64) (r size_t) {
	return ZSTD_compressBegin_usingCDict_internal(tls, cctx, cdict, fParams, pledgedSrcSize)
}

// C documentation
//
//	/* ZSTD_compressBegin_usingCDict() :
//	 * cdict must be != NULL */
func ZSTD_compressBegin_usingCDict_deprecated(tls *libc.TLS, cctx uintptr, cdict uintptr) (r size_t) {
	var fParams ZSTD_frameParameters
	_ = fParams
	fParams = ZSTD_frameParameters{}
	return ZSTD_compressBegin_usingCDict_internal(tls, cctx, cdict, fParams, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
}

func ZSTD_compressBegin_usingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr) (r size_t) {
	return ZSTD_compressBegin_usingCDict_deprecated(tls, cctx, cdict)
}

// C documentation
//
//	/*! ZSTD_compress_usingCDict_internal():
//	 * Implementation of various ZSTD_compress_usingCDict* functions.
//	 */
func ZSTD_compress_usingCDict_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, cdict uintptr, fParams ZSTD_frameParameters) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_compressBegin_usingCDict_internal(tls, cctx, cdict, fParams, srcSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	} /* will check if cdict != NULL */
	return ZSTD_compressEnd_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

// C documentation
//
//	/*! ZSTD_compress_usingCDict_advanced():
//	 * This function is DEPRECATED.
//	 */
func ZSTD_compress_usingCDict_advanced(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, cdict uintptr, fParams ZSTD_frameParameters) (r size_t) {
	return ZSTD_compress_usingCDict_internal(tls, cctx, dst, dstCapacity, src, srcSize, cdict, fParams)
}

// C documentation
//
//	/*! ZSTD_compress_usingCDict() :
//	 *  Compression using a digested Dictionary.
//	 *  Faster startup than ZSTD_compress_usingDict(), recommended when same dictionary is used multiple times.
//	 *  Note that compression parameters are decided at CDict creation time
//	 *  while frame parameters are hardcoded */
func ZSTD_compress_usingCDict(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, cdict uintptr) (r size_t) {
	var fParams ZSTD_frameParameters
	_ = fParams
	fParams = ZSTD_frameParameters{
		FcontentSizeFlag: int32(1),
	}
	return ZSTD_compress_usingCDict_internal(tls, cctx, dst, dstCapacity, src, srcSize, cdict, fParams)
}

/* ******************************************************************
*  Streaming
********************************************************************/

func ZSTD_createCStream(tls *libc.TLS) (r uintptr) {
	return ZSTD_createCStream_advanced(tls, ZSTD_defaultCMem)
}

func ZSTD_initStaticCStream(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	return ZSTD_initStaticCCtx(tls, workspace, workspaceSize)
}

func ZSTD_createCStream_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	/* CStream and CCtx are now same object */
	return ZSTD_createCCtx_advanced(tls, customMem)
}

func ZSTD_freeCStream(tls *libc.TLS, zcs uintptr) (r size_t) {
	return ZSTD_freeCCtx(tls, zcs) /* same object */
}

/*======   Initialization   ======*/

func ZSTD_CStreamInSize(tls *libc.TLS) (r size_t) {
	return uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
}

func ZSTD_CStreamOutSize(tls *libc.TLS) (r size_t) {
	return ZSTD_compressBound(tls, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))) + ZSTD_blockHeaderSize + uint64(4)
}

func ZSTD_getCParamMode(tls *libc.TLS, cdict uintptr, params uintptr, pledgedSrcSize U64) (r ZSTD_CParamMode_e) {
	if cdict != libc.UintptrFromInt32(0) && ZSTD_shouldAttachDict(tls, cdict, params, pledgedSrcSize) != 0 {
		return int32(ZSTD_cpm_attachDict)
	} else {
		return int32(ZSTD_cpm_noAttachDict)
	}
	return r
}

// C documentation
//
//	/* ZSTD_resetCStream():
//	 * pledgedSrcSize == 0 means "unknown" */
func ZSTD_resetCStream(tls *libc.TLS, zcs uintptr, pss uint64) (r size_t) {
	var err_code, err_code1 size_t
	var pledgedSrcSize U64
	var v1 uint64
	_, _, _, _ = err_code, err_code1, pledgedSrcSize, v1
	if pss == uint64(0) {
		v1 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v1 = pss
	}
	/* temporary : 0 interpreted as "unknown" during transition period.
	 * Users willing to specify "unknown" **must** use ZSTD_CONTENTSIZE_UNKNOWN.
	 * 0 will be interpreted as "empty" in the future.
	 */
	pledgedSrcSize = v1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_initCStream_internal() :
//	 *  Note : for lib/compress only. Used by zstdmt_compress.c.
//	 *  Assumption 1 : params are valid
//	 *  Assumption 2 : either dict, or cdict, is defined, not both */
func ZSTD_initCStream_internal(tls *libc.TLS, zcs uintptr, dict uintptr, dictSize size_t, cdict uintptr, params uintptr, pledgedSrcSize uint64) (r size_t) {
	var err_code, err_code1, err_code2, err_code3 size_t
	_, _, _, _ = err_code, err_code1, err_code2, err_code3
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	(*ZSTD_CStream)(unsafe.Pointer(zcs)).FrequestedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(params))
	/* either dict or cdict, not both */
	if dict != 0 {
		err_code2 = ZSTD_CCtx_loadDictionary(tls, zcs, dict, dictSize)
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code2
		}
	} else {
		/* Dictionary is cleared if !cdict */
		err_code3 = ZSTD_CCtx_refCDict(tls, zcs, cdict)
		if ERR_isError(tls, err_code3) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code3
		}
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_initCStream_usingCDict_advanced() :
//	 * same as ZSTD_initCStream_usingCDict(), with control over frame parameters */
func ZSTD_initCStream_usingCDict_advanced(tls *libc.TLS, zcs uintptr, cdict uintptr, fParams ZSTD_frameParameters, pledgedSrcSize uint64) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	(*ZSTD_CStream)(unsafe.Pointer(zcs)).FrequestedParams.FfParams = fParams
	err_code2 = ZSTD_CCtx_refCDict(tls, zcs, cdict)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

// C documentation
//
//	/* note : cdict must outlive compression session */
func ZSTD_initCStream_usingCDict(tls *libc.TLS, zcs uintptr, cdict uintptr) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_refCDict(tls, zcs, cdict)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_initCStream_advanced() :
//	 * pledgedSrcSize must be exact.
//	 * if srcSize is not known at init time, use value ZSTD_CONTENTSIZE_UNKNOWN.
//	 * dict is loaded with default parameters ZSTD_dct_auto and ZSTD_dlm_byCopy. */
func ZSTD_initCStream_advanced(tls *libc.TLS, zcs uintptr, dict uintptr, dictSize size_t, _params ZSTD_parameters, pss uint64) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var err_code, err_code1, err_code2, err_code3 size_t
	var pledgedSrcSize U64
	var v1 uint64
	_, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, pledgedSrcSize, v1
	if pss == uint64(0) && (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FfParams.FcontentSizeFlag == 0 {
		v1 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v1 = pss
	}
	/* for compatibility with older programs relying on this behavior.
	 * Users should now specify ZSTD_CONTENTSIZE_UNKNOWN.
	 * This line will be removed in the future.
	 */
	pledgedSrcSize = v1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_checkCParams(tls, (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	ZSTD_CCtxParams_setZstdParams(tls, zcs+16, bp)
	err_code3 = ZSTD_CCtx_loadDictionary(tls, zcs, dict, dictSize)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	return uint64(0)
}

func ZSTD_initCStream_usingDict(tls *libc.TLS, zcs uintptr, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setParameter(tls, zcs, int32(ZSTD_c_compressionLevel), compressionLevel)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_loadDictionary(tls, zcs, dict, dictSize)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

func ZSTD_initCStream_srcSize(tls *libc.TLS, zcs uintptr, compressionLevel int32, pss uint64) (r size_t) {
	var err_code, err_code1, err_code2, err_code3 size_t
	var pledgedSrcSize U64
	var v1 uint64
	_, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, pledgedSrcSize, v1
	if pss == uint64(0) {
		v1 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v1 = pss
	}
	/* temporary : 0 interpreted as "unknown" during transition period.
	 * Users willing to specify "unknown" **must** use ZSTD_CONTENTSIZE_UNKNOWN.
	 * 0 will be interpreted as "empty" in the future.
	 */
	pledgedSrcSize = v1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_refCDict(tls, zcs, libc.UintptrFromInt32(0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, zcs, int32(ZSTD_c_compressionLevel), compressionLevel)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	err_code3 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	return uint64(0)
}

func ZSTD_initCStream(tls *libc.TLS, zcs uintptr, compressionLevel int32) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_refCDict(tls, zcs, libc.UintptrFromInt32(0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, zcs, int32(ZSTD_c_compressionLevel), compressionLevel)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

/*======   Compression   ======*/

func ZSTD_nextInputSizeHint(tls *libc.TLS, cctx uintptr) (r size_t) {
	var hintInSize size_t
	_ = hintInSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		return (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed
	}
	hintInSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffTarget - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffPos
	if hintInSize == uint64(0) {
		hintInSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax
	}
	return hintInSize
	return r
}

// C documentation
//
//	/** ZSTD_compressStream_generic():
//	 *  internal function for all *compressStream*() variants
//	 * @return : hint size for next input to complete ongoing block */
func ZSTD_compressStream_generic(tls *libc.TLS, zcs uintptr, output uintptr, input uintptr, flushMode ZSTD_EndDirective) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cDst, iend, ip, istart, oend, op, ostart, v1, v2, v3, v4 uintptr
	var cSize, cSize1, err_code, err_code1, err_code2, flushed, iSize, loaded, oSize, toFlush, toLoad, v19 size_t
	var inputBuffered int32
	var lastBlock, lastBlock1 uint32
	var someMoreWork U32
	var v13, v14 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cDst, cSize, cSize1, err_code, err_code1, err_code2, flushed, iSize, iend, inputBuffered, ip, istart, lastBlock, lastBlock1, loaded, oSize, oend, op, ostart, someMoreWork, toFlush, toLoad, v1, v13, v14, v19, v2, v3, v4
	istart = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc
	if istart != libc.UintptrFromInt32(0) {
		v1 = istart + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize)
	} else {
		v1 = istart
	}
	iend = v1
	if istart != libc.UintptrFromInt32(0) {
		v2 = istart + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos)
	} else {
		v2 = istart
	}
	ip = v2
	ostart = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst
	if ostart != libc.UintptrFromInt32(0) {
		v3 = ostart + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize)
	} else {
		v3 = ostart
	}
	oend = v3
	if ostart != libc.UintptrFromInt32(0) {
		v4 = ostart + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos)
	} else {
		v4 = ostart
	}
	op = v4
	someMoreWork = uint32(1)
	/* check expectations */
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		*(*size_t)(unsafe.Pointer(input + 16)) -= (*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed
		if ip != 0 {
			ip = ip - uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed)
		}
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed = uint64(0)
	}
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered) {
	}
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_buffered) {
	}
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc == libc.UintptrFromInt32(0) {
	}
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst == libc.UintptrFromInt32(0) {
	}
	for someMoreWork != 0 {
		switch (*ZSTD_CStream)(unsafe.Pointer(zcs)).FstreamStage {
		case int32(zcss_init):
			goto _5
		case int32(zcss_load):
			goto _6
		case int32(zcss_flush):
			goto _7
		default:
			goto _8
		}
		goto _9
	_5:
		;
	_12:
		;
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4719, 0)
		}
		return uint64(-int32(ZSTD_error_init_missing))
		goto _11
	_11:
		;
		if 0 != 0 {
			goto _12
		}
		goto _10
	_10:
		;
	_6:
		;
		if flushMode == int32(ZSTD_e_end) && (uint64(int64(oend)-int64(op)) >= ZSTD_compressBound(tls, uint64(int64(iend)-int64(ip))) || (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable)) && (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos == uint64(0) {
			/* shortcut to compression pass directly into output buffer */
			cSize = ZSTD_compressEnd_public(tls, zcs, op, uint64(int64(oend)-int64(op)), ip, uint64(int64(iend)-int64(ip)))
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+4750, 0)
				}
				return err_code
			}
			ip = iend
			op = op + uintptr(cSize)
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded = uint32(1)
			ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
			someMoreWork = uint32(0)
			goto _9
		}
		/* complete loading into inBuffer in buffered mode */
		if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered) {
			toLoad = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget - (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos
			loaded = ZSTD_limitCopy(tls, (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos), toLoad, ip, uint64(int64(iend)-int64(ip)))
			*(*size_t)(unsafe.Pointer(zcs + 3592)) += loaded
			if ip != 0 {
				ip = ip + uintptr(loaded)
			}
			if flushMode == int32(ZSTD_e_continue) && (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos < (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget {
				/* not enough input to fill full block : stop here */
				someMoreWork = uint32(0)
				goto _9
			}
			if flushMode == int32(ZSTD_e_flush) && (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos == (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress {
				/* empty */
				someMoreWork = uint32(0)
				goto _9
			}
		} else {
			if flushMode == int32(ZSTD_e_continue) && uint64(int64(iend)-int64(ip)) < (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax {
				/* can't compress a full block : stop here */
				(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed = uint64(int64(iend) - int64(ip))
				ip = iend /* pretend to have consumed input */
				someMoreWork = uint32(0)
				goto _9
			}
			if flushMode == int32(ZSTD_e_flush) && ip == iend {
				/* empty */
				someMoreWork = uint32(0)
				goto _9
			}
		}
		/* compress current block (note : this stage cannot be stopped in the middle) */
		inputBuffered = libc.BoolInt32((*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered))
		oSize = uint64(int64(oend) - int64(op))
		if inputBuffered != 0 {
			v13 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos - (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress
		} else {
			if uint64(int64(iend)-int64(ip)) < (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax {
				v14 = uint64(int64(iend) - int64(ip))
			} else {
				v14 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax
			}
			v13 = v14
		}
		iSize = v13
		if oSize >= ZSTD_compressBound(tls, iSize) || (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable) {
			cDst = op
		} else {
			cDst = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuff
			oSize = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffSize
		}
		if inputBuffered != 0 {
			lastBlock = libc.BoolUint32(flushMode == int32(ZSTD_e_end) && ip == iend)
			if lastBlock != 0 {
				v13 = ZSTD_compressEnd_public(tls, zcs, cDst, oSize, (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress), iSize)
			} else {
				v13 = ZSTD_compressContinue_public(tls, zcs, cDst, oSize, (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress), iSize)
			}
			cSize1 = v13
			err_code1 = cSize1
			if ERR_isError(tls, err_code1) != 0 {
				if 0 != 0 {
					if lastBlock != 0 {
						v1 = __ccgo_ts + 4750
					} else {
						v1 = __ccgo_ts + 4774
					}
					_force_has_format_string(tls, __ccgo_ts+4400, libc.VaList(bp+8, v1))
				}
				return err_code1
			}
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded = lastBlock
			/* prepare next block */
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos + (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax
			if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget > (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffSize {
				(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos = uint64(0)
				(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax
			}
			if !(lastBlock != 0) {
			}
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos
		} else { /* !inputBuffered, hence ZSTD_bm_stable */
			lastBlock1 = libc.BoolUint32(flushMode == int32(ZSTD_e_end) && ip+uintptr(iSize) == iend)
			if lastBlock1 != 0 {
				v13 = ZSTD_compressEnd_public(tls, zcs, cDst, oSize, ip, iSize)
			} else {
				v13 = ZSTD_compressContinue_public(tls, zcs, cDst, oSize, ip, iSize)
			}
			cSize1 = v13
			/* Consume the input prior to error checking to mirror buffered mode. */
			if ip != 0 {
				ip = ip + uintptr(iSize)
			}
			err_code2 = cSize1
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					if lastBlock1 != 0 {
						v1 = __ccgo_ts + 4750
					} else {
						v1 = __ccgo_ts + 4774
					}
					_force_has_format_string(tls, __ccgo_ts+4400, libc.VaList(bp+8, v1))
				}
				return err_code2
			}
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded = lastBlock1
			if lastBlock1 != 0 {
			}
		}
		if cDst == op { /* no need to flush */
			op = op + uintptr(cSize1)
			if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
				someMoreWork = uint32(0)
				ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
			}
			goto _9
		}
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffContentSize = cSize1
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize = uint64(0)
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstreamStage = int32(zcss_flush) /* pass-through to flush stage */
	_7:
		;
		toFlush = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffContentSize - (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize
		flushed = ZSTD_limitCopy(tls, op, uint64(int64(oend)-int64(op)), (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize), toFlush)
		if flushed != 0 {
			op = op + uintptr(flushed)
		}
		*(*size_t)(unsafe.Pointer(zcs + 3632)) += flushed
		if toFlush != flushed {
			/* flush not fully completed, presumably because dst is too small */
			someMoreWork = uint32(0)
			goto _9
		}
		v19 = libc.Uint64FromInt32(0)
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize = v19
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffContentSize = v19
		if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
			someMoreWork = uint32(0)
			ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
			goto _9
		}
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstreamStage = int32(zcss_load)
		goto _9
	_8:
		; /* impossible */
	_9:
	}
	(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = uint64(int64(ip) - int64(istart))
	(*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos = uint64(int64(op) - int64(ostart))
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
		return uint64(0)
	}
	return ZSTD_nextInputSizeHint(tls, zcs)
}

func ZSTD_nextInputSizeHint_MTorST(tls *libc.TLS, cctx uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers >= int32(1) {
		return ZSTDMT_nextInputSizeHint(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	}
	return ZSTD_nextInputSizeHint(tls, cctx)
}

func ZSTD_compressStream(tls *libc.TLS, zcs uintptr, output uintptr, input uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_compressStream2(tls, zcs, output, input, int32(ZSTD_e_continue))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_nextInputSizeHint_MTorST(tls, zcs)
}

// C documentation
//
//	/* After a compression call set the expected input/output buffer.
//	 * This is validated at the start of the next compression call.
//	 */
func ZSTD_setBufferExpectations(tls *libc.TLS, cctx uintptr, output uintptr, input uintptr) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer = *(*ZSTD_inBuffer)(unsafe.Pointer(input))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedOutBufferSize = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize - (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
	}
}

// C documentation
//
//	/* Validate that the input/output buffers match the expectations set by
//	 * ZSTD_setBufferExpectations.
//	 */
func ZSTD_checkBufferStability(tls *libc.TLS, cctx uintptr, output uintptr, input uintptr, endOp ZSTD_EndDirective) (r size_t) {
	var expect ZSTD_inBuffer
	var outBufferSize size_t
	_, _ = expect, outBufferSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		expect = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer
		if expect.Fsrc != (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc || expect.Fpos != (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4803, 0)
			}
			return uint64(-int32(ZSTD_error_stabilityCondition_notRespected))
		}
	}
	_ = endOp
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable) {
		outBufferSize = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize - (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedOutBufferSize != outBufferSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4852, 0)
			}
			return uint64(-int32(ZSTD_error_stabilityCondition_notRespected))
		}
	}
	return uint64(0)
}

// C documentation
//
//	/*
//	 * If @endOp == ZSTD_e_end, @inSize becomes pledgedSrcSize.
//	 * Otherwise, it's ignored.
//	 * @return: 0 on success, or a ZSTD_error code otherwise.
//	 */
func ZSTD_CCtx_init_compressStream2(tls *libc.TLS, cctx uintptr, endOp ZSTD_EndDirective, inSize size_t) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var dictSize, err_code, err_code1, err_code2, v5 size_t
	var mode ZSTD_CParamMode_e
	var pledgedSrcSize U64
	var prefixDict ZSTD_prefixDict
	var v1, v2 uint64
	var v3 uint32
	var _ /* params at bp+0 */ ZSTD_CCtx_params
	_, _, _, _, _, _, _, _, _, _, _ = dictSize, err_code, err_code1, err_code2, mode, pledgedSrcSize, prefixDict, v1, v2, v3, v5
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams
	prefixDict = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict
	err_code = ZSTD_initLocalDict(tls, cctx)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	} /* Init the local dict if present. */
	libc.Xmemset(tls, cctx+3736, 0, libc.Uint64FromInt64(24)) /* single usage */
	/* only one can be set */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 && !((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fcdict != 0) {
		/* Let the cdict's compression level take priority over the requested params.
		 * But do not take the cdict's compression level if the "cdict" is actually a localDict
		 * generated from ZSTD_initLocalDict().
		 */
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FcompressionLevel
	}
	if endOp == int32(ZSTD_e_end) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne = inSize + uint64(1)
	} /* auto-determine pledgedSrcSize */
	if prefixDict.Fdict != 0 {
		v1 = prefixDict.FdictSize
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
			v2 = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize
		} else {
			v2 = uint64(0)
		}
		v1 = v2
	}
	dictSize = v1
	mode = ZSTD_getCParamMode(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = ZSTD_getCParamsFromCCtxParams(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1), dictSize, mode)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FpostBlockSplitter = ZSTD_resolveBlockSplitterMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FpostBlockSplitter, bp+4)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm = ZSTD_resolveEnableLdm(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm, bp+4)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, bp+4)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FvalidateSequences = ZSTD_resolveExternalSequenceValidation(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FvalidateSequences)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FmaxBlockSize = ZSTD_resolveMaxBlockSize(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FmaxBlockSize)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FsearchForExternalRepcodes = ZSTD_resolveExternalRepcodeSearch(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FsearchForExternalRepcodes, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel)
	/* If external matchfinder is enabled, make sure to fail before checking job size (for consistency) */
	if ZSTD_hasExtSeqProd(tls, bp) != 0 && (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers >= int32(1) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4908, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_combination_unsupported))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1) <= uint64(libc.Int32FromInt32(512)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers = 0 /* do not invoke multi-threading when src size is too small */
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers > 0 {
		/* mt context creation */
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx == libc.UintptrFromInt32(0) {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx = ZSTDMT_createCCtx_advanced(tls, uint32((*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fpool)
			if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx == libc.UintptrFromInt32(0) {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1377, 0)
				}
				return uint64(-int32(ZSTD_error_memory_allocation))
			}
		}
		/* mt compression */
		err_code1 = ZSTDMT_initCStream_internal(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx, prefixDict.Fdict, prefixDict.FdictSize, prefixDict.FdictContentType, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp)), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1))
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code1
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
			v3 = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictID
		} else {
			v3 = uint32(0)
		}
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = v3
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
			v1 = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize
		} else {
			v1 = prefixDict.FdictSize
		}
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = v1
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize = uint64(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize = uint64(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage = int32(zcss_load)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(bp))
	} else {
		pledgedSrcSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne - uint64(1)
		err_code2 = ZSTD_compressBegin_internal(tls, cctx, prefixDict.Fdict, prefixDict.FdictSize, prefixDict.FdictContentType, int32(ZSTD_dtlm_fast), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict, bp, pledgedSrcSize, int32(ZSTDb_buffered))
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code2
		}
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinToCompress = uint64(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffPos = uint64(0)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered) {
			/* for small input: avoid automatic flush on reaching end of block, since
			 * it would require to add a 3-bytes null block to end frame
			 */
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffTarget = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax + libc.BoolUint64((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax == pledgedSrcSize)
		} else {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffTarget = uint64(0)
		}
		v5 = libc.Uint64FromInt32(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffFlushedSize = v5
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffContentSize = v5
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage = int32(zcss_load)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FframeEnded = uint32(0)
	}
	return uint64(0)
}

// C documentation
//
//	/* @return provides a minimum amount of data remaining to be flushed from internal buffers
//	 */
func ZSTD_compressStream2(tls *libc.TLS, cctx uintptr, output uintptr, input uintptr, endOp ZSTD_EndDirective) (r size_t) {
	var err_code, err_code1, err_code2, err_code3, flushMin, inputSize, ipos, opos, totalInputSize size_t
	var v1 int32
	_, _, _, _, _, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, flushMin, inputSize, ipos, opos, totalInputSize, v1
	/* check conditions */
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos > (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4971, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos > (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4993, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if uint32(endOp) > uint32(ZSTD_e_end) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5014, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	/* transparent initialization stage */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage == int32(zcss_init) {
		inputSize = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize - (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos /* no obligation to start from pos==0 */
		totalInputSize = inputSize + (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode == int32(ZSTD_bm_stable) && endOp == int32(ZSTD_e_continue) && totalInputSize < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) { /* not even reached one block yet */
			if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed != 0 { /* not the first time */
				/* check stable source guarantees */
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc != (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer.Fsrc {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+5035, 0)
					}
					return uint64(-int32(ZSTD_error_stabilityCondition_notRespected))
				}
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos != (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer.Fsize {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+5093, 0)
					}
					return uint64(-int32(ZSTD_error_stabilityCondition_notRespected))
				}
			}
			/* pretend input was consumed, to give a sense forward progress */
			(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize
			/* save stable inBuffer, for later control, and flush/end */
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer = *(*ZSTD_inBuffer)(unsafe.Pointer(input))
			/* but actually input wasn't consumed, so keep track of position from where compression shall resume */
			*(*size_t)(unsafe.Pointer(cctx + 3672)) += inputSize
			/* don't initialize yet, wait for the first block of flush() order, for better parameters adaptation */
			if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.Fformat == int32(ZSTD_f_zstd1) {
				v1 = int32(6)
			} else {
				v1 = int32(2)
			}
			return uint64(v1) /* at least some header to produce */
		}
		err_code = ZSTD_CCtx_init_compressStream2(tls, cctx, endOp, totalInputSize)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5157, 0)
			}
			return err_code
		}
		ZSTD_setBufferExpectations(tls, cctx, output, input) /* Set initial buffer expectations now that we've initialized */
	}
	/* end of transparent initialization stage */
	err_code1 = ZSTD_checkBufferStability(tls, cctx, output, input, endOp)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5195, 0)
		}
		return err_code1
	}
	/* compression stage */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers > 0 {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcParamsChanged != 0 {
			ZSTDMT_updateCParams_whileCompressing(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx, cctx+16)
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcParamsChanged = 0
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed != 0 {
			/* some early data was skipped - make it available for consumption */
			*(*size_t)(unsafe.Pointer(input + 16)) -= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed = uint64(0)
		}
		for {
			ipos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos
			opos = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
			flushMin = ZSTDMT_compressStream_generic(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx, output, input, endOp)
			*(*uint64)(unsafe.Pointer(cctx + 792)) += (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos - ipos
			*(*uint64)(unsafe.Pointer(cctx + 800)) += (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos - opos
			if ZSTD_isError(tls, flushMin) != 0 || endOp == int32(ZSTD_e_end) && flushMin == uint64(0) { /* compression completed */
				if flushMin == uint64(0) {
					ZSTD_CCtx_trace(tls, cctx, uint64(0))
				}
				ZSTD_CCtx_reset(tls, cctx, int32(ZSTD_reset_session_only))
			}
			err_code2 = flushMin
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5211, 0)
				}
				return err_code2
			}
			if endOp == int32(ZSTD_e_continue) {
				/* We only require some progress with ZSTD_e_continue, not maximal progress.
				 * We're done if we've consumed or produced any bytes, or either buffer is
				 * full.
				 */
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos != ipos || (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos != opos || (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos == (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize || (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
					break
				}
			} else {
				/* We require maximal progress. We're done when the flush is complete or the
				 * output buffer is full.
				 */
				if flushMin == uint64(0) || (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
					break
				}
			}
			goto _2
		_2:
		}
		/* Either we don't require maximum forward progress, we've finished the
		 * flush, or we are out of output space.
		 */
		ZSTD_setBufferExpectations(tls, cctx, output, input)
		return flushMin
	}
	err_code3 = ZSTD_compressStream_generic(tls, cctx, output, input, endOp)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	ZSTD_setBufferExpectations(tls, cctx, output, input)
	return (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffContentSize - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffFlushedSize /* remaining to flush */
}

func ZSTD_compressStream2_simpleArgs(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, dstPos uintptr, src uintptr, srcSize size_t, srcPos uintptr, endOp ZSTD_EndDirective) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var cErr size_t
	var _ /* input at bp+24 */ ZSTD_inBuffer
	var _ /* output at bp+0 */ ZSTD_outBuffer
	_ = cErr
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fdst = dst
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fsize = dstCapacity
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos = *(*size_t)(unsafe.Pointer(dstPos))
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsrc = src
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsize = srcSize
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos = *(*size_t)(unsafe.Pointer(srcPos))
	/* ZSTD_compressStream2() will check validity of dstPos and srcPos */
	cErr = ZSTD_compressStream2(tls, cctx, bp, bp+24, endOp)
	*(*size_t)(unsafe.Pointer(dstPos)) = (*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos
	*(*size_t)(unsafe.Pointer(srcPos)) = (*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos
	return cErr
	return r
}

func ZSTD_compress2(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var err_code, result size_t
	var originalInBufferMode, originalOutBufferMode ZSTD_bufferMode_e
	var _ /* iPos at bp+8 */ size_t
	var _ /* oPos at bp+0 */ size_t
	_, _, _, _ = err_code, originalInBufferMode, originalOutBufferMode, result
	originalInBufferMode = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode
	originalOutBufferMode = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FoutBufferMode
	ZSTD_CCtx_reset(tls, cctx, int32(ZSTD_reset_session_only))
	/* Enable stable input/output buffers. */
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode = int32(ZSTD_bm_stable)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FoutBufferMode = int32(ZSTD_bm_stable)
	*(*size_t)(unsafe.Pointer(bp)) = uint64(0)
	*(*size_t)(unsafe.Pointer(bp + 8)) = uint64(0)
	result = ZSTD_compressStream2_simpleArgs(tls, cctx, dst, dstCapacity, bp, src, srcSize, bp+8, int32(ZSTD_e_end))
	/* Reset to the original values. */
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode = originalInBufferMode
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FoutBufferMode = originalOutBufferMode
	err_code = result
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5248, 0)
		}
		return err_code
	}
	if result != uint64(0) { /* compression not completed, due to lack of output space */
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* all input is expected consumed */
	return *(*size_t)(unsafe.Pointer(bp))
	return r
}

// C documentation
//
//	/* ZSTD_validateSequence() :
//	 * @offBase : must use the format required by ZSTD_storeSeq()
//	 * @returns a ZSTD error code if sequence is not valid
//	 */
func ZSTD_validateSequence(tls *libc.TLS, offBase U32, matchLength U32, minMatch U32, posInSrc size_t, windowLog U32, dictSize size_t, useSequenceProducer int32) (r size_t) {
	var matchLenLowerBound, offsetBound size_t
	var windowSize U32
	var v1 uint64
	var v2 int32
	_, _, _, _, _ = matchLenLowerBound, offsetBound, windowSize, v1, v2
	windowSize = uint32(1) << windowLog
	if posInSrc > uint64(windowSize) {
		v1 = uint64(windowSize)
	} else {
		v1 = posInSrc + dictSize
	}
	/* posInSrc represents the amount of data the decoder would decode up to this point.
	 * As long as the amount of data decoded is less than or equal to window size, offsets may be
	 * larger than the total length of output decoded in order to reference the dict, even larger than
	 * window size. After output surpasses windowSize, we're limited to windowSize offsets again.
	 */
	offsetBound = v1
	if minMatch == uint32(3) || useSequenceProducer != 0 {
		v2 = int32(3)
	} else {
		v2 = int32(4)
	}
	matchLenLowerBound = uint64(v2)
	if uint64(offBase) > offsetBound+libc.Uint64FromInt32(ZSTD_REP_NUM) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5287, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	/* Validate maxNbSeq is large enough for the given matchLength and minMatch */
	if uint64(matchLength) < matchLenLowerBound {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5305, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	return uint64(0)
}

// C documentation
//
//	/* Returns an offset code, given a sequence's raw offset, the ongoing repcode array, and whether litLength == 0 */
func ZSTD_finalizeOffBase(tls *libc.TLS, rawOffset U32, rep uintptr, ll0 U32) (r U32) {
	var offBase U32
	_ = offBase
	offBase = rawOffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
	if !(ll0 != 0) && rawOffset == *(*U32)(unsafe.Pointer(rep)) {
		offBase = uint32(libc.Int32FromInt32(1))
	} else {
		if rawOffset == *(*U32)(unsafe.Pointer(rep + 1*4)) {
			offBase = libc.Uint32FromInt32(2) - ll0
		} else {
			if rawOffset == *(*U32)(unsafe.Pointer(rep + 2*4)) {
				offBase = libc.Uint32FromInt32(3) - ll0
			} else {
				if ll0 != 0 && rawOffset == *(*U32)(unsafe.Pointer(rep))-uint32(1) {
					offBase = uint32(libc.Int32FromInt32(3))
				}
			}
		}
	}
	return offBase
}

// C documentation
//
//	/* This function scans through an array of ZSTD_Sequence,
//	 * storing the sequences it reads, until it reaches a block delimiter.
//	 * Note that the block delimiter includes the last literals of the block.
//	 * @blockSize must be == sum(sequence_lengths).
//	 * @returns @blockSize on success, and a ZSTD_error otherwise.
//	 */
func ZSTD_transferSequences_wBlockDelim(tls *libc.TLS, cctx uintptr, seqPos uintptr, inSeqs uintptr, inSeqsSize size_t, src uintptr, blockSize size_t, externalRepSearch ZSTD_ParamSwitch_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var dictSize, idx, lastSeqIdx, litLength, ll0, matchLength, offBase, startIdx U32
	var err_code size_t
	var iend, ip, rep uintptr
	var _ /* updatedRepcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _ = dictSize, err_code, idx, iend, ip, lastSeqIdx, litLength, ll0, matchLength, offBase, rep, startIdx
	idx = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx
	startIdx = idx
	ip = src
	iend = ip + uintptr(blockSize)
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
		dictSize = uint32((*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize)
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.Fdict != 0 {
			dictSize = uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictSize)
		} else {
			dictSize = uint32(0)
		}
	}
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	for {
		if !(uint64(idx) < inSeqsSize && ((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FmatchLength != uint32(0) || (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).Foffset != uint32(0))) {
			break
		}
		litLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength
		matchLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FmatchLength
		if externalRepSearch == int32(ZSTD_ps_disable) {
			offBase = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).Foffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
		} else {
			ll0 = libc.BoolUint32(litLength == libc.Uint32FromInt32(0))
			offBase = ZSTD_finalizeOffBase(tls, (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).Foffset, bp, ll0)
			ZSTD_updateRep(tls, bp, offBase, ll0)
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FvalidateSequences != 0 {
			*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64(litLength + matchLength)
			err_code = ZSTD_validateSequence(tls, offBase, matchLength, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch, (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSrc, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FwindowLog, uint64(dictSize), ZSTD_hasExtSeqProd(tls, cctx+240))
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5344, 0)
				}
				return err_code
			}
		}
		if uint64(idx-(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx) >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FmaxNbSeq {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5371, 0)
			}
			return uint64(-int32(ZSTD_error_externalSequences_invalid))
		}
		ZSTD_storeSeq(tls, cctx+976, uint64(litLength), ip, iend, offBase, uint64(matchLength))
		ip = ip + uintptr(matchLength+litLength)
		goto _1
	_1:
		;
		idx = idx + 1
	}
	if uint64(idx) == inSeqsSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5431, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	/* If we skipped repcode search while parsing, we need to update repcodes now */
	if externalRepSearch == int32(ZSTD_ps_disable) && idx != startIdx {
		rep = bp
		lastSeqIdx = idx - uint32(1) /* index of last non-block-delimiter sequence */
		if lastSeqIdx >= startIdx+uint32(2) {
			*(*U32)(unsafe.Pointer(rep + 2*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(2))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(1))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
		} else {
			if lastSeqIdx == startIdx+uint32(1) {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(1))*16))).Foffset
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
			} else {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep + 1*4))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
			}
		}
	}
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock+5616, bp, libc.Uint64FromInt64(12))
	if (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength != 0 {
		ZSTD_storeLastLiterals(tls, cctx+976, ip, uint64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength))
		ip = ip + uintptr((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength)
		*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength)
	}
	if ip != iend {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5458, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx = idx + uint32(1)
	return blockSize
}

// C documentation
//
//	/*
//	 * This function attempts to scan through @blockSize bytes in @src
//	 * represented by the sequences in @inSeqs,
//	 * storing any (partial) sequences.
//	 *
//	 * Occasionally, we may want to reduce the actual number of bytes consumed from @src
//	 * to avoid splitting a match, notably if it would produce a match smaller than MINMATCH.
//	 *
//	 * @returns the number of bytes consumed from @src, necessarily <= @blockSize.
//	 * Otherwise, it may return a ZSTD error if something went wrong.
//	 */
func ZSTD_transferSequences_noDelim(tls *libc.TLS, cctx uintptr, seqPos uintptr, inSeqs uintptr, inSeqsSize size_t, src uintptr, blockSize size_t, externalRepSearch ZSTD_ParamSwitch_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var bytesAdjustment, endPosInSequence, finalMatchSplit, firstHalfMatchLength, idx, lastLLSize, litLength, ll0, matchLength, offBase, rawOffset, secondHalfMatchLength, startPosInSequence U32
	var currSeq ZSTD_Sequence
	var dictSize, err_code size_t
	var iend, ip, istart uintptr
	var v1 uint32
	var _ /* updatedRepcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bytesAdjustment, currSeq, dictSize, endPosInSequence, err_code, finalMatchSplit, firstHalfMatchLength, idx, iend, ip, istart, lastLLSize, litLength, ll0, matchLength, offBase, rawOffset, secondHalfMatchLength, startPosInSequence, v1
	idx = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx
	startPosInSequence = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSequence
	endPosInSequence = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSequence + uint32(blockSize)
	istart = src
	ip = istart
	iend = istart + uintptr(blockSize)
	bytesAdjustment = uint32(0)
	finalMatchSplit = uint32(0)
	/* TODO(embg) support fast parsing mode in noBlockDelim mode */
	_ = externalRepSearch
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
		dictSize = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.Fdict != 0 {
			dictSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictSize
		} else {
			dictSize = uint64(0)
		}
	}
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	for endPosInSequence != 0 && uint64(idx) < inSeqsSize && !(finalMatchSplit != 0) {
		currSeq = *(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))
		litLength = currSeq.FlitLength
		matchLength = currSeq.FmatchLength
		rawOffset = currSeq.Foffset
		/* Modify the sequence depending on where endPosInSequence lies */
		if endPosInSequence >= currSeq.FlitLength+currSeq.FmatchLength {
			if startPosInSequence >= litLength {
				startPosInSequence = startPosInSequence - litLength
				litLength = uint32(0)
				matchLength = matchLength - startPosInSequence
			} else {
				litLength = litLength - startPosInSequence
			}
			/* Move to the next sequence */
			endPosInSequence = endPosInSequence - (currSeq.FlitLength + currSeq.FmatchLength)
			startPosInSequence = uint32(0)
		} else {
			/* This is the final (partial) sequence we're adding from inSeqs, and endPosInSequence
			   does not reach the end of the match. So, we have to split the sequence */
			if endPosInSequence > litLength {
				if startPosInSequence >= litLength {
					v1 = uint32(0)
				} else {
					v1 = litLength - startPosInSequence
				}
				litLength = v1
				firstHalfMatchLength = endPosInSequence - startPosInSequence - litLength
				if uint64(matchLength) > blockSize && firstHalfMatchLength >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch {
					/* Only ever split the match if it is larger than the block size */
					secondHalfMatchLength = currSeq.FmatchLength + currSeq.FlitLength - endPosInSequence
					if secondHalfMatchLength < (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch {
						/* Move the endPosInSequence backward so that it creates match of minMatch length */
						endPosInSequence = endPosInSequence - ((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch - secondHalfMatchLength)
						bytesAdjustment = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch - secondHalfMatchLength
						firstHalfMatchLength = firstHalfMatchLength - bytesAdjustment
					}
					matchLength = firstHalfMatchLength
					/* Flag that we split the last match - after storing the sequence, exit the loop,
					   but keep the value of endPosInSequence */
					finalMatchSplit = uint32(1)
				} else {
					/* Move the position in sequence backwards so that we don't split match, and break to store
					 * the last literals. We use the original currSeq.litLength as a marker for where endPosInSequence
					 * should go. We prefer to do this whenever it is not necessary to split the match, or if doing so
					 * would cause the first half of the match to be too small
					 */
					bytesAdjustment = endPosInSequence - currSeq.FlitLength
					endPosInSequence = currSeq.FlitLength
					break
				}
			} else {
				/* This sequence ends inside the literals, break to store the last literals */
				break
			}
		}
		/* Check if this offset can be represented with a repcode */
		ll0 = libc.BoolUint32(litLength == libc.Uint32FromInt32(0))
		offBase = ZSTD_finalizeOffBase(tls, rawOffset, bp, ll0)
		ZSTD_updateRep(tls, bp, offBase, ll0)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FvalidateSequences != 0 {
			*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64(litLength + matchLength)
			err_code = ZSTD_validateSequence(tls, offBase, matchLength, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch, (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSrc, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FwindowLog, dictSize, ZSTD_hasExtSeqProd(tls, cctx+240))
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5344, 0)
				}
				return err_code
			}
		}
		if uint64(idx-(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx) >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FmaxNbSeq {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5371, 0)
			}
			return uint64(-int32(ZSTD_error_externalSequences_invalid))
		}
		ZSTD_storeSeq(tls, cctx+976, uint64(litLength), ip, iend, offBase, uint64(matchLength))
		ip = ip + uintptr(matchLength+litLength)
		if !(finalMatchSplit != 0) {
			idx = idx + 1
		} /* Next Sequence */
	}
	(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx = idx
	(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSequence = endPosInSequence
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock+5616, bp, libc.Uint64FromInt64(12))
	iend = iend - uintptr(bytesAdjustment)
	if ip != iend {
		/* Store any last literals */
		lastLLSize = uint32(int64(iend) - int64(ip))
		ZSTD_storeLastLiterals(tls, cctx+976, ip, uint64(lastLLSize))
		*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64(lastLLSize)
	}
	return uint64(int64(iend) - int64(istart))
}

// C documentation
//
//	/* @seqPos represents a position within @inSeqs,
//	 * it is read and updated by this function,
//	 * once the goal to produce a block of size @blockSize is reached.
//	 * @return: nb of bytes consumed from @src, necessarily <= @blockSize.
//	 */
type ZSTD_SequenceCopier_f = uintptr

func ZSTD_selectSequenceCopier(tls *libc.TLS, mode ZSTD_SequenceFormat_e) (r ZSTD_SequenceCopier_f) {
	if mode == int32(ZSTD_sf_explicitBlockDelimiters) {
		return __ccgo_fp(ZSTD_transferSequences_wBlockDelim)
	}
	return __ccgo_fp(ZSTD_transferSequences_noDelim)
}

// C documentation
//
//	/* Discover the size of next block by searching for the delimiter.
//	 * Note that a block delimiter **must** exist in this mode,
//	 * otherwise it's an input error.
//	 * The block size retrieved will be later compared to ensure it remains within bounds */
func blockSize_explicitDelimiter(tls *libc.TLS, inSeqs uintptr, inSeqsSize size_t, seqPos ZSTD_SequencePosition) (r size_t) {
	var blockSize, spos size_t
	var end int32
	_, _, _ = blockSize, end, spos
	end = 0
	blockSize = uint64(0)
	spos = uint64(seqPos.Fidx)
	for spos < inSeqsSize {
		end = libc.BoolInt32((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).Foffset == uint32(0))
		blockSize = blockSize + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).FlitLength+(*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).FmatchLength)
		if end != 0 {
			if (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).FmatchLength != uint32(0) {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5504, 0)
				}
				return uint64(-int32(ZSTD_error_externalSequences_invalid))
			}
			break
		}
		spos = spos + 1
	}
	if !(end != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5570, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	return blockSize
}

func determine_blockSize(tls *libc.TLS, mode ZSTD_SequenceFormat_e, blockSize size_t, remaining size_t, inSeqs uintptr, inSeqsSize size_t, seqPos ZSTD_SequencePosition) (r size_t) {
	var err_code, explicitBlockSize size_t
	var v1 uint64
	_, _, _ = err_code, explicitBlockSize, v1
	if mode == int32(ZSTD_sf_noBlockDelimiters) {
		/* Note: more a "target" block size */
		if remaining < blockSize {
			v1 = remaining
		} else {
			v1 = blockSize
		}
		return v1
	}
	explicitBlockSize = blockSize_explicitDelimiter(tls, inSeqs, inSeqsSize, seqPos)
	err_code = explicitBlockSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5629, 0)
		}
		return err_code
	}
	if explicitBlockSize > blockSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5689, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	if explicitBlockSize > remaining {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5736, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	return explicitBlockSize
	return r
}

// C documentation
//
//	/* Compress all provided sequences, block-by-block.
//	 *
//	 * Returns the cumulative size of all compressed blocks (including their headers),
//	 * otherwise a ZSTD error.
//	 */
func ZSTD_compressSequences_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, inSeqsSize size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var blockSize, cBlockSize, cSize, compressedSeqsSize, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, remaining size_t
	var cBlockHeader, cBlockHeader24, lastBlock U32
	var ip, op uintptr
	var sequenceCopier ZSTD_SequenceCopier_f
	var _ /* seqPos at bp+0 */ ZSTD_SequencePosition
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, cBlockHeader, cBlockHeader24, cBlockSize, cSize, compressedSeqsSize, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, ip, lastBlock, op, remaining, sequenceCopier
	cSize = uint64(0)
	remaining = srcSize
	*(*ZSTD_SequencePosition)(unsafe.Pointer(bp)) = ZSTD_SequencePosition{}
	ip = src
	op = dst
	sequenceCopier = ZSTD_selectSequenceCopier(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FblockDelimiters)
	/* Special case: empty frame */
	if remaining == uint64(0) {
		cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1)
		if dstCapacity < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5780, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE32(tls, op, cBlockHeader24)
		op = op + uintptr(ZSTD_blockHeaderSize)
		dstCapacity = dstCapacity - ZSTD_blockHeaderSize
		cSize = cSize + ZSTD_blockHeaderSize
	}
	for remaining != 0 {
		blockSize = determine_blockSize(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FblockDelimiters, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax, remaining, inSeqs, inSeqsSize, *(*ZSTD_SequencePosition)(unsafe.Pointer(bp)))
		lastBlock = libc.BoolUint32(blockSize == remaining)
		err_code = blockSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5817, 0)
			}
			return err_code
		}
		ZSTD_resetSeqStore(tls, cctx+976)
		blockSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, size_t, uintptr, size_t, ZSTD_ParamSwitch_e) size_t)(unsafe.Pointer(&struct{ uintptr }{sequenceCopier})))(tls, cctx, bp, inSeqs, inSeqsSize, ip, blockSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FsearchForExternalRepcodes)
		err_code1 = blockSize
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5860, 0)
			}
			return err_code1
		}
		/* If blocks are too small, emit as a nocompress block */
		/* TODO: See 3090. We reduced MIN_CBLOCK_SIZE from 3 to 2 so to compensate we are adding
		 * additional 1. We need to revisit and change this logic to be more consistent */
		if blockSize < uint64(libc.Int32FromInt32(1)+libc.Int32FromInt32(1))+ZSTD_blockHeaderSize+uint64(1)+uint64(1) {
			cBlockSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, blockSize, lastBlock)
			err_code2 = cBlockSize
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3619, 0)
				}
				return err_code2
			}
			cSize = cSize + cBlockSize
			ip = ip + uintptr(blockSize)
			op = op + uintptr(cBlockSize)
			remaining = remaining - blockSize
			dstCapacity = dstCapacity - cBlockSize
			continue
		}
		if dstCapacity < ZSTD_blockHeaderSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5878, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		compressedSeqsSize = ZSTD_entropyCompressSeqStore(tls, cctx+976, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock, cctx+240, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, blockSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2)
		err_code3 = compressedSeqsSize
		if ERR_isError(tls, err_code3) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5933, 0)
			}
			return err_code3
		}
		if !((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock != 0) && ZSTD_maybeRLE(tls, cctx+976) != 0 && ZSTD_isRLE(tls, ip, blockSize) != 0 {
			/* Note: don't emit the first block as RLE even if it qualifies because
			 * doing so will cause the decoder (cli <= v1.4.3 only) to throw an (invalid) error
			 * "should consume all input error."
			 */
			compressedSeqsSize = uint64(1)
		}
		if compressedSeqsSize == uint64(0) {
			/* ZSTD_noCompressBlock writes the block header as well */
			cBlockSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, blockSize, lastBlock)
			err_code4 = cBlockSize
			if ERR_isError(tls, err_code4) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1741, 0)
				}
				return err_code4
			}
		} else {
			if compressedSeqsSize == uint64(1) {
				cBlockSize = ZSTD_rleCompressBlock(tls, op, dstCapacity, *(*BYTE)(unsafe.Pointer(ip)), blockSize, lastBlock)
				err_code5 = cBlockSize
				if ERR_isError(tls, err_code5) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+5971, 0)
					}
					return err_code5
				}
			} else {
				/* Error checking and repcodes update */
				ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, cctx+3224)
				if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
					(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
				}
				/* Write block header into beginning of block*/
				cBlockHeader = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(compressedSeqsSize<<libc.Int32FromInt32(3))
				MEM_writeLE24(tls, op, cBlockHeader)
				cBlockSize = ZSTD_blockHeaderSize + compressedSeqsSize
			}
		}
		cSize = cSize + cBlockSize
		if lastBlock != 0 {
			break
		} else {
			ip = ip + uintptr(blockSize)
			op = op + uintptr(cBlockSize)
			remaining = remaining - blockSize
			dstCapacity = dstCapacity - cBlockSize
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock = 0
		}
	}
	return cSize
}

func ZSTD_compressSequences(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, inSeqsSize size_t, src uintptr, srcSize size_t) (r size_t) {
	var cBlocksSize, cSize, err_code, err_code1, frameHeaderSize size_t
	var checksum U32
	var op uintptr
	_, _, _, _, _, _, _ = cBlocksSize, cSize, checksum, err_code, err_code1, frameHeaderSize, op
	op = dst
	cSize = uint64(0)
	/* Transparent initialization stage, same as compressStream2() */
	err_code = ZSTD_CCtx_init_compressStream2(tls, cctx, int32(ZSTD_e_end), srcSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6000, 0)
		}
		return err_code
	}
	/* Begin writing output, starting with frame header */
	frameHeaderSize = ZSTD_writeFrameHeader(tls, op, dstCapacity, cctx+240, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID)
	op = op + uintptr(frameHeaderSize)
	dstCapacity = dstCapacity - frameHeaderSize
	cSize = cSize + frameHeaderSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 && srcSize != 0 {
		XXH_INLINE_XXH64_update(tls, cctx+808, src, srcSize)
	}
	/* Now generate compressed blocks */
	cBlocksSize = ZSTD_compressSequences_internal(tls, cctx, op, dstCapacity, inSeqs, inSeqsSize, src, srcSize)
	err_code1 = cBlocksSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6027, 0)
		}
		return err_code1
	}
	cSize = cSize + cBlocksSize
	dstCapacity = dstCapacity - cBlocksSize
	/* Complete with frame checksum, if needed */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 {
		checksum = uint32(XXH_INLINE_XXH64_digest(tls, cctx+808))
		if dstCapacity < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4582, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE32(tls, dst+uintptr(cSize), checksum)
		cSize = cSize + uint64(4)
	}
	return cSize
}

func convertSequences_noRepcodes(tls *libc.TLS, dstSeqs uintptr, inSeqs uintptr, nbSequences size_t) (r size_t) {
	var longLen, n size_t
	_, _ = longLen, n
	longLen = uint64(0)
	n = uint64(0)
	for {
		if !(n < nbSequences) {
			break
		}
		(*(*SeqDef)(unsafe.Pointer(dstSeqs + uintptr(n)*8))).FoffBase = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).Foffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
		(*(*SeqDef)(unsafe.Pointer(dstSeqs + uintptr(n)*8))).FlitLength = uint16((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FlitLength)
		(*(*SeqDef)(unsafe.Pointer(dstSeqs + uintptr(n)*8))).FmlBase = uint16((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FmatchLength - libc.Uint32FromInt32(MINMATCH))
		/* check for long length > 65535 */
		if libc.BoolInt32((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FmatchLength > uint32(libc.Int32FromInt32(65535)+libc.Int32FromInt32(MINMATCH))) != 0 {
			longLen = n + uint64(1)
		}
		if libc.BoolInt32((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FlitLength > libc.Uint32FromInt32(65535)) != 0 {
			longLen = n + nbSequences + uint64(1)
		}
		goto _1
	_1:
		;
		n = n + 1
	}
	return longLen
}

// C documentation
//
//	/*
//	 * Precondition: Sequences must end on an explicit Block Delimiter
//	 * @return: 0 on success, or an error code.
//	 * Note: Sequence validation functionality has been disabled (removed).
//	 * This is helpful to generate a lean main pipeline, improving performance.
//	 * It may be re-inserted later.
//	 */
func ZSTD_convertBlockSequences(tls *libc.TLS, cctx uintptr, inSeqs uintptr, nbSequences size_t, repcodeResolution int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var lastSeqIdx, litLength, ll0, matchLength, offBase U32
	var longl, seqNb size_t
	var rep uintptr
	var _ /* updatedRepcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _ = lastSeqIdx, litLength, ll0, longl, matchLength, offBase, rep, seqNb
	seqNb = uint64(0)
	if nbSequences >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FmaxNbSeq {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5371, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	/* check end condition */
	/* Convert Sequences from public format to internal format */
	if !(repcodeResolution != 0) {
		longl = convertSequences_noRepcodes(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FsequencesStart, inSeqs, nbSequences-uint64(1))
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.Fsequences = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FsequencesStart + uintptr(nbSequences)*8 - uintptr(1)*8
		if longl != 0 {
			if longl <= nbSequences-uint64(1) {
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthType = int32(ZSTD_llt_matchLength)
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthPos = uint32(longl - libc.Uint64FromInt32(1))
			} else {
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthType = int32(ZSTD_llt_literalLength)
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthPos = uint32(longl - (nbSequences - libc.Uint64FromInt32(1)) - libc.Uint64FromInt32(1))
			}
		}
	} else {
		seqNb = uint64(0)
		for {
			if !(seqNb < nbSequences-uint64(1)) {
				break
			}
			litLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(seqNb)*16))).FlitLength
			matchLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(seqNb)*16))).FmatchLength
			ll0 = libc.BoolUint32(litLength == libc.Uint32FromInt32(0))
			offBase = ZSTD_finalizeOffBase(tls, (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(seqNb)*16))).Foffset, bp, ll0)
			ZSTD_storeSeqOnly(tls, cctx+976, uint64(litLength), offBase, uint64(matchLength))
			ZSTD_updateRep(tls, bp, offBase, ll0)
			goto _1
		_1:
			;
			seqNb = seqNb + 1
		}
	}
	/* If we skipped repcode search while parsing, we need to update repcodes now */
	if !(repcodeResolution != 0) && nbSequences > uint64(1) {
		rep = bp
		if nbSequences >= uint64(4) {
			lastSeqIdx = uint32(nbSequences) - uint32(2) /* index of last full sequence */
			*(*U32)(unsafe.Pointer(rep + 2*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(2))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(1))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
		} else {
			if nbSequences == uint64(3) {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs))).Foffset
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + 1*16))).Foffset
			} else {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep + 1*4))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs))).Foffset
			}
		}
	}
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock+5616, bp, libc.Uint64FromInt64(12))
	return uint64(0)
}

func ZSTD_get1BlockSummary(tls *libc.TLS, seqs uintptr, nbSeqs size_t) (r BlockSummary) {
	var bs, bs1 BlockSummary
	var litSize, n, totalMatchSize size_t
	_, _, _, _, _ = bs, bs1, litSize, n, totalMatchSize
	totalMatchSize = uint64(0)
	litSize = uint64(0)
	n = uint64(0)
	for {
		if !(n < nbSeqs) {
			break
		}
		totalMatchSize = totalMatchSize + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqs + uintptr(n)*16))).FmatchLength)
		litSize = litSize + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqs + uintptr(n)*16))).FlitLength)
		if (*(*ZSTD_Sequence)(unsafe.Pointer(seqs + uintptr(n)*16))).FmatchLength == uint32(0) {
			break
		}
		goto _1
	_1:
		;
		n = n + 1
	}
	if n == nbSeqs {
		bs.FnbSequences = uint64(-int32(ZSTD_error_externalSequences_invalid))
		return bs
	}
	bs1.FnbSequences = n + uint64(1)
	bs1.FblockSize = litSize + totalMatchSize
	bs1.FlitSize = litSize
	return bs1
	return r
}

func ZSTD_compressSequencesAndLiterals_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, nbSequences size_t, literals uintptr, litSize size_t, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var block BlockSummary
	var cBlockHeader, cBlockHeader24, lastBlock U32
	var cBlockSize, cSize, compressedSeqsSize, conversionStatus, err_code, err_code1, err_code2, remaining size_t
	var op uintptr
	var repcodeResolution int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = block, cBlockHeader, cBlockHeader24, cBlockSize, cSize, compressedSeqsSize, conversionStatus, err_code, err_code1, err_code2, lastBlock, op, remaining, repcodeResolution
	remaining = srcSize
	cSize = uint64(0)
	op = dst
	repcodeResolution = libc.BoolInt32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FsearchForExternalRepcodes == int32(ZSTD_ps_enable))
	if nbSequences == uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6054, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	/* Special case: empty frame */
	if nbSequences == uint64(1) && (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs))).FlitLength == uint32(0) {
		cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1)
		if dstCapacity < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5780, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE24(tls, op, cBlockHeader24)
		op = op + uintptr(ZSTD_blockHeaderSize)
		dstCapacity = dstCapacity - ZSTD_blockHeaderSize
		cSize = cSize + ZSTD_blockHeaderSize
	}
	for nbSequences != 0 {
		block = ZSTD_get1BlockSummary(tls, inSeqs, nbSequences)
		lastBlock = libc.BoolUint32(block.FnbSequences == nbSequences)
		err_code = block.FnbSequences
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6087, 0)
			}
			return err_code
		}
		if block.FlitSize > litSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6147, 0)
			}
			return uint64(-int32(ZSTD_error_externalSequences_invalid))
		}
		ZSTD_resetSeqStore(tls, cctx+976)
		conversionStatus = ZSTD_convertBlockSequences(tls, cctx, inSeqs, block.FnbSequences, repcodeResolution)
		err_code1 = conversionStatus
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6215, 0)
			}
			return err_code1
		}
		inSeqs = inSeqs + uintptr(block.FnbSequences)*16
		nbSequences = nbSequences - block.FnbSequences
		remaining = remaining - block.FblockSize
		/* Note: when blockSize is very small, other variant send it uncompressed.
		 * Here, we still send the sequences, because we don't have the original source to send it uncompressed.
		 * One could imagine in theory reproducing the source from the sequences,
		 * but that's complex and costly memory intensive, and goes against the objectives of this variant. */
		if dstCapacity < ZSTD_blockHeaderSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5878, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		compressedSeqsSize = ZSTD_entropyCompressSeqStore_internal(tls, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, literals, block.FlitSize, cctx+976, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock, cctx+240, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2)
		err_code2 = compressedSeqsSize
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5933, 0)
			}
			return err_code2
		}
		/* note: the spec forbids for any compressed block to be larger than maximum block size */
		if compressedSeqsSize > (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax {
			compressedSeqsSize = uint64(0)
		}
		litSize = litSize - block.FlitSize
		literals = literals + uintptr(block.FlitSize)
		/* Note: difficult to check source for RLE block when only Literals are provided,
		 * but it could be considered from analyzing the sequence directly */
		if compressedSeqsSize == uint64(0) {
			/* Sending uncompressed blocks is out of reach, because the source is not provided.
			 * In theory, one could use the sequences to regenerate the source, like a decompressor,
			 * but it's complex, and memory hungry, killing the purpose of this variant.
			 * Current outcome: generate an error code.
			 */
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6239, 0)
			}
			return uint64(-int32(ZSTD_error_cannotProduce_uncompressedBlock))
		} else {
			/* no RLE */
			/* Error checking and repcodes update */
			ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, cctx+3224)
			if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
				(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
			}
			/* Write block header into beginning of block*/
			cBlockHeader = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(compressedSeqsSize<<libc.Int32FromInt32(3))
			MEM_writeLE24(tls, op, cBlockHeader)
			cBlockSize = ZSTD_blockHeaderSize + compressedSeqsSize
		}
		cSize = cSize + cBlockSize
		op = op + uintptr(cBlockSize)
		dstCapacity = dstCapacity - cBlockSize
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock = 0
		if lastBlock != 0 {
			break
		}
	}
	if litSize != uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6311, 0)
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	if remaining != uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6358, libc.VaList(bp+8, srcSize))
		}
		return uint64(-int32(ZSTD_error_externalSequences_invalid))
	}
	return cSize
}

func ZSTD_compressSequencesAndLiterals(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, inSeqsSize size_t, literals uintptr, litSize size_t, litCapacity size_t, decompressedSize size_t) (r size_t) {
	var cBlocksSize, cSize, err_code, err_code1, frameHeaderSize size_t
	var op uintptr
	_, _, _, _, _, _ = cBlocksSize, cSize, err_code, err_code1, frameHeaderSize, op
	op = dst
	cSize = uint64(0)
	/* Transparent initialization stage, same as compressStream2() */
	if litCapacity < litSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6414, 0)
		}
		return uint64(-int32(ZSTD_error_workSpace_tooSmall))
	}
	err_code = ZSTD_CCtx_init_compressStream2(tls, cctx, int32(ZSTD_e_end), decompressedSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6000, 0)
		}
		return err_code
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FblockDelimiters == int32(ZSTD_sf_noBlockDelimiters) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6524, 0)
		}
		return uint64(-int32(ZSTD_error_frameParameter_unsupported))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FvalidateSequences != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6578, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_unsupported))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6631, 0)
		}
		return uint64(-int32(ZSTD_error_frameParameter_unsupported))
	}
	/* Begin writing output, starting with frame header */
	frameHeaderSize = ZSTD_writeFrameHeader(tls, op, dstCapacity, cctx+240, decompressedSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID)
	op = op + uintptr(frameHeaderSize)
	dstCapacity = dstCapacity - frameHeaderSize
	cSize = cSize + frameHeaderSize
	/* Now generate compressed blocks */
	cBlocksSize = ZSTD_compressSequencesAndLiterals_internal(tls, cctx, op, dstCapacity, inSeqs, inSeqsSize, literals, litSize, decompressedSize)
	err_code1 = cBlocksSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6027, 0)
		}
		return err_code1
	}
	cSize = cSize + cBlocksSize
	dstCapacity = dstCapacity - cBlocksSize
	return cSize
}

/*======   Finalize   ======*/

func inBuffer_forEndFlush(tls *libc.TLS, zcs uintptr) (r ZSTD_inBuffer) {
	var nullInput, v1 ZSTD_inBuffer
	var stableInput int32
	_, _, _ = nullInput, stableInput, v1
	nullInput = ZSTD_inBuffer{}
	stableInput = libc.BoolInt32((*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable))
	if stableInput != 0 {
		v1 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FexpectedInBuffer
	} else {
		v1 = nullInput
	}
	return v1
}

// C documentation
//
//	/*! ZSTD_flushStream() :
//	 * @return : amount of data remaining to flush */
func ZSTD_flushStream(tls *libc.TLS, zcs uintptr, output uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _ /* input at bp+0 */ ZSTD_inBuffer
	*(*ZSTD_inBuffer)(unsafe.Pointer(bp)) = inBuffer_forEndFlush(tls, zcs)
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp))).Fsize = (*(*ZSTD_inBuffer)(unsafe.Pointer(bp))).Fpos /* do not ingest more input during flush */
	return ZSTD_compressStream2(tls, zcs, output, bp, int32(ZSTD_e_flush))
}

func ZSTD_endStream(tls *libc.TLS, zcs uintptr, output uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var checksumSize, err_code, lastBlockSize, remainingToFlush, toFlush size_t
	var v1, v2 int32
	var _ /* input at bp+0 */ ZSTD_inBuffer
	_, _, _, _, _, _, _ = checksumSize, err_code, lastBlockSize, remainingToFlush, toFlush, v1, v2
	*(*ZSTD_inBuffer)(unsafe.Pointer(bp)) = inBuffer_forEndFlush(tls, zcs)
	remainingToFlush = ZSTD_compressStream2(tls, zcs, output, bp, int32(ZSTD_e_end))
	err_code = remainingToFlush
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6679, 0)
		}
		return err_code
	}
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FnbWorkers > 0 {
		return remainingToFlush
	} /* minimal estimation */
	/* single thread mode : attempt to calculate remaining to flush more precisely */
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
		v1 = 0
	} else {
		v1 = int32(ZSTD_BLOCKHEADERSIZE)
	}
	lastBlockSize = uint64(v1)
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
		v2 = 0
	} else {
		v2 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FfParams.FchecksumFlag * int32(4)
	}
	checksumSize = uint64(v2)
	toFlush = remainingToFlush + lastBlockSize + checksumSize
	return toFlush
	return r
}

/*-=====  Pre-defined compression levels  =====-*/
/**** start inlining clevels.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../zstd.h ****/

/*-=====  Pre-defined compression levels  =====-*/

var ZSTD_defaultCParameters = [4][23]ZSTD_compressionParameters{
	0: {
		0: {
			FwindowLog:    uint32(19),
			FchainLog:     uint32(12),
			FhashLog:      uint32(13),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(6),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(19),
			FchainLog:  uint32(13),
			FhashLog:   uint32(14),
			FsearchLog: uint32(1),
			FminMatch:  uint32(7),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(20),
			FchainLog:  uint32(15),
			FhashLog:   uint32(16),
			FsearchLog: uint32(1),
			FminMatch:  uint32(6),
			Fstrategy:  int32(ZSTD_fast),
		},
		3: {
			FwindowLog: uint32(21),
			FchainLog:  uint32(16),
			FhashLog:   uint32(17),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog: uint32(21),
			FchainLog:  uint32(18),
			FhashLog:   uint32(18),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		5: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		6: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		7: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(19),
			FhashLog:      uint32(20),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy),
		},
		8: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(19),
			FhashLog:      uint32(20),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(20),
			FhashLog:      uint32(21),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		10: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(21),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		11: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(21),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		12: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		13: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		14: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		15: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(23),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		16: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(48),
			Fstrategy:     int32(ZSTD_btopt),
		},
		17: {
			FwindowLog:    uint32(23),
			FchainLog:     uint32(23),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(64),
			Fstrategy:     int32(ZSTD_btopt),
		},
		18: {
			FwindowLog:    uint32(23),
			FchainLog:     uint32(23),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(64),
			Fstrategy:     int32(ZSTD_btultra),
		},
		19: {
			FwindowLog:    uint32(23),
			FchainLog:     uint32(24),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(25),
			FchainLog:     uint32(25),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(26),
			FchainLog:     uint32(26),
			FhashLog:      uint32(24),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(27),
			FchainLog:     uint32(27),
			FhashLog:      uint32(25),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
	1: {
		0: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(12),
			FhashLog:      uint32(13),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(5),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(18),
			FchainLog:  uint32(13),
			FhashLog:   uint32(14),
			FsearchLog: uint32(1),
			FminMatch:  uint32(6),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(18),
			FchainLog:  uint32(14),
			FhashLog:   uint32(14),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		3: {
			FwindowLog: uint32(18),
			FchainLog:  uint32(16),
			FhashLog:   uint32(16),
			FsearchLog: uint32(1),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		5: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(17),
			FhashLog:      uint32(18),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		6: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		7: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		8: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		10: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		11: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		12: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		13: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_btopt),
		},
		14: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(3),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btopt),
		},
		15: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btopt),
		},
		16: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra),
		},
		17: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra),
		},
		18: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		19: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(10),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(12),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(13),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
	2: {
		0: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(12),
			FhashLog:      uint32(12),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(5),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(12),
			FhashLog:   uint32(13),
			FsearchLog: uint32(1),
			FminMatch:  uint32(6),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(13),
			FhashLog:   uint32(15),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_fast),
		},
		3: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(15),
			FhashLog:   uint32(16),
			FsearchLog: uint32(2),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(17),
			FhashLog:   uint32(17),
			FsearchLog: uint32(2),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_dfast),
		},
		5: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		6: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		7: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		8: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		10: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		11: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(17),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		12: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		13: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btopt),
		},
		14: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(3),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btopt),
		},
		15: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btopt),
		},
		16: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra),
		},
		17: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra),
		},
		18: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(10),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra),
		},
		19: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(11),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
	3: {
		0: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(12),
			FhashLog:      uint32(13),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(5),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(14),
			FchainLog:  uint32(14),
			FhashLog:   uint32(15),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(14),
			FchainLog:  uint32(14),
			FhashLog:   uint32(15),
			FsearchLog: uint32(1),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_fast),
		},
		3: {
			FwindowLog: uint32(14),
			FchainLog:  uint32(14),
			FhashLog:   uint32(15),
			FsearchLog: uint32(2),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		5: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		6: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		7: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		8: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		10: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		11: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btopt),
		},
		12: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(3),
			FtargetLength: uint32(24),
			Fstrategy:     int32(ZSTD_btopt),
		},
		13: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(3),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btultra),
		},
		14: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(64),
			Fstrategy:     int32(ZSTD_btultra),
		},
		15: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra),
		},
		16: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(3),
			FtargetLength: uint32(48),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		17: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		18: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		19: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(10),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
}

/**** ended inlining clevels.h ****/

func ZSTD_maxCLevel(tls *libc.TLS) (r int32) {
	return int32(ZSTD_MAX_CLEVEL)
}

func ZSTD_minCLevel(tls *libc.TLS) (r int32) {
	return -(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
}

func ZSTD_defaultCLevel(tls *libc.TLS) (r int32) {
	return int32(ZSTD_CLEVEL_DEFAULT)
}

func ZSTD_dedicatedDictSearch_getCParams(tls *libc.TLS, compressionLevel int32, dictSize size_t) (r ZSTD_compressionParameters) {
	var cParams ZSTD_compressionParameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, uint64(0), dictSize, int32(ZSTD_cpm_createCDict))
	switch cParams.Fstrategy {
	case int32(ZSTD_fast):
		fallthrough
	case int32(ZSTD_dfast):
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		cParams.FhashLog += uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
		break
	}
	return cParams
}

func ZSTD_dedicatedDictSearch_isSupported(tls *libc.TLS, cParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_greedy) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy <= int32(ZSTD_lazy2) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog > (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog <= uint32(24))
}

// C documentation
//
//	/**
//	 * Reverses the adjustment applied to cparams when enabling dedicated dict
//	 * search. This is used to recover the params set to be used in the working
//	 * context. (Otherwise, those tables would also grow.)
//	 */
func ZSTD_dedicatedDictSearch_revertCParams(tls *libc.TLS, cParams uintptr) {
	switch (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy {
	case int32(ZSTD_fast):
		fallthrough
	case int32(ZSTD_dfast):
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		*(*uint32)(unsafe.Pointer(cParams + 8)) -= uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog < uint32(ZSTD_HASHLOG_MIN) {
			(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog = uint32(ZSTD_HASHLOG_MIN)
		}
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
		break
	}
}

func ZSTD_getCParamRowSize(tls *libc.TLS, srcSizeHint U64, dictSize size_t, mode ZSTD_CParamMode_e) (r U64) {
	var addedSize size_t
	var unknown, v1 int32
	var v2 uint64
	_, _, _, _ = addedSize, unknown, v1, v2
	switch mode {
	case int32(ZSTD_cpm_unknown):
		fallthrough
	case int32(ZSTD_cpm_noAttachDict):
		fallthrough
	case int32(ZSTD_cpm_createCDict):
	case int32(ZSTD_cpm_attachDict):
		dictSize = uint64(0)
	default:
		break
	}
	unknown = libc.BoolInt32(srcSizeHint == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
	if unknown != 0 && dictSize > uint64(0) {
		v1 = int32(500)
	} else {
		v1 = 0
	}
	addedSize = uint64(v1)
	if unknown != 0 && dictSize == uint64(0) {
		v2 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v2 = srcSizeHint + dictSize + addedSize
	}
	return v2
	return r
}

// C documentation
//
//	/*! ZSTD_getCParams_internal() :
//	 * @return ZSTD_compressionParameters structure for a selected compression level, srcSize and dictSize.
//	 *  Note: srcSizeHint 0 means 0, use ZSTD_CONTENTSIZE_UNKNOWN for unknown.
//	 *        Use dictSize == 0 for unknown or unused.
//	 *  Note: `mode` controls how we treat the `dictSize`. See docs for `ZSTD_CParamMode_e`. */
func ZSTD_getCParams_internal(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t, mode ZSTD_CParamMode_e) (r ZSTD_compressionParameters) {
	var clampedCompressionLevel, row, v1 int32
	var cp ZSTD_compressionParameters
	var rSize U64
	var tableID U32
	_, _, _, _, _, _ = clampedCompressionLevel, cp, rSize, row, tableID, v1
	rSize = ZSTD_getCParamRowSize(tls, srcSizeHint, dictSize, mode)
	tableID = uint32(libc.BoolInt32(rSize <= uint64(libc.Int32FromInt32(256)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(rSize <= uint64(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(rSize <= uint64(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	/* row */
	if compressionLevel == 0 {
		row = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		if compressionLevel < 0 {
			row = 0
		} else {
			if compressionLevel > int32(ZSTD_MAX_CLEVEL) {
				row = int32(ZSTD_MAX_CLEVEL)
			} else {
				row = compressionLevel
			}
		}
	}
	cp = *(*ZSTD_compressionParameters)(unsafe.Pointer(uintptr(unsafe.Pointer(&ZSTD_defaultCParameters)) + uintptr(tableID)*644 + uintptr(row)*28))
	/* acceleration factor */
	if compressionLevel < 0 {
		if ZSTD_minCLevel(tls) > compressionLevel {
			v1 = ZSTD_minCLevel(tls)
		} else {
			v1 = compressionLevel
		}
		clampedCompressionLevel = v1
		cp.FtargetLength = uint32(-clampedCompressionLevel)
	}
	/* refine parameters based on srcSize & dictSize */
	return ZSTD_adjustCParams_internal(tls, cp, srcSizeHint, dictSize, mode, int32(ZSTD_ps_auto))
	return r
}

// C documentation
//
//	/*! ZSTD_getCParams() :
//	 * @return ZSTD_compressionParameters structure for a selected compression level, srcSize and dictSize.
//	 *  Size values are optional, provide 0 if not known or unused */
func ZSTD_getCParams(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t) (r ZSTD_compressionParameters) {
	if srcSizeHint == uint64(0) {
		srcSizeHint = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	return ZSTD_getCParams_internal(tls, compressionLevel, srcSizeHint, dictSize, int32(ZSTD_cpm_unknown))
}

// C documentation
//
//	/*! ZSTD_getParams() :
//	 *  same idea as ZSTD_getCParams()
//	 * @return a `ZSTD_parameters` structure (instead of `ZSTD_compressionParameters`).
//	 *  Fields of `ZSTD_frameParameters` are set to default values */
func ZSTD_getParams_internal(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t, mode ZSTD_CParamMode_e) (r ZSTD_parameters) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var cParams ZSTD_compressionParameters
	var _ /* params at bp+0 */ ZSTD_parameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, srcSizeHint, dictSize, mode)
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(40))
	(*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams = cParams
	(*(*ZSTD_parameters)(unsafe.Pointer(bp))).FfParams.FcontentSizeFlag = int32(1)
	return *(*ZSTD_parameters)(unsafe.Pointer(bp))
}

// C documentation
//
//	/*! ZSTD_getParams() :
//	 *  same idea as ZSTD_getCParams()
//	 * @return a `ZSTD_parameters` structure (instead of `ZSTD_compressionParameters`).
//	 *  Fields of `ZSTD_frameParameters` are set to default values */
func ZSTD_getParams(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t) (r ZSTD_parameters) {
	if srcSizeHint == uint64(0) {
		srcSizeHint = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	return ZSTD_getParams_internal(tls, compressionLevel, srcSizeHint, dictSize, int32(ZSTD_cpm_unknown))
}

type __ccgo_fp__XZSTD_registerSequenceProducer_2 = func(*libc.TLS, uintptr, uintptr, uint64, uintptr, uint64, uintptr, uint64, int32, uint64) uint64

func ZSTD_registerSequenceProducer(tls *libc.TLS, zc uintptr, extSeqProdState uintptr, __ccgo_fp_extSeqProdFunc ZSTD_sequenceProducer_F) {
	ZSTD_CCtxParams_registerSequenceProducer(tls, zc+16, extSeqProdState, __ccgo_fp_extSeqProdFunc)
}

type __ccgo_fp__XZSTD_CCtxParams_registerSequenceProducer_2 = func(*libc.TLS, uintptr, uintptr, uint64, uintptr, uint64, uintptr, uint64, int32, uint64) uint64

func ZSTD_CCtxParams_registerSequenceProducer(tls *libc.TLS, params uintptr, extSeqProdState uintptr, __ccgo_fp_extSeqProdFunc ZSTD_sequenceProducer_F) {
	if __ccgo_fp_extSeqProdFunc != libc.UintptrFromInt32(0) {
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdFunc = __ccgo_fp_extSeqProdFunc
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdState = extSeqProdState
	} else {
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdFunc = libc.UintptrFromInt32(0)
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdState = libc.UintptrFromInt32(0)
	}
}

/**** ended inlining compress/zstd_compress.c ****/
/**** start inlining compress/zstd_double_fast.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_double_fast.h ****/

func ZSTD_fillDoubleHashTableForCDict(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashLarge, hashSmall, iend, ip uintptr
	var curr, fastHashFillStep, hBitsL, hBitsS, i, mls U32
	var lgHashAndTag, smHashAndTag size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBitsL, hBitsS, hashLarge, hashSmall, i, iend, ip, lgHashAndTag, mls, smHashAndTag
	cParams = ms + 256
	hashLarge = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Always insert every fastHashFillStep position into the hash tables.
	 * Insert the other positions into the large hash table if their entry
	 * is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep)-uintptr(1) <= iend) {
			break
		}
		curr = uint32(int64(ip) - int64(base))
		i = uint32(0)
		for {
			if !(i < fastHashFillStep) {
				break
			}
			smHashAndTag = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsS, mls)
			lgHashAndTag = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsL, uint32(8))
			if i == uint32(0) {
				ZSTD_writeTaggedIndex(tls, hashSmall, smHashAndTag, curr+i)
			}
			if i == uint32(0) || *(*U32)(unsafe.Pointer(hashLarge + uintptr(lgHashAndTag>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4)) == uint32(0) {
				ZSTD_writeTaggedIndex(tls, hashLarge, lgHashAndTag, curr+i)
			}
			/* Only load extra positions for ZSTD_dtlm_full */
			if dtlm == int32(ZSTD_dtlm_fast) {
				break
			}
			goto _2
		_2:
			;
			i = i + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillDoubleHashTableForCCtx(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashLarge, hashSmall, iend, ip uintptr
	var curr, fastHashFillStep, hBitsL, hBitsS, i, mls U32
	var lgHash, smHash size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBitsL, hBitsS, hashLarge, hashSmall, i, iend, ip, lgHash, mls, smHash
	cParams = ms + 256
	hashLarge = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Always insert every fastHashFillStep position into the hash tables.
	 * Insert the other positions into the large hash table if their entry
	 * is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep)-uintptr(1) <= iend) {
			break
		}
		curr = uint32(int64(ip) - int64(base))
		i = uint32(0)
		for {
			if !(i < fastHashFillStep) {
				break
			}
			smHash = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsS, mls)
			lgHash = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsL, uint32(8))
			if i == uint32(0) {
				*(*U32)(unsafe.Pointer(hashSmall + uintptr(smHash)*4)) = curr + i
			}
			if i == uint32(0) || *(*U32)(unsafe.Pointer(hashLarge + uintptr(lgHash)*4)) == uint32(0) {
				*(*U32)(unsafe.Pointer(hashLarge + uintptr(lgHash)*4)) = curr + i
			}
			/* Only load extra positions for ZSTD_dtlm_full */
			if dtlm == int32(ZSTD_dtlm_fast) {
				break
			}
			goto _2
		_2:
			;
			i = i + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillDoubleHashTable(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e) {
	if tfp == int32(ZSTD_tfp_forCDict) {
		ZSTD_fillDoubleHashTableForCDict(tls, ms, end, dtlm)
	} else {
		ZSTD_fillDoubleHashTableForCCtx(tls, ms, end, dtlm)
	}
}

func ZSTD_compressBlock_doubleFast_noDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var anchor, base, cParams, hashLong, hashSmall, iend, ilimit, ip, ip1, istart, matchl0, matchl0_safe, matchl1, matchs0, matchs0_safe, nextStep, prefixLowest uintptr
	var curr, current, endIndex, hBitsL, hBitsS, idxl0, idxl1, idxs0, indexToInsert, maxRep, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowestIndex, tmpOff, windowLow, v1 U32
	var hl0, hl1, hs0, kStepIncr, l1len, mLength, rLength, step size_t
	var v2 uint32
	var _ /* dummy at bp+0 */ [10]BYTE
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current, endIndex, hBitsL, hBitsS, hashLong, hashSmall, hl0, hl1, hs0, idxl0, idxl1, idxs0, iend, ilimit, indexToInsert, ip, ip1, istart, kStepIncr, l1len, mLength, matchl0, matchl0_safe, matchl1, matchs0, matchs0_safe, maxRep, nextStep, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowest, prefixLowestIndex, rLength, step, tmpOff, windowLow, v1, v2
	cParams = ms + 256
	hashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	anchor = istart
	endIndex = uint32(uint64(int64(istart)-int64(base)) + srcSize)
	/* presumes that, if there is a dictionary, it must be using Attach mode */
	prefixLowestIndex = ZSTD_getLowestPrefixIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	prefixLowest = base + uintptr(prefixLowestIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	/* how many positions to search before increasing step size */
	kStepIncr = uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(kSearchStrength)) /* matchs0 or safe address */
	ip = istart                                                                        /* the next position */
	/* Array of ~random data, should have low probability of matching data
	 * we load from here instead of from tables, if matchl0/matchl1 are
	 * invalid indices. Used to avoid unpredictable branches. */
	*(*[10]BYTE)(unsafe.Pointer(bp)) = [10]BYTE{
		0: uint8(0x12),
		1: uint8(0x34),
		2: uint8(0x56),
		3: uint8(0x78),
		4: uint8(0x9a),
		5: uint8(0xbc),
		6: uint8(0xde),
		7: uint8(0xf0),
		8: uint8(0xe2),
		9: uint8(0xb4),
	}
	/* init */
	ip = ip + libc.BoolUintptr(int64(ip)-int64(prefixLowest) == libc.Int64FromInt32(0))
	current = uint32(int64(ip) - int64(base))
	windowLow = ZSTD_getLowestPrefixIndex(tls, ms, current, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	maxRep = current - windowLow
	if offset_2 > maxRep {
		offsetSaved2 = offset_2
		offset_2 = libc.Uint32FromInt32(0)
	}
	if offset_1 > maxRep {
		offsetSaved1 = offset_1
		offset_1 = libc.Uint32FromInt32(0)
	}
	/* Outer Loop: one iteration per match found and stored */
	for int32(1) != 0 {
		step = uint64(1)
		nextStep = ip + uintptr(kStepIncr)
		ip1 = ip + uintptr(step)
		if ip1 > ilimit {
			goto _cleanup
		}
		hl0 = ZSTD_hashPtr(tls, ip, hBitsL, uint32(8))
		idxl0 = *(*U32)(unsafe.Pointer(hashLong + uintptr(hl0)*4))
		matchl0 = base + uintptr(idxl0)
		/* Inner Loop: one iteration per search / position */
		for cond := true; cond; cond = ip1 <= ilimit {
			hs0 = ZSTD_hashPtr(tls, ip, hBitsS, mls)
			idxs0 = *(*U32)(unsafe.Pointer(hashSmall + uintptr(hs0)*4))
			curr = uint32(int64(ip) - int64(base))
			matchs0 = base + uintptr(idxs0)
			v1 = curr
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(hs0)*4)) = v1
			*(*U32)(unsafe.Pointer(hashLong + uintptr(hl0)*4)) = v1 /* update hash tables */
			/* check noDict repcode */
			if libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip+uintptr(1)-uintptr(offset_1)) == MEM_read32(tls, ip+uintptr(1))) != 0 {
				mLength = ZSTD_count(tls, ip+uintptr(1)+uintptr(4), ip+uintptr(1)+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
				ip = ip + 1
				ZSTD_storeSeq(tls, seqStore, uint64(int64(ip)-int64(anchor)), anchor, iend, uint32(libc.Int32FromInt32(1)), mLength)
				goto _match_stored
			}
			hl1 = ZSTD_hashPtr(tls, ip1, hBitsL, uint32(8))
			/* idxl0 > prefixLowestIndex is a (somewhat) unpredictable branch.
			 * However expression below complies into conditional move. Since
			 * match is unlikely and we only *branch* on idxl0 > prefixLowestIndex
			 * if there is a match, all branches become predictable. */
			matchl0_safe = ZSTD_selectAddr(tls, idxl0, prefixLowestIndex, matchl0, bp)
			/* check prefix long match */
			if MEM_read64(tls, matchl0_safe) == MEM_read64(tls, ip) && matchl0_safe == matchl0 {
				mLength = ZSTD_count(tls, ip+uintptr(8), matchl0+uintptr(8), iend) + uint64(8)
				offset = uint32(int64(ip) - int64(matchl0))
				for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchl0 > prefixLowest) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(matchl0 + uintptr(-libc.Int32FromInt32(1))))) {
					ip = ip - 1
					matchl0 = matchl0 - 1
					mLength = mLength + 1
				} /* catch up */
				goto _match_found
			}
			idxl1 = *(*U32)(unsafe.Pointer(hashLong + uintptr(hl1)*4))
			matchl1 = base + uintptr(idxl1)
			/* Same optimization as matchl0 above */
			matchs0_safe = ZSTD_selectAddr(tls, idxs0, prefixLowestIndex, matchs0, bp)
			/* check prefix short match */
			if MEM_read32(tls, matchs0_safe) == MEM_read32(tls, ip) && matchs0_safe == matchs0 {
				goto _search_next_long
			}
			if ip1 >= nextStep {
				libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(64), libc.VaList(bp+24, 0, int32(3)))
				libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(128), libc.VaList(bp+24, 0, int32(3)))
				step = step + 1
				nextStep = nextStep + uintptr(kStepIncr)
			}
			ip = ip1
			ip1 = ip1 + uintptr(step)
			hl0 = hl1
			idxl0 = idxl1
			matchl0 = matchl1
		}
		goto _cleanup
	_cleanup:
		;
		/* If offset_1 started invalid (offsetSaved1 != 0) and became valid (offset_1 != 0),
		 * rotate saved offsets. See comment in ZSTD_compressBlock_fast_noDict for more context. */
		if offsetSaved1 != uint32(0) && offset_1 != uint32(0) {
			v2 = offsetSaved1
		} else {
			v2 = offsetSaved2
		}
		offsetSaved2 = v2
		/* save reps for next block */
		if offset_1 != 0 {
			v2 = offset_1
		} else {
			v2 = offsetSaved1
		}
		*(*U32)(unsafe.Pointer(rep)) = v2
		if offset_2 != 0 {
			v2 = offset_2
		} else {
			v2 = offsetSaved2
		}
		*(*U32)(unsafe.Pointer(rep + 1*4)) = v2
		/* Return the last literals size */
		return uint64(int64(iend) - int64(anchor))
		goto _search_next_long
	_search_next_long:
		;
		/* short match found: let's check for a longer one */
		mLength = ZSTD_count(tls, ip+uintptr(4), matchs0+uintptr(4), iend) + uint64(4)
		offset = uint32(int64(ip) - int64(matchs0))
		/* check long match at +1 position */
		if idxl1 > prefixLowestIndex && MEM_read64(tls, matchl1) == MEM_read64(tls, ip1) {
			l1len = ZSTD_count(tls, ip1+uintptr(8), matchl1+uintptr(8), iend) + uint64(8)
			if l1len > mLength {
				/* use the long match instead */
				ip = ip1
				mLength = l1len
				offset = uint32(int64(ip) - int64(matchl1))
				matchs0 = matchl1
			}
		}
		for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchs0 > prefixLowest) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(matchs0 + uintptr(-libc.Int32FromInt32(1))))) {
			ip = ip - 1
			matchs0 = matchs0 - 1
			mLength = mLength + 1
		} /* complete backward */
		/* fall-through */
		goto _match_found
	_match_found:
		; /* requires ip, offset, mLength */
		offset_2 = offset_1
		offset_1 = offset
		if step < uint64(4) {
			/* It is unsafe to write this value back to the hashtable when ip1 is
			 * greater than or equal to the new ip we will have after we're done
			 * processing this match. Rather than perform that test directly
			 * (ip1 >= ip + mLength), which costs speed in practice, we do a simpler
			 * more predictable test. The minmatch even if we take a short match is
			 * 4 bytes, so as long as step, the distance between ip and ip1
			 * (initially) is less than 4, we know ip1 < new ip. */
			*(*U32)(unsafe.Pointer(hashLong + uintptr(hl1)*4)) = uint32(int64(ip1) - int64(base))
		}
		ZSTD_storeSeq(tls, seqStore, uint64(int64(ip)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
		goto _match_stored
	_match_stored:
		;
		/* match found */
		ip = ip + uintptr(mLength)
		anchor = ip
		if ip <= ilimit {
			/* Complementary insertion */
			/* done after iLimit test, as candidates could be > iend-8 */
			indexToInsert = curr + uint32(2)
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsL, uint32(8)))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip-uintptr(2), hBitsL, uint32(8)))*4)) = uint32(int64(ip-libc.UintptrFromInt32(2)) - int64(base))
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsS, mls))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip-uintptr(1), hBitsS, mls))*4)) = uint32(int64(ip-libc.UintptrFromInt32(1)) - int64(base))
			/* check immediate repcode */
			for ip <= ilimit && libc.BoolInt32(offset_2 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_2))) != 0 {
				/* store sequence */
				rLength = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_2), iend) + uint64(4)
				tmpOff = offset_2
				offset_2 = offset_1
				offset_1 = tmpOff /* swap offset_2 <=> offset_1 */
				*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip, hBitsS, mls))*4)) = uint32(int64(ip) - int64(base))
				*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip, hBitsL, uint32(8)))*4)) = uint32(int64(ip) - int64(base))
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), rLength)
				ip = ip + uintptr(rLength)
				anchor = ip
				continue /* faster when present ... (?) */
			}
		}
	}
	return r
}

func ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _pos, _pos1, _size, _size1, chainTableBytes, dictHashAndTagL, dictHashAndTagL3, dictHashAndTagS, h, h2, hashTableBytes, hl3, mLength, repLength2 size_t
	var _ptr, _ptr1, anchor, base, cParams, dictBase, dictCParams, dictEnd, dictHashLong, dictHashSmall, dictMatchL, dictMatchL3, dictStart, dms, hashLong, hashSmall, iend, ilimit, ip, istart, match, matchL3, matchLong, prefixLowest, repEnd2, repMatch, repMatch2, repMatchEnd, v3, v5 uintptr
	var curr, current2, dictAndPrefixLength, dictHBitsL, dictHBitsS, dictIndexDelta, dictMatchIndexAndTagL, dictMatchIndexAndTagL3, dictMatchIndexAndTagS, dictMatchIndexL, dictMatchIndexL3, dictMatchIndexS, dictStartIndex, endIndex, hBitsL, hBitsS, indexToInsert, matchIndexL, matchIndexL3, matchIndexS, offset, offset_1, offset_2, prefixLowestIndex, repIndex, repIndex2, tmpOffset, v4 U32
	var dictTagsMatchL, dictTagsMatchL3, dictTagsMatchS int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _pos, _pos1, _ptr, _ptr1, _size, _size1, anchor, base, cParams, chainTableBytes, curr, current2, dictAndPrefixLength, dictBase, dictCParams, dictEnd, dictHBitsL, dictHBitsS, dictHashAndTagL, dictHashAndTagL3, dictHashAndTagS, dictHashLong, dictHashSmall, dictIndexDelta, dictMatchIndexAndTagL, dictMatchIndexAndTagL3, dictMatchIndexAndTagS, dictMatchIndexL, dictMatchIndexL3, dictMatchIndexS, dictMatchL, dictMatchL3, dictStart, dictStartIndex, dictTagsMatchL, dictTagsMatchL3, dictTagsMatchS, dms, endIndex, h, h2, hBitsL, hBitsS, hashLong, hashSmall, hashTableBytes, hl3, iend, ilimit, indexToInsert, ip, istart, mLength, match, matchIndexL, matchIndexL3, matchIndexS, matchL3, matchLong, offset, offset_1, offset_2, prefixLowest, prefixLowestIndex, repEnd2, repIndex, repIndex2, repLength2, repMatch, repMatch2, repMatchEnd, tmpOffset, v3, v4, v5
	cParams = ms + 256
	hashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	ip = istart
	anchor = istart
	endIndex = uint32(uint64(int64(istart)-int64(base)) + srcSize)
	/* presumes that, if there is a dictionary, it must be using Attach mode */
	prefixLowestIndex = ZSTD_getLowestPrefixIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	prefixLowest = base + uintptr(prefixLowestIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	dictCParams = dms + 256
	dictHashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
	dictHashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
	dictStartIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	dictStart = dictBase + uintptr(dictStartIndex)
	dictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	dictIndexDelta = prefixLowestIndex - uint32(int64(dictEnd)-int64(dictBase))
	dictHBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	dictHBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FchainLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	dictAndPrefixLength = uint32(int64(ip) - int64(prefixLowest) + (int64(dictEnd) - int64(dictStart)))
	/* if a dictionary is attached, it must be within window range */
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FprefetchCDictTables != 0 {
		hashTableBytes = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog * uint64(4)
		chainTableBytes = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FchainLog * uint64(4)
		_ptr = dictHashLong
		_size = hashTableBytes
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+8, 0, int32(2)))
			goto _1
		_1:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
		_ptr1 = dictHashSmall
		_size1 = chainTableBytes
		_pos1 = uint64(0)
		for {
			if !(_pos1 < _size1) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr1+uintptr(_pos1), libc.VaList(bp+8, 0, int32(2)))
			goto _2
		_2:
			;
			_pos1 = _pos1 + uint64(CACHELINE_SIZE)
		}
	}
	/* init */
	ip = ip + libc.BoolUintptr(dictAndPrefixLength == libc.Uint32FromInt32(0))
	/* dictMatchState repCode checks don't currently handle repCode == 0
	 * disabling. */
	/* Main Search Loop */
	for ip < ilimit {
		h2 = ZSTD_hashPtr(tls, ip, hBitsL, uint32(8))
		h = ZSTD_hashPtr(tls, ip, hBitsS, mls)
		dictHashAndTagL = ZSTD_hashPtr(tls, ip, dictHBitsL, uint32(8))
		dictHashAndTagS = ZSTD_hashPtr(tls, ip, dictHBitsS, mls)
		dictMatchIndexAndTagL = *(*U32)(unsafe.Pointer(dictHashLong + uintptr(dictHashAndTagL>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictMatchIndexAndTagS = *(*U32)(unsafe.Pointer(dictHashSmall + uintptr(dictHashAndTagS>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictTagsMatchL = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTagL), dictHashAndTagL)
		dictTagsMatchS = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTagS), dictHashAndTagS)
		curr = uint32(int64(ip) - int64(base))
		matchIndexL = *(*U32)(unsafe.Pointer(hashLong + uintptr(h2)*4))
		matchIndexS = *(*U32)(unsafe.Pointer(hashSmall + uintptr(h)*4))
		matchLong = base + uintptr(matchIndexL)
		match = base + uintptr(matchIndexS)
		repIndex = curr + uint32(1) - offset_1
		if repIndex < prefixLowestIndex {
			v3 = dictBase + uintptr(repIndex-dictIndexDelta)
		} else {
			v3 = base + uintptr(repIndex)
		}
		repMatch = v3
		v4 = curr
		*(*U32)(unsafe.Pointer(hashSmall + uintptr(h)*4)) = v4
		*(*U32)(unsafe.Pointer(hashLong + uintptr(h2)*4)) = v4 /* update hash tables */
		/* check repcode */
		if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip+uintptr(1)) {
			if repIndex < prefixLowestIndex {
				v3 = dictEnd
			} else {
				v3 = iend
			}
			repMatchEnd = v3
			mLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixLowest) + uint64(4)
			ip = ip + 1
			ZSTD_storeSeq(tls, seqStore, uint64(int64(ip)-int64(anchor)), anchor, iend, uint32(libc.Int32FromInt32(1)), mLength)
			goto _match_stored
		}
		if matchIndexL >= prefixLowestIndex && MEM_read64(tls, matchLong) == MEM_read64(tls, ip) {
			/* check prefix long match */
			mLength = ZSTD_count(tls, ip+uintptr(8), matchLong+uintptr(8), iend) + uint64(8)
			offset = uint32(int64(ip) - int64(matchLong))
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchLong > prefixLowest) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(matchLong + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				matchLong = matchLong - 1
				mLength = mLength + 1
			} /* catch up */
			goto _match_found
		} else {
			if dictTagsMatchL != 0 {
				/* check dictMatchState long match */
				dictMatchIndexL = dictMatchIndexAndTagL >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				dictMatchL = dictBase + uintptr(dictMatchIndexL)
				if dictMatchL > dictStart && MEM_read64(tls, dictMatchL) == MEM_read64(tls, ip) {
					mLength = ZSTD_count_2segments(tls, ip+uintptr(8), dictMatchL+uintptr(8), iend, dictEnd, prefixLowest) + uint64(8)
					offset = curr - dictMatchIndexL - dictIndexDelta
					for libc.BoolInt32(ip > anchor)&libc.BoolInt32(dictMatchL > dictStart) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(dictMatchL + uintptr(-libc.Int32FromInt32(1))))) {
						ip = ip - 1
						dictMatchL = dictMatchL - 1
						mLength = mLength + 1
					} /* catch up */
					goto _match_found
				}
			}
		}
		if matchIndexS > prefixLowestIndex {
			/* short match  candidate */
			if MEM_read32(tls, match) == MEM_read32(tls, ip) {
				goto _search_next_long
			}
		} else {
			if dictTagsMatchS != 0 {
				/* check dictMatchState short match */
				dictMatchIndexS = dictMatchIndexAndTagS >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				match = dictBase + uintptr(dictMatchIndexS)
				matchIndexS = dictMatchIndexS + dictIndexDelta
				if match > dictStart && MEM_read32(tls, match) == MEM_read32(tls, ip) {
					goto _search_next_long
				}
			}
		}
		ip = ip + uintptr((int64(ip)-int64(anchor))>>libc.Int32FromInt32(kSearchStrength)+int64(1))
		continue
		goto _search_next_long
	_search_next_long:
		;
		hl3 = ZSTD_hashPtr(tls, ip+uintptr(1), hBitsL, uint32(8))
		dictHashAndTagL3 = ZSTD_hashPtr(tls, ip+uintptr(1), dictHBitsL, uint32(8))
		matchIndexL3 = *(*U32)(unsafe.Pointer(hashLong + uintptr(hl3)*4))
		dictMatchIndexAndTagL3 = *(*U32)(unsafe.Pointer(dictHashLong + uintptr(dictHashAndTagL3>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictTagsMatchL3 = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTagL3), dictHashAndTagL3)
		matchL3 = base + uintptr(matchIndexL3)
		*(*U32)(unsafe.Pointer(hashLong + uintptr(hl3)*4)) = curr + uint32(1)
		/* check prefix long +1 match */
		if matchIndexL3 >= prefixLowestIndex && MEM_read64(tls, matchL3) == MEM_read64(tls, ip+uintptr(1)) {
			mLength = ZSTD_count(tls, ip+uintptr(9), matchL3+uintptr(8), iend) + uint64(8)
			ip = ip + 1
			offset = uint32(int64(ip) - int64(matchL3))
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchL3 > prefixLowest) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(matchL3 + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				matchL3 = matchL3 - 1
				mLength = mLength + 1
			} /* catch up */
			goto _match_found
		} else {
			if dictTagsMatchL3 != 0 {
				/* check dict long +1 match */
				dictMatchIndexL3 = dictMatchIndexAndTagL3 >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				dictMatchL3 = dictBase + uintptr(dictMatchIndexL3)
				if dictMatchL3 > dictStart && MEM_read64(tls, dictMatchL3) == MEM_read64(tls, ip+uintptr(1)) {
					mLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(8), dictMatchL3+uintptr(8), iend, dictEnd, prefixLowest) + uint64(8)
					ip = ip + 1
					offset = curr + libc.Uint32FromInt32(1) - dictMatchIndexL3 - dictIndexDelta
					for libc.BoolInt32(ip > anchor)&libc.BoolInt32(dictMatchL3 > dictStart) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(dictMatchL3 + uintptr(-libc.Int32FromInt32(1))))) {
						ip = ip - 1
						dictMatchL3 = dictMatchL3 - 1
						mLength = mLength + 1
					} /* catch up */
					goto _match_found
				}
			}
		}
		/* if no long +1 match, explore the short match we found */
		if matchIndexS < prefixLowestIndex {
			mLength = ZSTD_count_2segments(tls, ip+uintptr(4), match+uintptr(4), iend, dictEnd, prefixLowest) + uint64(4)
			offset = curr - matchIndexS
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match > dictStart) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				match = match - 1
				mLength = mLength + 1
			} /* catch up */
		} else {
			mLength = ZSTD_count(tls, ip+uintptr(4), match+uintptr(4), iend) + uint64(4)
			offset = uint32(int64(ip) - int64(match))
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match > prefixLowest) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				match = match - 1
				mLength = mLength + 1
			} /* catch up */
		}
		goto _match_found
	_match_found:
		;
		offset_2 = offset_1
		offset_1 = offset
		ZSTD_storeSeq(tls, seqStore, uint64(int64(ip)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
		goto _match_stored
	_match_stored:
		;
		/* match found */
		ip = ip + uintptr(mLength)
		anchor = ip
		if ip <= ilimit {
			/* Complementary insertion */
			/* done after iLimit test, as candidates could be > iend-8 */
			indexToInsert = curr + uint32(2)
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsL, uint32(8)))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip-uintptr(2), hBitsL, uint32(8)))*4)) = uint32(int64(ip-libc.UintptrFromInt32(2)) - int64(base))
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsS, mls))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip-uintptr(1), hBitsS, mls))*4)) = uint32(int64(ip-libc.UintptrFromInt32(1)) - int64(base))
			/* check immediate repcode */
			for ip <= ilimit {
				current2 = uint32(int64(ip) - int64(base))
				repIndex2 = current2 - offset_2
				if repIndex2 < prefixLowestIndex {
					v3 = dictBase + uintptr(repIndex2) - uintptr(dictIndexDelta)
				} else {
					v3 = base + uintptr(repIndex2)
				}
				repMatch2 = v3
				if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex2) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip) {
					if repIndex2 < prefixLowestIndex {
						v5 = dictEnd
					} else {
						v5 = iend
					}
					repEnd2 = v5
					repLength2 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixLowest) + uint64(4)
					tmpOffset = offset_2
					offset_2 = offset_1
					offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), repLength2)
					*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip, hBitsS, mls))*4)) = current2
					*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip, hBitsL, uint32(8)))*4)) = current2
					ip = ip + uintptr(repLength2)
					anchor = ip
					continue
				}
				break
			}
		}
	} /* while (ip < ilimit) */
	/* save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_doubleFast_noDict_4(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4))
}

func ZSTD_compressBlock_doubleFast_noDict_5(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5))
}

func ZSTD_compressBlock_doubleFast_noDict_6(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6))
}

func ZSTD_compressBlock_doubleFast_noDict_7(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_4(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_5(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_6(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_7(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7))
}

func ZSTD_compressBlock_doubleFast(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_doubleFast_noDict_4(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_doubleFast_noDict_5(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_doubleFast_noDict_6(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_doubleFast_noDict_7(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

func ZSTD_compressBlock_doubleFast_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_doubleFast_dictMatchState_4(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_doubleFast_dictMatchState_5(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_doubleFast_dictMatchState_6(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_doubleFast_dictMatchState_7(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

func ZSTD_compressBlock_doubleFast_extDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32) (r size_t) {
	var anchor, base, cParams, dictBase, dictEnd, dictStart, hashLong, hashSmall, iend, ilimit, ip, istart, lowMatchPtr, lowMatchPtr1, lowMatchPtr2, match, match3, match3Base, matchBase, matchEnd, matchEnd1, matchEnd2, matchLong, matchLongBase, prefixStart, repBase, repEnd2, repMatch, repMatch2, repMatchEnd, v2, v3, v4 uintptr
	var curr, current2, dictLimit, dictStartIndex, endIndex, hBitsL, hBitsS, indexToInsert, lowLimit, matchIndex, matchIndex3, matchLongIndex, offset, offset1, offset_1, offset_2, prefixStartIndex, repIndex, repIndex2, tmpOffset, v5 U32
	var h3, hLong, hSmall, mLength, repLength2 size_t
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current2, dictBase, dictEnd, dictLimit, dictStart, dictStartIndex, endIndex, h3, hBitsL, hBitsS, hLong, hSmall, hashLong, hashSmall, iend, ilimit, indexToInsert, ip, istart, lowLimit, lowMatchPtr, lowMatchPtr1, lowMatchPtr2, mLength, match, match3, match3Base, matchBase, matchEnd, matchEnd1, matchEnd2, matchIndex, matchIndex3, matchLong, matchLongBase, matchLongIndex, offset, offset1, offset_1, offset_2, prefixStart, prefixStartIndex, repBase, repEnd2, repIndex, repIndex2, repLength2, repMatch, repMatch2, repMatchEnd, tmpOffset, v1, v2, v3, v4, v5
	cParams = ms + 256
	hashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(8)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	endIndex = uint32(uint64(int64(istart)-int64(base)) + srcSize)
	lowLimit = ZSTD_getLowestMatchIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	dictStartIndex = lowLimit
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if dictLimit > lowLimit {
		v1 = dictLimit
	} else {
		v1 = lowLimit
	}
	prefixStartIndex = v1
	prefixStart = base + uintptr(prefixStartIndex)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictStart = dictBase + uintptr(dictStartIndex)
	dictEnd = dictBase + uintptr(prefixStartIndex)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	/* if extDict is invalidated due to maxDistance, switch to "regular" variant */
	if prefixStartIndex == dictStartIndex {
		return ZSTD_compressBlock_doubleFast(tls, ms, seqStore, rep, src, srcSize)
	}
	/* Search Loop */
	for ip < ilimit { /* < instead of <=, because (ip+1) */
		hSmall = ZSTD_hashPtr(tls, ip, hBitsS, mls)
		matchIndex = *(*U32)(unsafe.Pointer(hashSmall + uintptr(hSmall)*4))
		if matchIndex < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		matchBase = v2
		match = matchBase + uintptr(matchIndex)
		hLong = ZSTD_hashPtr(tls, ip, hBitsL, uint32(8))
		matchLongIndex = *(*U32)(unsafe.Pointer(hashLong + uintptr(hLong)*4))
		if matchLongIndex < prefixStartIndex {
			v3 = dictBase
		} else {
			v3 = base
		}
		matchLongBase = v3
		matchLong = matchLongBase + uintptr(matchLongIndex)
		curr = uint32(int64(ip) - int64(base))
		repIndex = curr + uint32(1) - offset_1
		if repIndex < prefixStartIndex {
			v4 = dictBase
		} else {
			v4 = base
		} /* offset_1 expected <= curr +1 */
		repBase = v4
		repMatch = repBase + uintptr(repIndex)
		v5 = curr
		*(*U32)(unsafe.Pointer(hashLong + uintptr(hLong)*4)) = v5
		*(*U32)(unsafe.Pointer(hashSmall + uintptr(hSmall)*4)) = v5 /* update hash table */
		if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex)&libc.BoolInt32(offset_1 <= curr+uint32(1)-dictStartIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip+uintptr(1)) {
			if repIndex < prefixStartIndex {
				v2 = dictEnd
			} else {
				v2 = iend
			}
			repMatchEnd = v2
			mLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixStart) + uint64(4)
			ip = ip + 1
			ZSTD_storeSeq(tls, seqStore, uint64(int64(ip)-int64(anchor)), anchor, iend, uint32(libc.Int32FromInt32(1)), mLength)
		} else {
			if matchLongIndex > dictStartIndex && MEM_read64(tls, matchLong) == MEM_read64(tls, ip) {
				if matchLongIndex < prefixStartIndex {
					v2 = dictEnd
				} else {
					v2 = iend
				}
				matchEnd = v2
				if matchLongIndex < prefixStartIndex {
					v3 = dictStart
				} else {
					v3 = prefixStart
				}
				lowMatchPtr = v3
				mLength = ZSTD_count_2segments(tls, ip+uintptr(8), matchLong+uintptr(8), iend, matchEnd, prefixStart) + uint64(8)
				offset = curr - matchLongIndex
				for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchLong > lowMatchPtr) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(matchLong + uintptr(-libc.Int32FromInt32(1))))) {
					ip = ip - 1
					matchLong = matchLong - 1
					mLength = mLength + 1
				} /* catch up */
				offset_2 = offset_1
				offset_1 = offset
				ZSTD_storeSeq(tls, seqStore, uint64(int64(ip)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
			} else {
				if matchIndex > dictStartIndex && MEM_read32(tls, match) == MEM_read32(tls, ip) {
					h3 = ZSTD_hashPtr(tls, ip+uintptr(1), hBitsL, uint32(8))
					matchIndex3 = *(*U32)(unsafe.Pointer(hashLong + uintptr(h3)*4))
					if matchIndex3 < prefixStartIndex {
						v2 = dictBase
					} else {
						v2 = base
					}
					match3Base = v2
					match3 = match3Base + uintptr(matchIndex3)
					*(*U32)(unsafe.Pointer(hashLong + uintptr(h3)*4)) = curr + uint32(1)
					if matchIndex3 > dictStartIndex && MEM_read64(tls, match3) == MEM_read64(tls, ip+uintptr(1)) {
						if matchIndex3 < prefixStartIndex {
							v2 = dictEnd
						} else {
							v2 = iend
						}
						matchEnd1 = v2
						if matchIndex3 < prefixStartIndex {
							v3 = dictStart
						} else {
							v3 = prefixStart
						}
						lowMatchPtr1 = v3
						mLength = ZSTD_count_2segments(tls, ip+uintptr(9), match3+uintptr(8), iend, matchEnd1, prefixStart) + uint64(8)
						ip = ip + 1
						offset1 = curr + uint32(1) - matchIndex3
						for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match3 > lowMatchPtr1) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match3 + uintptr(-libc.Int32FromInt32(1))))) {
							ip = ip - 1
							match3 = match3 - 1
							mLength = mLength + 1
						} /* catch up */
					} else {
						if matchIndex < prefixStartIndex {
							v2 = dictEnd
						} else {
							v2 = iend
						}
						matchEnd2 = v2
						if matchIndex < prefixStartIndex {
							v3 = dictStart
						} else {
							v3 = prefixStart
						}
						lowMatchPtr2 = v3
						mLength = ZSTD_count_2segments(tls, ip+uintptr(4), match+uintptr(4), iend, matchEnd2, prefixStart) + uint64(4)
						offset1 = curr - matchIndex
						for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match > lowMatchPtr2) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
							ip = ip - 1
							match = match - 1
							mLength = mLength + 1
						} /* catch up */
					}
					offset_2 = offset_1
					offset_1 = offset1
					ZSTD_storeSeq(tls, seqStore, uint64(int64(ip)-int64(anchor)), anchor, iend, offset1+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
				} else {
					ip = ip + uintptr((int64(ip)-int64(anchor))>>libc.Int32FromInt32(kSearchStrength)+int64(1))
					continue
				}
			}
		}
		/* move to next sequence start */
		ip = ip + uintptr(mLength)
		anchor = ip
		if ip <= ilimit {
			/* Complementary insertion */
			/* done after iLimit test, as candidates could be > iend-8 */
			indexToInsert = curr + uint32(2)
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsL, uint32(8)))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip-uintptr(2), hBitsL, uint32(8)))*4)) = uint32(int64(ip-libc.UintptrFromInt32(2)) - int64(base))
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsS, mls))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip-uintptr(1), hBitsS, mls))*4)) = uint32(int64(ip-libc.UintptrFromInt32(1)) - int64(base))
			/* check immediate repcode */
			for ip <= ilimit {
				current2 = uint32(int64(ip) - int64(base))
				repIndex2 = current2 - offset_2
				if repIndex2 < prefixStartIndex {
					v2 = dictBase + uintptr(repIndex2)
				} else {
					v2 = base + uintptr(repIndex2)
				}
				repMatch2 = v2
				if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex2)&libc.BoolInt32(offset_2 <= current2-dictStartIndex) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip) {
					if repIndex2 < prefixStartIndex {
						v3 = dictEnd
					} else {
						v3 = iend
					}
					repEnd2 = v3
					repLength2 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
					tmpOffset = offset_2
					offset_2 = offset_1
					offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), repLength2)
					*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip, hBitsS, mls))*4)) = current2
					*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip, hBitsL, uint32(8)))*4)) = current2
					ip = ip + uintptr(repLength2)
					anchor = ip
					continue
				}
				break
			}
		}
	}
	/* save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_doubleFast_extDict_4(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4))
}

func ZSTD_compressBlock_doubleFast_extDict_5(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5))
}

func ZSTD_compressBlock_doubleFast_extDict_6(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6))
}

func ZSTD_compressBlock_doubleFast_extDict_7(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7))
}

func ZSTD_compressBlock_doubleFast_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_doubleFast_extDict_4(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_doubleFast_extDict_5(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_doubleFast_extDict_6(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_doubleFast_extDict_7(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

/**** ended inlining compress/zstd_double_fast.c ****/
/**** start inlining compress/zstd_fast.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_fast.h ****/

func ZSTD_fillHashTableForCDict(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashTable, iend, ip uintptr
	var curr, fastHashFillStep, hBits, mls, p U32
	var hashAndTag, hashAndTag1 size_t
	_, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBits, hashAndTag, hashAndTag1, hashTable, iend, ip, mls, p
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBits = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Currently, we always use ZSTD_dtlm_full for filling CDict tables.
	 * Feel free to remove this assert if there's a good reason! */
	/* Always insert every fastHashFillStep position into the hash table.
	 * Insert the other positions if their hash entry is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep) < iend+uintptr(2)) {
			break
		}
		curr = uint32(int64(ip) - int64(base))
		hashAndTag = ZSTD_hashPtr(tls, ip, hBits, mls)
		ZSTD_writeTaggedIndex(tls, hashTable, hashAndTag, curr)
		if dtlm == int32(ZSTD_dtlm_fast) {
			goto _1
		}
		/* Only load extra positions for ZSTD_dtlm_full */
		p = uint32(1)
		for {
			if !(p < fastHashFillStep) {
				break
			}
			hashAndTag1 = ZSTD_hashPtr(tls, ip+uintptr(p), hBits, mls)
			if *(*U32)(unsafe.Pointer(hashTable + uintptr(hashAndTag1>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4)) == uint32(0) { /* not yet filled */
				ZSTD_writeTaggedIndex(tls, hashTable, hashAndTag1, curr+p)
			}
			goto _2
		_2:
			;
			p = p + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillHashTableForCCtx(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashTable, iend, ip uintptr
	var curr, fastHashFillStep, hBits, mls, p U32
	var hash, hash0 size_t
	_, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBits, hash, hash0, hashTable, iend, ip, mls, p
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBits = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Currently, we always use ZSTD_dtlm_fast for filling CCtx tables.
	 * Feel free to remove this assert if there's a good reason! */
	/* Always insert every fastHashFillStep position into the hash table.
	 * Insert the other positions if their hash entry is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep) < iend+uintptr(2)) {
			break
		}
		curr = uint32(int64(ip) - int64(base))
		hash0 = ZSTD_hashPtr(tls, ip, hBits, mls)
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = curr
		if dtlm == int32(ZSTD_dtlm_fast) {
			goto _1
		}
		/* Only load extra positions for ZSTD_dtlm_full */
		p = uint32(1)
		for {
			if !(p < fastHashFillStep) {
				break
			}
			hash = ZSTD_hashPtr(tls, ip+uintptr(p), hBits, mls)
			if *(*U32)(unsafe.Pointer(hashTable + uintptr(hash)*4)) == uint32(0) { /* not yet filled */
				*(*U32)(unsafe.Pointer(hashTable + uintptr(hash)*4)) = curr + p
			}
			goto _2
		_2:
			;
			p = p + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillHashTable(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e) {
	if tfp == int32(ZSTD_tfp_forCDict) {
		ZSTD_fillHashTableForCDict(tls, ms, end, dtlm)
	} else {
		ZSTD_fillHashTableForCCtx(tls, ms, end, dtlm)
	}
}

type ZSTD_match4Found = uintptr

func ZSTD_match4Found_cmov(tls *libc.TLS, currentPtr uintptr, matchAddress uintptr, matchIdx U32, idxLowLimit U32) (r int32) {
	var mvalAddr uintptr
	_ = mvalAddr
	/* currentIdx >= lowLimit is a (somewhat) unpredictable branch.
	 * However expression below compiles into conditional move.
	 */
	mvalAddr = ZSTD_selectAddr(tls, matchIdx, idxLowLimit, matchAddress, uintptr(unsafe.Pointer(&dummy)))
	/* Note: this used to be written as : return test1 && test2;
	 * Unfortunately, once inlined, these tests become branches,
	 * in which case it becomes critical that they are executed in the right order (test1 then test2).
	 * So we have to write these tests in a specific manner to ensure their ordering.
	 */
	if MEM_read32(tls, currentPtr) != MEM_read32(tls, mvalAddr) {
		return 0
	}
	/* force ordering of these tests, which matters once the function is inlined, as they become branches */
	return libc.BoolInt32(matchIdx >= idxLowLimit)
}

/* Array of ~random data, should have low probability of matching data.
 * Load from here if the index is invalid.
 * Used to avoid unpredictable branches. */
var dummy = [4]BYTE{
	0: uint8(0x12),
	1: uint8(0x34),
	2: uint8(0x56),
	3: uint8(0x78),
}

func ZSTD_match4Found_branch(tls *libc.TLS, currentPtr uintptr, matchAddress uintptr, matchIdx U32, idxLowLimit U32) (r int32) {
	var mval U32
	_ = mval
	if matchIdx >= idxLowLimit {
		mval = MEM_read32(tls, matchAddress)
	} else {
		mval = MEM_read32(tls, currentPtr) ^ uint32(1) /* guaranteed to not match. */
	}
	return libc.BoolInt32(MEM_read32(tls, currentPtr) == mval)
}

// C documentation
//
//	/**
//	 * If you squint hard enough (and ignore repcodes), the search operation at any
//	 * given position is broken into 4 stages:
//	 *
//	 * 1. Hash   (map position to hash value via input read)
//	 * 2. Lookup (map hash val to index via hashtable read)
//	 * 3. Load   (map index to value at that position via input read)
//	 * 4. Compare
//	 *
//	 * Each of these steps involves a memory read at an address which is computed
//	 * from the previous step. This means these steps must be sequenced and their
//	 * latencies are cumulative.
//	 *
//	 * Rather than do 1->2->3->4 sequentially for a single position before moving
//	 * onto the next, this implementation interleaves these operations across the
//	 * next few positions:
//	 *
//	 * R = Repcode Read & Compare
//	 * H = Hash
//	 * T = Table Lookup
//	 * M = Match Read & Compare
//	 *
//	 * Pos | Time -->
//	 * ----+-------------------
//	 * N   | ... M
//	 * N+1 | ...   TM
//	 * N+2 |    R H   T M
//	 * N+3 |         H    TM
//	 * N+4 |           R H   T M
//	 * N+5 |                H   ...
//	 * N+6 |                  R ...
//	 *
//	 * This is very much analogous to the pipelining of execution in a CPU. And just
//	 * like a CPU, we have to dump the pipeline when we find a match (i.e., take a
//	 * branch).
//	 *
//	 * When this happens, we throw away our current state, and do the following prep
//	 * to re-enter the loop:
//	 *
//	 * Pos | Time -->
//	 * ----+-------------------
//	 * N   | H T
//	 * N+1 |  H
//	 *
//	 * This is also the work we do at the beginning to enter the loop initially.
//	 */
func ZSTD_compressBlock_fast_noDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32, useCmov int32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, cParams, hashTable, iend, ilimit, ip0, ip1, ip2, ip3, istart, match0, nextStep, prefixStart, v1 uintptr
	var curr, current0, endIndex, hlog, matchIdx, maxRep, offcode, offsetSaved1, offsetSaved2, prefixStartIndex, rep_offset1, rep_offset2, rval, tmpOff, windowLow U32
	var hash0, hash1, kStepIncr, mLength, rLength, step, stepSize size_t
	var matchFound ZSTD_match4Found
	var v2 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current0, endIndex, hash0, hash1, hashTable, hlog, iend, ilimit, ip0, ip1, ip2, ip3, istart, kStepIncr, mLength, match0, matchFound, matchIdx, maxRep, nextStep, offcode, offsetSaved1, offsetSaved2, prefixStart, prefixStartIndex, rLength, rep_offset1, rep_offset2, rval, step, stepSize, tmpOff, windowLow, v1, v2
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hlog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	stepSize = uint64((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength + libc.BoolUint32(!((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength != 0)) + uint32(1)) /* min 2 */
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	endIndex = uint32(uint64(int64(istart)-int64(base)) + srcSize)
	prefixStartIndex = ZSTD_getLowestPrefixIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	prefixStart = base + uintptr(prefixStartIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	anchor = istart
	ip0 = istart
	rep_offset1 = *(*U32)(unsafe.Pointer(rep))
	rep_offset2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	kStepIncr = uint64(libc.Int32FromInt32(1) << (libc.Int32FromInt32(kSearchStrength) - libc.Int32FromInt32(1)))
	if useCmov != 0 {
		v1 = __ccgo_fp(ZSTD_match4Found_cmov)
	} else {
		v1 = __ccgo_fp(ZSTD_match4Found_branch)
	}
	matchFound = v1
	ip0 = ip0 + libc.BoolUintptr(ip0 == prefixStart)
	curr = uint32(int64(ip0) - int64(base))
	windowLow = ZSTD_getLowestPrefixIndex(tls, ms, curr, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	maxRep = curr - windowLow
	if rep_offset2 > maxRep {
		offsetSaved2 = rep_offset2
		rep_offset2 = libc.Uint32FromInt32(0)
	}
	if rep_offset1 > maxRep {
		offsetSaved1 = rep_offset1
		rep_offset1 = libc.Uint32FromInt32(0)
	}
	/* start each op */
	goto _start
_start:
	; /* Requires: ip0 */
	step = stepSize
	nextStep = ip0 + uintptr(kStepIncr)
	/* calculate positions, ip0 - anchor == 0, so we skip step calc */
	ip1 = ip0 + uintptr(1)
	ip2 = ip0 + uintptr(step)
	ip3 = ip2 + uintptr(1)
	if ip3 >= ilimit {
		goto _cleanup
	}
	hash0 = ZSTD_hashPtr(tls, ip0, hlog, mls)
	hash1 = ZSTD_hashPtr(tls, ip1, hlog, mls)
	matchIdx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4))
	for cond := true; cond; cond = ip3 < ilimit {
		/* load repcode match for ip[2]*/
		rval = MEM_read32(tls, ip2-uintptr(rep_offset1))
		/* write back hash table entry */
		current0 = uint32(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		/* check repcode at ip[2] */
		if libc.BoolInt32(MEM_read32(tls, ip2) == rval)&libc.BoolInt32(rep_offset1 > uint32(0)) != 0 {
			ip0 = ip2
			match0 = ip0 - uintptr(rep_offset1)
			mLength = libc.BoolUint64(int32(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))))
			ip0 = ip0 - uintptr(mLength)
			match0 = match0 - uintptr(mLength)
			offcode = uint32(libc.Int32FromInt32(1))
			mLength = mLength + uint64(4)
			/* Write next hash table entry: it's already calculated.
			 * This write is known to be safe because ip1 is before the
			 * repcode (ip2). */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = uint32(int64(ip1) - int64(base))
			goto _match
		}
		if (*(*func(*libc.TLS, uintptr, uintptr, U32, U32) int32)(unsafe.Pointer(&struct{ uintptr }{matchFound})))(tls, ip0, base+uintptr(matchIdx), matchIdx, prefixStartIndex) != 0 {
			/* Write next hash table entry (it's already calculated).
			 * This write is known to be safe because the ip1 == ip0 + 1,
			 * so searching will resume after ip1 */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = uint32(int64(ip1) - int64(base))
			goto _offset
		}
		/* lookup ip[1] */
		matchIdx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip3
		/* write back hash table entry */
		current0 = uint32(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		if (*(*func(*libc.TLS, uintptr, uintptr, U32, U32) int32)(unsafe.Pointer(&struct{ uintptr }{matchFound})))(tls, ip0, base+uintptr(matchIdx), matchIdx, prefixStartIndex) != 0 {
			/* Write next hash table entry, since it's already calculated */
			if step <= uint64(4) {
				/* Avoid writing an index if it's >= position where search will resume.
				 * The minimum possible match has length 4, so search can resume at ip0 + 4.
				 */
				*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = uint32(int64(ip1) - int64(base))
			}
			goto _offset
		}
		/* lookup ip[1] */
		matchIdx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip0 + uintptr(step)
		ip3 = ip1 + uintptr(step)
		/* calculate step */
		if ip2 >= nextStep {
			step = step + 1
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(64), libc.VaList(bp+8, 0, int32(3)))
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(128), libc.VaList(bp+8, 0, int32(3)))
			nextStep = nextStep + uintptr(kStepIncr)
		}
	}
	goto _cleanup
_cleanup:
	;
	/* Note that there are probably still a couple positions one could search.
	 * However, it seems to be a meaningful performance hit to try to search
	 * them. So let's not. */
	/* When the repcodes are outside of the prefix, we set them to zero before the loop.
	 * When the offsets are still zero, we need to restore them after the block to have a correct
	 * repcode history. If only one offset was invalid, it is easy. The tricky case is when both
	 * offsets were invalid. We need to figure out which offset to refill with.
	 *     - If both offsets are zero they are in the same order.
	 *     - If both offsets are non-zero, we won't restore the offsets from `offsetSaved[12]`.
	 *     - If only one is zero, we need to decide which offset to restore.
	 *         - If rep_offset1 is non-zero, then rep_offset2 must be offsetSaved1.
	 *         - It is impossible for rep_offset2 to be non-zero.
	 *
	 * So if rep_offset1 started invalid (offsetSaved1 != 0) and became valid (rep_offset1 != 0), then
	 * set rep[0] = rep_offset1 and rep[1] = offsetSaved1.
	 */
	if offsetSaved1 != uint32(0) && rep_offset1 != uint32(0) {
		v2 = offsetSaved1
	} else {
		v2 = offsetSaved2
	}
	offsetSaved2 = v2
	/* save reps for next block */
	if rep_offset1 != 0 {
		v2 = rep_offset1
	} else {
		v2 = offsetSaved1
	}
	*(*U32)(unsafe.Pointer(rep)) = v2
	if rep_offset2 != 0 {
		v2 = rep_offset2
	} else {
		v2 = offsetSaved2
	}
	*(*U32)(unsafe.Pointer(rep + 1*4)) = v2
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
	goto _offset
_offset:
	; /* Requires: ip0, idx */
	/* Compute the offset code. */
	match0 = base + uintptr(matchIdx)
	rep_offset2 = rep_offset1
	rep_offset1 = uint32(int64(ip0) - int64(match0))
	offcode = rep_offset1 + libc.Uint32FromInt32(ZSTD_REP_NUM)
	mLength = uint64(4)
	/* Count the backwards match length. */
	for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(match0 > prefixStart) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))) {
		ip0 = ip0 - 1
		match0 = match0 - 1
		mLength = mLength + 1
	}
	goto _match
_match:
	; /* Requires: ip0, match0, offcode */
	/* Count the forward length. */
	mLength = mLength + ZSTD_count(tls, ip0+uintptr(mLength), match0+uintptr(mLength), iend)
	ZSTD_storeSeq(tls, seqStore, uint64(int64(ip0)-int64(anchor)), anchor, iend, offcode, mLength)
	ip0 = ip0 + uintptr(mLength)
	anchor = ip0
	/* Fill table and check for immediate repcode. */
	if ip0 <= ilimit {
		/* Fill Table */
		/* check base overflow */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, base+uintptr(current0)+uintptr(2), hlog, mls))*4)) = current0 + uint32(2) /* here because current+2 could be > iend-8 */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0-uintptr(2), hlog, mls))*4)) = uint32(int64(ip0-libc.UintptrFromInt32(2)) - int64(base))
		if rep_offset2 > uint32(0) { /* rep_offset2==0 means rep_offset2 is invalidated */
			for ip0 <= ilimit && MEM_read32(tls, ip0) == MEM_read32(tls, ip0-uintptr(rep_offset2)) {
				/* store sequence */
				rLength = ZSTD_count(tls, ip0+uintptr(4), ip0+uintptr(4)-uintptr(rep_offset2), iend) + uint64(4)
				tmpOff = rep_offset2
				rep_offset2 = rep_offset1
				rep_offset1 = tmpOff /* swap rep_offset2 <=> rep_offset1 */
				*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0, hlog, mls))*4)) = uint32(int64(ip0) - int64(base))
				ip0 = ip0 + uintptr(rLength)
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), rLength)
				anchor = ip0
				continue /* faster when present (confirmed on gcc-8) ... (?) */
			}
		}
	}
	goto _start
	return r
}

func ZSTD_compressBlock_fast_noDict_4_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), int32(1))
}

func ZSTD_compressBlock_fast_noDict_5_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), int32(1))
}

func ZSTD_compressBlock_fast_noDict_6_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), int32(1))
}

func ZSTD_compressBlock_fast_noDict_7_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), int32(1))
}

func ZSTD_compressBlock_fast_noDict_4_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), 0)
}

func ZSTD_compressBlock_fast_noDict_5_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), 0)
}

func ZSTD_compressBlock_fast_noDict_6_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), 0)
}

func ZSTD_compressBlock_fast_noDict_7_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), 0)
}

func ZSTD_compressBlock_fast(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mml U32
	var useCmov int32
	_, _ = mml, useCmov
	mml = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	/* use cmov when "candidate in range" branch is likely unpredictable */
	useCmov = libc.BoolInt32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FwindowLog < uint32(19))
	if useCmov != 0 {
		switch mml {
		default: /* includes case 3 */
			fallthrough
		case uint32(4):
			return ZSTD_compressBlock_fast_noDict_4_1(tls, ms, seqStore, rep, src, srcSize)
		case uint32(5):
			return ZSTD_compressBlock_fast_noDict_5_1(tls, ms, seqStore, rep, src, srcSize)
		case uint32(6):
			return ZSTD_compressBlock_fast_noDict_6_1(tls, ms, seqStore, rep, src, srcSize)
		case uint32(7):
			return ZSTD_compressBlock_fast_noDict_7_1(tls, ms, seqStore, rep, src, srcSize)
		}
	} else {
		/* use a branch instead */
		switch mml {
		default: /* includes case 3 */
			fallthrough
		case uint32(4):
			return ZSTD_compressBlock_fast_noDict_4_0(tls, ms, seqStore, rep, src, srcSize)
		case uint32(5):
			return ZSTD_compressBlock_fast_noDict_5_0(tls, ms, seqStore, rep, src, srcSize)
		case uint32(6):
			return ZSTD_compressBlock_fast_noDict_6_0(tls, ms, seqStore, rep, src, srcSize)
		case uint32(7):
			return ZSTD_compressBlock_fast_noDict_7_0(tls, ms, seqStore, rep, src, srcSize)
		}
	}
	return r
}

func ZSTD_compressBlock_fast_dictMatchState_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32, hasStep U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _pos, _size, dictHashAndTag0, dictHashAndTag1, hash0, hash1, hashTableBytes, kStepIncr, mLength, repLength2, step size_t
	var _ptr, anchor, base, cParams, dictBase, dictCParams, dictEnd, dictHashTable, dictMatch, dictStart, dms, hashTable, iend, ilimit, ip0, ip1, istart, match, nextStep, prefixStart, repEnd2, repMatch, repMatch2, repMatchEnd, v2, v3 uintptr
	var curr, current2, dictAndPrefixLength, dictHBits, dictIndexDelta, dictMatchIndex, dictMatchIndexAndTag, dictStartIndex, endIndex, hlog, matchIndex, maxDistance, offset, offset1, offset_1, offset_2, prefixStartIndex, repIndex, repIndex2, stepSize, tmpOffset U32
	var dictTagsMatch int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _pos, _ptr, _size, anchor, base, cParams, curr, current2, dictAndPrefixLength, dictBase, dictCParams, dictEnd, dictHBits, dictHashAndTag0, dictHashAndTag1, dictHashTable, dictIndexDelta, dictMatch, dictMatchIndex, dictMatchIndexAndTag, dictStart, dictStartIndex, dictTagsMatch, dms, endIndex, hash0, hash1, hashTable, hashTableBytes, hlog, iend, ilimit, ip0, ip1, istart, kStepIncr, mLength, match, matchIndex, maxDistance, nextStep, offset, offset1, offset_1, offset_2, prefixStart, prefixStartIndex, repEnd2, repIndex, repIndex2, repLength2, repMatch, repMatch2, repMatchEnd, step, stepSize, tmpOffset, v2, v3
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hlog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	/* support stepSize of 0 */
	stepSize = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength + libc.BoolUint32(!((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength != 0))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	ip0 = istart
	ip1 = ip0 + uintptr(stepSize) /* we assert below that stepSize >= 1 */
	anchor = istart
	prefixStartIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(prefixStartIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	dictCParams = dms + 256
	dictHashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
	dictStartIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	dictStart = dictBase + uintptr(dictStartIndex)
	dictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	dictIndexDelta = prefixStartIndex - uint32(int64(dictEnd)-int64(dictBase))
	dictAndPrefixLength = uint32(int64(uintptr(int64(istart)-int64(prefixStart))+dictEnd) - int64(dictStart))
	dictHBits = (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	/* if a dictionary is still attached, it necessarily means that
	 * it is within window size. So we just check it. */
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	endIndex = uint32(uint64(int64(istart)-int64(base)) + srcSize)
	_ = maxDistance
	_ = endIndex /* these variables are not used when assert() is disabled */
	_ = hasStep  /* not currently specialized on whether it's accelerated */
	/* ensure there will be no underflow
	 * when translating a dict index into a local index */
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FprefetchCDictTables != 0 {
		hashTableBytes = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog * uint64(4)
		_ptr = dictHashTable
		_size = hashTableBytes
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+8, 0, int32(2)))
			goto _1
		_1:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
	}
	/* init */
	ip0 = ip0 + libc.BoolUintptr(dictAndPrefixLength == libc.Uint32FromInt32(0))
	/* dictMatchState repCode checks don't currently handle repCode == 0
	 * disabling. */
	/* Outer search loop */
	for ip1 <= ilimit {
		hash0 = ZSTD_hashPtr(tls, ip0, hlog, mls)
		dictHashAndTag0 = ZSTD_hashPtr(tls, ip0, dictHBits, mls)
		dictMatchIndexAndTag = *(*U32)(unsafe.Pointer(dictHashTable + uintptr(dictHashAndTag0>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictTagsMatch = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTag), dictHashAndTag0)
		matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4))
		curr = uint32(int64(ip0) - int64(base))
		step = uint64(stepSize)
		kStepIncr = uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(kSearchStrength))
		nextStep = ip0 + uintptr(kStepIncr)
		/* Inner search loop */
		for int32(1) != 0 {
			match = base + uintptr(matchIndex)
			repIndex = curr + uint32(1) - offset_1
			if repIndex < prefixStartIndex {
				v2 = dictBase + uintptr(repIndex-dictIndexDelta)
			} else {
				v2 = base + uintptr(repIndex)
			}
			repMatch = v2
			hash1 = ZSTD_hashPtr(tls, ip1, hlog, mls)
			dictHashAndTag1 = ZSTD_hashPtr(tls, ip1, dictHBits, mls)
			*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = curr /* update hash table */
			if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip0+uintptr(1)) {
				if repIndex < prefixStartIndex {
					v2 = dictEnd
				} else {
					v2 = iend
				}
				repMatchEnd = v2
				mLength = ZSTD_count_2segments(tls, ip0+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixStart) + uint64(4)
				ip0 = ip0 + 1
				ZSTD_storeSeq(tls, seqStore, uint64(int64(ip0)-int64(anchor)), anchor, iend, uint32(libc.Int32FromInt32(1)), mLength)
				break
			}
			if dictTagsMatch != 0 {
				/* Found a possible dict match */
				dictMatchIndex = dictMatchIndexAndTag >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				dictMatch = dictBase + uintptr(dictMatchIndex)
				if dictMatchIndex > dictStartIndex && MEM_read32(tls, dictMatch) == MEM_read32(tls, ip0) {
					/* To replicate extDict parse behavior, we only use dict matches when the normal matchIndex is invalid */
					if matchIndex <= prefixStartIndex {
						offset = curr - dictMatchIndex - dictIndexDelta
						mLength = ZSTD_count_2segments(tls, ip0+uintptr(4), dictMatch+uintptr(4), iend, dictEnd, prefixStart) + uint64(4)
						for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(dictMatch > dictStart) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(dictMatch + uintptr(-libc.Int32FromInt32(1))))) {
							ip0 = ip0 - 1
							dictMatch = dictMatch - 1
							mLength = mLength + 1
						} /* catch up */
						offset_2 = offset_1
						offset_1 = offset
						ZSTD_storeSeq(tls, seqStore, uint64(int64(ip0)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
						break
					}
				}
			}
			if ZSTD_match4Found_cmov(tls, ip0, match, matchIndex, prefixStartIndex) != 0 {
				/* found a regular match of size >= 4 */
				offset1 = uint32(int64(ip0) - int64(match))
				mLength = ZSTD_count(tls, ip0+uintptr(4), match+uintptr(4), iend) + uint64(4)
				for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(match > prefixStart) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
					ip0 = ip0 - 1
					match = match - 1
					mLength = mLength + 1
				} /* catch up */
				offset_2 = offset_1
				offset_1 = offset1
				ZSTD_storeSeq(tls, seqStore, uint64(int64(ip0)-int64(anchor)), anchor, iend, offset1+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
				break
			}
			/* Prepare for next iteration */
			dictMatchIndexAndTag = *(*U32)(unsafe.Pointer(dictHashTable + uintptr(dictHashAndTag1>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
			dictTagsMatch = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTag), dictHashAndTag1)
			matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
			if ip1 >= nextStep {
				step = step + 1
				nextStep = nextStep + uintptr(kStepIncr)
			}
			ip0 = ip1
			ip1 = ip1 + uintptr(step)
			if ip1 > ilimit {
				goto _cleanup
			}
			curr = uint32(int64(ip0) - int64(base))
			hash0 = hash1
		} /* end inner search loop */
		/* match found */
		ip0 = ip0 + uintptr(mLength)
		anchor = ip0
		if ip0 <= ilimit {
			/* Fill Table */
			/* check base overflow */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, base+uintptr(curr)+uintptr(2), hlog, mls))*4)) = curr + uint32(2) /* here because curr+2 could be > iend-8 */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0-uintptr(2), hlog, mls))*4)) = uint32(int64(ip0-libc.UintptrFromInt32(2)) - int64(base))
			/* check immediate repcode */
			for ip0 <= ilimit {
				current2 = uint32(int64(ip0) - int64(base))
				repIndex2 = current2 - offset_2
				if repIndex2 < prefixStartIndex {
					v2 = dictBase - uintptr(dictIndexDelta) + uintptr(repIndex2)
				} else {
					v2 = base + uintptr(repIndex2)
				}
				repMatch2 = v2
				if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex2) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip0) {
					if repIndex2 < prefixStartIndex {
						v3 = dictEnd
					} else {
						v3 = iend
					}
					repEnd2 = v3
					repLength2 = ZSTD_count_2segments(tls, ip0+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
					tmpOffset = offset_2
					offset_2 = offset_1
					offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), repLength2)
					*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0, hlog, mls))*4)) = current2
					ip0 = ip0 + uintptr(repLength2)
					anchor = ip0
					continue
				}
				break
			}
		}
		/* Prepare for next iteration */
		ip1 = ip0 + uintptr(stepSize)
	}
	goto _cleanup
_cleanup:
	;
	/* save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_fast_dictMatchState_4_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState_5_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState_6_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState_7_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_fast_dictMatchState_4_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_fast_dictMatchState_5_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_fast_dictMatchState_6_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_fast_dictMatchState_7_0(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

func ZSTD_compressBlock_fast_extDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32, hasStep U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, cParams, dictBase, dictEnd, dictStart, hashTable, idxBase, iend, ilimit, ip0, ip1, ip2, ip3, istart, lowMatchPtr, match0, matchEnd, nextStep, prefixStart, repBase, repEnd2, repMatch2, v2, v3 uintptr
	var curr, current0, current2, dictLimit, dictStartIndex, endIndex, hlog, idx, lowLimit, maxRep, mval, mval1, offcode, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixStartIndex, repIndex, repIndex2, rval, tmpOffset U32
	var hash0, hash1, kStepIncr, mLength, repLength2, step, stepSize size_t
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current0, current2, dictBase, dictEnd, dictLimit, dictStart, dictStartIndex, endIndex, hash0, hash1, hashTable, hlog, idx, idxBase, iend, ilimit, ip0, ip1, ip2, ip3, istart, kStepIncr, lowLimit, lowMatchPtr, mLength, match0, matchEnd, maxRep, mval, mval1, nextStep, offcode, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixStart, prefixStartIndex, repBase, repEnd2, repIndex, repIndex2, repLength2, repMatch2, rval, step, stepSize, tmpOffset, v1, v2, v3
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hlog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	/* support stepSize of 0 */
	stepSize = uint64((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength + libc.BoolUint32(!((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength != 0)) + uint32(1))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	istart = src
	anchor = istart
	endIndex = uint32(uint64(int64(istart)-int64(base)) + srcSize)
	lowLimit = ZSTD_getLowestMatchIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	dictStartIndex = lowLimit
	dictStart = dictBase + uintptr(dictStartIndex)
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if dictLimit < lowLimit {
		v1 = lowLimit
	} else {
		v1 = dictLimit
	}
	prefixStartIndex = v1
	prefixStart = base + uintptr(prefixStartIndex)
	dictEnd = dictBase + uintptr(prefixStartIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(8)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	ip0 = istart
	matchEnd = uintptr(0)
	kStepIncr = uint64(libc.Int32FromInt32(1) << (libc.Int32FromInt32(kSearchStrength) - libc.Int32FromInt32(1)))
	_ = hasStep /* not currently specialized on whether it's accelerated */
	/* switch to "regular" variant if extDict is invalidated due to maxDistance */
	if prefixStartIndex == dictStartIndex {
		return ZSTD_compressBlock_fast(tls, ms, seqStore, rep, src, srcSize)
	}
	curr = uint32(int64(ip0) - int64(base))
	maxRep = curr - dictStartIndex
	if offset_2 >= maxRep {
		offsetSaved2 = offset_2
		offset_2 = libc.Uint32FromInt32(0)
	}
	if offset_1 >= maxRep {
		offsetSaved1 = offset_1
		offset_1 = libc.Uint32FromInt32(0)
	}
	/* start each op */
	goto _start
_start:
	; /* Requires: ip0 */
	step = stepSize
	nextStep = ip0 + uintptr(kStepIncr)
	/* calculate positions, ip0 - anchor == 0, so we skip step calc */
	ip1 = ip0 + uintptr(1)
	ip2 = ip0 + uintptr(step)
	ip3 = ip2 + uintptr(1)
	if ip3 >= ilimit {
		goto _cleanup
	}
	hash0 = ZSTD_hashPtr(tls, ip0, hlog, mls)
	hash1 = ZSTD_hashPtr(tls, ip1, hlog, mls)
	idx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4))
	if idx < prefixStartIndex {
		v2 = dictBase
	} else {
		v2 = base
	}
	idxBase = v2
	for cond := true; cond; cond = ip3 < ilimit {
		/* load repcode match for ip[2] */
		current2 = uint32(int64(ip2) - int64(base))
		repIndex = current2 - offset_1
		if repIndex < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		repBase = v2
		if libc.BoolInt32(prefixStartIndex-repIndex >= uint32(4))&libc.BoolInt32(offset_1 > uint32(0)) != 0 {
			rval = MEM_read32(tls, repBase+uintptr(repIndex))
		} else {
			rval = MEM_read32(tls, ip2) ^ uint32(1) /* guaranteed to not match. */
		}
		/* write back hash table entry */
		current0 = uint32(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		/* check repcode at ip[2] */
		if MEM_read32(tls, ip2) == rval {
			ip0 = ip2
			match0 = repBase + uintptr(repIndex)
			if repIndex < prefixStartIndex {
				v2 = dictEnd
			} else {
				v2 = iend
			}
			matchEnd = v2
			mLength = libc.BoolUint64(int32(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))))
			ip0 = ip0 - uintptr(mLength)
			match0 = match0 - uintptr(mLength)
			offcode = uint32(libc.Int32FromInt32(1))
			mLength = mLength + uint64(4)
			goto _match
		}
		if idx >= dictStartIndex {
			v1 = MEM_read32(tls, idxBase+uintptr(idx))
		} else {
			v1 = MEM_read32(tls, ip0) ^ uint32(1)
		} /* load match for ip[0] */
		mval = v1 /* guaranteed not to match */
		/* check match at ip[0] */
		if MEM_read32(tls, ip0) == mval {
			/* found a match! */
			goto _offset
		}
		/* lookup ip[1] */
		idx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		if idx < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		idxBase = v2
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip3
		/* write back hash table entry */
		current0 = uint32(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		if idx >= dictStartIndex {
			v1 = MEM_read32(tls, idxBase+uintptr(idx))
		} else {
			v1 = MEM_read32(tls, ip0) ^ uint32(1)
		} /* load match for ip[0] */
		mval1 = v1 /* guaranteed not to match */
		/* check match at ip[0] */
		if MEM_read32(tls, ip0) == mval1 {
			/* found a match! */
			goto _offset
		}
		/* lookup ip[1] */
		idx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		if idx < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		idxBase = v2
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip0 + uintptr(step)
		ip3 = ip1 + uintptr(step)
		/* calculate step */
		if ip2 >= nextStep {
			step = step + 1
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(64), libc.VaList(bp+8, 0, int32(3)))
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(128), libc.VaList(bp+8, 0, int32(3)))
			nextStep = nextStep + uintptr(kStepIncr)
		}
	}
	goto _cleanup
_cleanup:
	;
	/* Note that there are probably still a couple positions we could search.
	 * However, it seems to be a meaningful performance hit to try to search
	 * them. So let's not. */
	/* If offset_1 started invalid (offsetSaved1 != 0) and became valid (offset_1 != 0),
	 * rotate saved offsets. See comment in ZSTD_compressBlock_fast_noDict for more context. */
	if offsetSaved1 != uint32(0) && offset_1 != uint32(0) {
		v1 = offsetSaved1
	} else {
		v1 = offsetSaved2
	}
	offsetSaved2 = v1
	/* save reps for next block */
	if offset_1 != 0 {
		v1 = offset_1
	} else {
		v1 = offsetSaved1
	}
	*(*U32)(unsafe.Pointer(rep)) = v1
	if offset_2 != 0 {
		v1 = offset_2
	} else {
		v1 = offsetSaved2
	}
	*(*U32)(unsafe.Pointer(rep + 1*4)) = v1
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
	goto _offset
_offset:
	; /* Requires: ip0, idx, idxBase */
	/* Compute the offset code. */
	offset = current0 - idx
	if idx < prefixStartIndex {
		v2 = dictStart
	} else {
		v2 = prefixStart
	}
	lowMatchPtr = v2
	if idx < prefixStartIndex {
		v3 = dictEnd
	} else {
		v3 = iend
	}
	matchEnd = v3
	match0 = idxBase + uintptr(idx)
	offset_2 = offset_1
	offset_1 = offset
	offcode = offset + libc.Uint32FromInt32(ZSTD_REP_NUM)
	mLength = uint64(4)
	/* Count the backwards match length. */
	for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(match0 > lowMatchPtr) != 0 && int32(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))) {
		ip0 = ip0 - 1
		match0 = match0 - 1
		mLength = mLength + 1
	}
	goto _match
_match:
	; /* Requires: ip0, match0, offcode, matchEnd */
	/* Count the forward length. */
	mLength = mLength + ZSTD_count_2segments(tls, ip0+uintptr(mLength), match0+uintptr(mLength), iend, matchEnd, prefixStart)
	ZSTD_storeSeq(tls, seqStore, uint64(int64(ip0)-int64(anchor)), anchor, iend, offcode, mLength)
	ip0 = ip0 + uintptr(mLength)
	anchor = ip0
	/* write next hash table entry */
	if ip1 < ip0 {
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = uint32(int64(ip1) - int64(base))
	}
	/* Fill table and check for immediate repcode. */
	if ip0 <= ilimit {
		/* Fill Table */
		/* check base overflow */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, base+uintptr(current0)+uintptr(2), hlog, mls))*4)) = current0 + uint32(2) /* here because current+2 could be > iend-8 */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0-uintptr(2), hlog, mls))*4)) = uint32(int64(ip0-libc.UintptrFromInt32(2)) - int64(base))
		for ip0 <= ilimit {
			repIndex2 = uint32(int64(ip0)-int64(base)) - offset_2
			if repIndex2 < prefixStartIndex {
				v2 = dictBase + uintptr(repIndex2)
			} else {
				v2 = base + uintptr(repIndex2)
			}
			repMatch2 = v2
			if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex2)&libc.BoolInt32(offset_2 > uint32(0)) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip0) {
				if repIndex2 < prefixStartIndex {
					v3 = dictEnd
				} else {
					v3 = iend
				}
				repEnd2 = v3
				repLength2 = ZSTD_count_2segments(tls, ip0+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
				tmpOffset = offset_2
				offset_2 = offset_1
				offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), repLength2)
				*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0, hlog, mls))*4)) = uint32(int64(ip0) - int64(base))
				ip0 = ip0 + uintptr(repLength2)
				anchor = ip0
				continue
			}
			break
		}
	}
	goto _start
	return r
}

func ZSTD_compressBlock_fast_extDict_4_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), uint32(0))
}

func ZSTD_compressBlock_fast_extDict_5_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), uint32(0))
}

func ZSTD_compressBlock_fast_extDict_6_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), uint32(0))
}

func ZSTD_compressBlock_fast_extDict_7_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), uint32(0))
}

func ZSTD_compressBlock_fast_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_fast_extDict_4_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_fast_extDict_5_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_fast_extDict_6_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_fast_extDict_7_0(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

/**** ended inlining compress/zstd_fast.c ****/
/**** start inlining compress/zstd_lazy.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_lazy.h ****/
/**** skipping file: ../common/bits.h ****/

/*-*************************************
*  Binary Tree search
***************************************/

func ZSTD_updateDUBT(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, mls U32) {
	var base, bt, cParams, hashTable, nextCandidatePtr, sortMarkPtr uintptr
	var btLog, btMask, hashLog, idx, matchIndex, target U32
	var h size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _ = base, bt, btLog, btMask, cParams, h, hashLog, hashTable, idx, matchIndex, nextCandidatePtr, sortMarkPtr, target
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = uint32(int32(1)<<btLog - int32(1))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = uint32(int64(ip) - int64(base))
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	if idx != target {
	}
	/* condition for ZSTD_hashPtr */
	_ = iend
	/* condition for valid base+idx */
	for {
		if !(idx < target) {
			break
		}
		h = ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, mls) /* assumption : ip + 8 <= iend */
		matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
		nextCandidatePtr = bt + uintptr(uint32(2)*(idx&btMask))*4
		sortMarkPtr = nextCandidatePtr + uintptr(1)*4
		*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = idx /* Update Hash Table */
		*(*U32)(unsafe.Pointer(nextCandidatePtr)) = matchIndex  /* update BT like a chain */
		*(*U32)(unsafe.Pointer(sortMarkPtr)) = uint32(ZSTD_DUBT_UNSORTED_MARK)
		goto _1
	_1:
		;
		idx = idx + 1
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

// C documentation
//
//	/** ZSTD_insertDUBT1() :
//	 *  sort one already inserted but unsorted position
//	 *  assumption : curr >= btlow == (curr - btmask)
//	 *  doesn't fail */
func ZSTD_insertDUBT1(tls *libc.TLS, ms uintptr, curr U32, inputEnd uintptr, nbCompares U32, btLow U32, dictMode ZSTD_dictMode_e) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, iend, ip, largerPtr, mBase, match, nextPtr, prefixStart, smallerPtr, v1, v2 uintptr
	var btLog, btMask, dictLimit, matchIndex, maxDistance, windowLow, windowValid, v7 U32
	var commonLengthLarger, commonLengthSmaller, matchLength size_t
	var v3 uint32
	var v5 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bt, btLog, btMask, cParams, commonLengthLarger, commonLengthSmaller, dictBase, dictEnd, dictLimit, iend, ip, largerPtr, mBase, match, matchIndex, matchLength, maxDistance, nextPtr, prefixStart, smallerPtr, windowLow, windowValid, v1, v2, v3, v5, v7
	cParams = ms + 256
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = uint32(int32(1)<<btLog - int32(1))
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if curr >= dictLimit {
		v1 = base + uintptr(curr)
	} else {
		v1 = dictBase + uintptr(curr)
	}
	ip = v1
	if curr >= dictLimit {
		v2 = inputEnd
	} else {
		v2 = dictBase + uintptr(dictLimit)
	}
	iend = v2
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = smallerPtr + uintptr(1)*4
	matchIndex = *(*U32)(unsafe.Pointer(smallerPtr)) /* to be nullified at the end */
	windowValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	if curr-windowValid > maxDistance {
		v3 = curr - maxDistance
	} else {
		v3 = windowValid
	}
	windowLow = v3
	/* condition for ZSTD_count */
	for {
		if !(nbCompares != 0 && matchIndex > windowLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v5 = commonLengthSmaller
		} else {
			v5 = commonLengthLarger
		}
		matchLength = v5 /* guaranteed minimum nb of common bytes */
		/* note : all candidates are now supposed sorted,
		 * but it's still possible to have nextPtr[1] == ZSTD_DUBT_UNSORTED_MARK
		 * when a real index has the same value as ZSTD_DUBT_UNSORTED_MARK */
		if dictMode != int32(ZSTD_extDict) || uint64(matchIndex)+matchLength >= uint64(dictLimit) || curr < dictLimit {
			if dictMode != int32(ZSTD_extDict) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				v1 = base
			} else {
				v1 = dictBase
			}
			mBase = v1
			match = mBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend)
		} else {
			match = dictBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match = base + uintptr(matchIndex)
			} /* preparation for next read of match[matchLength] */
		}
		if ip+uintptr(matchLength) == iend { /* equal : no way to know if inf or sup */
			break /* drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt tree */
		}
		if int32(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) { /* necessarily within buffer */
			/* match is smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop searching */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new "candidate" => larger than match, which was smaller than target */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex, larger than previous and closer to current */
		} else {
			/* match is larger than current */
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop searching */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _4
	_4:
		;
		nbCompares = nbCompares - 1
	}
	v7 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v7
	*(*U32)(unsafe.Pointer(smallerPtr)) = v7
}

func ZSTD_DUBT_findBetterDictMatch(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, offsetPtr uintptr, bestLength size_t, nbCompares U32, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	var base, dictBase, dictBt, dictEnd, dictHashTable, dms, dmsCParams, match, nextPtr, prefixStart uintptr
	var btLog, btLow, btMask, curr, dictHighLimit, dictIndexDelta, dictLowLimit, dictMatchIndex, hashLog, mIndex, matchIndex U32
	var commonLengthLarger, commonLengthSmaller, h, matchLength size_t
	var v1 uint32
	var v3 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, btLog, btLow, btMask, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictBt, dictEnd, dictHashTable, dictHighLimit, dictIndexDelta, dictLowLimit, dictMatchIndex, dms, dmsCParams, h, hashLog, mIndex, match, matchIndex, matchLength, nextPtr, prefixStart, v1, v3
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	dmsCParams = dms + 256
	dictHashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FhashLog
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	dictMatchIndex = *(*U32)(unsafe.Pointer(dictHashTable + uintptr(h)*4))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	prefixStart = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit)
	curr = uint32(int64(ip) - int64(base))
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	dictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	dictHighLimit = uint32(int64((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase))
	dictLowLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FlowLimit
	dictIndexDelta = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit - dictHighLimit
	dictBt = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FchainLog - uint32(1)
	btMask = uint32(int32(1)<<btLog - int32(1))
	if btMask >= dictHighLimit-dictLowLimit {
		v1 = dictLowLimit
	} else {
		v1 = dictHighLimit - btMask
	}
	btLow = v1
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	_ = dictMode
	for {
		if !(nbCompares != 0 && dictMatchIndex > dictLowLimit) {
			break
		}
		nextPtr = dictBt + uintptr(uint32(2)*(dictMatchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v3 = commonLengthSmaller
		} else {
			v3 = commonLengthLarger
		}
		matchLength = v3 /* guaranteed minimum nb of common bytes */
		match = dictBase + uintptr(dictMatchIndex)
		matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
		if uint64(dictMatchIndex)+matchLength >= uint64(dictHighLimit) {
			match = base + uintptr(dictMatchIndex) + uintptr(dictIndexDelta)
		} /* to prepare for next usage of match[matchLength] */
		if matchLength > bestLength {
			matchIndex = dictMatchIndex + dictIndexDelta
			if int32(4)*int32(matchLength-bestLength) > int32(ZSTD_highbit32(tls, curr-matchIndex+uint32(1))-ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(offsetPtr)))+uint32(1))) {
				bestLength = matchLength
				*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM))
			}
			if ip+uintptr(matchLength) == iend { /* reached end of input : ip[matchLength] is not valid, no way to know if it's larger or smaller than match */
				break /* drop, to guarantee consistency (miss a little bit of compression) */
			}
		}
		if int32(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) {
			if dictMatchIndex <= btLow {
				break
			} /* beyond tree size, stop the search */
			commonLengthSmaller = matchLength                       /* all smaller will now have at least this guaranteed common length */
			dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex larger than previous (closer to current) */
		} else {
			/* match is larger than current */
			if dictMatchIndex <= btLow {
				break
			} /* beyond tree size, stop the search */
			commonLengthLarger = matchLength
			dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _2
	_2:
		;
		nbCompares = nbCompares - 1
	}
	if bestLength >= uint64(MINMATCH) {
		mIndex = curr - uint32(*(*size_t)(unsafe.Pointer(offsetPtr))-libc.Uint64FromInt32(ZSTD_REP_NUM))
		_ = mIndex
	}
	return bestLength
}

func ZSTD_DUBT_findBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, offBasePtr uintptr, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, hashTable, largerPtr, match, nextCandidate, nextCandidateIdxPtr, nextPtr, prefixStart, smallerPtr, unsortedMark uintptr
	var bestLength, commonLengthLarger, commonLengthSmaller, h, matchLength size_t
	var btLog, btLow, btMask, curr, dictLimit, hashLog, mIndex, matchEndIdx, matchIndex, nbCandidates, nbCompares, nextCandidateIdx, previousCandidate, unsortLimit, windowLow, v3 U32
	var v1, v2 uint32
	var v5 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bestLength, bt, btLog, btLow, btMask, cParams, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictEnd, dictLimit, h, hashLog, hashTable, largerPtr, mIndex, match, matchEndIdx, matchIndex, matchLength, nbCandidates, nbCompares, nextCandidate, nextCandidateIdx, nextCandidateIdxPtr, nextPtr, prefixStart, previousCandidate, smallerPtr, unsortLimit, unsortedMark, windowLow, v1, v2, v3, v5
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	curr = uint32(int64(ip) - int64(base))
	windowLow = ZSTD_getLowestMatchIndex(tls, ms, curr, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = uint32(int32(1)<<btLog - int32(1))
	if btMask >= curr {
		v1 = uint32(0)
	} else {
		v1 = curr - btMask
	}
	btLow = v1
	if btLow > windowLow {
		v2 = btLow
	} else {
		v2 = windowLow
	}
	unsortLimit = v2
	nextCandidate = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
	unsortedMark = bt + uintptr(uint32(2)*(matchIndex&btMask))*4 + uintptr(1)*4
	nbCompares = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	nbCandidates = nbCompares
	previousCandidate = uint32(0)
	/* required for h calculation */
	/* reach end of unsorted candidates list */
	for matchIndex > unsortLimit && *(*U32)(unsafe.Pointer(unsortedMark)) == uint32(ZSTD_DUBT_UNSORTED_MARK) && nbCandidates > uint32(1) {
		*(*U32)(unsafe.Pointer(unsortedMark)) = previousCandidate /* the unsortedMark becomes a reversed chain, to move up back to original position */
		previousCandidate = matchIndex
		matchIndex = *(*U32)(unsafe.Pointer(nextCandidate))
		nextCandidate = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		unsortedMark = bt + uintptr(uint32(2)*(matchIndex&btMask))*4 + uintptr(1)*4
		nbCandidates = nbCandidates - 1
	}
	/* nullify last candidate if it's still unsorted
	 * simplification, detrimental to compression ratio, beneficial for speed */
	if matchIndex > unsortLimit && *(*U32)(unsafe.Pointer(unsortedMark)) == uint32(ZSTD_DUBT_UNSORTED_MARK) {
		v3 = libc.Uint32FromInt32(0)
		*(*U32)(unsafe.Pointer(unsortedMark)) = v3
		*(*U32)(unsafe.Pointer(nextCandidate)) = v3
	}
	/* batch sort stacked candidates */
	matchIndex = previousCandidate
	for matchIndex != 0 { /* will end on matchIndex == 0 */
		nextCandidateIdxPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4 + uintptr(1)*4
		nextCandidateIdx = *(*U32)(unsafe.Pointer(nextCandidateIdxPtr))
		ZSTD_insertDUBT1(tls, ms, matchIndex, iend, nbCandidates, unsortLimit, dictMode)
		matchIndex = nextCandidateIdx
		nbCandidates = nbCandidates + 1
	}
	/* find longest match */
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4 + uintptr(1)*4
	matchEndIdx = curr + uint32(8) + uint32(1) /* to be nullified at the end */
	bestLength = uint64(0)
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = curr /* Update Hash Table */
	for {
		if !(nbCompares != 0 && matchIndex > windowLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v5 = commonLengthSmaller
		} else {
			v5 = commonLengthLarger
		}
		matchLength = v5
		if dictMode != int32(ZSTD_extDict) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
			match = base + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend)
		} else {
			match = dictBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match = base + uintptr(matchIndex)
			} /* to prepare for next usage of match[matchLength] */
		}
		if matchLength > bestLength {
			if matchLength > uint64(matchEndIdx-matchIndex) {
				matchEndIdx = matchIndex + uint32(matchLength)
			}
			if int32(4)*int32(matchLength-bestLength) > int32(ZSTD_highbit32(tls, curr-matchIndex+uint32(1))-ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(offBasePtr))))) {
				bestLength = matchLength
				*(*size_t)(unsafe.Pointer(offBasePtr)) = uint64(curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM))
			}
			if ip+uintptr(matchLength) == iend { /* equal : no way to know if inf or sup */
				if dictMode == int32(ZSTD_dictMatchState) {
					nbCompares = uint32(0) /* in addition to avoiding checking any
					 * further in this loop, make sure we
					 * skip checking in the dictionary. */
				}
				break /* drop, to guarantee consistency (miss a little bit of compression) */
			}
		}
		if int32(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) {
			/* match is smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop the search */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new "smaller" => larger of match */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex larger than previous (closer to current) */
		} else {
			/* match is larger than current */
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop the search */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _4
	_4:
		;
		nbCompares = nbCompares - 1
	}
	v3 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v3
	*(*U32)(unsafe.Pointer(smallerPtr)) = v3
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dictMatchState) && nbCompares != 0 {
		bestLength = ZSTD_DUBT_findBetterDictMatch(tls, ms, ip, iend, offBasePtr, bestLength, nbCompares, mls, dictMode)
	}
	/* ensure nextToUpdate is increased */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = matchEndIdx - uint32(8) /* skip repetitive patterns */
	if bestLength >= uint64(MINMATCH) {
		mIndex = curr - uint32(*(*size_t)(unsafe.Pointer(offBasePtr))-libc.Uint64FromInt32(ZSTD_REP_NUM))
		_ = mIndex
	}
	return bestLength
	return r
}

// C documentation
//
//	/** ZSTD_BtFindBestMatch() : Tree updater, providing best match */
func ZSTD_BtFindBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	if ip < (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase+uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate) {
		return uint64(0)
	} /* skipped area */
	ZSTD_updateDUBT(tls, ms, ip, iLimit, mls)
	return ZSTD_DUBT_findBestMatch(tls, ms, ip, iLimit, offBasePtr, mls, dictMode)
}

/***********************************
* Dedicated dict search
***********************************/

func ZSTD_dedicatedDictSearch_lazy_loadDictionary(tls *libc.TLS, ms uintptr, ip uintptr) {
	var base, chainTable, hashTable, tmpChainTable, tmpHashTable uintptr
	var bucketIdx, bucketSize, cacheSize, chainAttempts, chainLimit, chainPackedPointer, chainPos, chainSize, count, countBeyondMinChain, h, h1, hashIdx, hashLog, i, i1, i2, idx, minChain, target, tmpChainSize, tmpMinChain, v8 U32
	var v1, v2, v3 uint32
	var v9 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bucketIdx, bucketSize, cacheSize, chainAttempts, chainLimit, chainPackedPointer, chainPos, chainSize, chainTable, count, countBeyondMinChain, h, h1, hashIdx, hashLog, hashTable, i, i1, i2, idx, minChain, target, tmpChainSize, tmpChainTable, tmpHashTable, tmpMinChain, v1, v2, v3, v8, v9
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = uint32(int64(ip) - int64(base))
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	chainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	chainSize = uint32(int32(1) << (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FchainLog)
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	if chainSize < target-idx {
		v1 = target - chainSize
	} else {
		v1 = idx
	}
	minChain = v1
	bucketSize = uint32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_LAZY_DDSS_BUCKET_LOG))
	cacheSize = bucketSize - uint32(1)
	chainAttempts = uint32(libc.Int32FromInt32(1)<<(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog) - cacheSize
	if chainAttempts > uint32(255) {
		v2 = uint32(255)
	} else {
		v2 = chainAttempts
	}
	chainLimit = v2
	/* We know the hashtable is oversized by a factor of `bucketSize`.
	 * We are going to temporarily pretend `bucketSize == 1`, keeping only a
	 * single entry. We will use the rest of the space to construct a temporary
	 * chaintable.
	 */
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FhashLog - uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	tmpHashTable = hashTable
	tmpChainTable = hashTable + uintptr(libc.Uint64FromInt32(1)<<hashLog)*4
	tmpChainSize = uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_LAZY_DDSS_BUCKET_LOG)-libc.Int32FromInt32(1)) << hashLog
	if tmpChainSize < target {
		v3 = target - tmpChainSize
	} else {
		v3 = idx
	}
	tmpMinChain = v3
	/* fill conventional hash table and conventional chain table */
	for {
		if !(idx < target) {
			break
		}
		h = uint32(ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch))
		if idx >= tmpMinChain {
			*(*U32)(unsafe.Pointer(tmpChainTable + uintptr(idx-tmpMinChain)*4)) = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
		}
		*(*U32)(unsafe.Pointer(tmpHashTable + uintptr(h)*4)) = idx
		goto _4
	_4:
		;
		idx = idx + 1
	}
	/* sort chains into ddss chain table */
	chainPos = uint32(0)
	hashIdx = uint32(0)
	for {
		if !(hashIdx < uint32(1)<<hashLog) {
			break
		}
		countBeyondMinChain = uint32(0)
		i = *(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4))
		count = uint32(0)
		for {
			if !(i >= tmpMinChain && count < cacheSize) {
				break
			}
			/* skip through the chain to the first position that won't be
			 * in the hash cache bucket */
			if i < minChain {
				countBeyondMinChain = countBeyondMinChain + 1
			}
			i = *(*U32)(unsafe.Pointer(tmpChainTable + uintptr(i-tmpMinChain)*4))
			goto _6
		_6:
			;
			count = count + 1
		}
		if count == cacheSize {
			count = uint32(0)
			for {
				if !(count < chainLimit) {
					break
				}
				if i < minChain {
					if v9 = !(i != 0); !v9 {
						countBeyondMinChain = countBeyondMinChain + 1
						v8 = countBeyondMinChain
					}
					if v9 || v8 > cacheSize {
						/* only allow pulling `cacheSize` number of entries
						 * into the cache or chainTable beyond `minChain`,
						 * to replace the entries pulled out of the
						 * chainTable into the cache. This lets us reach
						 * back further without increasing the total number
						 * of entries in the chainTable, guaranteeing the
						 * DDSS chain table will fit into the space
						 * allocated for the regular one. */
						break
					}
				}
				v8 = chainPos
				chainPos = chainPos + 1
				*(*U32)(unsafe.Pointer(chainTable + uintptr(v8)*4)) = i
				count = count + 1
				if i < tmpMinChain {
					break
				}
				i = *(*U32)(unsafe.Pointer(tmpChainTable + uintptr(i-tmpMinChain)*4))
				goto _7
			_7:
			}
		} else {
			count = uint32(0)
		}
		if count != 0 {
			*(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4)) = (chainPos-count)<<int32(8) + count
		} else {
			*(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4)) = uint32(0)
		}
		goto _5
	_5:
		;
		hashIdx = hashIdx + 1
	}
	/* I believe this is guaranteed... */
	/* move chain pointers into the last entry of each hash bucket */
	hashIdx = uint32(libc.Int32FromInt32(1) << hashLog)
	for {
		if !(hashIdx != 0) {
			break
		}
		hashIdx = hashIdx - 1
		v8 = hashIdx
		bucketIdx = v8 << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		chainPackedPointer = *(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4))
		i1 = uint32(0)
		for {
			if !(i1 < cacheSize) {
				break
			}
			*(*U32)(unsafe.Pointer(hashTable + uintptr(bucketIdx+i1)*4)) = uint32(0)
			goto _13
		_13:
			;
			i1 = i1 + 1
		}
		*(*U32)(unsafe.Pointer(hashTable + uintptr(bucketIdx+bucketSize-uint32(1))*4)) = chainPackedPointer
		goto _11
	_11:
	}
	/* fill the buckets of the hash table */
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	for {
		if !(idx < target) {
			break
		}
		h1 = uint32(ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch)) << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		/* Shift hash cache down 1. */
		i2 = cacheSize - uint32(1)
		for {
			if !(i2 != 0) {
				break
			}
			*(*U32)(unsafe.Pointer(hashTable + uintptr(h1+i2)*4)) = *(*U32)(unsafe.Pointer(hashTable + uintptr(h1+i2-uint32(1))*4))
			goto _15
		_15:
			;
			i2 = i2 - 1
		}
		*(*U32)(unsafe.Pointer(hashTable + uintptr(h1)*4)) = idx
		goto _14
	_14:
		;
		idx = idx + 1
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

// C documentation
//
//	/* Returns the longest match length found in the dedicated dict search structure.
//	 * If none are longer than the argument ml, then ml will be returned.
//	 */
func ZSTD_dedicatedDictSearch_lazy_search(tls *libc.TLS, offsetPtr uintptr, ml size_t, nbAttempts U32, dms uintptr, ip uintptr, iLimit uintptr, prefixStart uintptr, curr U32, dictLimit U32, ddsIdx size_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var bucketLimit, bucketSize, chainAttempt, chainAttempts, chainIndex, chainIndex1, chainLength, chainLimit, chainPackedPointer, chainPackedPointer1, ddsAttempt, ddsIndexDelta, ddsLowestIndex, ddsSize, matchIndex U32
	var currentMl, currentMl1 size_t
	var ddsBase, ddsEnd, match, match1 uintptr
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bucketLimit, bucketSize, chainAttempt, chainAttempts, chainIndex, chainIndex1, chainLength, chainLimit, chainPackedPointer, chainPackedPointer1, currentMl, currentMl1, ddsAttempt, ddsBase, ddsEnd, ddsIndexDelta, ddsLowestIndex, ddsSize, match, match1, matchIndex, v1
	ddsLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	ddsBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	ddsEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	ddsSize = uint32(int64(ddsEnd) - int64(ddsBase))
	ddsIndexDelta = dictLimit - ddsSize
	bucketSize = uint32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_LAZY_DDSS_BUCKET_LOG))
	if nbAttempts < bucketSize-uint32(1) {
		v1 = nbAttempts
	} else {
		v1 = bucketSize - uint32(1)
	}
	bucketLimit = v1
	ddsAttempt = uint32(0)
	for {
		if !(ddsAttempt < bucketSize-uint32(1)) {
			break
		}
		libc.X__builtin_prefetch(tls, ddsBase+uintptr(*(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(ddsAttempt))*4))), libc.VaList(bp+8, 0, int32(3)))
		goto _2
	_2:
		;
		ddsAttempt = ddsAttempt + 1
	}
	chainPackedPointer = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(bucketSize)-uint64(1))*4))
	chainIndex = chainPackedPointer >> int32(8)
	libc.X__builtin_prefetch(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable+uintptr(chainIndex)*4, libc.VaList(bp+8, 0, int32(3)))
	ddsAttempt = uint32(0)
	for {
		if !(ddsAttempt < bucketLimit) {
			break
		}
		currentMl = uint64(0)
		matchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(ddsAttempt))*4))
		match = ddsBase + uintptr(matchIndex)
		if !(matchIndex != 0) {
			return ml
		}
		/* guaranteed by table construction */
		_ = ddsLowestIndex
		if MEM_read32(tls, match) == MEM_read32(tls, ip) {
			/* assumption : matchIndex <= dictLimit-4 (by table construction) */
			currentMl = ZSTD_count_2segments(tls, ip+uintptr(4), match+uintptr(4), iLimit, ddsEnd, prefixStart) + uint64(4)
		}
		/* save best solution */
		if currentMl > ml {
			ml = currentMl
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex + ddsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl) == iLimit {
				/* best possible, avoids read overflow on next attempt */
				return ml
			}
		}
		goto _3
	_3:
		;
		ddsAttempt = ddsAttempt + 1
	}
	chainPackedPointer1 = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(bucketSize)-uint64(1))*4))
	chainIndex1 = chainPackedPointer1 >> int32(8)
	chainLength = chainPackedPointer1 & uint32(0xFF)
	chainAttempts = nbAttempts - ddsAttempt
	if chainAttempts > chainLength {
		v1 = chainLength
	} else {
		v1 = chainAttempts
	}
	chainLimit = v1
	chainAttempt = uint32(0)
	for {
		if !(chainAttempt < chainLimit) {
			break
		}
		libc.X__builtin_prefetch(tls, ddsBase+uintptr(*(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable + uintptr(chainIndex1+chainAttempt)*4))), libc.VaList(bp+8, 0, int32(3)))
		goto _5
	_5:
		;
		chainAttempt = chainAttempt + 1
	}
	chainAttempt = uint32(0)
	for {
		if !(chainAttempt < chainLimit) {
			break
		}
		currentMl1 = uint64(0)
		matchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable + uintptr(chainIndex1)*4))
		match1 = ddsBase + uintptr(matchIndex)
		/* guaranteed by table construction */
		if MEM_read32(tls, match1) == MEM_read32(tls, ip) {
			/* assumption : matchIndex <= dictLimit-4 (by table construction) */
			currentMl1 = ZSTD_count_2segments(tls, ip+uintptr(4), match1+uintptr(4), iLimit, ddsEnd, prefixStart) + uint64(4)
		}
		/* save best solution */
		if currentMl1 > ml {
			ml = currentMl1
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex + ddsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl1) == iLimit {
				break
			} /* best possible, avoids read overflow on next attempt */
		}
		goto _6
	_6:
		;
		chainAttempt = chainAttempt + 1
		chainIndex1 = chainIndex1 + 1
	}
	return ml
}

/* *********************************
*  Hash Chain
***********************************/

// C documentation
//
//	/* Update chains up to ip (excluded)
//	   Assumption : always within prefix (i.e. not within extDict) */
func ZSTD_insertAndFindFirstIndex_internal(tls *libc.TLS, ms uintptr, cParams uintptr, ip uintptr, mls U32, lazySkipping U32) (r U32) {
	var base, chainTable, hashTable uintptr
	var chainMask, hashLog, idx, target U32
	var h size_t
	_, _, _, _, _, _, _, _ = base, chainMask, chainTable, h, hashLog, hashTable, idx, target
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	chainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	chainMask = uint32(int32(1)<<(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - int32(1))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = uint32(int64(ip) - int64(base))
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	for idx < target { /* catch up */
		h = ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, mls)
		*(*U32)(unsafe.Pointer(chainTable + uintptr(idx&chainMask)*4)) = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = idx
		idx = idx + 1
		/* Stop inserting every position when in the lazy skipping mode. */
		if lazySkipping != 0 {
			break
		}
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
	return *(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip, hashLog, mls))*4))
}

func ZSTD_insertAndFindFirstIndex(tls *libc.TLS, ms uintptr, ip uintptr) (r U32) {
	var cParams uintptr
	_ = cParams
	cParams = ms + 256
	return ZSTD_insertAndFindFirstIndex_internal(tls, ms, cParams, ip, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch, uint32(0))
}

// C documentation
//
//	/* inlining is important to hardwire a hot branch (template emulation) */
func ZSTD_HcFindBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var base, cParams, chainTable, dictBase, dictEnd, dms, dmsBase, dmsChainTable, dmsEnd, entry, match, match1, match2, prefixStart uintptr
	var chainMask, chainSize, curr, ddsHashLog, dictLimit, dmsChainMask, dmsChainSize, dmsIndexDelta, dmsLowestIndex, dmsMinChain, dmsSize, isDictionary, lowLimit, lowestValid, matchIndex, maxDistance, minChain, nbAttempts, withinMaxDistance U32
	var currentMl, currentMl1, ddsIdx, ml size_t
	var v1, v2, v3, v4 uint32
	var v5 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, chainMask, chainSize, chainTable, curr, currentMl, currentMl1, ddsHashLog, ddsIdx, dictBase, dictEnd, dictLimit, dms, dmsBase, dmsChainMask, dmsChainSize, dmsChainTable, dmsEnd, dmsIndexDelta, dmsLowestIndex, dmsMinChain, dmsSize, entry, isDictionary, lowLimit, lowestValid, match, match1, match2, matchIndex, maxDistance, minChain, ml, nbAttempts, prefixStart, withinMaxDistance, v1, v2, v3, v4, v5
	cParams = ms + 256
	chainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	chainSize = uint32(libc.Int32FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog)
	chainMask = chainSize - uint32(1)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(dictLimit)
	dictEnd = dictBase + uintptr(dictLimit)
	curr = uint32(int64(ip) - int64(base))
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinMaxDistance = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinMaxDistance
	}
	lowLimit = v2
	if curr > chainSize {
		v3 = curr - chainSize
	} else {
		v3 = uint32(0)
	}
	minChain = v3
	nbAttempts = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	ml = uint64(libc.Int32FromInt32(4) - libc.Int32FromInt32(1))
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FhashLog - uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	} else {
		v4 = uint32(0)
	}
	ddsHashLog = v4
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		v5 = ZSTD_hashPtr(tls, ip, ddsHashLog, mls) << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	} else {
		v5 = uint64(0)
	}
	ddsIdx = v5
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		entry = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx)*4
		libc.X__builtin_prefetch(tls, entry, libc.VaList(bp+8, 0, int32(3)))
	}
	/* HC4 match finder */
	matchIndex = ZSTD_insertAndFindFirstIndex_internal(tls, ms, cParams, ip, mls, uint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping))
	for {
		if !(libc.BoolInt32(matchIndex >= lowLimit)&libc.BoolInt32(nbAttempts > uint32(0)) != 0) {
			break
		}
		currentMl = uint64(0)
		if dictMode != int32(ZSTD_extDict) || matchIndex >= dictLimit {
			match = base + uintptr(matchIndex)
			/* ensures this is true if dictMode != ZSTD_extDict */
			/* read 4B starting from (match + ml + 1 - sizeof(U32)) */
			if MEM_read32(tls, match+uintptr(ml)-uintptr(3)) == MEM_read32(tls, ip+uintptr(ml)-uintptr(3)) { /* potentially better */
				currentMl = ZSTD_count(tls, ip, match, iLimit)
			}
		} else {
			match1 = dictBase + uintptr(matchIndex)
			if MEM_read32(tls, match1) == MEM_read32(tls, ip) { /* assumption : matchIndex <= dictLimit-4 (by table construction) */
				currentMl = ZSTD_count_2segments(tls, ip+uintptr(4), match1+uintptr(4), iLimit, dictEnd, prefixStart) + uint64(4)
			}
		}
		/* save best solution */
		if currentMl > ml {
			ml = currentMl
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl) == iLimit {
				break
			} /* best possible, avoids read overflow on next attempt */
		}
		if matchIndex <= minChain {
			break
		}
		matchIndex = *(*U32)(unsafe.Pointer(chainTable + uintptr(matchIndex&chainMask)*4))
		goto _6
	_6:
		;
		nbAttempts = nbAttempts - 1
	}
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		ml = ZSTD_dedicatedDictSearch_lazy_search(tls, offsetPtr, ml, nbAttempts, dms, ip, iLimit, prefixStart, curr, dictLimit, ddsIdx)
	} else {
		if dictMode == int32(ZSTD_dictMatchState) {
			dmsChainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
			dmsChainSize = uint32(libc.Int32FromInt32(1) << (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FchainLog)
			dmsChainMask = dmsChainSize - uint32(1)
			dmsLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
			dmsBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
			dmsEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
			dmsSize = uint32(int64(dmsEnd) - int64(dmsBase))
			dmsIndexDelta = dictLimit - dmsSize
			if dmsSize > dmsChainSize {
				v1 = dmsSize - dmsChainSize
			} else {
				v1 = uint32(0)
			}
			dmsMinChain = v1
			matchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ZSTD_hashPtr(tls, ip, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FhashLog, mls))*4))
			for {
				if !(libc.BoolInt32(matchIndex >= dmsLowestIndex)&libc.BoolInt32(nbAttempts > uint32(0)) != 0) {
					break
				}
				currentMl1 = uint64(0)
				match2 = dmsBase + uintptr(matchIndex)
				if MEM_read32(tls, match2) == MEM_read32(tls, ip) { /* assumption : matchIndex <= dictLimit-4 (by table construction) */
					currentMl1 = ZSTD_count_2segments(tls, ip+uintptr(4), match2+uintptr(4), iLimit, dmsEnd, prefixStart) + uint64(4)
				}
				/* save best solution */
				if currentMl1 > ml {
					ml = currentMl1
					*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex + dmsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
					if ip+uintptr(currentMl1) == iLimit {
						break
					} /* best possible, avoids read overflow on next attempt */
				}
				if matchIndex <= dmsMinChain {
					break
				}
				matchIndex = *(*U32)(unsafe.Pointer(dmsChainTable + uintptr(matchIndex&dmsChainMask)*4))
				goto _8
			_8:
				;
				nbAttempts = nbAttempts - 1
			}
		}
	}
	return ml
}

/* *********************************
* (SIMD) Row-based matchfinder
***********************************/
/* Constants for row-based hash */

type ZSTD_VecMask = uint64 /* Clarifies when we are interacting with a U64 representing a mask of matches */

// C documentation
//
//	/* ZSTD_VecMask_next():
//	 * Starting from the LSB, returns the idx of the next non-zero bit.
//	 * Basically counting the nb of trailing zeroes.
//	 */
func ZSTD_VecMask_next(tls *libc.TLS, val ZSTD_VecMask) (r U32) {
	return ZSTD_countTrailingZeros64(tls, val)
}

// C documentation
//
//	/* ZSTD_row_nextIndex():
//	 * Returns the next index to insert at within a tagTable row, and updates the "head"
//	 * value to reflect the update. Essentially cycles backwards from [1, {entries per row})
//	 */
func ZSTD_row_nextIndex(tls *libc.TLS, tagRow uintptr, rowMask U32) (r U32) {
	var next U32
	var v1 uint32
	_, _ = next, v1
	next = uint32(int32(*(*BYTE)(unsafe.Pointer(tagRow)))-libc.Int32FromInt32(1)) & rowMask
	if next == uint32(0) {
		v1 = rowMask
	} else {
		v1 = uint32(0)
	}
	next = next + v1 /* skip first position */
	*(*BYTE)(unsafe.Pointer(tagRow)) = uint8(next)
	return next
}

// C documentation
//
//	/* ZSTD_isAligned():
//	 * Checks that a pointer is aligned to "align" bytes which must be a power of 2.
//	 */
func ZSTD_isAligned(tls *libc.TLS, ptr uintptr, align size_t) (r int32) {
	return libc.BoolInt32(uint64(ptr)&(align-uint64(1)) == uint64(0))
}

// C documentation
//
//	/* ZSTD_row_prefetch():
//	 * Performs prefetching for the hashTable and tagTable at a given row.
//	 */
func ZSTD_row_prefetch(tls *libc.TLS, hashTable uintptr, tagTable uintptr, relRow U32, rowLog U32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	libc.X__builtin_prefetch(tls, hashTable+uintptr(relRow)*4, libc.VaList(bp+8, 0, int32(3)))
	if rowLog >= uint32(5) {
		libc.X__builtin_prefetch(tls, hashTable+uintptr(relRow)*4+libc.UintptrFromInt32(16)*4, libc.VaList(bp+8, 0, int32(3)))
		/* Note: prefetching more of the hash table does not appear to be beneficial for 128-entry rows */
	}
	libc.X__builtin_prefetch(tls, tagTable+uintptr(relRow), libc.VaList(bp+8, 0, int32(3)))
	if rowLog == uint32(6) {
		libc.X__builtin_prefetch(tls, tagTable+uintptr(relRow)+libc.UintptrFromInt32(32), libc.VaList(bp+8, 0, int32(3)))
	}
	/* prefetched hash row always 64-byte aligned */
	/* prefetched tagRow sits on correct multiple of bytes (32,64,128) */
}

// C documentation
//
//	/* ZSTD_row_fillHashCache():
//	 * Fill up the hash cache starting at idx, prefetching up to ZSTD_ROW_HASH_CACHE_SIZE entries,
//	 * but not beyond iLimit.
//	 */
func ZSTD_row_fillHashCache(tls *libc.TLS, ms uintptr, base uintptr, rowLog U32, mls U32, idx U32, iLimit uintptr) {
	var hash, hashLog, lim, maxElemsToPrefetch, row U32
	var hashTable, tagTable uintptr
	var v1, v2 uint32
	_, _, _, _, _, _, _, _, _ = hash, hashLog, hashTable, lim, maxElemsToPrefetch, row, tagTable, v1, v2
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	tagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog
	if base+uintptr(idx) > iLimit {
		v1 = uint32(0)
	} else {
		v1 = uint32(int64(iLimit) - int64(base+uintptr(idx)) + libc.Int64FromInt32(1))
	}
	maxElemsToPrefetch = v1
	if uint32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)) < maxElemsToPrefetch {
		v2 = uint32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE))
	} else {
		v2 = maxElemsToPrefetch
	}
	lim = idx + v2
	for {
		if !(idx < lim) {
			break
		}
		hash = uint32(ZSTD_hashPtrSalted(tls, base+uintptr(idx), hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt))
		row = hash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
		ZSTD_row_prefetch(tls, hashTable, tagTable, row, rowLog)
		*(*U32)(unsafe.Pointer(ms + 64 + uintptr(idx&uint32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)-libc.Int32FromInt32(1)))*4)) = hash
		goto _3
	_3:
		;
		idx = idx + 1
	}
}

// C documentation
//
//	/* ZSTD_row_nextCachedHash():
//	 * Returns the hash of base + idx, and replaces the hash in the hash cache with the byte at
//	 * base + idx + ZSTD_ROW_HASH_CACHE_SIZE. Also prefetches the appropriate rows from hashTable and tagTable.
//	 */
func ZSTD_row_nextCachedHash(tls *libc.TLS, cache uintptr, hashTable uintptr, tagTable uintptr, base uintptr, idx U32, hashLog U32, rowLog U32, mls U32, hashSalt U64) (r U32) {
	var hash, newHash, row U32
	_, _, _ = hash, newHash, row
	newHash = uint32(ZSTD_hashPtrSalted(tls, base+uintptr(idx)+uintptr(ZSTD_ROW_HASH_CACHE_SIZE), hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, hashSalt))
	row = newHash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
	ZSTD_row_prefetch(tls, hashTable, tagTable, row, rowLog)
	hash = *(*U32)(unsafe.Pointer(cache + uintptr(idx&uint32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)-libc.Int32FromInt32(1)))*4))
	*(*U32)(unsafe.Pointer(cache + uintptr(idx&uint32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)-libc.Int32FromInt32(1)))*4)) = newHash
	return hash
	return r
}

// C documentation
//
//	/* ZSTD_row_update_internalImpl():
//	 * Updates the hash table with positions starting from updateStartIdx until updateEndIdx.
//	 */
func ZSTD_row_update_internalImpl(tls *libc.TLS, ms uintptr, updateStartIdx U32, updateEndIdx U32, mls U32, rowLog U32, rowMask U32, useCache U32) {
	var base, hashTable, row, tagRow, tagTable uintptr
	var hash, hashLog, pos, relRow U32
	var v2 uint32
	_, _, _, _, _, _, _, _, _, _ = base, hash, hashLog, hashTable, pos, relRow, row, tagRow, tagTable, v2
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	tagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	for {
		if !(updateStartIdx < updateEndIdx) {
			break
		}
		if useCache != 0 {
			v2 = ZSTD_row_nextCachedHash(tls, ms+64, hashTable, tagTable, base, updateStartIdx, hashLog, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt)
		} else {
			v2 = uint32(ZSTD_hashPtrSalted(tls, base+uintptr(updateStartIdx), hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt))
		}
		hash = v2
		relRow = hash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
		row = hashTable + uintptr(relRow)*4
		tagRow = tagTable + uintptr(relRow)
		pos = ZSTD_row_nextIndex(tls, tagRow, rowMask)
		*(*BYTE)(unsafe.Pointer(tagRow + uintptr(pos))) = uint8(hash & (libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) - libc.Uint32FromInt32(1)))
		*(*U32)(unsafe.Pointer(row + uintptr(pos)*4)) = updateStartIdx
		goto _1
	_1:
		;
		updateStartIdx = updateStartIdx + 1
	}
}

// C documentation
//
//	/* ZSTD_row_update_internal():
//	 * Inserts the byte at ip into the appropriate position in the hash table, and updates ms->nextToUpdate.
//	 * Skips sections of long matches as is necessary.
//	 */
func ZSTD_row_update_internal(tls *libc.TLS, ms uintptr, ip uintptr, mls U32, rowLog U32, rowMask U32, useCache U32) {
	var base uintptr
	var bound, idx, kMaxMatchEndPositionsToUpdate, kMaxMatchStartPositionsToUpdate, kSkipThreshold, target U32
	_, _, _, _, _, _, _ = base, bound, idx, kMaxMatchEndPositionsToUpdate, kMaxMatchStartPositionsToUpdate, kSkipThreshold, target
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = uint32(int64(ip) - int64(base))
	kSkipThreshold = uint32(384)
	kMaxMatchStartPositionsToUpdate = uint32(96)
	kMaxMatchEndPositionsToUpdate = uint32(32)
	if useCache != 0 {
		/* Only skip positions when using hash cache, i.e.
		 * if we are loading a dict, don't skip anything.
		 * If we decide to skip, then we only update a set number
		 * of positions at the beginning and end of the match.
		 */
		if libc.BoolInt32(target-idx > kSkipThreshold) != 0 {
			bound = idx + kMaxMatchStartPositionsToUpdate
			ZSTD_row_update_internalImpl(tls, ms, idx, bound, mls, rowLog, rowMask, useCache)
			idx = target - kMaxMatchEndPositionsToUpdate
			ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, idx, ip+uintptr(1))
		}
	}
	ZSTD_row_update_internalImpl(tls, ms, idx, target, mls, rowLog, rowMask, useCache)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

// C documentation
//
//	/* ZSTD_row_update():
//	 * External wrapper for ZSTD_row_update_internal(). Used for filling the hashtable during dictionary
//	 * processing.
//	 */
func ZSTD_row_update(tls *libc.TLS, ms uintptr, ip uintptr) {
	var mls, rowLog, rowMask U32
	var v1, v2, v3, v4 uint32
	_, _, _, _, _, _, _ = mls, rowLog, rowMask, v1, v2, v3, v4
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < uint32(libc.Int32FromInt32(6)) {
		v2 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
	} else {
		v2 = uint32(libc.Int32FromInt32(6))
	}
	if uint32(libc.Int32FromInt32(4)) > v2 {
		v1 = uint32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < uint32(libc.Int32FromInt32(6)) {
			v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
		} else {
			v3 = uint32(libc.Int32FromInt32(6))
		}
		v1 = v3
	}
	rowLog = v1
	rowMask = uint32(1)<<rowLog - uint32(1)
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < uint32(libc.Int32FromInt32(6)) {
		v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v4 = uint32(libc.Int32FromInt32(6))
	}
	mls = v4
	ZSTD_row_update_internal(tls, ms, ip, mls, rowLog, rowMask, uint32(0))
}

// C documentation
//
//	/* Returns the mask width of bits group of which will be set to 1. Given not all
//	 * architectures have easy movemask instruction, this helps to iterate over
//	 * groups of bits easier and faster.
//	 */
func ZSTD_row_matchMaskGroupWidth(tls *libc.TLS, rowEntries U32) (r U32) {
	_ = rowEntries
	return uint32(1)
}

// C documentation
//
//	/* Returns a ZSTD_VecMask (U64) that has the nth group (determined by
//	 * ZSTD_row_matchMaskGroupWidth) of bits set to 1 if the newly-computed "tag"
//	 * matches the hash at the nth position in a row of the tagTable.
//	 * Each row is a circular buffer beginning at the value of "headGrouped". So we
//	 * must rotate the "matches" bitfield to match up with the actual layout of the
//	 * entries within the hashTable */
func ZSTD_row_getMatchMask(tls *libc.TLS, tagRow uintptr, tag BYTE, headGrouped U32, rowEntries U32) (r ZSTD_VecMask) {
	var chunk, chunk1, extractMagic, extractMagic1, msb, shiftAmount, splatChar, x01, x80, xFF size_t
	var chunkSize, i int32
	var matches ZSTD_VecMask
	var src uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = chunk, chunk1, chunkSize, extractMagic, extractMagic1, i, matches, msb, shiftAmount, splatChar, src, x01, x80, xFF
	src = tagRow
	/* SWAR */
	chunkSize = int32(8)
	shiftAmount = uint64(chunkSize*libc.Int32FromInt32(8) - chunkSize)
	xFF = ^libc.Uint64FromInt32(0)
	x01 = xFF / uint64(0xFF)
	x80 = x01 << int32(7)
	splatChar = uint64(tag) * x01
	matches = uint64(0)
	i = int32(rowEntries - uint32(chunkSize))
	if MEM_isLittleEndian(tls) != 0 { /* runtime check so have two loops */
		extractMagic = xFF / uint64(0x7F) >> chunkSize
		for cond := true; cond; cond = i >= 0 {
			chunk = MEM_readST(tls, src+uintptr(i))
			chunk = chunk ^ splatChar
			chunk = (chunk | x80 - x01 | chunk) & x80
			matches = matches << uint64(chunkSize)
			matches = matches | chunk*extractMagic>>shiftAmount
			i = i - chunkSize
		}
	} else { /* big endian: reverse bits during extraction */
		msb = xFF ^ xFF>>libc.Int32FromInt32(1)
		extractMagic1 = msb/uint64(0x1FF) | msb
		for cond := true; cond; cond = i >= 0 {
			chunk1 = MEM_readST(tls, src+uintptr(i))
			chunk1 = chunk1 ^ splatChar
			chunk1 = (chunk1 | x80 - x01 | chunk1) & x80
			matches = matches << uint64(chunkSize)
			matches = matches | chunk1>>libc.Int32FromInt32(7)*extractMagic1>>shiftAmount
			i = i - chunkSize
		}
	}
	matches = ^matches
	if rowEntries == uint32(16) {
		return uint64(ZSTD_rotateRight_U16(tls, uint16(matches), headGrouped))
	} else {
		if rowEntries == uint32(32) {
			return uint64(ZSTD_rotateRight_U32(tls, uint32(matches), headGrouped))
		} else {
			return ZSTD_rotateRight_U64(tls, matches, headGrouped)
		}
	}
	return r
}

// C documentation
//
//	/* The high-level approach of the SIMD row based match finder is as follows:
//	 * - Figure out where to insert the new entry:
//	 *      - Generate a hash for current input position and split it into a one byte of tag and `rowHashLog` bits of index.
//	 *           - The hash is salted by a value that changes on every context reset, so when the same table is used
//	 *             we will avoid collisions that would otherwise slow us down by introducing phantom matches.
//	 *      - The hashTable is effectively split into groups or "rows" of 15 or 31 entries of U32, and the index determines
//	 *        which row to insert into.
//	 *      - Determine the correct position within the row to insert the entry into. Each row of 15 or 31 can
//	 *        be considered as a circular buffer with a "head" index that resides in the tagTable (overall 16 or 32 bytes
//	 *        per row).
//	 * - Use SIMD to efficiently compare the tags in the tagTable to the 1-byte tag calculated for the position and
//	 *   generate a bitfield that we can cycle through to check the collisions in the hash table.
//	 * - Pick the longest match.
//	 * - Insert the tag into the equivalent row and position in the tagTable.
//	 */
func ZSTD_RowFindBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr, mls U32, dictMode ZSTD_dictMode_e, rowLog U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var base, cParams, dictBase, dictEnd, dms, dmsBase, dmsEnd, dmsHashTable, dmsRow, dmsTagRow, dmsTagTable, hashCache, hashTable, match, match1, match2, prefixStart, row, tagRow, tagTable, v8 uintptr
	var cappedSearchLog, curr, ddsExtraAttempts, ddsHashLog, dictLimit, dmsHash, dmsIndexDelta, dmsLowestIndex, dmsRelRow, dmsSize, dmsTag, groupWidth, hash, hashLog, headGrouped, headGrouped1, isDictionary, lowLimit, lowestValid, matchIndex, matchIndex1, matchIndex2, matchIndex3, matchPos, matchPos1, maxDistance, nbAttempts, pos, relRow, rowEntries, rowMask, tag, withinMaxDistance, v7 U32
	var currMatch, currMatch1, currentMl, currentMl1, ddsIdx, ml, numMatches, numMatches1, v6 size_t
	var hashSalt U64
	var matchBuffer, matchBuffer1 [64]U32
	var matches, matches1 ZSTD_VecMask
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, cappedSearchLog, curr, currMatch, currMatch1, currentMl, currentMl1, ddsExtraAttempts, ddsHashLog, ddsIdx, dictBase, dictEnd, dictLimit, dms, dmsBase, dmsEnd, dmsHash, dmsHashTable, dmsIndexDelta, dmsLowestIndex, dmsRelRow, dmsRow, dmsSize, dmsTag, dmsTagRow, dmsTagTable, groupWidth, hash, hashCache, hashLog, hashSalt, hashTable, headGrouped, headGrouped1, isDictionary, lowLimit, lowestValid, match, match1, match2, matchBuffer, matchBuffer1, matchIndex, matchIndex1, matchIndex2, matchIndex3, matchPos, matchPos1, matches, matches1, maxDistance, ml, nbAttempts, numMatches, numMatches1, pos, prefixStart, relRow, row, rowEntries, rowMask, tag, tagRow, tagTable, withinMaxDistance, v1, v2, v3, v6, v7, v8
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	tagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable
	hashCache = ms + 64
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog
	cParams = ms + 256
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(dictLimit)
	dictEnd = dictBase + uintptr(dictLimit)
	curr = uint32(int64(ip) - int64(base))
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinMaxDistance = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinMaxDistance
	}
	lowLimit = v2
	rowEntries = libc.Uint32FromUint32(1) << rowLog
	rowMask = rowEntries - uint32(1)
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog < rowLog {
		v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	} else {
		v3 = rowLog
	}
	cappedSearchLog = v3 /* nb of searches is capped at nb entries per row */
	groupWidth = ZSTD_row_matchMaskGroupWidth(tls, rowEntries)
	hashSalt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt
	nbAttempts = uint32(1) << cappedSearchLog
	ml = uint64(libc.Int32FromInt32(4) - libc.Int32FromInt32(1))
	/* DMS/DDS variables that may be referenced laster */
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	/* Initialize the following variables to satisfy static analyzer */
	ddsIdx = uint64(0)
	ddsExtraAttempts = uint32(0) /* cctx hash tables are limited in searches, but allow extra searches into DDS */
	dmsTag = uint32(0)
	dmsRow = libc.UintptrFromInt32(0)
	dmsTagRow = libc.UintptrFromInt32(0)
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		ddsHashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FhashLog - uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		/* Prefetch DDS hashtable entry */
		ddsIdx = ZSTD_hashPtr(tls, ip, ddsHashLog, mls) << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		libc.X__builtin_prefetch(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable+uintptr(ddsIdx)*4, libc.VaList(bp+8, 0, int32(3)))
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog > rowLog {
			v1 = uint32(1) << ((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog - rowLog)
		} else {
			v1 = uint32(0)
		}
		ddsExtraAttempts = v1
	}
	if dictMode == int32(ZSTD_dictMatchState) {
		/* Prefetch DMS rows */
		dmsHashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
		dmsTagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FtagTable
		dmsHash = uint32(ZSTD_hashPtr(tls, ip, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FrowHashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls))
		dmsRelRow = dmsHash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
		dmsTag = dmsHash & (libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) - libc.Uint32FromInt32(1))
		dmsTagRow = dmsTagTable + uintptr(dmsRelRow)
		dmsRow = dmsHashTable + uintptr(dmsRelRow)*4
		ZSTD_row_prefetch(tls, dmsHashTable, dmsTagTable, dmsRelRow, rowLog)
	}
	/* Update the hashTable and tagTable up to (but not including) ip */
	if !((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping != 0) {
		ZSTD_row_update_internal(tls, ms, ip, mls, rowLog, rowMask, uint32(1))
		hash = ZSTD_row_nextCachedHash(tls, hashCache, hashTable, tagTable, base, curr, hashLog, rowLog, mls, hashSalt)
	} else {
		/* Stop inserting every position when in the lazy skipping mode.
		 * The hash cache is also not kept up to date in this mode.
		 */
		hash = uint32(ZSTD_hashPtrSalted(tls, ip, hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, hashSalt))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr
	}
	*(*U32)(unsafe.Pointer(ms + 104)) += hash /* collect salt entropy */
	/* Get the hash for ip, compute the appropriate row */
	relRow = hash >> int32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
	tag = hash & (libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) - libc.Uint32FromInt32(1))
	row = hashTable + uintptr(relRow)*4
	tagRow = tagTable + uintptr(relRow)
	headGrouped = uint32(*(*BYTE)(unsafe.Pointer(tagRow))) & rowMask * groupWidth
	numMatches = uint64(0)
	currMatch = uint64(0)
	matches = ZSTD_row_getMatchMask(tls, tagRow, uint8(tag), headGrouped, rowEntries)
	/* Cycle through the matches and prefetch */
	for {
		if !(matches > uint64(0) && nbAttempts > uint32(0)) {
			break
		}
		matchPos = (headGrouped + ZSTD_VecMask_next(tls, matches)) / groupWidth & rowMask
		matchIndex = *(*U32)(unsafe.Pointer(row + uintptr(matchPos)*4))
		if matchPos == uint32(0) {
			goto _5
		}
		if matchIndex < lowLimit {
			break
		}
		if dictMode != int32(ZSTD_extDict) || matchIndex >= dictLimit {
			libc.X__builtin_prefetch(tls, base+uintptr(matchIndex), libc.VaList(bp+8, 0, int32(3)))
		} else {
			libc.X__builtin_prefetch(tls, dictBase+uintptr(matchIndex), libc.VaList(bp+8, 0, int32(3)))
		}
		v6 = numMatches
		numMatches = numMatches + 1
		matchBuffer[v6] = matchIndex
		nbAttempts = nbAttempts - 1
		goto _5
	_5:
		;
		matches = matches & (matches - uint64(1))
	}
	/* Speed opt: insert current byte into hashtable too. This allows us to avoid one iteration of the loop
	   in ZSTD_row_update_internal() at the next search. */
	pos = ZSTD_row_nextIndex(tls, tagRow, rowMask)
	*(*BYTE)(unsafe.Pointer(tagRow + uintptr(pos))) = uint8(tag)
	v8 = ms + 44
	v7 = *(*U32)(unsafe.Pointer(v8))
	*(*U32)(unsafe.Pointer(v8)) = *(*U32)(unsafe.Pointer(v8)) + 1
	*(*U32)(unsafe.Pointer(row + uintptr(pos)*4)) = v7
	/* Return the longest match */
	for {
		if !(currMatch < numMatches) {
			break
		}
		matchIndex1 = matchBuffer[currMatch]
		currentMl = uint64(0)
		if dictMode != int32(ZSTD_extDict) || matchIndex1 >= dictLimit {
			match = base + uintptr(matchIndex1)
			/* ensures this is true if dictMode != ZSTD_extDict */
			/* read 4B starting from (match + ml + 1 - sizeof(U32)) */
			if MEM_read32(tls, match+uintptr(ml)-uintptr(3)) == MEM_read32(tls, ip+uintptr(ml)-uintptr(3)) { /* potentially better */
				currentMl = ZSTD_count(tls, ip, match, iLimit)
			}
		} else {
			match1 = dictBase + uintptr(matchIndex1)
			if MEM_read32(tls, match1) == MEM_read32(tls, ip) { /* assumption : matchIndex <= dictLimit-4 (by table construction) */
				currentMl = ZSTD_count_2segments(tls, ip+uintptr(4), match1+uintptr(4), iLimit, dictEnd, prefixStart) + uint64(4)
			}
		}
		/* Save best solution */
		if currentMl > ml {
			ml = currentMl
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - matchIndex1 + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl) == iLimit {
				break
			} /* best possible, avoids read overflow on next attempt */
		}
		goto _9
	_9:
		;
		currMatch = currMatch + 1
	}
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		ml = ZSTD_dedicatedDictSearch_lazy_search(tls, offsetPtr, ml, nbAttempts+ddsExtraAttempts, dms, ip, iLimit, prefixStart, curr, dictLimit, ddsIdx)
	} else {
		if dictMode == int32(ZSTD_dictMatchState) {
			/* TODO: Measure and potentially add prefetching to DMS */
			dmsLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
			dmsBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
			dmsEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
			dmsSize = uint32(int64(dmsEnd) - int64(dmsBase))
			dmsIndexDelta = dictLimit - dmsSize
			headGrouped1 = uint32(*(*BYTE)(unsafe.Pointer(dmsTagRow))) & rowMask * groupWidth
			numMatches1 = uint64(0)
			currMatch1 = uint64(0)
			matches1 = ZSTD_row_getMatchMask(tls, dmsTagRow, uint8(dmsTag), headGrouped1, rowEntries)
			for {
				if !(matches1 > uint64(0) && nbAttempts > uint32(0)) {
					break
				}
				matchPos1 = (headGrouped1 + ZSTD_VecMask_next(tls, matches1)) / groupWidth & rowMask
				matchIndex2 = *(*U32)(unsafe.Pointer(dmsRow + uintptr(matchPos1)*4))
				if matchPos1 == uint32(0) {
					goto _10
				}
				if matchIndex2 < dmsLowestIndex {
					break
				}
				libc.X__builtin_prefetch(tls, dmsBase+uintptr(matchIndex2), libc.VaList(bp+8, 0, int32(3)))
				v6 = numMatches1
				numMatches1 = numMatches1 + 1
				matchBuffer1[v6] = matchIndex2
				nbAttempts = nbAttempts - 1
				goto _10
			_10:
				;
				matches1 = matches1 & (matches1 - uint64(1))
			}
			/* Return the longest match */
			for {
				if !(currMatch1 < numMatches1) {
					break
				}
				matchIndex3 = matchBuffer1[currMatch1]
				currentMl1 = uint64(0)
				match2 = dmsBase + uintptr(matchIndex3)
				if MEM_read32(tls, match2) == MEM_read32(tls, ip) {
					currentMl1 = ZSTD_count_2segments(tls, ip+uintptr(4), match2+uintptr(4), iLimit, dmsEnd, prefixStart) + uint64(4)
				}
				if currentMl1 > ml {
					ml = currentMl1
					*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex3 + dmsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
					if ip+uintptr(currentMl1) == iLimit {
						break
					}
				}
				goto _12
			_12:
				;
				currMatch1 = currMatch1 + 1
			}
		}
	}
	return ml
}

/**
 * Generate search functions templated on (dictMode, mls, rowLog).
 * These functions are outlined for code size & compilation time.
 * ZSTD_searchMax() dispatches to the correct implementation function.
 *
 * TODO: The start of the search function involves loading and calculating a
 * bunch of constants from the ZSTD_MatchState_t. These computations could be
 * done in an initialization function, and saved somewhere in the match state.
 * Then we could pass a pointer to the saved state instead of the match state,
 * and avoid duplicate computations.
 *
 * TODO: Move the match re-winding into searchMax. This improves compression
 * ratio, and unlocks further simplifications with the next TODO.
 *
 * TODO: Try moving the repcode search into searchMax. After the re-winding
 * and repcode search are in searchMax, there is no more logic in the match
 * finder loop that requires knowledge about the dictMode. So we should be
 * able to avoid force inlining it, and we can join the extDict loop with
 * the single segment loop. It should go in searchMax instead of its own
 * function to avoid having multiple virtual function calls per search.
 */

// C documentation
//
//	/* Generate row search fns for each combination of (dictMode, mls, rowLog) */
func ZSTD_RowFindBestMatch_noDict_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict), uint32(4))
}

func ZSTD_RowFindBestMatch_noDict_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict), uint32(5))
}

func ZSTD_RowFindBestMatch_noDict_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict), uint32(6))
}

func ZSTD_RowFindBestMatch_noDict_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict), uint32(4))
}

func ZSTD_RowFindBestMatch_noDict_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict), uint32(5))
}

func ZSTD_RowFindBestMatch_noDict_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict), uint32(6))
}

func ZSTD_RowFindBestMatch_noDict_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict), uint32(4))
}

func ZSTD_RowFindBestMatch_noDict_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict), uint32(5))
}

func ZSTD_RowFindBestMatch_noDict_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict), uint32(6))
}

func ZSTD_RowFindBestMatch_extDict_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict), uint32(4))
}

func ZSTD_RowFindBestMatch_extDict_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict), uint32(5))
}

func ZSTD_RowFindBestMatch_extDict_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict), uint32(6))
}

func ZSTD_RowFindBestMatch_extDict_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict), uint32(4))
}

func ZSTD_RowFindBestMatch_extDict_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict), uint32(5))
}

func ZSTD_RowFindBestMatch_extDict_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict), uint32(6))
}

func ZSTD_RowFindBestMatch_extDict_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict), uint32(4))
}

func ZSTD_RowFindBestMatch_extDict_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict), uint32(5))
}

func ZSTD_RowFindBestMatch_extDict_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict), uint32(6))
}

func ZSTD_RowFindBestMatch_dictMatchState_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_RowFindBestMatch_dictMatchState_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_RowFindBestMatch_dictMatchState_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_RowFindBestMatch_dictMatchState_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_RowFindBestMatch_dictMatchState_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_RowFindBestMatch_dictMatchState_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_RowFindBestMatch_dictMatchState_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_RowFindBestMatch_dictMatchState_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_RowFindBestMatch_dictMatchState_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch), uint32(4))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch), uint32(5))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch), uint32(6))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch), uint32(4))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch), uint32(5))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch), uint32(6))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch), uint32(4))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch), uint32(5))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch), uint32(6))
}

// C documentation
//
//	/* Generate binary Tree search fns for each combination of (dictMode, mls) */
func ZSTD_BtFindBestMatch_noDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_noDict))
}

func ZSTD_BtFindBestMatch_noDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_noDict))
}

func ZSTD_BtFindBestMatch_noDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_noDict))
}

func ZSTD_BtFindBestMatch_extDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_extDict))
}

func ZSTD_BtFindBestMatch_extDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_extDict))
}

func ZSTD_BtFindBestMatch_extDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_extDict))
}

func ZSTD_BtFindBestMatch_dictMatchState_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_dictMatchState))
}

func ZSTD_BtFindBestMatch_dictMatchState_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_dictMatchState))
}

func ZSTD_BtFindBestMatch_dictMatchState_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_dictMatchState))
}

func ZSTD_BtFindBestMatch_dedicatedDictSearch_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_BtFindBestMatch_dedicatedDictSearch_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_BtFindBestMatch_dedicatedDictSearch_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_dedicatedDictSearch))
}

// C documentation
//
//	/* Generate hash chain search fns for each combination of (dictMode, mls) */
func ZSTD_HcFindBestMatch_noDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict))
}

func ZSTD_HcFindBestMatch_noDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict))
}

func ZSTD_HcFindBestMatch_noDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict))
}

func ZSTD_HcFindBestMatch_extDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict))
}

func ZSTD_HcFindBestMatch_extDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict))
}

func ZSTD_HcFindBestMatch_extDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict))
}

func ZSTD_HcFindBestMatch_dictMatchState_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState))
}

func ZSTD_HcFindBestMatch_dictMatchState_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState))
}

func ZSTD_HcFindBestMatch_dictMatchState_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState))
}

func ZSTD_HcFindBestMatch_dedicatedDictSearch_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_HcFindBestMatch_dedicatedDictSearch_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_HcFindBestMatch_dedicatedDictSearch_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch))
}

type searchMethod_e = int32

const search_hashChain = 0
const search_binaryTree = 1
const search_rowHash = 2

// C documentation
//
//	/**
//	 * Searches for the longest match at @p ip.
//	 * Dispatches to the correct implementation function based on the
//	 * (searchMethod, dictMode, mls, rowLog). We use switch statements
//	 * here instead of using an indirect function call through a function
//	 * pointer because after Spectre and Meltdown mitigations, indirect
//	 * function calls can be very costly, especially in the kernel.
//	 *
//	 * NOTE: dictMode and searchMethod should be templated, so those switch
//	 * statements should be optimized out. Only the mls & rowLog switches
//	 * should be left.
//	 *
//	 * @param ms The match state.
//	 * @param ip The position to search at.
//	 * @param iend The end of the input data.
//	 * @param[out] offsetPtr Stores the match offset into this pointer.
//	 * @param mls The minimum search length, in the range [4, 6].
//	 * @param rowLog The row log (if applicable), in the range [4, 6].
//	 * @param searchMethod The search method to use (templated).
//	 * @param dictMode The dictMode (templated).
//	 *
//	 * @returns The length of the longest match found, or < mls if no match is found.
//	 * If a match is found its offset is stored in @p offsetPtr.
//	 */
func ZSTD_searchMax(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, offsetPtr uintptr, mls U32, rowLog U32, searchMethod searchMethod_e, dictMode ZSTD_dictMode_e) (r size_t) {
	if dictMode == int32(ZSTD_noDict) {
		switch searchMethod {
		case int32(search_hashChain):
			switch mls {
			case uint32(4):
				return ZSTD_HcFindBestMatch_noDict_4(tls, ms, ip, iend, offsetPtr)
			case uint32(5):
				return ZSTD_HcFindBestMatch_noDict_5(tls, ms, ip, iend, offsetPtr)
			case uint32(6):
				return ZSTD_HcFindBestMatch_noDict_6(tls, ms, ip, iend, offsetPtr)
			}
		case int32(search_binaryTree):
			switch mls {
			case uint32(4):
				return ZSTD_BtFindBestMatch_noDict_4(tls, ms, ip, iend, offsetPtr)
			case uint32(5):
				return ZSTD_BtFindBestMatch_noDict_5(tls, ms, ip, iend, offsetPtr)
			case uint32(6):
				return ZSTD_BtFindBestMatch_noDict_6(tls, ms, ip, iend, offsetPtr)
			}
		case int32(search_rowHash):
			switch mls {
			case uint32(4):
				switch rowLog {
				case uint32(4):
					return ZSTD_RowFindBestMatch_noDict_4_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_RowFindBestMatch_noDict_4_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_RowFindBestMatch_noDict_4_6(tls, ms, ip, iend, offsetPtr)
				}
				libc.X__builtin_unreachable(tls)
			case uint32(5):
				switch rowLog {
				case uint32(4):
					return ZSTD_RowFindBestMatch_noDict_5_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_RowFindBestMatch_noDict_5_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_RowFindBestMatch_noDict_5_6(tls, ms, ip, iend, offsetPtr)
				}
				libc.X__builtin_unreachable(tls)
			case uint32(6):
				switch rowLog {
				case uint32(4):
					return ZSTD_RowFindBestMatch_noDict_6_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_RowFindBestMatch_noDict_6_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_RowFindBestMatch_noDict_6_6(tls, ms, ip, iend, offsetPtr)
				}
				libc.X__builtin_unreachable(tls)
				break
			}
			break
		}
		libc.X__builtin_unreachable(tls)
	} else {
		if dictMode == int32(ZSTD_extDict) {
			switch searchMethod {
			case int32(search_hashChain):
				switch mls {
				case uint32(4):
					return ZSTD_HcFindBestMatch_extDict_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_HcFindBestMatch_extDict_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_HcFindBestMatch_extDict_6(tls, ms, ip, iend, offsetPtr)
				}
			case int32(search_binaryTree):
				switch mls {
				case uint32(4):
					return ZSTD_BtFindBestMatch_extDict_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_BtFindBestMatch_extDict_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_BtFindBestMatch_extDict_6(tls, ms, ip, iend, offsetPtr)
				}
			case int32(search_rowHash):
				switch mls {
				case uint32(4):
					switch rowLog {
					case uint32(4):
						return ZSTD_RowFindBestMatch_extDict_4_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_RowFindBestMatch_extDict_4_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_RowFindBestMatch_extDict_4_6(tls, ms, ip, iend, offsetPtr)
					}
					libc.X__builtin_unreachable(tls)
				case uint32(5):
					switch rowLog {
					case uint32(4):
						return ZSTD_RowFindBestMatch_extDict_5_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_RowFindBestMatch_extDict_5_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_RowFindBestMatch_extDict_5_6(tls, ms, ip, iend, offsetPtr)
					}
					libc.X__builtin_unreachable(tls)
				case uint32(6):
					switch rowLog {
					case uint32(4):
						return ZSTD_RowFindBestMatch_extDict_6_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_RowFindBestMatch_extDict_6_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_RowFindBestMatch_extDict_6_6(tls, ms, ip, iend, offsetPtr)
					}
					libc.X__builtin_unreachable(tls)
					break
				}
				break
			}
			libc.X__builtin_unreachable(tls)
		} else {
			if dictMode == int32(ZSTD_dictMatchState) {
				switch searchMethod {
				case int32(search_hashChain):
					switch mls {
					case uint32(4):
						return ZSTD_HcFindBestMatch_dictMatchState_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_HcFindBestMatch_dictMatchState_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_HcFindBestMatch_dictMatchState_6(tls, ms, ip, iend, offsetPtr)
					}
				case int32(search_binaryTree):
					switch mls {
					case uint32(4):
						return ZSTD_BtFindBestMatch_dictMatchState_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_BtFindBestMatch_dictMatchState_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_BtFindBestMatch_dictMatchState_6(tls, ms, ip, iend, offsetPtr)
					}
				case int32(search_rowHash):
					switch mls {
					case uint32(4):
						switch rowLog {
						case uint32(4):
							return ZSTD_RowFindBestMatch_dictMatchState_4_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_RowFindBestMatch_dictMatchState_4_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_RowFindBestMatch_dictMatchState_4_6(tls, ms, ip, iend, offsetPtr)
						}
						libc.X__builtin_unreachable(tls)
					case uint32(5):
						switch rowLog {
						case uint32(4):
							return ZSTD_RowFindBestMatch_dictMatchState_5_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_RowFindBestMatch_dictMatchState_5_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_RowFindBestMatch_dictMatchState_5_6(tls, ms, ip, iend, offsetPtr)
						}
						libc.X__builtin_unreachable(tls)
					case uint32(6):
						switch rowLog {
						case uint32(4):
							return ZSTD_RowFindBestMatch_dictMatchState_6_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_RowFindBestMatch_dictMatchState_6_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_RowFindBestMatch_dictMatchState_6_6(tls, ms, ip, iend, offsetPtr)
						}
						libc.X__builtin_unreachable(tls)
						break
					}
					break
				}
				libc.X__builtin_unreachable(tls)
			} else {
				if dictMode == int32(ZSTD_dedicatedDictSearch) {
					switch searchMethod {
					case int32(search_hashChain):
						switch mls {
						case uint32(4):
							return ZSTD_HcFindBestMatch_dedicatedDictSearch_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_HcFindBestMatch_dedicatedDictSearch_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_HcFindBestMatch_dedicatedDictSearch_6(tls, ms, ip, iend, offsetPtr)
						}
					case int32(search_binaryTree):
						switch mls {
						case uint32(4):
							return ZSTD_BtFindBestMatch_dedicatedDictSearch_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_BtFindBestMatch_dedicatedDictSearch_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_BtFindBestMatch_dedicatedDictSearch_6(tls, ms, ip, iend, offsetPtr)
						}
					case int32(search_rowHash):
						switch mls {
						case uint32(4):
							switch rowLog {
							case uint32(4):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_4_4(tls, ms, ip, iend, offsetPtr)
							case uint32(5):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_4_5(tls, ms, ip, iend, offsetPtr)
							case uint32(6):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_4_6(tls, ms, ip, iend, offsetPtr)
							}
							libc.X__builtin_unreachable(tls)
						case uint32(5):
							switch rowLog {
							case uint32(4):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_5_4(tls, ms, ip, iend, offsetPtr)
							case uint32(5):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_5_5(tls, ms, ip, iend, offsetPtr)
							case uint32(6):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_5_6(tls, ms, ip, iend, offsetPtr)
							}
							libc.X__builtin_unreachable(tls)
						case uint32(6):
							switch rowLog {
							case uint32(4):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_6_4(tls, ms, ip, iend, offsetPtr)
							case uint32(5):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_6_5(tls, ms, ip, iend, offsetPtr)
							case uint32(6):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_6_6(tls, ms, ip, iend, offsetPtr)
							}
							libc.X__builtin_unreachable(tls)
							break
						}
						break
					}
					libc.X__builtin_unreachable(tls)
				}
			}
		}
	}
	libc.X__builtin_unreachable(tls)
	return uint64(0)
}

/* *******************************
*  Common parser - lazy strategy
*********************************/
func ZSTD_compressBlock_lazy_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, searchMethod searchMethod_e, depth U32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, dictBase, dictEnd, dictLowest, dms, iend, ilimit, ip, istart, mStart, match, prefixLowest, repEnd2, repMatch, repMatch1, repMatch2, repMatch3, repMatchEnd, repMatchEnd1, repMatchEnd2, start, v1, v10, v11, v9 uintptr
	var curr, current2, dictAndPrefixLength, dictIndexDelta, dictLowestIndex, matchIndex, maxRep, mls, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowestIndex, repIndex, repIndex1, repIndex2, repIndex3, rowLog, windowLow U32
	var gain1, gain11, gain12, gain13, gain14, gain15, gain2, gain21, gain22, gain23, gain24, gain25, isDDS, isDMS, isDxS int32
	var litLength, matchLength, ml2, ml21, ml22, mlRep, mlRep1, mlRep2, mlRep3, offBase, step size_t
	var v12, v2, v3, v4, v5, v6, v7, v8 uint32
	var v19 bool
	var _ /* ofbCandidate at bp+16 */ size_t
	var _ /* ofbCandidate at bp+8 */ size_t
	var _ /* offbaseFound at bp+0 */ size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, curr, current2, dictAndPrefixLength, dictBase, dictEnd, dictIndexDelta, dictLowest, dictLowestIndex, dms, gain1, gain11, gain12, gain13, gain14, gain15, gain2, gain21, gain22, gain23, gain24, gain25, iend, ilimit, ip, isDDS, isDMS, isDxS, istart, litLength, mStart, match, matchIndex, matchLength, maxRep, ml2, ml21, ml22, mlRep, mlRep1, mlRep2, mlRep3, mls, offBase, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowest, prefixLowestIndex, repEnd2, repIndex, repIndex1, repIndex2, repIndex3, repMatch, repMatch1, repMatch2, repMatch3, repMatchEnd, repMatchEnd1, repMatchEnd2, rowLog, start, step, windowLow, v1, v10, v11, v12, v19, v2, v3, v4, v5, v6, v7, v8, v9
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	if searchMethod == int32(search_rowHash) {
		v1 = iend - uintptr(8) - uintptr(ZSTD_ROW_HASH_CACHE_SIZE)
	} else {
		v1 = iend - uintptr(8)
	}
	ilimit = v1
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	prefixLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixLowest = base + uintptr(prefixLowestIndex)
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < uint32(libc.Int32FromInt32(6)) {
		v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v3 = uint32(libc.Int32FromInt32(6))
	}
	if uint32(libc.Int32FromInt32(4)) > v3 {
		v2 = uint32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < uint32(libc.Int32FromInt32(6)) {
			v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
		} else {
			v4 = uint32(libc.Int32FromInt32(6))
		}
		v2 = v4
	}
	mls = v2
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < uint32(libc.Int32FromInt32(6)) {
		v6 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
	} else {
		v6 = uint32(libc.Int32FromInt32(6))
	}
	if uint32(libc.Int32FromInt32(4)) > v6 {
		v5 = uint32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < uint32(libc.Int32FromInt32(6)) {
			v7 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
		} else {
			v7 = uint32(libc.Int32FromInt32(6))
		}
		v5 = v7
	}
	rowLog = v5
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	isDMS = libc.BoolInt32(dictMode == int32(ZSTD_dictMatchState))
	isDDS = libc.BoolInt32(dictMode == int32(ZSTD_dedicatedDictSearch))
	isDxS = libc.BoolInt32(isDMS != 0 || isDDS != 0)
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	if isDxS != 0 {
		v8 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	} else {
		v8 = uint32(0)
	}
	dictLowestIndex = v8
	if isDxS != 0 {
		v9 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	} else {
		v9 = libc.UintptrFromInt32(0)
	}
	dictBase = v9
	if isDxS != 0 {
		v10 = dictBase + uintptr(dictLowestIndex)
	} else {
		v10 = libc.UintptrFromInt32(0)
	}
	dictLowest = v10
	if isDxS != 0 {
		v11 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	} else {
		v11 = libc.UintptrFromInt32(0)
	}
	dictEnd = v11
	if isDxS != 0 {
		v12 = prefixLowestIndex - uint32(int64(dictEnd)-int64(dictBase))
	} else {
		v12 = uint32(0)
	}
	dictIndexDelta = v12
	dictAndPrefixLength = uint32(int64(ip) - int64(prefixLowest) + (int64(dictEnd) - int64(dictLowest)))
	ip = ip + libc.BoolUintptr(dictAndPrefixLength == libc.Uint32FromInt32(0))
	if dictMode == int32(ZSTD_noDict) {
		curr = uint32(int64(ip) - int64(base))
		windowLow = ZSTD_getLowestPrefixIndex(tls, ms, curr, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FwindowLog)
		maxRep = curr - windowLow
		if offset_2 > maxRep {
			offsetSaved2 = offset_2
			offset_2 = libc.Uint32FromInt32(0)
		}
		if offset_1 > maxRep {
			offsetSaved1 = offset_1
			offset_1 = libc.Uint32FromInt32(0)
		}
	}
	if isDxS != 0 {
		/* dictMatchState repCode checks don't currently handle repCode == 0
		 * disabling. */
	}
	/* Reset the lazy skipping state */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
	if searchMethod == int32(search_rowHash) {
		ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
	}
	/* Match Loop */
	for ip < ilimit {
		matchLength = uint64(0)
		offBase = uint64(libc.Int32FromInt32(1))
		start = ip + uintptr(1)
		/* check repCode */
		if isDxS != 0 {
			repIndex = uint32(int64(ip)-int64(base)) + uint32(1) - offset_1
			if (dictMode == int32(ZSTD_dictMatchState) || dictMode == int32(ZSTD_dedicatedDictSearch)) && repIndex < prefixLowestIndex {
				v1 = dictBase + uintptr(repIndex-dictIndexDelta)
			} else {
				v1 = base + uintptr(repIndex)
			}
			repMatch = v1
			if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip+uintptr(1)) {
				if repIndex < prefixLowestIndex {
					v9 = dictEnd
				} else {
					v9 = iend
				}
				repMatchEnd = v9
				matchLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixLowest) + uint64(4)
				if depth == uint32(0) {
					goto _storeSequence
				}
			}
		}
		if dictMode == int32(ZSTD_noDict) && libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip+uintptr(1)-uintptr(offset_1)) == MEM_read32(tls, ip+uintptr(1))) != 0 {
			matchLength = ZSTD_count(tls, ip+uintptr(1)+uintptr(4), ip+uintptr(1)+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
			if depth == uint32(0) {
				goto _storeSequence
			}
		}
		/* first search (depth 0) */
		*(*size_t)(unsafe.Pointer(bp)) = uint64(999999999)
		ml2 = ZSTD_searchMax(tls, ms, ip, iend, bp, mls, rowLog, searchMethod, dictMode)
		if ml2 > matchLength {
			matchLength = ml2
			start = ip
			offBase = *(*size_t)(unsafe.Pointer(bp))
		}
		if matchLength < uint64(4) {
			step = uint64(int64(ip)-int64(anchor))>>libc.Int32FromInt32(kSearchStrength) + uint64(1) /* jump faster over incompressible sections */
			ip = ip + uintptr(step)
			/* Enter the lazy skipping mode once we are skipping more than 8 bytes at a time.
			 * In this mode we stop inserting every position into our tables, and only insert
			 * positions that we search, which is one in step positions.
			 * The exact cutoff is flexible, I've just chosen a number that is reasonably high,
			 * so we minimize the compression ratio loss in "normal" scenarios. This mode gets
			 * triggered once we've gone 2KB without finding any matches.
			 */
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = libc.BoolInt32(step > uint64(kLazySkippingStep))
			continue
		}
		/* let's try to find a better solution */
		if depth >= uint32(1) {
			for ip < ilimit {
				ip = ip + 1
				if dictMode == int32(ZSTD_noDict) && offBase != 0 && libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_1))) != 0 {
					mlRep = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
					gain2 = int32(mlRep * libc.Uint64FromInt32(3))
					gain1 = int32(matchLength*libc.Uint64FromInt32(3) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
					if mlRep >= uint64(4) && gain2 > gain1 {
						matchLength = mlRep
						offBase = uint64(libc.Int32FromInt32(1))
						start = ip
					}
				}
				if isDxS != 0 {
					repIndex1 = uint32(int64(ip)-int64(base)) - offset_1
					if repIndex1 < prefixLowestIndex {
						v1 = dictBase + uintptr(repIndex1-dictIndexDelta)
					} else {
						v1 = base + uintptr(repIndex1)
					}
					repMatch1 = v1
					if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex1) != 0 && MEM_read32(tls, repMatch1) == MEM_read32(tls, ip) {
						if repIndex1 < prefixLowestIndex {
							v9 = dictEnd
						} else {
							v9 = iend
						}
						repMatchEnd1 = v9
						mlRep1 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch1+uintptr(4), iend, repMatchEnd1, prefixLowest) + uint64(4)
						gain21 = int32(mlRep1 * libc.Uint64FromInt32(3))
						gain11 = int32(matchLength*libc.Uint64FromInt32(3) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
						if mlRep1 >= uint64(4) && gain21 > gain11 {
							matchLength = mlRep1
							offBase = uint64(libc.Int32FromInt32(1))
							start = ip
						}
					}
				}
				*(*size_t)(unsafe.Pointer(bp + 8)) = uint64(999999999)
				ml21 = ZSTD_searchMax(tls, ms, ip, iend, bp+8, mls, rowLog, searchMethod, dictMode)
				gain22 = int32(ml21*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 8)))))) /* raw approx */
				gain12 = int32(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(4))
				if ml21 >= uint64(4) && gain22 > gain12 {
					matchLength = ml21
					offBase = *(*size_t)(unsafe.Pointer(bp + 8))
					start = ip
					continue /* search a better one */
				}
				/* let's find an even better one */
				if depth == uint32(2) && ip < ilimit {
					ip = ip + 1
					if dictMode == int32(ZSTD_noDict) && offBase != 0 && libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_1))) != 0 {
						mlRep2 = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
						gain23 = int32(mlRep2 * libc.Uint64FromInt32(4))
						gain13 = int32(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
						if mlRep2 >= uint64(4) && gain23 > gain13 {
							matchLength = mlRep2
							offBase = uint64(libc.Int32FromInt32(1))
							start = ip
						}
					}
					if isDxS != 0 {
						repIndex2 = uint32(int64(ip)-int64(base)) - offset_1
						if repIndex2 < prefixLowestIndex {
							v1 = dictBase + uintptr(repIndex2-dictIndexDelta)
						} else {
							v1 = base + uintptr(repIndex2)
						}
						repMatch2 = v1
						if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex2) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip) {
							if repIndex2 < prefixLowestIndex {
								v9 = dictEnd
							} else {
								v9 = iend
							}
							repMatchEnd2 = v9
							mlRep3 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repMatchEnd2, prefixLowest) + uint64(4)
							gain24 = int32(mlRep3 * libc.Uint64FromInt32(4))
							gain14 = int32(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
							if mlRep3 >= uint64(4) && gain24 > gain14 {
								matchLength = mlRep3
								offBase = uint64(libc.Int32FromInt32(1))
								start = ip
							}
						}
					}
					*(*size_t)(unsafe.Pointer(bp + 16)) = uint64(999999999)
					ml22 = ZSTD_searchMax(tls, ms, ip, iend, bp+16, mls, rowLog, searchMethod, dictMode)
					gain25 = int32(ml22*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 16)))))) /* raw approx */
					gain15 = int32(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(7))
					if ml22 >= uint64(4) && gain25 > gain15 {
						matchLength = ml22
						offBase = *(*size_t)(unsafe.Pointer(bp + 16))
						start = ip
						continue
					}
				}
				break /* nothing found : store previous solution */
			}
		}
		/* NOTE:
		 * Pay attention that `start[-value]` can lead to strange undefined behavior
		 * notably if `value` is unsigned, resulting in a large positive `-value`.
		 */
		/* catch up */
		if offBase > uint64(ZSTD_REP_NUM) {
			if dictMode == int32(ZSTD_noDict) {
				for {
					if v19 = libc.BoolInt32(start > anchor)&libc.BoolInt32(start-uintptr(offBase-libc.Uint64FromInt32(ZSTD_REP_NUM)) > prefixLowest) != 0; v19 {
					}
					if !(v19 && int32(*(*BYTE)(unsafe.Pointer(start + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(start - uintptr(offBase-libc.Uint64FromInt32(ZSTD_REP_NUM)) + uintptr(-libc.Int32FromInt32(1)))))) {
						break
					} /* only search for offset within prefix */
					start = start - 1
					matchLength = matchLength + 1
				}
			}
			if isDxS != 0 {
				matchIndex = uint32(uint64(int64(start)-int64(base)) - (offBase - libc.Uint64FromInt32(ZSTD_REP_NUM)))
				if matchIndex < prefixLowestIndex {
					v1 = dictBase + uintptr(matchIndex) - uintptr(dictIndexDelta)
				} else {
					v1 = base + uintptr(matchIndex)
				}
				match = v1
				if matchIndex < prefixLowestIndex {
					v9 = dictLowest
				} else {
					v9 = prefixLowest
				}
				mStart = v9
				for start > anchor && match > mStart && int32(*(*BYTE)(unsafe.Pointer(start + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
					start = start - 1
					match = match - 1
					matchLength = matchLength + 1
				} /* catch up */
			}
			offset_2 = offset_1
			offset_1 = uint32(offBase - libc.Uint64FromInt32(ZSTD_REP_NUM))
		}
		/* store sequence */
		goto _storeSequence
	_storeSequence:
		;
		litLength = uint64(int64(start) - int64(anchor))
		ZSTD_storeSeq(tls, seqStore, litLength, anchor, iend, uint32(offBase), matchLength)
		v1 = start + uintptr(matchLength)
		ip = v1
		anchor = v1
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping != 0 {
			/* We've found a match, disable lazy skipping mode, and refill the hash cache. */
			if searchMethod == int32(search_rowHash) {
				ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
			}
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
		}
		/* check immediate repcode */
		if isDxS != 0 {
			for ip <= ilimit {
				current2 = uint32(int64(ip) - int64(base))
				repIndex3 = current2 - offset_2
				if repIndex3 < prefixLowestIndex {
					v1 = dictBase - uintptr(dictIndexDelta) + uintptr(repIndex3)
				} else {
					v1 = base + uintptr(repIndex3)
				}
				repMatch3 = v1
				if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex3) != 0 && MEM_read32(tls, repMatch3) == MEM_read32(tls, ip) {
					if repIndex3 < prefixLowestIndex {
						v9 = dictEnd
					} else {
						v9 = iend
					}
					repEnd2 = v9
					matchLength = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch3+uintptr(4), iend, repEnd2, prefixLowest) + uint64(4)
					offBase = uint64(offset_2)
					offset_2 = offset_1
					offset_1 = uint32(offBase) /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), matchLength)
					ip = ip + uintptr(matchLength)
					anchor = ip
					continue
				}
				break
			}
		}
		if dictMode == int32(ZSTD_noDict) {
			for libc.BoolInt32(ip <= ilimit)&libc.BoolInt32(offset_2 > uint32(0)) != 0 && MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_2)) {
				/* store sequence */
				matchLength = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_2), iend) + uint64(4)
				offBase = uint64(offset_2)
				offset_2 = offset_1
				offset_1 = uint32(offBase) /* swap repcodes */
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), matchLength)
				ip = ip + uintptr(matchLength)
				anchor = ip
				continue /* faster when present ... (?) */
			}
		}
	}
	/* If offset_1 started invalid (offsetSaved1 != 0) and became valid (offset_1 != 0),
	 * rotate saved offsets. See comment in ZSTD_compressBlock_fast_noDict for more context. */
	if offsetSaved1 != uint32(0) && offset_1 != uint32(0) {
		v2 = offsetSaved1
	} else {
		v2 = offsetSaved2
	}
	offsetSaved2 = v2
	/* save reps for next block */
	if offset_1 != 0 {
		v2 = offset_1
	} else {
		v2 = offsetSaved1
	}
	*(*U32)(unsafe.Pointer(rep)) = v2
	if offset_2 != 0 {
		v2 = offset_2
	} else {
		v2 = offsetSaved2
	}
	*(*U32)(unsafe.Pointer(rep + 1*4)) = v2
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_greedy(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_greedy_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_greedy_dedicatedDictSearch(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_greedy_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_greedy_dictMatchState_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_greedy_dedicatedDictSearch_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy_dedicatedDictSearch(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy_dictMatchState_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy_dedicatedDictSearch_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy2_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy2_dedicatedDictSearch(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy2_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy2_dictMatchState_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy2_dedicatedDictSearch_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_btlazy2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_binaryTree), uint32(2), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_btlazy2_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_binaryTree), uint32(2), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy_extDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, searchMethod searchMethod_e, depth U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, dictBase, dictEnd, dictStart, iend, ilimit, ip, istart, mStart, match, prefixStart, repBase, repBase1, repBase2, repBase3, repEnd, repEnd1, repEnd2, repEnd3, repMatch, repMatch1, repMatch2, repMatch3, start, v1, v8 uintptr
	var curr, dictLimit, matchIndex, mls, offset_1, offset_2, repCurrent, repIndex, repIndex1, repIndex2, repIndex3, rowLog, windowLog, windowLow, windowLow1, windowLow2, windowLow3 U32
	var gain1, gain11, gain12, gain13, gain2, gain21, gain22, gain23 int32
	var litLength, matchLength, ml2, ml21, ml22, offBase, repLength, repLength1, step size_t
	var v2, v3, v4, v5, v6, v7 uint32
	var _ /* ofbCandidate at bp+0 */ size_t
	var _ /* ofbCandidate at bp+16 */ size_t
	var _ /* ofbCandidate at bp+8 */ size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, curr, dictBase, dictEnd, dictLimit, dictStart, gain1, gain11, gain12, gain13, gain2, gain21, gain22, gain23, iend, ilimit, ip, istart, litLength, mStart, match, matchIndex, matchLength, ml2, ml21, ml22, mls, offBase, offset_1, offset_2, prefixStart, repBase, repBase1, repBase2, repBase3, repCurrent, repEnd, repEnd1, repEnd2, repEnd3, repIndex, repIndex1, repIndex2, repIndex3, repLength, repLength1, repMatch, repMatch1, repMatch2, repMatch3, rowLog, start, step, windowLog, windowLow, windowLow1, windowLow2, windowLow3, v1, v2, v3, v4, v5, v6, v7, v8
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	if searchMethod == int32(search_rowHash) {
		v1 = iend - uintptr(8) - uintptr(ZSTD_ROW_HASH_CACHE_SIZE)
	} else {
		v1 = iend - uintptr(8)
	}
	ilimit = v1
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(dictLimit)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictEnd = dictBase + uintptr(dictLimit)
	dictStart = dictBase + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit)
	windowLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FwindowLog
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < uint32(libc.Int32FromInt32(6)) {
		v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v3 = uint32(libc.Int32FromInt32(6))
	}
	if uint32(libc.Int32FromInt32(4)) > v3 {
		v2 = uint32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < uint32(libc.Int32FromInt32(6)) {
			v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
		} else {
			v4 = uint32(libc.Int32FromInt32(6))
		}
		v2 = v4
	}
	mls = v2
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < uint32(libc.Int32FromInt32(6)) {
		v6 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
	} else {
		v6 = uint32(libc.Int32FromInt32(6))
	}
	if uint32(libc.Int32FromInt32(4)) > v6 {
		v5 = uint32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < uint32(libc.Int32FromInt32(6)) {
			v7 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
		} else {
			v7 = uint32(libc.Int32FromInt32(6))
		}
		v5 = v7
	}
	rowLog = v5
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	/* Reset the lazy skipping state */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
	/* init */
	ip = ip + libc.BoolUintptr(ip == prefixStart)
	if searchMethod == int32(search_rowHash) {
		ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
	}
	/* Match Loop */
	for ip < ilimit {
		matchLength = uint64(0)
		offBase = uint64(libc.Int32FromInt32(1))
		start = ip + uintptr(1)
		curr = uint32(int64(ip) - int64(base))
		/* check repCode */
		windowLow = ZSTD_getLowestMatchIndex(tls, ms, curr+uint32(1), windowLog)
		repIndex = curr + libc.Uint32FromInt32(1) - offset_1
		if repIndex < dictLimit {
			v1 = dictBase
		} else {
			v1 = base
		}
		repBase = v1
		repMatch = repBase + uintptr(repIndex)
		if ZSTD_index_overlap_check(tls, dictLimit, repIndex)&libc.BoolInt32(offset_1 <= curr+uint32(1)-windowLow) != 0 { /* note: we are searching at curr+1 */
			if MEM_read32(tls, ip+uintptr(1)) == MEM_read32(tls, repMatch) {
				if repIndex < dictLimit {
					v8 = dictEnd
				} else {
					v8 = iend
				}
				/* repcode detected we should take it */
				repEnd = v8
				matchLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repEnd, prefixStart) + uint64(4)
				if depth == uint32(0) {
					goto _storeSequence
				}
			}
		}
		/* first search (depth 0) */
		*(*size_t)(unsafe.Pointer(bp)) = uint64(999999999)
		ml2 = ZSTD_searchMax(tls, ms, ip, iend, bp, mls, rowLog, searchMethod, int32(ZSTD_extDict))
		if ml2 > matchLength {
			matchLength = ml2
			start = ip
			offBase = *(*size_t)(unsafe.Pointer(bp))
		}
		if matchLength < uint64(4) {
			step = uint64(int64(ip)-int64(anchor)) >> libc.Int32FromInt32(kSearchStrength)
			ip = ip + uintptr(step+uint64(1)) /* jump faster over incompressible sections */
			/* Enter the lazy skipping mode once we are skipping more than 8 bytes at a time.
			 * In this mode we stop inserting every position into our tables, and only insert
			 * positions that we search, which is one in step positions.
			 * The exact cutoff is flexible, I've just chosen a number that is reasonably high,
			 * so we minimize the compression ratio loss in "normal" scenarios. This mode gets
			 * triggered once we've gone 2KB without finding any matches.
			 */
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = libc.BoolInt32(step > uint64(kLazySkippingStep))
			continue
		}
		/* let's try to find a better solution */
		if depth >= uint32(1) {
			for ip < ilimit {
				ip = ip + 1
				curr = curr + 1
				/* check repCode */
				if offBase != 0 {
					windowLow1 = ZSTD_getLowestMatchIndex(tls, ms, curr, windowLog)
					repIndex1 = curr - offset_1
					if repIndex1 < dictLimit {
						v1 = dictBase
					} else {
						v1 = base
					}
					repBase1 = v1
					repMatch1 = repBase1 + uintptr(repIndex1)
					if ZSTD_index_overlap_check(tls, dictLimit, repIndex1)&libc.BoolInt32(offset_1 <= curr-windowLow1) != 0 { /* equivalent to `curr > repIndex >= windowLow` */
						if MEM_read32(tls, ip) == MEM_read32(tls, repMatch1) {
							if repIndex1 < dictLimit {
								v8 = dictEnd
							} else {
								v8 = iend
							}
							/* repcode detected */
							repEnd1 = v8
							repLength = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch1+uintptr(4), iend, repEnd1, prefixStart) + uint64(4)
							gain2 = int32(repLength * libc.Uint64FromInt32(3))
							gain1 = int32(matchLength*libc.Uint64FromInt32(3) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
							if repLength >= uint64(4) && gain2 > gain1 {
								matchLength = repLength
								offBase = uint64(libc.Int32FromInt32(1))
								start = ip
							}
						}
					}
				}
				/* search match, depth 1 */
				*(*size_t)(unsafe.Pointer(bp + 8)) = uint64(999999999)
				ml21 = ZSTD_searchMax(tls, ms, ip, iend, bp+8, mls, rowLog, searchMethod, int32(ZSTD_extDict))
				gain21 = int32(ml21*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 8)))))) /* raw approx */
				gain11 = int32(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(4))
				if ml21 >= uint64(4) && gain21 > gain11 {
					matchLength = ml21
					offBase = *(*size_t)(unsafe.Pointer(bp + 8))
					start = ip
					continue /* search a better one */
				}
				/* let's find an even better one */
				if depth == uint32(2) && ip < ilimit {
					ip = ip + 1
					curr = curr + 1
					/* check repCode */
					if offBase != 0 {
						windowLow2 = ZSTD_getLowestMatchIndex(tls, ms, curr, windowLog)
						repIndex2 = curr - offset_1
						if repIndex2 < dictLimit {
							v1 = dictBase
						} else {
							v1 = base
						}
						repBase2 = v1
						repMatch2 = repBase2 + uintptr(repIndex2)
						if ZSTD_index_overlap_check(tls, dictLimit, repIndex2)&libc.BoolInt32(offset_1 <= curr-windowLow2) != 0 { /* equivalent to `curr > repIndex >= windowLow` */
							if MEM_read32(tls, ip) == MEM_read32(tls, repMatch2) {
								if repIndex2 < dictLimit {
									v8 = dictEnd
								} else {
									v8 = iend
								}
								/* repcode detected */
								repEnd2 = v8
								repLength1 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
								gain22 = int32(repLength1 * libc.Uint64FromInt32(4))
								gain12 = int32(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
								if repLength1 >= uint64(4) && gain22 > gain12 {
									matchLength = repLength1
									offBase = uint64(libc.Int32FromInt32(1))
									start = ip
								}
							}
						}
					}
					/* search match, depth 2 */
					*(*size_t)(unsafe.Pointer(bp + 16)) = uint64(999999999)
					ml22 = ZSTD_searchMax(tls, ms, ip, iend, bp+16, mls, rowLog, searchMethod, int32(ZSTD_extDict))
					gain23 = int32(ml22*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 16)))))) /* raw approx */
					gain13 = int32(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(7))
					if ml22 >= uint64(4) && gain23 > gain13 {
						matchLength = ml22
						offBase = *(*size_t)(unsafe.Pointer(bp + 16))
						start = ip
						continue
					}
				}
				break /* nothing found : store previous solution */
			}
		}
		/* catch up */
		if offBase > uint64(ZSTD_REP_NUM) {
			matchIndex = uint32(uint64(int64(start)-int64(base)) - (offBase - libc.Uint64FromInt32(ZSTD_REP_NUM)))
			if matchIndex < dictLimit {
				v1 = dictBase + uintptr(matchIndex)
			} else {
				v1 = base + uintptr(matchIndex)
			}
			match = v1
			if matchIndex < dictLimit {
				v8 = dictStart
			} else {
				v8 = prefixStart
			}
			mStart = v8
			for start > anchor && match > mStart && int32(*(*BYTE)(unsafe.Pointer(start + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
				start = start - 1
				match = match - 1
				matchLength = matchLength + 1
			} /* catch up */
			offset_2 = offset_1
			offset_1 = uint32(offBase - libc.Uint64FromInt32(ZSTD_REP_NUM))
		}
		/* store sequence */
		goto _storeSequence
	_storeSequence:
		;
		litLength = uint64(int64(start) - int64(anchor))
		ZSTD_storeSeq(tls, seqStore, litLength, anchor, iend, uint32(offBase), matchLength)
		v1 = start + uintptr(matchLength)
		ip = v1
		anchor = v1
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping != 0 {
			/* We've found a match, disable lazy skipping mode, and refill the hash cache. */
			if searchMethod == int32(search_rowHash) {
				ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
			}
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
		}
		/* check immediate repcode */
		for ip <= ilimit {
			repCurrent = uint32(int64(ip) - int64(base))
			windowLow3 = ZSTD_getLowestMatchIndex(tls, ms, repCurrent, windowLog)
			repIndex3 = repCurrent - offset_2
			if repIndex3 < dictLimit {
				v1 = dictBase
			} else {
				v1 = base
			}
			repBase3 = v1
			repMatch3 = repBase3 + uintptr(repIndex3)
			if ZSTD_index_overlap_check(tls, dictLimit, repIndex3)&libc.BoolInt32(offset_2 <= repCurrent-windowLow3) != 0 { /* equivalent to `curr > repIndex >= windowLow` */
				if MEM_read32(tls, ip) == MEM_read32(tls, repMatch3) {
					if repIndex3 < dictLimit {
						v8 = dictEnd
					} else {
						v8 = iend
					}
					/* repcode detected we should take it */
					repEnd3 = v8
					matchLength = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch3+uintptr(4), iend, repEnd3, prefixStart) + uint64(4)
					offBase = uint64(offset_2)
					offset_2 = offset_1
					offset_1 = uint32(offBase) /* swap offset history */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, uint32(libc.Int32FromInt32(1)), matchLength)
					ip = ip + uintptr(matchLength)
					anchor = ip
					continue /* faster when present ... (?) */
				}
			}
			break
		}
	}
	/* Save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_greedy_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0))
}

func ZSTD_compressBlock_greedy_extDict_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0))
}

func ZSTD_compressBlock_lazy_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1))
}

func ZSTD_compressBlock_lazy_extDict_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1))
}

func ZSTD_compressBlock_lazy2_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2))
}

func ZSTD_compressBlock_lazy2_extDict_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2))
}

func ZSTD_compressBlock_btlazy2_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_binaryTree), uint32(2))
}

/**** ended inlining compress/zstd_lazy.c ****/
/**** start inlining compress/zstd_ldm.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_ldm.h ****/

/**** skipping file: ../common/debug.h ****/
/**** skipping file: ../common/xxhash.h ****/
/**** skipping file: zstd_fast.h ****/
/**** skipping file: zstd_double_fast.h ****/
/**** start inlining zstd_ldm_geartab.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/mem.h ****/

var ZSTD_ldm_gearTab = [256]U64{
	0:   uint64(0xf5b8f72c5f77775c),
	1:   uint64(0x84935f266b7ac412),
	2:   uint64(0xb647ada9ca730ccc),
	3:   uint64(0xb065bb4b114fb1de),
	4:   uint64(0x34584e7e8c3a9fd0),
	5:   uint64(0x4e97e17c6ae26b05),
	6:   uint64(0x3a03d743bc99a604),
	7:   uint64(0xcecd042422c4044f),
	8:   uint64(0x76de76c58524259e),
	9:   uint64(0x9c8528f65badeaca),
	10:  uint64(0x86563706e2097529),
	11:  uint64(0x2902475fa375d889),
	12:  uint64(0xafb32a9739a5ebe6),
	13:  uint64(0xce2714da3883e639),
	14:  uint64(0x21eaf821722e69e),
	15:  uint64(0x37b628620b628),
	16:  uint64(0x49a8d455d88caf5),
	17:  uint64(0x8556d711e6958140),
	18:  uint64(0x4f7ae74fc605c1f),
	19:  uint64(0x829f0c3468bd3a20),
	20:  uint64(0x4ffdc885c625179e),
	21:  uint64(0x8473de048a3daf1b),
	22:  uint64(0x51008822b05646b2),
	23:  uint64(0x69d75d12b2d1cc5f),
	24:  uint64(0x8c9d4a19159154bc),
	25:  uint64(0xc3cc10f4abbd4003),
	26:  uint64(0xd06ddc1cecb97391),
	27:  uint64(0xbe48e6e7ed80302e),
	28:  uint64(0x3481db31cee03547),
	29:  uint64(0xacc3f67cdaa1d210),
	30:  uint64(0x65cb771d8c7f96cc),
	31:  uint64(0x8eb27177055723dd),
	32:  uint64(0xc789950d44cd94be),
	33:  uint64(0x934feadc3700b12b),
	34:  uint64(0x5e485f11edbdf182),
	35:  uint64(0x1e2e2a46fd64767a),
	36:  uint64(0x2969ca71d82efa7c),
	37:  uint64(0x9d46e9935ebbba2e),
	38:  uint64(0xe056b67e05e6822b),
	39:  uint64(0x94d73f55739d03a0),
	40:  uint64(0xcd7010bdb69b5a03),
	41:  uint64(0x455ef9fcd79b82f4),
	42:  uint64(0x869cb54a8749c161),
	43:  uint64(0x38d1a4fa6185d225),
	44:  uint64(0xb475166f94bbe9bb),
	45:  uint64(0xa4143548720959f1),
	46:  uint64(0x7aed4780ba6b26ba),
	47:  uint64(0xd0ce264439e02312),
	48:  uint64(0x84366d746078d508),
	49:  uint64(0xa8ce973c72ed17be),
	50:  uint64(0x21c323a29a430b01),
	51:  uint64(0x9962d617e3af80ee),
	52:  uint64(0xab0ce91d9c8cf75b),
	53:  uint64(0x530e8ee6d19a4dbc),
	54:  uint64(0x2ef68c0cf53f5d72),
	55:  uint64(0xc03a681640a85506),
	56:  uint64(0x496e4e9f9c310967),
	57:  uint64(0x78580472b59b14a0),
	58:  uint64(0x273824c23b388577),
	59:  uint64(0x66bf923ad45cb553),
	60:  uint64(0x47ae1a5a2492ba86),
	61:  uint64(0x35e304569e229659),
	62:  uint64(0x4765182a46870b6f),
	63:  uint64(0x6cbab625e9099412),
	64:  uint64(0xddac9a2e598522c1),
	65:  uint64(0x7172086e666624f2),
	66:  uint64(0xdf5003ca503b7837),
	67:  uint64(0x88c0c1db78563d09),
	68:  uint64(0x58d51865acfc289d),
	69:  uint64(0x177671aec65224f1),
	70:  uint64(0xfb79d8a241e967d7),
	71:  uint64(0x2be1e101cad9a49a),
	72:  uint64(0x6625682f6e29186b),
	73:  uint64(0x399553457ac06e50),
	74:  uint64(0x35dffb4c23abb74),
	75:  uint64(0x429db2591f54aade),
	76:  uint64(0xc52802a8037d1009),
	77:  uint64(0x6acb27381f0b25f3),
	78:  uint64(0xf45e2551ee4f823b),
	79:  uint64(0x8b0ea2d99580c2f7),
	80:  uint64(0x3bed519cbcb4e1e1),
	81:  uint64(0xff452823dbb010a),
	82:  uint64(0x9d42ed614f3dd267),
	83:  uint64(0x5b9313c06257c57b),
	84:  uint64(0xa114b8008b5e1442),
	85:  uint64(0xc1fe311c11c13d4b),
	86:  uint64(0x66e8763ea34c5568),
	87:  uint64(0x8b982af1c262f05d),
	88:  uint64(0xee8876faaa75fbb7),
	89:  uint64(0x8a62a4d0d172bb2a),
	90:  uint64(0xc13d94a3b7449a97),
	91:  uint64(0x6dbbba9dc15d037c),
	92:  uint64(0xc786101f1d92e0f1),
	93:  uint64(0xd78681a907a0b79b),
	94:  uint64(0xf61aaf2962c9abb9),
	95:  uint64(0x2cfd16fcd3cb7ad9),
	96:  uint64(0x868c5b6744624d21),
	97:  uint64(0x25e650899c74ddd7),
	98:  uint64(0xba042af4a7c37463),
	99:  uint64(0x4eb1a539465a3eca),
	100: uint64(0xbe09dbf03b05d5ca),
	101: uint64(0x774e5a362b5472ba),
	102: uint64(0x47a1221229d183cd),
	103: uint64(0x504b0ca18ef5a2df),
	104: uint64(0xdffbdfbde2456eb9),
	105: uint64(0x46cd2b2fbee34634),
	106: uint64(0xf2aef8fe819d98c3),
	107: uint64(0x357f5276d4599d61),
	108: uint64(0x24a5483879c453e3),
	109: uint64(0x88026889192b4b9),
	110: uint64(0x28da96671782dbec),
	111: uint64(0x4ef37c40588e9aaa),
	112: uint64(0x8837b90651bc9fb3),
	113: uint64(0xc164f741d3f0e5d6),
	114: uint64(0xbc135a0a704b70ba),
	115: uint64(0x69cd868f7622ada),
	116: uint64(0xbc37ba89e0b9c0ab),
	117: uint64(0x47c14a01323552f6),
	118: uint64(0x4f00794bacee98bb),
	119: uint64(0x7107de7d637a69d5),
	120: uint64(0x88af793bb6f2255e),
	121: uint64(0xf3c6466b8799b598),
	122: uint64(0xc288c616aa7f3b59),
	123: uint64(0x81ca63cf42fca3fd),
	124: uint64(0x88d85ace36a2674b),
	125: uint64(0xd056bd3792389e7),
	126: uint64(0xe55c396c4e9dd32d),
	127: uint64(0xbefb504571e6c0a6),
	128: uint64(0x96ab32115e91e8cc),
	129: uint64(0xbf8acb18de8f38d1),
	130: uint64(0x66dae58801672606),
	131: uint64(0x833b6017872317fb),
	132: uint64(0xb87c16f2d1c92864),
	133: uint64(0xdb766a74e58b669c),
	134: uint64(0x89659f85c61417be),
	135: uint64(0xc8daad856011ea0c),
	136: uint64(0x76a4b565b6fe7eae),
	137: uint64(0xa469d085f6237312),
	138: uint64(0xaaf0365683a3e96c),
	139: uint64(0x4dbb746f8424f7b8),
	140: uint64(0x638755af4e4acc1),
	141: uint64(0x3d7807f5bde64486),
	142: uint64(0x17be6d8f5bbb7639),
	143: uint64(0x903f0cd44dc35dc),
	144: uint64(0x67b672eafdf1196c),
	145: uint64(0xa676ff93ed4c82f1),
	146: uint64(0x521d1004c5053d9d),
	147: uint64(0x37ba9ad09ccc9202),
	148: uint64(0x84e54d297aacfb51),
	149: uint64(0xa0b4b776a143445),
	150: uint64(0x820d471e20b348e),
	151: uint64(0x1874383cb83d46dc),
	152: uint64(0x97edeec7a1efe11c),
	153: uint64(0xb330e50b1bdc42aa),
	154: uint64(0x1dd91955ce70e032),
	155: uint64(0xa514cdb88f2939d5),
	156: uint64(0x2791233fd90db9d3),
	157: uint64(0x7b670a4cc50f7a9b),
	158: uint64(0x77c07d2a05c6dfa5),
	159: uint64(0xe3778b6646d0a6fa),
	160: uint64(0xb39c8eda47b56749),
	161: uint64(0x933ed448addbef28),
	162: uint64(0xaf846af6ab7d0bf4),
	163: uint64(0xe5af208eb666e49),
	164: uint64(0x5e6622f73534cd6a),
	165: uint64(0x297daeca42ef5b6e),
	166: uint64(0x862daef3d35539a6),
	167: uint64(0xe68722498f8e1ea9),
	168: uint64(0x981c53093dc0d572),
	169: uint64(0xfa09b0bfbf86fbf5),
	170: uint64(0x30b1e96166219f15),
	171: uint64(0x70e7d466bdc4fb83),
	172: uint64(0x5a66736e35f2a8e9),
	173: uint64(0xcddb59d2b7c1baef),
	174: uint64(0xd6c7d247d26d8996),
	175: uint64(0xea4e39eac8de1ba3),
	176: uint64(0x539c8bb19fa3aff2),
	177: uint64(0x9f90e4c5fd508d8),
	178: uint64(0xa34e5956fbaf3385),
	179: uint64(0x2e2f8e151d3ef375),
	180: uint64(0x173691e9b83faec1),
	181: uint64(0xb85a8d56bf016379),
	182: uint64(0x8382381267408ae3),
	183: uint64(0xb90f901bbdc0096d),
	184: uint64(0x7c6ad32933bcec65),
	185: uint64(0x76bb5e2f2c8ad595),
	186: uint64(0x390f851a6cf46d28),
	187: uint64(0xc3e6064da1c2da72),
	188: uint64(0xc52a0c101cfa5389),
	189: uint64(0xd78eaf84a3fbc530),
	190: uint64(0x3781b9e2288b997e),
	191: uint64(0x73c2f6dea83d05c4),
	192: uint64(0x4228e364c5b5ed7),
	193: uint64(0x9d7a3edf0da43911),
	194: uint64(0x8edcfeda24686756),
	195: uint64(0x5e7667a7b7a9b3a1),
	196: uint64(0x4c4f389fa143791d),
	197: uint64(0xb08bc1023da7cddc),
	198: uint64(0x7ab4be3ae529b1cc),
	199: uint64(0x754e6132dbe74ff9),
	200: uint64(0x71635442a839df45),
	201: uint64(0x2f6fb1643fbe52de),
	202: uint64(0x961e0a42cf7a8177),
	203: uint64(0xf3b45d83d89ef2ea),
	204: uint64(0xee3de4cf4a6e3e9b),
	205: uint64(0xcd6848542c3295e7),
	206: uint64(0xe4cee1664c78662f),
	207: uint64(0x9947548b474c68c4),
	208: uint64(0x25d73777a5ed8b0b),
	209: uint64(0xc915b1d636b7fc),
	210: uint64(0x21c2ba75d9b0d2da),
	211: uint64(0x5f6b5dcf608a64a1),
	212: uint64(0xdcf333255ff9570c),
	213: uint64(0x633b922418ced4ee),
	214: uint64(0xc136dde0b004b34a),
	215: uint64(0x58cc83b05d4b2f5a),
	216: uint64(0x5eb424dda28e42d2),
	217: uint64(0x62df47369739cd98),
	218: uint64(0xb4e0b42485e4ce17),
	219: uint64(0x16e1f0c1f9a8d1e7),
	220: uint64(0x8ec3916707560ebf),
	221: uint64(0x62ba6e2df2cc9db3),
	222: uint64(0xcbf9f4ff77d83a16),
	223: uint64(0x78d9d7d07d2bbcc4),
	224: uint64(0xef554ce1e02c41f4),
	225: uint64(0x8d7581127eccf94d),
	226: uint64(0xa9b53336cb3c8a05),
	227: uint64(0x38c42c0bf45c4f91),
	228: uint64(0x640893cdf4488863),
	229: uint64(0x80ec34bc575ea568),
	230: uint64(0x39f324f5b48eaa40),
	231: uint64(0xe9d9ed1f8eff527f),
	232: uint64(0x9224fc058cc5a214),
	233: uint64(0xbaba00b04cfe7741),
	234: uint64(0x309a9f120fcf52af),
	235: uint64(0xa558f3ec65626212),
	236: uint64(0x424bec8b7adabe2f),
	237: uint64(0x41622513a6aea433),
	238: uint64(0xb88da2d5324ca798),
	239: uint64(0xd287733b245528a4),
	240: uint64(0x9a44697e6d68aec3),
	241: uint64(0x7b1093be2f49bb28),
	242: uint64(0x50bbec632e3d8aad),
	243: uint64(0x6cd90723e1ea8283),
	244: uint64(0x897b9e7431b02bf3),
	245: uint64(0x219efdcb338a7047),
	246: uint64(0x3b0311f0a27c0656),
	247: uint64(0xdb17bf91c0db96e7),
	248: uint64(0x8cd4fd6b4e85a5b2),
	249: uint64(0xfab071054ba6409d),
	250: uint64(0x40d6fe831fa9dfd9),
	251: uint64(0xaf358debad7d791e),
	252: uint64(0xeb8d0e25a65e3e58),
	253: uint64(0xbbcbd3df14e08580),
	254: uint64(0xcf751f27ecdab2b),
	255: uint64(0x2b4da14f2613d8f4),
}

/**** ended inlining zstd_ldm_geartab.h ****/

type ldmRollingHashState_t = struct {
	Frolling  U64
	FstopMask U64
}

// C documentation
//
//	/** ZSTD_ldm_gear_init():
//	 *
//	 * Initializes the rolling hash state such that it will honor the
//	 * settings in params. */
func ZSTD_ldm_gear_init(tls *libc.TLS, state uintptr, params uintptr) {
	var hashRateLog, maxBitsInMask, v1 uint32
	_, _, _ = hashRateLog, maxBitsInMask, v1
	if (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength < uint32(libc.Int32FromInt32(64)) {
		v1 = (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength
	} else {
		v1 = uint32(libc.Int32FromInt32(64))
	}
	maxBitsInMask = v1
	hashRateLog = (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog
	(*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling = uint64(^libc.Uint32FromInt32(0))
	/* The choice of the splitting criterion is subject to two conditions:
	 *   1. it has to trigger on average every 2^(hashRateLog) bytes;
	 *   2. ideally, it has to depend on a window of minMatchLength bytes.
	 *
	 * In the gear hash algorithm, bit n depends on the last n bytes;
	 * so in order to obtain a good quality splitting criterion it is
	 * preferable to use bits with high weight.
	 *
	 * To match condition 1 we use a mask with hashRateLog bits set
	 * and, because of the previous remark, we make sure these bits
	 * have the highest possible weight while still respecting
	 * condition 2.
	 */
	if hashRateLog > uint32(0) && hashRateLog <= maxBitsInMask {
		(*ldmRollingHashState_t)(unsafe.Pointer(state)).FstopMask = (libc.Uint64FromInt32(1)<<hashRateLog - uint64(1)) << (maxBitsInMask - hashRateLog)
	} else {
		/* In this degenerate case we simply honor the hash rate. */
		(*ldmRollingHashState_t)(unsafe.Pointer(state)).FstopMask = libc.Uint64FromInt32(1)<<hashRateLog - uint64(1)
	}
}

// C documentation
//
//	/** ZSTD_ldm_gear_reset()
//	 * Feeds [data, data + minMatchLength) into the hash without registering any
//	 * splits. This effectively resets the hash state. This is used when skipping
//	 * over data, either at the beginning of a block, or skipping sections.
//	 */
func ZSTD_ldm_gear_reset(tls *libc.TLS, state uintptr, data uintptr, minMatchLength size_t) {
	var hash U64
	var n size_t
	_, _ = hash, n
	hash = (*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling
	n = uint64(0)
	for n+uint64(3) < minMatchLength {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
	}
	for n < minMatchLength {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
	}
}

// C documentation
//
//	/** ZSTD_ldm_gear_feed():
//	 *
//	 * Registers in the splits array all the split points found in the first
//	 * size bytes following the data pointer. This function terminates when
//	 * either all the data has been processed or LDM_BATCH_SIZE splits are
//	 * present in the splits array.
//	 *
//	 * Precondition: The splits array must not be full.
//	 * Returns: The number of bytes processed. */
func ZSTD_ldm_gear_feed(tls *libc.TLS, state uintptr, data uintptr, size size_t, splits uintptr, numSplits uintptr) (r size_t) {
	var hash, mask U64
	var n size_t
	_, _, _ = hash, mask, n
	hash = (*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling
	mask = (*ldmRollingHashState_t)(unsafe.Pointer(state)).FstopMask
	n = uint64(0)
	for n+uint64(3) < size {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt32(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt32(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt32(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt32(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
	}
	for n < size {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[int32(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt32(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
	}
	goto done
done:
	;
	(*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling = hash
	return n
}

func ZSTD_ldm_adjustParameters(tls *libc.TLS, params uintptr, cParams uintptr) {
	var v1, v2, v3 uint32
	_, _, _ = v1, v2, v3
	(*ldmParams_t)(unsafe.Pointer(params)).FwindowLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	_ = libc.Uint64FromInt64(1)
	if (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog == uint32(0) {
		if (*ldmParams_t)(unsafe.Pointer(params)).FhashLog > uint32(0) {
			/* if params->hashLog is set, derive hashRateLog from it */
			if (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog > (*ldmParams_t)(unsafe.Pointer(params)).FhashLog {
				(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog = (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog - (*ldmParams_t)(unsafe.Pointer(params)).FhashLog
			}
		} else {
			/* mapping from [fast, rate7] to [btultra2, rate4] */
			(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog = uint32(int32(7) - (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy/int32(3))
		}
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FhashLog == uint32(0) {
		if (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog-(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog < uint32(libc.Int32FromInt32(30)) {
			v2 = (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog - (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog
		} else {
			v2 = uint32(libc.Int32FromInt32(30))
		}
		if uint32(libc.Int32FromInt32(ZSTD_HASHLOG_MIN)) > v2 {
			v1 = uint32(libc.Int32FromInt32(ZSTD_HASHLOG_MIN))
		} else {
			if (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog-(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog < uint32(libc.Int32FromInt32(30)) {
				v3 = (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog - (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog
			} else {
				v3 = uint32(libc.Int32FromInt32(30))
			}
			v1 = v3
		}
		(*ldmParams_t)(unsafe.Pointer(params)).FhashLog = v1
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength == uint32(0) {
		(*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength = uint32(LDM_MIN_MATCH_LENGTH)
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btultra) {
			*(*U32)(unsafe.Pointer(params + 12)) /= uint32(2)
		}
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog == uint32(0) {
		if uint32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy) < uint32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX)) {
			v2 = uint32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy)
		} else {
			v2 = uint32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX))
		}
		if uint32(libc.Int32FromInt32(LDM_BUCKET_SIZE_LOG)) > v2 {
			v1 = uint32(libc.Int32FromInt32(LDM_BUCKET_SIZE_LOG))
		} else {
			if uint32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy) < uint32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX)) {
				v3 = uint32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy)
			} else {
				v3 = uint32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX))
			}
			v1 = v3
		}
		(*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog = v1
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog < (*ldmParams_t)(unsafe.Pointer(params)).FhashLog {
		v1 = (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	} else {
		v1 = (*ldmParams_t)(unsafe.Pointer(params)).FhashLog
	}
	(*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog = v1
}

func ZSTD_ldm_getTableSize(tls *libc.TLS, params ldmParams_t) (r size_t) {
	var ldmBucketSize, ldmBucketSizeLog, ldmHSize, totalSize size_t
	var v1 uint32
	var v2 uint64
	_, _, _, _, _, _ = ldmBucketSize, ldmBucketSizeLog, ldmHSize, totalSize, v1, v2
	ldmHSize = libc.Uint64FromInt32(1) << params.FhashLog
	if params.FbucketSizeLog < params.FhashLog {
		v1 = params.FbucketSizeLog
	} else {
		v1 = params.FhashLog
	}
	ldmBucketSizeLog = uint64(v1)
	ldmBucketSize = libc.Uint64FromInt32(1) << (uint64(params.FhashLog) - ldmBucketSizeLog)
	totalSize = ZSTD_cwksp_alloc_size(tls, ldmBucketSize) + ZSTD_cwksp_alloc_size(tls, ldmHSize*uint64(8))
	if params.FenableLdm == int32(ZSTD_ps_enable) {
		v2 = totalSize
	} else {
		v2 = uint64(0)
	}
	return v2
}

func ZSTD_ldm_getMaxNbSeq(tls *libc.TLS, params ldmParams_t, maxChunkSize size_t) (r size_t) {
	var v1 uint64
	_ = v1
	if params.FenableLdm == int32(ZSTD_ps_enable) {
		v1 = maxChunkSize / uint64(params.FminMatchLength)
	} else {
		v1 = uint64(0)
	}
	return v1
}

// C documentation
//
//	/** ZSTD_ldm_getBucket() :
//	 *  Returns a pointer to the start of the bucket associated with hash. */
func ZSTD_ldm_getBucket(tls *libc.TLS, ldmState uintptr, hash size_t, bucketSizeLog U32) (r uintptr) {
	return (*ldmState_t)(unsafe.Pointer(ldmState)).FhashTable + uintptr(hash<<bucketSizeLog)*8
}

// C documentation
//
//	/** ZSTD_ldm_insertEntry() :
//	 *  Insert the entry with corresponding hash into the hash table */
func ZSTD_ldm_insertEntry(tls *libc.TLS, ldmState uintptr, hash size_t, entry ldmEntry_t, bucketSizeLog U32) {
	var offset uint32
	var pOffset uintptr
	_, _ = offset, pOffset
	pOffset = (*ldmState_t)(unsafe.Pointer(ldmState)).FbucketOffsets + uintptr(hash)
	offset = uint32(*(*BYTE)(unsafe.Pointer(pOffset)))
	*(*ldmEntry_t)(unsafe.Pointer(ZSTD_ldm_getBucket(tls, ldmState, hash, bucketSizeLog) + uintptr(offset)*8)) = entry
	*(*BYTE)(unsafe.Pointer(pOffset)) = uint8((offset + libc.Uint32FromInt32(1)) & (libc.Uint32FromUint32(1)<<bucketSizeLog - libc.Uint32FromInt32(1)))
}

// C documentation
//
//	/** ZSTD_ldm_countBackwardsMatch() :
//	 *  Returns the number of bytes that match backwards before pIn and pMatch.
//	 *
//	 *  We count only bytes where pMatch >= pBase and pIn >= pAnchor. */
func ZSTD_ldm_countBackwardsMatch(tls *libc.TLS, pIn uintptr, pAnchor uintptr, pMatch uintptr, pMatchBase uintptr) (r size_t) {
	var matchLength size_t
	_ = matchLength
	matchLength = uint64(0)
	for pIn > pAnchor && pMatch > pMatchBase && int32(*(*BYTE)(unsafe.Pointer(pIn + uintptr(-libc.Int32FromInt32(1))))) == int32(*(*BYTE)(unsafe.Pointer(pMatch + uintptr(-libc.Int32FromInt32(1))))) {
		pIn = pIn - 1
		pMatch = pMatch - 1
		matchLength = matchLength + 1
	}
	return matchLength
}

// C documentation
//
//	/** ZSTD_ldm_countBackwardsMatch_2segments() :
//	 *  Returns the number of bytes that match backwards from pMatch,
//	 *  even with the backwards match spanning 2 different segments.
//	 *
//	 *  On reaching `pMatchBase`, start counting from mEnd */
func ZSTD_ldm_countBackwardsMatch_2segments(tls *libc.TLS, pIn uintptr, pAnchor uintptr, pMatch uintptr, pMatchBase uintptr, pExtDictStart uintptr, pExtDictEnd uintptr) (r size_t) {
	var matchLength size_t
	_ = matchLength
	matchLength = ZSTD_ldm_countBackwardsMatch(tls, pIn, pAnchor, pMatch, pMatchBase)
	if pMatch-uintptr(matchLength) != pMatchBase || pMatchBase == pExtDictStart {
		/* If backwards match is entirely in the extDict or prefix, immediately return */
		return matchLength
	}
	matchLength = matchLength + ZSTD_ldm_countBackwardsMatch(tls, pIn-uintptr(matchLength), pAnchor, pExtDictEnd, pExtDictStart)
	return matchLength
}

// C documentation
//
//	/** ZSTD_ldm_fillFastTables() :
//	 *
//	 *  Fills the relevant tables for the ZSTD_fast and ZSTD_dfast strategies.
//	 *  This is similar to ZSTD_loadDictionaryContent.
//	 *
//	 *  The tables for the other strategies are filled within their
//	 *  block compressors. */
func ZSTD_ldm_fillFastTables(tls *libc.TLS, ms uintptr, end uintptr) (r size_t) {
	var iend uintptr
	_ = iend
	iend = end
	switch (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.Fstrategy {
	case int32(ZSTD_fast):
		ZSTD_fillHashTable(tls, ms, iend, int32(ZSTD_dtlm_fast), int32(ZSTD_tfp_forCCtx))
	case int32(ZSTD_dfast):
		ZSTD_fillDoubleHashTable(tls, ms, iend, int32(ZSTD_dtlm_fast), int32(ZSTD_tfp_forCCtx))
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		fallthrough
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
	default:
		/* not possible : not a valid strategy id */
	}
	return uint64(0)
}

func ZSTD_ldm_fillHashTable(tls *libc.TLS, ldmState uintptr, ip uintptr, iend uintptr, params uintptr) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var base, istart, split, splits uintptr
	var bucketSizeLog, hBits, hash, minMatchLength U32
	var entry ldmEntry_t
	var hashed size_t
	var n uint32
	var xxhash U64
	var _ /* hashState at bp+0 */ ldmRollingHashState_t
	var _ /* numSplits at bp+16 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _ = base, bucketSizeLog, entry, hBits, hash, hashed, istart, minMatchLength, n, split, splits, xxhash
	minMatchLength = (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength
	bucketSizeLog = (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	hBits = (*ldmParams_t)(unsafe.Pointer(params)).FhashLog - bucketSizeLog
	base = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.Fbase
	istart = ip
	splits = ldmState + 64
	ZSTD_ldm_gear_init(tls, bp, params)
	for ip < iend {
		*(*uint32)(unsafe.Pointer(bp + 16)) = uint32(0)
		hashed = ZSTD_ldm_gear_feed(tls, bp, ip, uint64(int64(iend)-int64(ip)), splits, bp+16)
		n = uint32(0)
		for {
			if !(n < *(*uint32)(unsafe.Pointer(bp + 16))) {
				break
			}
			if ip+uintptr(*(*size_t)(unsafe.Pointer(splits + uintptr(n)*8))) >= istart+uintptr(minMatchLength) {
				split = ip + uintptr(*(*size_t)(unsafe.Pointer(splits + uintptr(n)*8))) - uintptr(minMatchLength)
				xxhash = XXH_INLINE_XXH64(tls, split, uint64(minMatchLength), uint64(0))
				hash = uint32(xxhash & uint64(libc.Uint32FromInt32(1)<<hBits-libc.Uint32FromInt32(1)))
				entry.Foffset = uint32(int64(split) - int64(base))
				entry.Fchecksum = uint32(xxhash >> libc.Int32FromInt32(32))
				ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash), entry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
			}
			goto _1
		_1:
			;
			n = n + 1
		}
		ip = ip + uintptr(hashed)
	}
}

// C documentation
//
//	/** ZSTD_ldm_limitTableUpdate() :
//	 *
//	 *  Sets cctx->nextToUpdate to a position corresponding closer to anchor
//	 *  if it is far way
//	 *  (after a long match, only update tables a limited amount). */
func ZSTD_ldm_limitTableUpdate(tls *libc.TLS, ms uintptr, anchor uintptr) {
	var curr U32
	var v1 uint32
	_, _ = curr, v1
	curr = uint32(int64(anchor) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	if curr > (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate+uint32(1024) {
		if uint32(libc.Int32FromInt32(512)) < curr-(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate-uint32(1024) {
			v1 = uint32(libc.Int32FromInt32(512))
		} else {
			v1 = curr - (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate - uint32(1024)
		}
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr - v1
	}
}

func ZSTD_ldm_generateSequences_internal(tls *libc.TLS, ldmState uintptr, rawSeqStore uintptr, params uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var anchor, base, bestEntry, bucket, candidates, cur, curMatchBase, dictBase, dictEnd, dictStart, iend, ilimit, ip, istart, lowMatchPtr, lowPrefixPtr, matchEnd, pMatch, pMatch1, seq, split, split1, splits, v2, v3, v4 uintptr
	var backwardMatchLength, bestMatchLength, curBackwardMatchLength, curForwardMatchLength, curTotalMatchLength, forwardMatchLength, hashed, mLength size_t
	var checksum, dictLimit, entsPerBucket, hBits, hash, hash1, lowestIndex, minMatchLength, offset U32
	var extDict int32
	var n, v1 uint32
	var newEntry ldmEntry_t
	var xxhash U64
	var _ /* hashState at bp+0 */ ldmRollingHashState_t
	var _ /* numSplits at bp+16 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, backwardMatchLength, base, bestEntry, bestMatchLength, bucket, candidates, checksum, cur, curBackwardMatchLength, curForwardMatchLength, curMatchBase, curTotalMatchLength, dictBase, dictEnd, dictLimit, dictStart, entsPerBucket, extDict, forwardMatchLength, hBits, hash, hash1, hashed, iend, ilimit, ip, istart, lowMatchPtr, lowPrefixPtr, lowestIndex, mLength, matchEnd, minMatchLength, n, newEntry, offset, pMatch, pMatch1, seq, split, split1, splits, xxhash, v1, v2, v3, v4
	/* LDM parameters */
	extDict = int32(ZSTD_window_hasExtDict(tls, (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow))
	minMatchLength = (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength
	entsPerBucket = uint32(1) << (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	hBits = (*ldmParams_t)(unsafe.Pointer(params)).FhashLog - (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	/* Prefix and extDict parameters */
	dictLimit = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.FdictLimit
	if extDict != 0 {
		v1 = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.FlowLimit
	} else {
		v1 = dictLimit
	}
	lowestIndex = v1
	base = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.Fbase
	if extDict != 0 {
		v2 = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.FdictBase
	} else {
		v2 = libc.UintptrFromInt32(0)
	}
	dictBase = v2
	if extDict != 0 {
		v3 = dictBase + uintptr(lowestIndex)
	} else {
		v3 = libc.UintptrFromInt32(0)
	}
	dictStart = v3
	if extDict != 0 {
		v4 = dictBase + uintptr(dictLimit)
	} else {
		v4 = libc.UintptrFromInt32(0)
	}
	dictEnd = v4
	lowPrefixPtr = base + uintptr(dictLimit)
	/* Input bounds */
	istart = src
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	/* Input positions */
	anchor = istart
	ip = istart
	/* Arrays for staged-processing */
	splits = ldmState + 64
	candidates = ldmState + 576
	if srcSize < uint64(minMatchLength) {
		return uint64(int64(iend) - int64(anchor))
	}
	/* Initialize the rolling hash state with the first minMatchLength bytes */
	ZSTD_ldm_gear_init(tls, bp, params)
	ZSTD_ldm_gear_reset(tls, bp, ip, uint64(minMatchLength))
	ip = ip + uintptr(minMatchLength)
	for ip < ilimit {
		*(*uint32)(unsafe.Pointer(bp + 16)) = uint32(0)
		hashed = ZSTD_ldm_gear_feed(tls, bp, ip, uint64(int64(ilimit)-int64(ip)), splits, bp+16)
		n = uint32(0)
		for {
			if !(n < *(*uint32)(unsafe.Pointer(bp + 16))) {
				break
			}
			split = ip + uintptr(*(*size_t)(unsafe.Pointer(splits + uintptr(n)*8))) - uintptr(minMatchLength)
			xxhash = XXH_INLINE_XXH64(tls, split, uint64(minMatchLength), uint64(0))
			hash = uint32(xxhash & uint64(libc.Uint32FromInt32(1)<<hBits-libc.Uint32FromInt32(1)))
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fsplit = split
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fhash = hash
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fchecksum = uint32(xxhash >> libc.Int32FromInt32(32))
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fbucket = ZSTD_ldm_getBucket(tls, ldmState, uint64(hash), (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
			libc.X__builtin_prefetch(tls, (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fbucket, libc.VaList(bp+32, 0, int32(3)))
			goto _5
		_5:
			;
			n = n + 1
		}
		n = uint32(0)
		for {
			if !(n < *(*uint32)(unsafe.Pointer(bp + 16))) {
				break
			}
			forwardMatchLength = uint64(0)
			backwardMatchLength = uint64(0)
			bestMatchLength = uint64(0)
			split1 = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fsplit
			checksum = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fchecksum
			hash1 = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fhash
			bucket = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fbucket
			bestEntry = libc.UintptrFromInt32(0)
			newEntry.Foffset = uint32(int64(split1) - int64(base))
			newEntry.Fchecksum = checksum
			/* If a split point would generate a sequence overlapping with
			 * the previous one, we merely register it in the hash table and
			 * move on */
			if split1 < anchor {
				ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash1), newEntry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
				goto _6
			}
			cur = bucket
			for {
				if !(cur < bucket+uintptr(entsPerBucket)*8) {
					break
				}
				if (*ldmEntry_t)(unsafe.Pointer(cur)).Fchecksum != checksum || (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset <= lowestIndex {
					goto _7
				}
				if extDict != 0 {
					if (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset < dictLimit {
						v2 = dictBase
					} else {
						v2 = base
					}
					curMatchBase = v2
					pMatch = curMatchBase + uintptr((*ldmEntry_t)(unsafe.Pointer(cur)).Foffset)
					if (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset < dictLimit {
						v3 = dictEnd
					} else {
						v3 = iend
					}
					matchEnd = v3
					if (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset < dictLimit {
						v4 = dictStart
					} else {
						v4 = lowPrefixPtr
					}
					lowMatchPtr = v4
					curForwardMatchLength = ZSTD_count_2segments(tls, split1, pMatch, iend, matchEnd, lowPrefixPtr)
					if curForwardMatchLength < uint64(minMatchLength) {
						goto _7
					}
					curBackwardMatchLength = ZSTD_ldm_countBackwardsMatch_2segments(tls, split1, anchor, pMatch, lowMatchPtr, dictStart, dictEnd)
				} else { /* !extDict */
					pMatch1 = base + uintptr((*ldmEntry_t)(unsafe.Pointer(cur)).Foffset)
					curForwardMatchLength = ZSTD_count(tls, split1, pMatch1, iend)
					if curForwardMatchLength < uint64(minMatchLength) {
						goto _7
					}
					curBackwardMatchLength = ZSTD_ldm_countBackwardsMatch(tls, split1, anchor, pMatch1, lowPrefixPtr)
				}
				curTotalMatchLength = curForwardMatchLength + curBackwardMatchLength
				if curTotalMatchLength > bestMatchLength {
					bestMatchLength = curTotalMatchLength
					forwardMatchLength = curForwardMatchLength
					backwardMatchLength = curBackwardMatchLength
					bestEntry = cur
				}
				goto _7
			_7:
				;
				cur += 8
			}
			/* No match found -- insert an entry into the hash table
			 * and process the next candidate match */
			if bestEntry == libc.UintptrFromInt32(0) {
				ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash1), newEntry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
				goto _6
			}
			/* Match found */
			offset = uint32(int64(split1)-int64(base)) - (*ldmEntry_t)(unsafe.Pointer(bestEntry)).Foffset
			mLength = forwardMatchLength + backwardMatchLength
			seq = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize)*12
			/* Out of sequence storage */
			if (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize == (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fcapacity {
				return uint64(-int32(ZSTD_error_dstSize_tooSmall))
			}
			(*rawSeq)(unsafe.Pointer(seq)).FlitLength = uint32(int64(split1-uintptr(backwardMatchLength)) - int64(anchor))
			(*rawSeq)(unsafe.Pointer(seq)).FmatchLength = uint32(mLength)
			(*rawSeq)(unsafe.Pointer(seq)).Foffset = offset
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize + 1
			/* Insert the current entry into the hash table --- it must be
			 * done after the previous block to avoid clobbering bestEntry */
			ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash1), newEntry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
			anchor = split1 + uintptr(forwardMatchLength)
			/* If we find a match that ends after the data that we've hashed
			 * then we have a repeating, overlapping, pattern. E.g. all zeros.
			 * If one repetition of the pattern matches our `stopMask` then all
			 * repetitions will. We don't need to insert them all into out table,
			 * only the first one. So skip over overlapping matches.
			 * This is a major speed boost (20x) for compressing a single byte
			 * repeated, when that byte ends up in the table.
			 */
			if anchor > ip+uintptr(hashed) {
				ZSTD_ldm_gear_reset(tls, bp, anchor-uintptr(minMatchLength), uint64(minMatchLength))
				/* Continue the outer loop at anchor (ip + hashed == anchor). */
				ip = anchor - uintptr(hashed)
				break
			}
			goto _6
		_6:
			;
			n = n + 1
		}
		ip = ip + uintptr(hashed)
	}
	return uint64(int64(iend) - int64(anchor))
}

// C documentation
//
//	/*! ZSTD_ldm_reduceTable() :
//	 *  reduce table indexes by `reducerValue` */
func ZSTD_ldm_reduceTable(tls *libc.TLS, table uintptr, size U32, reducerValue U32) {
	var u U32
	_ = u
	u = uint32(0)
	for {
		if !(u < size) {
			break
		}
		if (*(*ldmEntry_t)(unsafe.Pointer(table + uintptr(u)*8))).Foffset < reducerValue {
			(*(*ldmEntry_t)(unsafe.Pointer(table + uintptr(u)*8))).Foffset = uint32(0)
		} else {
			(*(*ldmEntry_t)(unsafe.Pointer(table + uintptr(u)*8))).Foffset -= reducerValue
		}
		goto _1
	_1:
		;
		u = u + 1
	}
}

func ZSTD_ldm_generateSequences(tls *libc.TLS, ldmState uintptr, sequences uintptr, params uintptr, src uintptr, srcSize size_t) (r size_t) {
	var chunk, chunkSize, kMaxChunkSize, leftoverSize, nbChunks, newLeftoverSize, prevSize, remaining size_t
	var chunkEnd, chunkStart, iend, istart, v2 uintptr
	var correction, ldmHSize, maxDist U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = chunk, chunkEnd, chunkSize, chunkStart, correction, iend, istart, kMaxChunkSize, ldmHSize, leftoverSize, maxDist, nbChunks, newLeftoverSize, prevSize, remaining, v2
	maxDist = uint32(1) << (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog
	istart = src
	iend = istart + uintptr(srcSize)
	kMaxChunkSize = uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
	nbChunks = srcSize/kMaxChunkSize + libc.BoolUint64(srcSize%kMaxChunkSize != libc.Uint64FromInt32(0))
	leftoverSize = uint64(0)
	/* Check that ZSTD_window_update() has been called for this chunk prior
	 * to passing it to this function.
	 */
	/* The input could be very large (in zstdmt), so it must be broken up into
	 * chunks to enforce the maximum distance and handle overflow correction.
	 */
	chunk = uint64(0)
	for {
		if !(chunk < nbChunks && (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fsize < (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fcapacity) {
			break
		}
		chunkStart = istart + uintptr(chunk*kMaxChunkSize)
		remaining = uint64(int64(iend) - int64(chunkStart))
		if remaining < kMaxChunkSize {
			v2 = iend
		} else {
			v2 = chunkStart + uintptr(kMaxChunkSize)
		}
		chunkEnd = v2
		chunkSize = uint64(int64(chunkEnd) - int64(chunkStart))
		prevSize = (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fsize
		/* 1. Perform overflow correction if necessary. */
		if ZSTD_window_needOverflowCorrection(tls, (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow, uint32(0), maxDist, (*ldmState_t)(unsafe.Pointer(ldmState)).FloadedDictEnd, chunkStart, chunkEnd) != 0 {
			ldmHSize = uint32(1) << (*ldmParams_t)(unsafe.Pointer(params)).FhashLog
			correction = ZSTD_window_correctOverflow(tls, ldmState, uint32(0), maxDist, chunkStart)
			ZSTD_ldm_reduceTable(tls, (*ldmState_t)(unsafe.Pointer(ldmState)).FhashTable, ldmHSize, correction)
			/* invalidate dictionaries on overflow correction */
			(*ldmState_t)(unsafe.Pointer(ldmState)).FloadedDictEnd = uint32(0)
		}
		/* 2. We enforce the maximum offset allowed.
		 *
		 * kMaxChunkSize should be small enough that we don't lose too much of
		 * the window through early invalidation.
		 * TODO: * Test the chunk size.
		 *       * Try invalidation after the sequence generation and test the
		 *         offset against maxDist directly.
		 *
		 * NOTE: Because of dictionaries + sequence splitting we MUST make sure
		 * that any offset used is valid at the END of the sequence, since it may
		 * be split into two sequences. This condition holds when using
		 * ZSTD_window_enforceMaxDist(), but if we move to checking offsets
		 * against maxDist directly, we'll have to carefully handle that case.
		 */
		ZSTD_window_enforceMaxDist(tls, ldmState, chunkEnd, maxDist, ldmState+48, libc.UintptrFromInt32(0))
		/* 3. Generate the sequences for the chunk, and get newLeftoverSize. */
		newLeftoverSize = ZSTD_ldm_generateSequences_internal(tls, ldmState, sequences, params, chunkStart, chunkSize)
		if ZSTD_isError(tls, newLeftoverSize) != 0 {
			return newLeftoverSize
		}
		/* 4. We add the leftover literals from previous iterations to the first
		 *    newly generated sequence, or add the `newLeftoverSize` if none are
		 *    generated.
		 */
		/* Prepend the leftover literals from the last call */
		if prevSize < (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fsize {
			(*(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(sequences)).Fseq + uintptr(prevSize)*12))).FlitLength += uint32(leftoverSize)
			leftoverSize = newLeftoverSize
		} else {
			leftoverSize = leftoverSize + chunkSize
		}
		goto _1
	_1:
		;
		chunk = chunk + 1
	}
	return uint64(0)
}

func ZSTD_ldm_skipSequences(tls *libc.TLS, rawSeqStore uintptr, srcSize size_t, minMatch U32) {
	var seq uintptr
	_ = seq
	for srcSize > uint64(0) && (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		seq = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12
		if srcSize <= uint64((*rawSeq)(unsafe.Pointer(seq)).FlitLength) {
			/* Skip past srcSize literals */
			*(*U32)(unsafe.Pointer(seq + 4)) -= uint32(srcSize)
			return
		}
		srcSize = srcSize - uint64((*rawSeq)(unsafe.Pointer(seq)).FlitLength)
		(*rawSeq)(unsafe.Pointer(seq)).FlitLength = uint32(0)
		if srcSize < uint64((*rawSeq)(unsafe.Pointer(seq)).FmatchLength) {
			/* Skip past the first srcSize of the match */
			*(*U32)(unsafe.Pointer(seq + 8)) -= uint32(srcSize)
			if (*rawSeq)(unsafe.Pointer(seq)).FmatchLength < minMatch {
				/* The match is too short, omit it */
				if (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos+uint64(1) < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
					(*(*rawSeq)(unsafe.Pointer(seq + 1*12))).FlitLength += (*(*rawSeq)(unsafe.Pointer(seq))).FmatchLength
				}
				(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
			}
			return
		}
		srcSize = srcSize - uint64((*rawSeq)(unsafe.Pointer(seq)).FmatchLength)
		(*rawSeq)(unsafe.Pointer(seq)).FmatchLength = uint32(0)
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
	}
}

// C documentation
//
//	/**
//	 * If the sequence length is longer than remaining then the sequence is split
//	 * between this block and the next.
//	 *
//	 * Returns the current sequence to handle, or if the rest of the block should
//	 * be literals, it returns a sequence with offset == 0.
//	 */
func maybeSplitSequence(tls *libc.TLS, rawSeqStore uintptr, remaining U32, minMatch U32) (r rawSeq) {
	var sequence rawSeq
	_ = sequence
	sequence = *(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12))
	/* Likely: No partial sequence */
	if remaining >= sequence.FlitLength+sequence.FmatchLength {
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
		return sequence
	}
	/* Cut the sequence short (offset == 0 ==> rest is literals). */
	if remaining <= sequence.FlitLength {
		sequence.Foffset = uint32(0)
	} else {
		if remaining < sequence.FlitLength+sequence.FmatchLength {
			sequence.FmatchLength = remaining - sequence.FlitLength
			if sequence.FmatchLength < minMatch {
				sequence.Foffset = uint32(0)
			}
		}
	}
	/* Skip past `remaining` bytes for the future sequences. */
	ZSTD_ldm_skipSequences(tls, rawSeqStore, uint64(remaining), minMatch)
	return sequence
}

func ZSTD_ldm_skipRawSeqStoreBytes(tls *libc.TLS, rawSeqStore uintptr, nbBytes size_t) {
	var currPos U32
	var currSeq rawSeq
	_, _ = currPos, currSeq
	currPos = uint32((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence + nbBytes)
	for currPos != 0 && (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		currSeq = *(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12))
		if currPos >= currSeq.FlitLength+currSeq.FmatchLength {
			currPos = currPos - (currSeq.FlitLength + currSeq.FmatchLength)
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
		} else {
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(currPos)
			break
		}
	}
	if currPos == uint32(0) || (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos == (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(0)
	}
}

func ZSTD_ldm_blockCompress(tls *libc.TLS, rawSeqStore uintptr, ms uintptr, seqStore uintptr, rep uintptr, useRowMatchFinder ZSTD_ParamSwitch_e, src uintptr, srcSize size_t) (r size_t) {
	var blockCompressor ZSTD_BlockCompressor_f
	var cParams, iend, ip, istart uintptr
	var i int32
	var lastLLSize, newLitLength size_t
	var minMatch uint32
	var sequence rawSeq
	_, _, _, _, _, _, _, _, _, _ = blockCompressor, cParams, i, iend, ip, istart, lastLLSize, minMatch, newLitLength, sequence
	cParams = ms + 256
	minMatch = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	blockCompressor = ZSTD_selectBlockCompressor(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder, ZSTD_matchState_dictMode(tls, ms))
	/* Input bounds */
	istart = src
	iend = istart + uintptr(srcSize)
	/* Input positions */
	ip = istart
	/* If using opt parser, use LDMs only as candidates rather than always accepting them */
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) {
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = rawSeqStore
		lastLLSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, seqStore, rep, src, srcSize)
		ZSTD_ldm_skipRawSeqStoreBytes(tls, rawSeqStore, srcSize)
		return lastLLSize
	}
	/* Loop through each sequence and apply the block compressor to the literals */
	for (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize && ip < iend {
		/* maybeSplitSequence updates rawSeqStore->pos */
		sequence = maybeSplitSequence(tls, rawSeqStore, uint32(int64(iend)-int64(ip)), minMatch)
		/* End signal */
		if sequence.Foffset == uint32(0) {
			break
		}
		/* Fill tables for block compressor */
		ZSTD_ldm_limitTableUpdate(tls, ms, ip)
		ZSTD_ldm_fillFastTables(tls, ms, ip)
		/* Run the block compressor */
		newLitLength = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, seqStore, rep, ip, uint64(sequence.FlitLength))
		ip = ip + uintptr(sequence.FlitLength)
		/* Update the repcodes */
		i = libc.Int32FromInt32(ZSTD_REP_NUM) - libc.Int32FromInt32(1)
		for {
			if !(i > 0) {
				break
			}
			*(*U32)(unsafe.Pointer(rep + uintptr(i)*4)) = *(*U32)(unsafe.Pointer(rep + uintptr(i-int32(1))*4))
			goto _1
		_1:
			;
			i = i - 1
		}
		*(*U32)(unsafe.Pointer(rep)) = sequence.Foffset
		/* Store the sequence */
		ZSTD_storeSeq(tls, seqStore, newLitLength, ip-uintptr(newLitLength), iend, sequence.Foffset+libc.Uint32FromInt32(ZSTD_REP_NUM), uint64(sequence.FmatchLength))
		ip = ip + uintptr(sequence.FmatchLength)
	}
	/* Fill the tables for the block compressor */
	ZSTD_ldm_limitTableUpdate(tls, ms, ip)
	ZSTD_ldm_fillFastTables(tls, ms, ip)
	/* Compress the last literals */
	return (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, seqStore, rep, ip, uint64(int64(iend)-int64(ip)))
}

/**** ended inlining compress/zstd_ldm.c ****/
/**** start inlining compress/zstd_opt.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: zstd_opt.h ****/

/*-*************************************
*  Price functions for optimal parser
***************************************/

// C documentation
//
//	/* ZSTD_bitWeight() :
//	 * provide estimated "cost" of a stat in full bits only */
func ZSTD_bitWeight(tls *libc.TLS, stat U32) (r U32) {
	return ZSTD_highbit32(tls, stat+uint32(1)) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
}

// C documentation
//
//	/* ZSTD_fracWeight() :
//	 * provide fractional-bit "cost" of a stat,
//	 * using linear interpolation approximation */
func ZSTD_fracWeight(tls *libc.TLS, rawStat U32) (r U32) {
	var BWeight, FWeight, hb, stat, weight U32
	_, _, _, _, _ = BWeight, FWeight, hb, stat, weight
	stat = rawStat + uint32(1)
	hb = ZSTD_highbit32(tls, stat)
	BWeight = hb * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	/* Fweight was meant for "Fractional weight"
	 * but it's effectively a value between 1 and 2
	 * using fixed point arithmetic */
	FWeight = stat << libc.Int32FromInt32(BITCOST_ACCURACY) >> hb
	weight = BWeight + FWeight
	return weight
}

func ZSTD_compressedLiterals(tls *libc.TLS, optPtr uintptr) (r int32) {
	return libc.BoolInt32((*optState_t)(unsafe.Pointer(optPtr)).FliteralCompressionMode != int32(ZSTD_ps_disable))
}

func ZSTD_setBasePrices(tls *libc.TLS, optPtr uintptr, optLevel int32) {
	var v1 uint32
	_ = v1
	if ZSTD_compressedLiterals(tls, optPtr) != 0 {
		if optLevel != 0 {
			v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitSum)
		} else {
			v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitSum)
		}
		(*optState_t)(unsafe.Pointer(optPtr)).FlitSumBasePrice = v1
	}
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum)
	} else {
		v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum)
	}
	(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSumBasePrice = v1
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum)
	} else {
		v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum)
	}
	(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSumBasePrice = v1
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum)
	} else {
		v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum)
	}
	(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSumBasePrice = v1
}

func sum_u32(tls *libc.TLS, table uintptr, nbElts size_t) (r U32) {
	var n size_t
	var total U32
	_, _ = n, total
	total = uint32(0)
	n = uint64(0)
	for {
		if !(n < nbElts) {
			break
		}
		total = total + *(*uint32)(unsafe.Pointer(table + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	return total
}

type base_directive_e = int32

const base_0possible = 0
const base_1guaranteed = 1

func ZSTD_downscaleStats(tls *libc.TLS, table uintptr, lastEltIndex U32, shift U32, base1 base_directive_e) (r U32) {
	var base, newStat uint32
	var s, sum U32
	var v2 int32
	_, _, _, _, _ = base, newStat, s, sum, v2
	sum = uint32(0)
	s = uint32(0)
	for {
		if !(s < lastEltIndex+uint32(1)) {
			break
		}
		if base1 != 0 {
			v2 = int32(1)
		} else {
			v2 = libc.BoolInt32(*(*uint32)(unsafe.Pointer(table + uintptr(s)*4)) > uint32(0))
		}
		base = uint32(v2)
		newStat = base + *(*uint32)(unsafe.Pointer(table + uintptr(s)*4))>>shift
		sum = sum + newStat
		*(*uint32)(unsafe.Pointer(table + uintptr(s)*4)) = newStat
		goto _1
	_1:
		;
		s = s + 1
	}
	return sum
}

// C documentation
//
//	/* ZSTD_scaleStats() :
//	 * reduce all elt frequencies in table if sum too large
//	 * return the resulting sum of elements */
func ZSTD_scaleStats(tls *libc.TLS, table uintptr, lastEltIndex U32, logTarget U32) (r U32) {
	var factor, prevsum U32
	_, _ = factor, prevsum
	prevsum = sum_u32(tls, table, uint64(lastEltIndex+uint32(1)))
	factor = prevsum >> logTarget
	if factor <= uint32(1) {
		return prevsum
	}
	return ZSTD_downscaleStats(tls, table, lastEltIndex, ZSTD_highbit32(tls, factor), int32(base_1guaranteed))
}

// C documentation
//
//	/* ZSTD_rescaleFreqs() :
//	 * if first block (detected by optPtr->litLengthSum == 0) : init statistics
//	 *    take hints from dictionary if there is one
//	 *    and init from zero if there is none,
//	 *    using src for literals stats, and baseline stats for sequence symbols
//	 * otherwise downscale existing stats, to be used as seed for next block.
//	 */
func ZSTD_rescaleFreqs(tls *libc.TLS, optPtr uintptr, src uintptr, srcSize size_t, optLevel int32) {
	bp := tls.Alloc(384)
	defer tls.Free(384)
	var bitCost, bitCost1, bitCost2, bitCost3, scaleLog, scaleLog1, scaleLog2, scaleLog3 U32
	var compressedLiterals, v2 int32
	var lit, ll, ml, ml1, of uint32
	var _ /* baseLLfreqs at bp+100 */ [36]uint32
	var _ /* baseOFCfreqs at bp+244 */ [32]uint32
	var _ /* lit at bp+96 */ uint32
	var _ /* llstate at bp+0 */ FSE_CState_t
	var _ /* mlstate at bp+32 */ FSE_CState_t
	var _ /* ofstate at bp+64 */ FSE_CState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bitCost, bitCost1, bitCost2, bitCost3, compressedLiterals, lit, ll, ml, ml1, of, scaleLog, scaleLog1, scaleLog2, scaleLog3, v2
	compressedLiterals = ZSTD_compressedLiterals(tls, optPtr)
	(*optState_t)(unsafe.Pointer(optPtr)).FpriceType = int32(zop_dynamic)
	if (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum == uint32(0) { /* no literals stats collected -> first block assumed -> init */
		/* heuristic: use pre-defined stats for too small inputs */
		if srcSize <= uint64(ZSTD_PREDEF_THRESHOLD) {
			(*optState_t)(unsafe.Pointer(optPtr)).FpriceType = int32(zop_predef)
		}
		if (*ZSTD_entropyCTables_t)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts)).Fhuf.FrepeatMode == int32(HUF_repeat_valid) {
			/* huffman stats covering the full value set : table presumed generated by dictionary */
			(*optState_t)(unsafe.Pointer(optPtr)).FpriceType = int32(zop_dynamic)
			if compressedLiterals != 0 {
				(*optState_t)(unsafe.Pointer(optPtr)).FlitSum = uint32(0)
				lit = uint32(0)
				for {
					if !(lit <= uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits)-libc.Int32FromInt32(1))) {
						break
					}
					scaleLog = uint32(11) /* scale to 2K */
					bitCost = HUF_getNbBitsFromCTable(tls, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts, lit)
					if bitCost != 0 {
						v2 = int32(1) << (scaleLog - bitCost)
					} else {
						v2 = int32(1)
					}
					*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(lit)*4)) = uint32(v2)
					*(*U32)(unsafe.Pointer(optPtr + 48)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(lit)*4))
					goto _1
				_1:
					;
					lit = lit + 1
				}
			}
			FSE_initCState(tls, bp, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts+2064+2224)
			(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = uint32(0)
			ll = uint32(0)
			for {
				if !(ll <= uint32(MaxLL)) {
					break
				}
				scaleLog1 = uint32(10) /* scale to 1K */
				bitCost1 = FSE_getMaxNbBits(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp))).FsymbolTT, ll)
				if bitCost1 != 0 {
					v2 = int32(1) << (scaleLog1 - bitCost1)
				} else {
					v2 = int32(1)
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(ll)*4)) = uint32(v2)
				*(*U32)(unsafe.Pointer(optPtr + 52)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(ll)*4))
				goto _3
			_3:
				;
				ll = ll + 1
			}
			FSE_initCState(tls, bp+32, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts+2064+772)
			(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = uint32(0)
			ml = uint32(0)
			for {
				if !(ml <= uint32(MaxML)) {
					break
				}
				scaleLog2 = uint32(10)
				bitCost2 = FSE_getMaxNbBits(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp + 32))).FsymbolTT, ml)
				if bitCost2 != 0 {
					v2 = int32(1) << (scaleLog2 - bitCost2)
				} else {
					v2 = int32(1)
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(ml)*4)) = uint32(v2)
				*(*U32)(unsafe.Pointer(optPtr + 56)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(ml)*4))
				goto _5
			_5:
				;
				ml = ml + 1
			}
			FSE_initCState(tls, bp+64, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts+2064)
			(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = uint32(0)
			of = uint32(0)
			for {
				if !(of <= uint32(MaxOff)) {
					break
				}
				scaleLog3 = uint32(10)
				bitCost3 = FSE_getMaxNbBits(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp + 64))).FsymbolTT, of)
				if bitCost3 != 0 {
					v2 = int32(1) << (scaleLog3 - bitCost3)
				} else {
					v2 = int32(1)
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(of)*4)) = uint32(v2)
				*(*U32)(unsafe.Pointer(optPtr + 60)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(of)*4))
				goto _7
			_7:
				;
				of = of + 1
			}
		} else { /* first block, no dictionary */
			if compressedLiterals != 0 {
				/* base initial cost of literals on direct frequency within src */
				*(*uint32)(unsafe.Pointer(bp + 96)) = uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits) - libc.Int32FromInt32(1))
				HIST_count_simple(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitFreq, bp+96, src, srcSize) /* use raw first block to init statistics */
				(*optState_t)(unsafe.Pointer(optPtr)).FlitSum = ZSTD_downscaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitFreq, uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits)-libc.Int32FromInt32(1)), uint32(8), int32(base_0possible))
			}
			*(*[36]uint32)(unsafe.Pointer(bp + 100)) = [36]uint32{
				0:  uint32(4),
				1:  uint32(2),
				2:  uint32(1),
				3:  uint32(1),
				4:  uint32(1),
				5:  uint32(1),
				6:  uint32(1),
				7:  uint32(1),
				8:  uint32(1),
				9:  uint32(1),
				10: uint32(1),
				11: uint32(1),
				12: uint32(1),
				13: uint32(1),
				14: uint32(1),
				15: uint32(1),
				16: uint32(1),
				17: uint32(1),
				18: uint32(1),
				19: uint32(1),
				20: uint32(1),
				21: uint32(1),
				22: uint32(1),
				23: uint32(1),
				24: uint32(1),
				25: uint32(1),
				26: uint32(1),
				27: uint32(1),
				28: uint32(1),
				29: uint32(1),
				30: uint32(1),
				31: uint32(1),
				32: uint32(1),
				33: uint32(1),
				34: uint32(1),
				35: uint32(1),
			}
			libc.Xmemcpy(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq, bp+100, libc.Uint64FromInt64(144))
			(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = sum_u32(tls, bp+100, uint64(libc.Int32FromInt32(MaxLL)+libc.Int32FromInt32(1)))
			ml1 = uint32(0)
			for {
				if !(ml1 <= uint32(MaxML)) {
					break
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(ml1)*4)) = uint32(1)
				goto _9
			_9:
				;
				ml1 = ml1 + 1
			}
			(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = uint32(libc.Int32FromInt32(MaxML) + libc.Int32FromInt32(1))
			*(*[32]uint32)(unsafe.Pointer(bp + 244)) = [32]uint32{
				0:  uint32(6),
				1:  uint32(2),
				2:  uint32(1),
				3:  uint32(1),
				4:  uint32(2),
				5:  uint32(3),
				6:  uint32(4),
				7:  uint32(4),
				8:  uint32(4),
				9:  uint32(3),
				10: uint32(2),
				11: uint32(1),
				12: uint32(1),
				13: uint32(1),
				14: uint32(1),
				15: uint32(1),
				16: uint32(1),
				17: uint32(1),
				18: uint32(1),
				19: uint32(1),
				20: uint32(1),
				21: uint32(1),
				22: uint32(1),
				23: uint32(1),
				24: uint32(1),
				25: uint32(1),
				26: uint32(1),
				27: uint32(1),
				28: uint32(1),
				29: uint32(1),
				30: uint32(1),
				31: uint32(1),
			}
			libc.Xmemcpy(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq, bp+244, libc.Uint64FromInt64(128))
			(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = sum_u32(tls, bp+244, uint64(libc.Int32FromInt32(MaxOff)+libc.Int32FromInt32(1)))
		}
	} else { /* new block : scale down accumulated statistics */
		if compressedLiterals != 0 {
			(*optState_t)(unsafe.Pointer(optPtr)).FlitSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitFreq, uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits)-libc.Int32FromInt32(1)), uint32(12))
		}
		(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq, uint32(MaxLL), uint32(11))
		(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq, uint32(MaxML), uint32(11))
		(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq, uint32(MaxOff), uint32(11))
	}
	ZSTD_setBasePrices(tls, optPtr, optLevel)
}

// C documentation
//
//	/* ZSTD_rawLiteralsCost() :
//	 * price of literals (only) in specified segment (which length can be 0).
//	 * does not include price of literalLength symbol */
func ZSTD_rawLiteralsCost(tls *libc.TLS, literals uintptr, litLength U32, optPtr uintptr, optLevel int32) (r U32) {
	var litPrice, litPriceMax, price, u U32
	var v2 uint32
	_, _, _, _, _ = litPrice, litPriceMax, price, u, v2
	if litLength == uint32(0) {
		return uint32(0)
	}
	if !(ZSTD_compressedLiterals(tls, optPtr) != 0) {
		return litLength << libc.Int32FromInt32(3) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* Uncompressed - 8 bytes per literal. */
	if (*optState_t)(unsafe.Pointer(optPtr)).FpriceType == int32(zop_predef) {
		return litLength * uint32(6) * uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* 6 bit per literal - no statistic used */
	/* dynamic statistics */
	price = (*optState_t)(unsafe.Pointer(optPtr)).FlitSumBasePrice * litLength
	litPriceMax = (*optState_t)(unsafe.Pointer(optPtr)).FlitSumBasePrice - uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	u = uint32(0)
	for {
		if !(u < litLength) {
			break
		}
		if optLevel != 0 {
			v2 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(*(*BYTE)(unsafe.Pointer(literals + uintptr(u))))*4)))
		} else {
			v2 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(*(*BYTE)(unsafe.Pointer(literals + uintptr(u))))*4)))
		}
		litPrice = v2
		if libc.BoolInt32(litPrice > litPriceMax) != 0 {
			litPrice = litPriceMax
		}
		price = price - litPrice
		goto _1
	_1:
		;
		u = u + 1
	}
	return price
	return r
}

// C documentation
//
//	/* ZSTD_litLengthPrice() :
//	 * cost of literalLength symbol */
func ZSTD_litLengthPrice(tls *libc.TLS, litLength U32, optPtr uintptr, optLevel int32) (r U32) {
	var llCode U32
	var v1 uint32
	_, _ = llCode, v1
	if (*optState_t)(unsafe.Pointer(optPtr)).FpriceType == int32(zop_predef) {
		if optLevel != 0 {
			v1 = ZSTD_fracWeight(tls, litLength)
		} else {
			v1 = ZSTD_bitWeight(tls, litLength)
		}
		return v1
	}
	/* ZSTD_LLcode() can't compute litLength price for sizes >= ZSTD_BLOCKSIZE_MAX
	 * because it isn't representable in the zstd format.
	 * So instead just pretend it would cost 1 bit more than ZSTD_BLOCKSIZE_MAX - 1.
	 * In such a case, the block would be all literals.
	 */
	if litLength == uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		return uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)) + ZSTD_litLengthPrice(tls, uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)-libc.Int32FromInt32(1)), optPtr, optLevel)
	}
	/* dynamic statistics */
	llCode = ZSTD_LLcode(tls, litLength)
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)))
	} else {
		v1 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)))
	}
	return uint32(int32(LL_bits[llCode])*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))) + (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSumBasePrice - v1
	return r
}

// C documentation
//
//	/* ZSTD_getMatchPrice() :
//	 * Provides the cost of the match part (offset + matchLength) of a sequence.
//	 * Must be combined with ZSTD_fullLiteralsCost() to get the full cost of a sequence.
//	 * @offBase : sumtype, representing an offset or a repcode, and using numeric representation of ZSTD_storeSeq()
//	 * @optLevel: when <2, favors small offset for decompression speed (improved cache efficiency)
//	 */
func ZSTD_getMatchPrice(tls *libc.TLS, offBase U32, matchLength U32, optPtr uintptr, optLevel int32) (r U32) {
	var mlBase, mlCode, offCode, price U32
	var v1 uint32
	_, _, _, _, _ = mlBase, mlCode, offCode, price, v1
	offCode = ZSTD_highbit32(tls, offBase)
	mlBase = matchLength - uint32(MINMATCH)
	if (*optState_t)(unsafe.Pointer(optPtr)).FpriceType == int32(zop_predef) { /* fixed scheme, does not use statistics */
		if optLevel != 0 {
			v1 = ZSTD_fracWeight(tls, mlBase)
		} else {
			v1 = ZSTD_bitWeight(tls, mlBase)
		}
		return v1 + (uint32(16)+offCode)*uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* emulated offset cost */
	/* dynamic statistics */
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)))
	} else {
		v1 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)))
	}
	price = offCode*uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)) + ((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSumBasePrice - v1)
	if optLevel < int32(2) && offCode >= uint32(20) {
		price = price + (offCode-uint32(19))*uint32(2)*uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* handicap for long distance offsets, favor decompression speed */
	/* match Length */
	mlCode = ZSTD_MLcode(tls, mlBase)
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)))
	} else {
		v1 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)))
	}
	price = price + (uint32(int32(ML_bits[mlCode])*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))) + ((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSumBasePrice - v1))
	price = price + uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)/libc.Int32FromInt32(5)) /* heuristic : make matches a bit more costly to favor less sequences -> faster decompression speed */
	return price
}

// C documentation
//
//	/* ZSTD_updateStats() :
//	 * assumption : literals + litLength <= iend */
func ZSTD_updateStats(tls *libc.TLS, optPtr uintptr, litLength U32, literals uintptr, offBase U32, matchLength U32) {
	var llCode, mlBase, mlCode, offCode, u U32
	_, _, _, _, _ = llCode, mlBase, mlCode, offCode, u
	/* literals */
	if ZSTD_compressedLiterals(tls, optPtr) != 0 {
		u = uint32(0)
		for {
			if !(u < litLength) {
				break
			}
			*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(*(*BYTE)(unsafe.Pointer(literals + uintptr(u))))*4)) += uint32(ZSTD_LITFREQ_ADD)
			goto _1
		_1:
			;
			u = u + 1
		}
		*(*U32)(unsafe.Pointer(optPtr + 48)) += litLength * uint32(ZSTD_LITFREQ_ADD)
	}
	/* literal Length */
	llCode = ZSTD_LLcode(tls, litLength)
	*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)) = *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)) + 1
	(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum + 1
	/* offset code : follows storeSeq() numeric representation */
	offCode = ZSTD_highbit32(tls, offBase)
	*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)) = *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)) + 1
	(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum + 1
	/* match Length */
	mlBase = matchLength - uint32(MINMATCH)
	mlCode = ZSTD_MLcode(tls, mlBase)
	*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)) = *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)) + 1
	(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum + 1
}

// C documentation
//
//	/* ZSTD_readMINMATCH() :
//	 * function safe only for comparisons
//	 * assumption : memPtr must be at least 4 bytes before end of buffer */
func ZSTD_readMINMATCH(tls *libc.TLS, memPtr uintptr, length U32) (r U32) {
	switch length {
	default:
		fallthrough
	case uint32(4):
		return MEM_read32(tls, memPtr)
	case uint32(3):
		if MEM_isLittleEndian(tls) != 0 {
			return MEM_read32(tls, memPtr) << int32(8)
		} else {
			return MEM_read32(tls, memPtr) >> int32(8)
		}
	}
	return r
}

// C documentation
//
//	/* Update hashTable3 up to ip (excluded)
//	   Assumption : always within prefix (i.e. not within extDict) */
func ZSTD_insertAndFindFirstIndexHash3(tls *libc.TLS, ms uintptr, nextToUpdate3 uintptr, ip uintptr) (r U32) {
	var base, hashTable3 uintptr
	var hash3 size_t
	var hashLog3, idx, target U32
	_, _, _, _, _, _ = base, hash3, hashLog3, hashTable3, idx, target
	hashTable3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable3
	hashLog3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	idx = *(*U32)(unsafe.Pointer(nextToUpdate3))
	target = uint32(int64(ip) - int64(base))
	hash3 = ZSTD_hash3Ptr(tls, ip, hashLog3)
	for idx < target {
		*(*U32)(unsafe.Pointer(hashTable3 + uintptr(ZSTD_hash3Ptr(tls, base+uintptr(idx), hashLog3))*4)) = idx
		idx = idx + 1
	}
	*(*U32)(unsafe.Pointer(nextToUpdate3)) = target
	return *(*U32)(unsafe.Pointer(hashTable3 + uintptr(hash3)*4))
}

// C documentation
//
//	/*-*************************************
//	*  Binary Tree search
//	***************************************/
//	/** ZSTD_insertBt1() : add one or multiple positions to tree.
//	 * @param ip assumed <= iend-8 .
//	 * @param target The target of ZSTD_updateTree_internal() - we are filling to this position
//	 * @return : nb of positions added */
func ZSTD_insertBt1(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, target U32, mls U32, extDict int32) (r U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, hashTable, largerPtr, match, nextPtr, prefixStart, smallerPtr uintptr
	var bestLength, commonLengthLarger, commonLengthSmaller, h, matchLength size_t
	var btLog, btLow, btMask, curr, dictLimit, hashLog, matchEndIdx, matchIndex, nbCompares, positions, windowLow, v4 U32
	var v1 uint32
	var v3 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bestLength, bt, btLog, btLow, btMask, cParams, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictEnd, dictLimit, h, hashLog, hashTable, largerPtr, match, matchEndIdx, matchIndex, matchLength, nbCompares, nextPtr, positions, prefixStart, smallerPtr, windowLow, v1, v3, v4
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = uint32(int32(1)<<btLog - int32(1))
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	curr = uint32(int64(ip) - int64(base))
	if btMask >= curr {
		v1 = uint32(0)
	} else {
		v1 = curr - btMask
	}
	btLow = v1
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = smallerPtr + uintptr(1)*4 /* to be nullified at the end */
	/* windowLow is based on target because
	 * we only need positions that will be in the window at the end of the tree update.
	 */
	windowLow = ZSTD_getLowestMatchIndex(tls, ms, target, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	matchEndIdx = curr + uint32(8) + uint32(1)
	bestLength = uint64(8)
	nbCompares = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	/* required for h calculation */
	*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = curr /* Update Hash Table */
	for {
		if !(nbCompares != 0 && matchIndex >= windowLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v3 = commonLengthSmaller
		} else {
			v3 = commonLengthLarger
		}
		matchLength = v3 /* guaranteed minimum nb of common bytes */
		if !(extDict != 0) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
			/* might be wrong if actually extDict */
			match = base + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend)
		} else {
			match = dictBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match = base + uintptr(matchIndex)
			} /* to prepare for next usage of match[matchLength] */
		}
		if matchLength > bestLength {
			bestLength = matchLength
			if matchLength > uint64(matchEndIdx-matchIndex) {
				matchEndIdx = matchIndex + uint32(matchLength)
			}
		}
		if ip+uintptr(matchLength) == iend { /* equal : no way to know if inf or sup */
			break /* drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt tree */
		}
		if int32(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) { /* necessarily within buffer */
			/* match is smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop searching */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new "candidate" => larger than match, which was smaller than target */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex, larger than previous and closer to current */
		} else {
			/* match is larger than current */
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop searching */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _2
	_2:
		;
		nbCompares = nbCompares - 1
	}
	v4 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v4
	*(*U32)(unsafe.Pointer(smallerPtr)) = v4
	positions = uint32(0)
	if bestLength > uint64(384) {
		if uint32(libc.Int32FromInt32(192)) < uint32(bestLength-libc.Uint64FromInt32(384)) {
			v1 = uint32(libc.Int32FromInt32(192))
		} else {
			v1 = uint32(bestLength - libc.Uint64FromInt32(384))
		}
		positions = v1
	} /* speed optimization */
	if positions > matchEndIdx-(curr+uint32(8)) {
		v1 = positions
	} else {
		v1 = matchEndIdx - (curr + uint32(8))
	}
	return v1
	return r
}

func ZSTD_updateTree_internal(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, mls U32, dictMode ZSTD_dictMode_e) {
	var base uintptr
	var forward, idx, target U32
	_, _, _, _ = base, forward, idx, target
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = uint32(int64(ip) - int64(base))
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	for idx < target {
		forward = ZSTD_insertBt1(tls, ms, base+uintptr(idx), iend, target, mls, libc.BoolInt32(dictMode == int32(ZSTD_extDict)))
		idx = idx + forward
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

func ZSTD_updateTree(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr) {
	ZSTD_updateTree_internal(tls, ms, ip, iend, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch, int32(ZSTD_noDict))
}

func ZSTD_insertBtAndGetAllMatches(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iLimit uintptr, dictMode ZSTD_dictMode_e, rep uintptr, ll0 U32, lengthToBeat U32, mls U32) (r U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, dms, dmsBase, dmsBt, dmsCParams, dmsEnd, hashTable, largerPtr, match, match1, match2, match3, nextPtr, nextPtr1, prefixStart, repMatch, smallerPtr, v5, v6, v7, v8 uintptr
	var bestLength, commonLengthLarger, commonLengthSmaller, dmsH, h, matchLength, matchLength1, mlen, v22 size_t
	var btLog, btLow, btMask, curr, dictLimit, dictMatchIndex, dmsBtLog, dmsBtLow, dmsBtMask, dmsHashLog, dmsHighLimit, dmsIndexDelta, dmsLowLimit, hashLog, lastR, matchEndIdx, matchIndex, matchIndex3, matchLow, minMatch, mnum, nbCompares, repCode, repIndex, repLen, repOffset, sufficient_len, windowLow, v21 U32
	var v1, v10, v11, v12, v13, v14, v15, v3, v4, v9 uint32
	var v2 int32
	var v20 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bestLength, bt, btLog, btLow, btMask, cParams, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictEnd, dictLimit, dictMatchIndex, dms, dmsBase, dmsBt, dmsBtLog, dmsBtLow, dmsBtMask, dmsCParams, dmsEnd, dmsH, dmsHashLog, dmsHighLimit, dmsIndexDelta, dmsLowLimit, h, hashLog, hashTable, largerPtr, lastR, match, match1, match2, match3, matchEndIdx, matchIndex, matchIndex3, matchLength, matchLength1, matchLow, minMatch, mlen, mnum, nbCompares, nextPtr, nextPtr1, prefixStart, repCode, repIndex, repLen, repMatch, repOffset, smallerPtr, sufficient_len, windowLow, v1, v10, v11, v12, v13, v14, v15, v2, v20, v21, v22, v3, v4, v5, v6, v7, v8, v9
	cParams = ms + 256
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength < uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)-libc.Int32FromInt32(1)) {
		v1 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength
	} else {
		v1 = uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12) - libc.Int32FromInt32(1))
	}
	sufficient_len = v1
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	curr = uint32(int64(ip) - int64(base))
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	if mls == uint32(3) {
		v2 = int32(3)
	} else {
		v2 = int32(4)
	}
	minMatch = uint32(v2)
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = uint32(1)<<btLog - uint32(1)
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	if btMask >= curr {
		v3 = uint32(0)
	} else {
		v3 = curr - btMask
	}
	btLow = v3
	windowLow = ZSTD_getLowestMatchIndex(tls, ms, curr, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	if windowLow != 0 {
		v4 = windowLow
	} else {
		v4 = uint32(1)
	}
	matchLow = v4
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4 + uintptr(1)*4
	matchEndIdx = curr + uint32(8) + uint32(1) /* to be nullified at the end */
	mnum = uint32(0)
	nbCompares = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	if dictMode == int32(ZSTD_dictMatchState) {
		v5 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	} else {
		v5 = libc.UintptrFromInt32(0)
	}
	dms = v5
	if dictMode == int32(ZSTD_dictMatchState) {
		v6 = dms + 256
	} else {
		v6 = libc.UintptrFromInt32(0)
	}
	dmsCParams = v6
	if dictMode == int32(ZSTD_dictMatchState) {
		v7 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	} else {
		v7 = libc.UintptrFromInt32(0)
	}
	dmsBase = v7
	if dictMode == int32(ZSTD_dictMatchState) {
		v8 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	} else {
		v8 = libc.UintptrFromInt32(0)
	}
	dmsEnd = v8
	if dictMode == int32(ZSTD_dictMatchState) {
		v9 = uint32(int64(dmsEnd) - int64(dmsBase))
	} else {
		v9 = uint32(0)
	}
	dmsHighLimit = v9
	if dictMode == int32(ZSTD_dictMatchState) {
		v10 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FlowLimit
	} else {
		v10 = uint32(0)
	}
	dmsLowLimit = v10
	if dictMode == int32(ZSTD_dictMatchState) {
		v11 = windowLow - dmsHighLimit
	} else {
		v11 = uint32(0)
	}
	dmsIndexDelta = v11
	if dictMode == int32(ZSTD_dictMatchState) {
		v12 = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FhashLog
	} else {
		v12 = hashLog
	}
	dmsHashLog = v12
	if dictMode == int32(ZSTD_dictMatchState) {
		v13 = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FchainLog - uint32(1)
	} else {
		v13 = btLog
	}
	dmsBtLog = v13
	if dictMode == int32(ZSTD_dictMatchState) {
		v14 = uint32(1)<<dmsBtLog - uint32(1)
	} else {
		v14 = uint32(0)
	}
	dmsBtMask = v14
	if dictMode == int32(ZSTD_dictMatchState) && dmsBtMask < dmsHighLimit-dmsLowLimit {
		v15 = dmsHighLimit - dmsBtMask
	} else {
		v15 = dmsLowLimit
	}
	dmsBtLow = v15
	bestLength = uint64(lengthToBeat - uint32(1))
	/* check repCode */
	/* necessarily 1 or 0 */
	lastR = uint32(ZSTD_REP_NUM) + ll0
	repCode = ll0
	for {
		if !(repCode < lastR) {
			break
		}
		if repCode == uint32(ZSTD_REP_NUM) {
			v1 = *(*U32)(unsafe.Pointer(rep)) - uint32(1)
		} else {
			v1 = *(*U32)(unsafe.Pointer(rep + uintptr(repCode)*4))
		}
		repOffset = v1
		repIndex = curr - repOffset
		repLen = uint32(0)
		if repOffset-uint32(1) < curr-dictLimit { /* equivalent to `curr > repIndex >= dictLimit` */
			/* We must validate the repcode offset because when we're using a dictionary the
			 * valid offset range shrinks when the dictionary goes out of bounds.
			 */
			if libc.BoolInt32(repIndex >= windowLow)&libc.BoolInt32(ZSTD_readMINMATCH(tls, ip, minMatch) == ZSTD_readMINMATCH(tls, ip-uintptr(repOffset), minMatch)) != 0 {
				repLen = uint32(ZSTD_count(tls, ip+uintptr(minMatch), ip+uintptr(minMatch)-uintptr(repOffset), iLimit)) + minMatch
			}
		} else {
			if dictMode == int32(ZSTD_dictMatchState) {
				v5 = dmsBase + uintptr(repIndex) - uintptr(dmsIndexDelta)
			} else {
				v5 = dictBase + uintptr(repIndex)
			} /* repIndex < dictLimit || repIndex >= curr */
			repMatch = v5
			if dictMode == int32(ZSTD_extDict) && libc.BoolInt32(repOffset-uint32(1) < curr-windowLow)&ZSTD_index_overlap_check(tls, dictLimit, repIndex) != 0 && ZSTD_readMINMATCH(tls, ip, minMatch) == ZSTD_readMINMATCH(tls, repMatch, minMatch) {
				repLen = uint32(ZSTD_count_2segments(tls, ip+uintptr(minMatch), repMatch+uintptr(minMatch), iLimit, dictEnd, prefixStart)) + minMatch
			}
			if dictMode == int32(ZSTD_dictMatchState) && libc.BoolInt32(repOffset-uint32(1) < curr-(dmsLowLimit+dmsIndexDelta))&ZSTD_index_overlap_check(tls, dictLimit, repIndex) != 0 && ZSTD_readMINMATCH(tls, ip, minMatch) == ZSTD_readMINMATCH(tls, repMatch, minMatch) {
				repLen = uint32(ZSTD_count_2segments(tls, ip+uintptr(minMatch), repMatch+uintptr(minMatch), iLimit, dmsEnd, prefixStart)) + minMatch
			}
		}
		/* save longer solution */
		if uint64(repLen) > bestLength {
			bestLength = uint64(repLen)
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Foff = repCode - ll0 + libc.Uint32FromInt32(1) /* expect value between 1 and 3 */
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Flen1 = repLen
			mnum = mnum + 1
			if libc.BoolInt32(repLen > sufficient_len)|libc.BoolInt32(ip+uintptr(repLen) == iLimit) != 0 { /* best possible */
				return mnum
			}
		}
		goto _16
	_16:
		;
		repCode = repCode + 1
	}
	/* HC3 match finder */
	if mls == uint32(3) && bestLength < uint64(mls) {
		matchIndex3 = ZSTD_insertAndFindFirstIndexHash3(tls, ms, nextToUpdate3, ip)
		if libc.BoolInt32(matchIndex3 >= matchLow)&libc.BoolInt32(curr-matchIndex3 < uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(18))) != 0 {
			if dictMode == int32(ZSTD_noDict) || dictMode == int32(ZSTD_dictMatchState) || matchIndex3 >= dictLimit {
				match = base + uintptr(matchIndex3)
				mlen = ZSTD_count(tls, ip, match, iLimit)
			} else {
				match1 = dictBase + uintptr(matchIndex3)
				mlen = ZSTD_count_2segments(tls, ip, match1, iLimit, dictEnd, prefixStart)
			}
			/* save best solution */
			if mlen >= uint64(mls) {
				bestLength = mlen
				/* no prior solution */
				(*(*ZSTD_match_t)(unsafe.Pointer(matches))).Foff = curr - matchIndex3 + libc.Uint32FromInt32(ZSTD_REP_NUM)
				(*(*ZSTD_match_t)(unsafe.Pointer(matches))).Flen1 = uint32(mlen)
				mnum = uint32(1)
				if libc.BoolInt32(mlen > uint64(sufficient_len))|libc.BoolInt32(ip+uintptr(mlen) == iLimit) != 0 { /* best possible length */
					(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr + uint32(1) /* skip insertion */
					return uint32(1)
				}
			}
		}
		/* no dictMatchState lookup: dicts don't have a populated HC3 table */
	} /* if (mls == 3) */
	*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = curr /* Update Hash Table */
	for {
		if !(nbCompares != 0 && matchIndex >= matchLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v20 = commonLengthSmaller
		} else {
			v20 = commonLengthLarger
		}
		matchLength = v20 /* guaranteed minimum nb of common bytes */
		if dictMode == int32(ZSTD_noDict) || dictMode == int32(ZSTD_dictMatchState) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
			/* ensure the condition is correct when !extDict */
			match2 = base + uintptr(matchIndex)
			if matchIndex >= dictLimit {
			} /* ensure early section of match is equal as expected */
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match2+uintptr(matchLength), iLimit)
		} else {
			match2 = dictBase + uintptr(matchIndex)
			/* ensure early section of match is equal as expected */
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match2+uintptr(matchLength), iLimit, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match2 = base + uintptr(matchIndex)
			} /* prepare for match[matchLength] read */
		}
		if matchLength > bestLength {
			if matchLength > uint64(matchEndIdx-matchIndex) {
				matchEndIdx = matchIndex + uint32(matchLength)
			}
			bestLength = matchLength
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Foff = curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM)
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Flen1 = uint32(matchLength)
			mnum = mnum + 1
			if libc.BoolInt32(matchLength > uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)))|libc.BoolInt32(ip+uintptr(matchLength) == iLimit) != 0 {
				if dictMode == int32(ZSTD_dictMatchState) {
					nbCompares = uint32(0)
				} /* break should also skip searching dms */
				break /* drop, to preserve bt consistency (miss a little bit of compression) */
			}
		}
		if int32(*(*BYTE)(unsafe.Pointer(match2 + uintptr(matchLength)))) < int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) {
			/* match smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop the search */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new candidate => larger than match, which was smaller than current */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex, larger than previous, closer to current */
		} else {
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop the search */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _19
	_19:
		;
		nbCompares = nbCompares - 1
	}
	v21 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v21
	*(*U32)(unsafe.Pointer(smallerPtr)) = v21
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dictMatchState) && nbCompares != 0 {
		dmsH = ZSTD_hashPtr(tls, ip, dmsHashLog, mls)
		dictMatchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(dmsH)*4))
		dmsBt = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
		v22 = libc.Uint64FromInt32(0)
		commonLengthLarger = v22
		commonLengthSmaller = v22
		for {
			if !(nbCompares != 0 && dictMatchIndex > dmsLowLimit) {
				break
			}
			nextPtr1 = dmsBt + uintptr(uint32(2)*(dictMatchIndex&dmsBtMask))*4
			if commonLengthSmaller < commonLengthLarger {
				v20 = commonLengthSmaller
			} else {
				v20 = commonLengthLarger
			}
			matchLength1 = v20 /* guaranteed minimum nb of common bytes */
			match3 = dmsBase + uintptr(dictMatchIndex)
			matchLength1 = matchLength1 + ZSTD_count_2segments(tls, ip+uintptr(matchLength1), match3+uintptr(matchLength1), iLimit, dmsEnd, prefixStart)
			if uint64(dictMatchIndex)+matchLength1 >= uint64(dmsHighLimit) {
				match3 = base + uintptr(dictMatchIndex) + uintptr(dmsIndexDelta)
			} /* to prepare for next usage of match[matchLength] */
			if matchLength1 > bestLength {
				matchIndex = dictMatchIndex + dmsIndexDelta
				if matchLength1 > uint64(matchEndIdx-matchIndex) {
					matchEndIdx = matchIndex + uint32(matchLength1)
				}
				bestLength = matchLength1
				(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Foff = curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM)
				(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Flen1 = uint32(matchLength1)
				mnum = mnum + 1
				if libc.BoolInt32(matchLength1 > uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)))|libc.BoolInt32(ip+uintptr(matchLength1) == iLimit) != 0 {
					break /* drop, to guarantee consistency (miss a little bit of compression) */
				}
			}
			if dictMatchIndex <= dmsBtLow {
				break
			} /* beyond tree size, stop the search */
			if int32(*(*BYTE)(unsafe.Pointer(match3 + uintptr(matchLength1)))) < int32(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength1)))) {
				commonLengthSmaller = matchLength1                       /* all smaller will now have at least this guaranteed common length */
				dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr1 + 1*4)) /* new matchIndex larger than previous (closer to current) */
			} else {
				/* match is larger than current */
				commonLengthLarger = matchLength1
				dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr1))
			}
			goto _23
		_23:
			;
			nbCompares = nbCompares - 1
		}
	} /* if (dictMode == ZSTD_dictMatchState) */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = matchEndIdx - uint32(8) /* skip repetitive patterns */
	return mnum
}

type ZSTD_getAllMatchesFn = uintptr

func ZSTD_btGetAllMatches_internal(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32, dictMode ZSTD_dictMode_e, mls U32) (r U32) {
	if ip < (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase+uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate) {
		return uint32(0)
	} /* skipped area */
	ZSTD_updateTree_internal(tls, ms, ip, iHighLimit, mls, dictMode)
	return ZSTD_insertBtAndGetAllMatches(tls, matches, ms, nextToUpdate3, ip, iHighLimit, dictMode, rep, ll0, lengthToBeat, mls)
}

func ZSTD_btGetAllMatches_noDict_3(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(3))
}

func ZSTD_btGetAllMatches_noDict_4(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(4))
}

func ZSTD_btGetAllMatches_noDict_5(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(5))
}

func ZSTD_btGetAllMatches_noDict_6(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(6))
}

func ZSTD_btGetAllMatches_extDict_3(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(3))
}

func ZSTD_btGetAllMatches_extDict_4(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(4))
}

func ZSTD_btGetAllMatches_extDict_5(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(5))
}

func ZSTD_btGetAllMatches_extDict_6(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(6))
}

func ZSTD_btGetAllMatches_dictMatchState_3(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(3))
}

func ZSTD_btGetAllMatches_dictMatchState_4(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_btGetAllMatches_dictMatchState_5(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_btGetAllMatches_dictMatchState_6(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_selectBtGetAllMatches(tls *libc.TLS, ms uintptr, dictMode ZSTD_dictMode_e) (r ZSTD_getAllMatchesFn) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var mls U32
	var v1, v2, v3 uint32
	var _ /* getAllMatchesFns at bp+0 */ [3][4]ZSTD_getAllMatchesFn
	_, _, _, _ = mls, v1, v2, v3
	*(*[3][4]ZSTD_getAllMatchesFn)(unsafe.Pointer(bp)) = [3][4]ZSTD_getAllMatchesFn{
		0: {
			0: __ccgo_fp(ZSTD_btGetAllMatches_noDict_3),
			1: __ccgo_fp(ZSTD_btGetAllMatches_noDict_4),
			2: __ccgo_fp(ZSTD_btGetAllMatches_noDict_5),
			3: __ccgo_fp(ZSTD_btGetAllMatches_noDict_6),
		},
		1: {
			0: __ccgo_fp(ZSTD_btGetAllMatches_extDict_3),
			1: __ccgo_fp(ZSTD_btGetAllMatches_extDict_4),
			2: __ccgo_fp(ZSTD_btGetAllMatches_extDict_5),
			3: __ccgo_fp(ZSTD_btGetAllMatches_extDict_6),
		},
		2: {
			0: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_3),
			1: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_4),
			2: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_5),
			3: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_6),
		},
	}
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < uint32(libc.Int32FromInt32(6)) {
		v2 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v2 = uint32(libc.Int32FromInt32(6))
	}
	if uint32(libc.Int32FromInt32(3)) > v2 {
		v1 = uint32(libc.Int32FromInt32(3))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < uint32(libc.Int32FromInt32(6)) {
			v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
		} else {
			v3 = uint32(libc.Int32FromInt32(6))
		}
		v1 = v3
	}
	mls = v1
	return *(*ZSTD_getAllMatchesFn)(unsafe.Pointer(bp + uintptr(dictMode)*32 + uintptr(mls-uint32(3))*8))
}

/*************************
*  LDM helper functions  *
*************************/

// C documentation
//
//	/* Struct containing info needed to make decision about ldm inclusion */
type ZSTD_optLdm_t = struct {
	FseqStore        RawSeqStore_t
	FstartPosInBlock U32
	FendPosInBlock   U32
	Foffset          U32
}

// C documentation
//
//	/* ZSTD_optLdm_skipRawSeqStoreBytes():
//	 * Moves forward in @rawSeqStore by @nbBytes,
//	 * which will update the fields 'pos' and 'posInSequence'.
//	 */
func ZSTD_optLdm_skipRawSeqStoreBytes(tls *libc.TLS, rawSeqStore uintptr, nbBytes size_t) {
	var currPos U32
	var currSeq rawSeq
	_, _ = currPos, currSeq
	currPos = uint32((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence + nbBytes)
	for currPos != 0 && (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		currSeq = *(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12))
		if currPos >= currSeq.FlitLength+currSeq.FmatchLength {
			currPos = currPos - (currSeq.FlitLength + currSeq.FmatchLength)
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
		} else {
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(currPos)
			break
		}
	}
	if currPos == uint32(0) || (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos == (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(0)
	}
}

// C documentation
//
//	/* ZSTD_opt_getNextMatchAndUpdateSeqStore():
//	 * Calculates the beginning and end of the next match in the current block.
//	 * Updates 'pos' and 'posInSequence' of the ldmSeqStore.
//	 */
func ZSTD_opt_getNextMatchAndUpdateSeqStore(tls *libc.TLS, optLdm uintptr, currPosInBlock U32, blockBytesRemaining U32) {
	var currBlockEndPos, literalsBytesRemaining, matchBytesRemaining U32
	var currSeq rawSeq
	var v1 uint32
	_, _, _, _, _ = currBlockEndPos, currSeq, literalsBytesRemaining, matchBytesRemaining, v1
	/* Setting match end position to MAX to ensure we never use an LDM during this block */
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize == uint64(0) || (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fpos >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize {
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock = libc.Uint32FromInt32(__INT_MAX__)*libc.Uint32FromUint32(2) + libc.Uint32FromUint32(1)
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = libc.Uint32FromInt32(__INT_MAX__)*libc.Uint32FromUint32(2) + libc.Uint32FromUint32(1)
		return
	}
	/* Calculate appropriate bytes left in matchLength and litLength
	 * after adjusting based on ldmSeqStore->posInSequence */
	currSeq = *(*rawSeq)(unsafe.Pointer((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fseq + uintptr((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fpos)*12))
	currBlockEndPos = currPosInBlock + blockBytesRemaining
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.FposInSequence < uint64(currSeq.FlitLength) {
		v1 = currSeq.FlitLength - uint32((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.FposInSequence)
	} else {
		v1 = uint32(0)
	}
	literalsBytesRemaining = v1
	if literalsBytesRemaining == uint32(0) {
		v1 = currSeq.FmatchLength - (uint32((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.FposInSequence) - currSeq.FlitLength)
	} else {
		v1 = currSeq.FmatchLength
	}
	matchBytesRemaining = v1
	/* If there are more literal bytes than bytes remaining in block, no ldm is possible */
	if literalsBytesRemaining >= blockBytesRemaining {
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock = libc.Uint32FromInt32(__INT_MAX__)*libc.Uint32FromUint32(2) + libc.Uint32FromUint32(1)
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = libc.Uint32FromInt32(__INT_MAX__)*libc.Uint32FromUint32(2) + libc.Uint32FromUint32(1)
		ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(blockBytesRemaining))
		return
	}
	/* Matches may be < minMatch by this process. In that case, we will reject them
	   when we are deciding whether or not to add the ldm */
	(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock = currPosInBlock + literalsBytesRemaining
	(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock + matchBytesRemaining
	(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).Foffset = currSeq.Foffset
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock > currBlockEndPos {
		/* Match ends after the block ends, we can't use the whole match */
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = currBlockEndPos
		ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(currBlockEndPos-currPosInBlock))
	} else {
		/* Consume nb of bytes equal to size of sequence left */
		ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(literalsBytesRemaining+matchBytesRemaining))
	}
}

// C documentation
//
//	/* ZSTD_optLdm_maybeAddMatch():
//	 * Adds a match if it's long enough,
//	 * based on it's 'matchStartPosInBlock' and 'matchEndPosInBlock',
//	 * into 'matches'. Maintains the correct ordering of 'matches'.
//	 */
func ZSTD_optLdm_maybeAddMatch(tls *libc.TLS, matches uintptr, nbMatches uintptr, optLdm uintptr, currPosInBlock U32, minMatch U32) {
	var candidateMatchLength, candidateOffBase, posDiff U32
	_, _, _ = candidateMatchLength, candidateOffBase, posDiff
	posDiff = currPosInBlock - (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock
	/* Note: ZSTD_match_t actually contains offBase and matchLength (before subtracting MINMATCH) */
	candidateMatchLength = (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock - (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock - posDiff
	/* Ensure that current block position is not outside of the match */
	if currPosInBlock < (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock || currPosInBlock >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock || candidateMatchLength < minMatch {
		return
	}
	if *(*U32)(unsafe.Pointer(nbMatches)) == uint32(0) || candidateMatchLength > (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(nbMatches))-uint32(1))*8))).Flen1 && *(*U32)(unsafe.Pointer(nbMatches)) < uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)) {
		candidateOffBase = (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).Foffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
		(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(nbMatches)))*8))).Flen1 = candidateMatchLength
		(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(nbMatches)))*8))).Foff = candidateOffBase
		*(*U32)(unsafe.Pointer(nbMatches)) = *(*U32)(unsafe.Pointer(nbMatches)) + 1
	}
}

// C documentation
//
//	/* ZSTD_optLdm_processMatchCandidate():
//	 * Wrapper function to update ldm seq store and call ldm functions as necessary.
//	 */
func ZSTD_optLdm_processMatchCandidate(tls *libc.TLS, optLdm uintptr, matches uintptr, nbMatches uintptr, currPosInBlock U32, remainingBytes U32, minMatch U32) {
	var posOvershoot U32
	_ = posOvershoot
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize == uint64(0) || (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fpos >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize {
		return
	}
	if currPosInBlock >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock {
		if currPosInBlock > (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock {
			/* The position at which ZSTD_optLdm_processMatchCandidate() is called is not necessarily
			 * at the end of a match from the ldm seq store, and will often be some bytes
			 * over beyond matchEndPosInBlock. As such, we need to correct for these "overshoots"
			 */
			posOvershoot = currPosInBlock - (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock
			ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(posOvershoot))
		}
		ZSTD_opt_getNextMatchAndUpdateSeqStore(tls, optLdm, currPosInBlock, remainingBytes)
	}
	ZSTD_optLdm_maybeAddMatch(tls, matches, nbMatches, optLdm, currPosInBlock, minMatch)
}

/*-*******************************
*  Optimal parser
*********************************/
func ZSTD_compressBlock_opt_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, optLevel int32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	var advance, cur, end, lastML, last_pos, litlen, litlen1, ll0, ll01, llen, longestML, matchNb, matchNb1, maxML, maxOffBase, minMatch, mlen, mlen1, offBase, offBase1, offset, pos, pos1, prev, prev1, startML, storeEnd, storePos, storeStart, stretchPos, sufficient_len, v4, v5 U32
	var anchor, base, cParams, iend, ilimit, inr, ip, istart, matches, opt, optStatePtr, prefixStart uintptr
	var basePrice, matchPrice, previousPrice, price, price1, sequencePrice, with1literal, withMoreLiterals, v2 int32
	var getAllMatches ZSTD_getAllMatchesFn
	var nextStretch, prevMatch ZSTD_optimal_t
	var v1 uint32
	var v3 RawSeqStore_t
	var _ /* lastStretch at bp+4 */ ZSTD_optimal_t
	var _ /* nbMatches at bp+116 */ U32
	var _ /* nbMatches at bp+88 */ U32
	var _ /* newReps at bp+104 */ Repcodes_t
	var _ /* newReps at bp+92 */ Repcodes_t
	var _ /* nextToUpdate3 at bp+0 */ U32
	var _ /* optLdm at bp+32 */ ZSTD_optLdm_t
	var _ /* reps at bp+120 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = advance, anchor, base, basePrice, cParams, cur, end, getAllMatches, iend, ilimit, inr, ip, istart, lastML, last_pos, litlen, litlen1, ll0, ll01, llen, longestML, matchNb, matchNb1, matchPrice, matches, maxML, maxOffBase, minMatch, mlen, mlen1, nextStretch, offBase, offBase1, offset, opt, optStatePtr, pos, pos1, prefixStart, prev, prev1, prevMatch, previousPrice, price, price1, sequencePrice, startML, storeEnd, storePos, storeStart, stretchPos, sufficient_len, with1literal, withMoreLiterals, v1, v2, v3, v4, v5
	optStatePtr = ms + 144
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(8)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	prefixStart = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit)
	cParams = ms + 256
	getAllMatches = ZSTD_selectBtGetAllMatches(tls, ms, dictMode)
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength < uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)-libc.Int32FromInt32(1)) {
		v1 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength
	} else {
		v1 = uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12) - libc.Int32FromInt32(1))
	}
	sufficient_len = v1
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch == uint32(3) {
		v2 = int32(3)
	} else {
		v2 = int32(4)
	}
	minMatch = uint32(v2)
	*(*U32)(unsafe.Pointer(bp)) = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	opt = (*optState_t)(unsafe.Pointer(optStatePtr)).FpriceTable
	matches = (*optState_t)(unsafe.Pointer(optStatePtr)).FmatchTable
	libc.Xmemset(tls, bp+4, 0, libc.Uint64FromInt64(28))
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore != 0 {
		v3 = *(*RawSeqStore_t)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore))
	} else {
		v3 = kNullRawSeqStore
	}
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).FseqStore = v3
	v5 = libc.Uint32FromInt32(0)
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).Foffset = v5
	v4 = v5
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).FstartPosInBlock = v4
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).FendPosInBlock = v4
	ZSTD_opt_getNextMatchAndUpdateSeqStore(tls, bp+32, uint32(int64(ip)-int64(istart)), uint32(int64(iend)-int64(ip)))
	/* init */
	ZSTD_rescaleFreqs(tls, optStatePtr, src, srcSize, optLevel)
	ip = ip + libc.BoolUintptr(ip == prefixStart)
	/* Match Loop */
	for ip < ilimit {
		last_pos = uint32(0)
		/* find first match */
		litlen = uint32(int64(ip) - int64(anchor))
		ll0 = libc.BoolUint32(!(litlen != 0))
		*(*U32)(unsafe.Pointer(bp + 88)) = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, uintptr, uintptr, U32, U32) U32)(unsafe.Pointer(&struct{ uintptr }{getAllMatches})))(tls, matches, ms, bp, ip, iend, rep, ll0, minMatch)
		ZSTD_optLdm_processMatchCandidate(tls, bp+32, matches, bp+88, uint32(int64(ip)-int64(istart)), uint32(int64(iend)-int64(ip)), minMatch)
		if !(*(*U32)(unsafe.Pointer(bp + 88)) != 0) {
			ip = ip + 1
			continue
		}
		/* Match found: let's store this solution, and eventually find more candidates.
		 * During this forward pass, @opt is used to store stretches,
		 * defined as "a match followed by N literals".
		 * Note how this is different from a Sequence, which is "N literals followed by a match".
		 * Storing stretches allows us to store different match predecessors
		 * for each literal position part of a literals run. */
		/* initialize opt[0] */
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Fmlen = uint32(0) /* there are only literals so far */
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Flitlen = litlen
		/* No need to include the actual price of the literals before the first match
		 * because it is static for the duration of the forward pass, and is included
		 * in every subsequent price. But, we include the literal length because
		 * the cost variation of litlen depends on the value of litlen.
		 */
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Fprice = int32(ZSTD_litLengthPrice(tls, litlen, optStatePtr, optLevel))
		_ = libc.Uint64FromInt64(1)
		libc.Xmemcpy(tls, opt+16, rep, libc.Uint64FromInt64(12))
		/* large match -> immediate encoding */
		maxML = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 88))-uint32(1))*8))).Flen1
		maxOffBase = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 88))-uint32(1))*8))).Foff
		if maxML > sufficient_len {
			(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen = uint32(0)
			(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen = maxML
			(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Foff = maxOffBase
			cur = uint32(0)
			last_pos = maxML
			goto _shortestPath
		}
		/* set prices for first matches starting position == 0 */
		pos = uint32(1)
		for {
			if !(pos < minMatch) {
				break
			}
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fmlen = uint32(0)
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Flitlen = litlen + pos
			goto _6
		_6:
			;
			pos = pos + 1
		}
		matchNb = uint32(0)
		for {
			if !(matchNb < *(*U32)(unsafe.Pointer(bp + 88))) {
				break
			}
			offBase = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb)*8))).Foff
			end = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb)*8))).Flen1
			for {
				if !(pos <= end) {
					break
				}
				matchPrice = int32(ZSTD_getMatchPrice(tls, offBase, pos, optStatePtr, optLevel))
				sequencePrice = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Fprice + matchPrice
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fmlen = pos
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Foff = offBase
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Flitlen = uint32(0) /* end of match */
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fprice = sequencePrice + int32(ZSTD_litLengthPrice(tls, uint32(0), optStatePtr, optLevel))
				goto _8
			_8:
				;
				pos = pos + 1
			}
			goto _7
		_7:
			;
			matchNb = matchNb + 1
		}
		last_pos = pos - uint32(1)
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
		/* check further positions */
		cur = uint32(1)
		for {
			if !(cur <= last_pos) {
				break
			}
			inr = ip + uintptr(cur)
			/* Fix current position with one literal if cheaper */
			litlen1 = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur-uint32(1))*28))).Flitlen + uint32(1)
			price = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur-uint32(1))*28))).Fprice + int32(ZSTD_rawLiteralsCost(tls, ip+uintptr(cur)-uintptr(1), uint32(1), optStatePtr, optLevel)) + (int32(ZSTD_litLengthPrice(tls, litlen1, optStatePtr, optLevel)) - int32(ZSTD_litLengthPrice(tls, litlen1-uint32(1), optStatePtr, optLevel)))
			/* overflow check */
			if price <= (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice {
				prevMatch = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))
				*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28)) = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur-uint32(1))*28))
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen = litlen1
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice = price
				if optLevel >= int32(1) && prevMatch.Flitlen == uint32(0) && int32(ZSTD_litLengthPrice(tls, uint32(1), optStatePtr, optLevel))-int32(ZSTD_litLengthPrice(tls, uint32(libc.Int32FromInt32(1)-libc.Int32FromInt32(1)), optStatePtr, optLevel)) < 0 && libc.BoolInt32(ip+uintptr(cur) < iend) != 0 {
					/* check next position, in case it would be cheaper */
					with1literal = prevMatch.Fprice + int32(ZSTD_rawLiteralsCost(tls, ip+uintptr(cur), uint32(1), optStatePtr, optLevel)) + (int32(ZSTD_litLengthPrice(tls, uint32(1), optStatePtr, optLevel)) - int32(ZSTD_litLengthPrice(tls, uint32(libc.Int32FromInt32(1)-libc.Int32FromInt32(1)), optStatePtr, optLevel)))
					withMoreLiterals = price + int32(ZSTD_rawLiteralsCost(tls, ip+uintptr(cur), uint32(1), optStatePtr, optLevel)) + (int32(ZSTD_litLengthPrice(tls, litlen1+uint32(1), optStatePtr, optLevel)) - int32(ZSTD_litLengthPrice(tls, litlen1+uint32(1)-uint32(1), optStatePtr, optLevel)))
					if with1literal < withMoreLiterals && with1literal < (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Fprice {
						/* update offset history - before it disappears */
						prev = cur - prevMatch.Fmlen
						*(*Repcodes_t)(unsafe.Pointer(bp + 92)) = ZSTD_newRep(tls, opt+uintptr(prev)*28+16, prevMatch.Foff, libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(prev)*28))).Flitlen == uint32(0)))
						*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28)) = prevMatch /* mlen & offbase */
						libc.Xmemcpy(tls, opt+uintptr(cur+uint32(1))*28+16, bp+92, libc.Uint64FromInt64(12))
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Flitlen = uint32(1)
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Fprice = with1literal
						if last_pos < cur+uint32(1) {
							last_pos = cur + uint32(1)
						}
					}
				}
			} else {
			}
			/* Offset history is not updated during match comparison.
			 * Do it here, now that the match is selected and confirmed.
			 */
			_ = libc.Uint64FromInt64(1)
			if (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen == uint32(0) {
				/* just finished a match => alter offset history */
				prev1 = cur - (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fmlen
				*(*Repcodes_t)(unsafe.Pointer(bp + 104)) = ZSTD_newRep(tls, opt+uintptr(prev1)*28+16, (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Foff, libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(prev1)*28))).Flitlen == uint32(0)))
				libc.Xmemcpy(tls, opt+uintptr(cur)*28+16, bp+104, libc.Uint64FromInt64(12))
			}
			/* last match must start at a minimum distance of 8 from oend */
			if inr > ilimit {
				goto _9
			}
			if cur == last_pos {
				break
			}
			if optLevel == 0 && (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Fprice <= (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice+libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)/libc.Int32FromInt32(2) {
				goto _9 /* skip unpromising positions; about ~+6% speed, -0.01 ratio */
			}
			ll01 = libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen == libc.Uint32FromInt32(0))
			previousPrice = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice
			basePrice = previousPrice + int32(ZSTD_litLengthPrice(tls, uint32(0), optStatePtr, optLevel))
			*(*U32)(unsafe.Pointer(bp + 116)) = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, uintptr, uintptr, U32, U32) U32)(unsafe.Pointer(&struct{ uintptr }{getAllMatches})))(tls, matches, ms, bp, inr, iend, opt+uintptr(cur)*28+16, ll01, minMatch)
			ZSTD_optLdm_processMatchCandidate(tls, bp+32, matches, bp+116, uint32(int64(inr)-int64(istart)), uint32(int64(iend)-int64(inr)), minMatch)
			if !(*(*U32)(unsafe.Pointer(bp + 116)) != 0) {
				goto _9
			}
			longestML = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 116))-uint32(1))*8))).Flen1
			if longestML > sufficient_len || cur+longestML >= uint32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)) || ip+uintptr(cur)+uintptr(longestML) >= iend {
				(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen = longestML
				(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Foff = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 116))-uint32(1))*8))).Foff
				(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen = uint32(0)
				last_pos = cur + longestML
				goto _shortestPath
			}
			/* set prices using matches found at position == cur */
			matchNb1 = uint32(0)
			for {
				if !(matchNb1 < *(*U32)(unsafe.Pointer(bp + 116))) {
					break
				}
				offset = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb1)*8))).Foff
				lastML = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb1)*8))).Flen1
				if matchNb1 > uint32(0) {
					v1 = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb1-uint32(1))*8))).Flen1 + uint32(1)
				} else {
					v1 = minMatch
				}
				startML = v1
				mlen = lastML
				for {
					if !(mlen >= startML) {
						break
					} /* scan downward */
					pos1 = cur + mlen
					price1 = basePrice + int32(ZSTD_getMatchPrice(tls, offset, mlen, optStatePtr, optLevel))
					if pos1 > last_pos || price1 < (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Fprice {
						for last_pos < pos1 {
							/* fill empty positions, for future comparisons */
							last_pos = last_pos + 1
							(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos)*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
							(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos)*28))).Flitlen = libc.BoolUint32(!(libc.Int32FromInt32(0) != 0)) /* just needs to be != 0, to mean "not an end of match" */
						}
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Fmlen = mlen
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Foff = offset
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Flitlen = uint32(0)
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Fprice = price1
					} else {
						if optLevel == 0 {
							break
						} /* early update abort; gets ~+10% speed for about -0.01 ratio loss */
					}
					goto _12
				_12:
					;
					mlen = mlen - 1
				}
				goto _10
			_10:
				;
				matchNb1 = matchNb1 + 1
			}
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos+uint32(1))*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
			goto _9
		_9:
			;
			cur = cur + 1
		} /* for (cur = 1; cur <= last_pos; cur++) */
		*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4)) = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos)*28))
		cur = last_pos - (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen
		goto _shortestPath
	_shortestPath:
		; /* cur, last_pos, best_mlen, best_off have to be set */
		if (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen == uint32(0) {
			/* no solution : all matches have been converted into literals */
			ip = ip + uintptr(last_pos)
			continue
		}
		/* Update offset history */
		if (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen == uint32(0) {
			/* finishing on a match : update offset history */
			*(*Repcodes_t)(unsafe.Pointer(bp + 120)) = ZSTD_newRep(tls, opt+uintptr(cur)*28+16, (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Foff, libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen == uint32(0)))
			libc.Xmemcpy(tls, rep, bp+120, libc.Uint64FromInt64(12))
		} else {
			libc.Xmemcpy(tls, rep, bp+4+16, libc.Uint64FromInt64(12))
			cur = cur - (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen
		}
		/* Let's write the shortest path solution.
		 * It is stored in @opt in reverse order,
		 * starting from @storeEnd (==cur+2),
		 * effectively partially @opt overwriting.
		 * Content is changed too:
		 * - So far, @opt stored stretches, aka a match followed by literals
		 * - Now, it will store sequences, aka literals followed by a match
		 */
		storeEnd = cur + uint32(2)
		storeStart = storeEnd
		stretchPos = cur
		_ = last_pos
		if (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen > uint32(0) {
			/* last "sequence" is unfinished: just a bunch of literals */
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeEnd)*28))).Flitlen = (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeEnd)*28))).Fmlen = uint32(0)
			storeStart = storeEnd - uint32(1)
			*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeStart)*28)) = *(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))
		}
		*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeEnd)*28)) = *(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4)) /* note: litlen will be fixed */
		storeStart = storeEnd
		for int32(1) != 0 {
			nextStretch = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(stretchPos)*28))
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeStart)*28))).Flitlen = nextStretch.Flitlen
			if nextStretch.Fmlen == uint32(0) {
				/* reaching beginning of segment */
				break
			}
			storeStart = storeStart - 1
			*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeStart)*28)) = nextStretch /* note: litlen will be fixed */
			stretchPos = stretchPos - (nextStretch.Flitlen + nextStretch.Fmlen)
		}
		/* save sequences */
		storePos = storeStart
		for {
			if !(storePos <= storeEnd) {
				break
			}
			llen = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storePos)*28))).Flitlen
			mlen1 = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storePos)*28))).Fmlen
			offBase1 = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storePos)*28))).Foff
			advance = llen + mlen1
			if mlen1 == uint32(0) { /* only literals => must be last "sequence", actually starting a new stream of sequences */
				/* must be last sequence */
				ip = anchor + uintptr(llen) /* last "sequence" is a bunch of literals => don't progress anchor */
				goto _13                    /* will finish */
			}
			ZSTD_updateStats(tls, optStatePtr, llen, anchor, offBase1, mlen1)
			ZSTD_storeSeq(tls, seqStore, uint64(llen), anchor, iend, offBase1, uint64(mlen1))
			anchor = anchor + uintptr(advance)
			ip = anchor
			goto _13
		_13:
			;
			storePos = storePos + 1
		}
		/* update all costs */
		ZSTD_setBasePrices(tls, optStatePtr, optLevel)
	} /* while (ip < ilimit) */
	/* Return the last literals size */
	return uint64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_opt0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, dictMode ZSTD_dictMode_e) (r size_t) {
	return ZSTD_compressBlock_opt_generic(tls, ms, seqStore, rep, src, srcSize, 0, dictMode)
}

func ZSTD_compressBlock_opt2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, dictMode ZSTD_dictMode_e) (r size_t) {
	return ZSTD_compressBlock_opt_generic(tls, ms, seqStore, rep, src, srcSize, int32(2), dictMode)
}

func ZSTD_compressBlock_btopt(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt0(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_noDict))
}

// C documentation
//
//	/* ZSTD_initStats_ultra():
//	 * make a first compression pass, just to seed stats with more accurate starting values.
//	 * only works on first block, with no dictionary and no ldm.
//	 * this function cannot error out, its narrow contract must be respected.
//	 */
func ZSTD_initStats_ultra(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* tmpRep at bp+0 */ [3]U32 /* updated rep codes will sink here */
	libc.Xmemcpy(tls, bp, rep, libc.Uint64FromInt64(12))
	/* first block */
	/* no ldm */
	/* no dictionary */
	/* no prefix (note: intentional overflow, defined as 2-complement) */
	ZSTD_compressBlock_opt2(tls, ms, seqStore, bp, src, srcSize, int32(ZSTD_noDict)) /* generate stats into ms->opt*/
	/* invalidate first scan from history, only keep entropy stats */
	ZSTD_resetSeqStore(tls, seqStore)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase -= uintptr(srcSize)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit += uint32(srcSize)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
}

func ZSTD_compressBlock_btultra(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_noDict))
}

func ZSTD_compressBlock_btultra2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var curr U32
	_ = curr
	curr = uint32(int64(src) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	/* 2-passes strategy:
	 * this strategy makes a first pass over first block to collect statistics
	 * in order to seed next round's statistics with it.
	 * After 1st pass, function forgets history, and starts a new block.
	 * Consequently, this can only work if no data has been previously loaded in tables,
	 * aka, no dictionary, no prefix, no ldm preprocessing.
	 * The compression ratio gain is generally small (~0.5% on first block),
	 * the cost is 2x cpu time on first block. */
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitLengthSum == uint32(0) && (*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences == (*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart && (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit == (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit && curr == (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit && srcSize > uint64(ZSTD_PREDEF_THRESHOLD) {
		ZSTD_initStats_ultra(tls, ms, seqStore, rep, src, srcSize)
	}
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_noDict))
}

func ZSTD_compressBlock_btopt_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt0(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_btopt_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt0(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_extDict))
}

func ZSTD_compressBlock_btultra_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_btultra_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_extDict))
}

/* note : no btultra2 variant for extDict nor dictMatchState,
 * because btultra2 is not meant to work with dictionaries
 * and is only specific for the first block (no prefix) */
/**** ended inlining compress/zstd_opt.c ****/
/**** start inlining compress/zstdmt_compress.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ======   Compiler specifics   ====== */

/* ======   Dependencies   ====== */
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/pool.h ****/
/**** skipping file: ../common/threading.h ****/
/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_ldm.h ****/
/**** skipping file: zstdmt_compress.h ****/

/* Guards code to support resizing the SeqPool.
 * We will want to resize the SeqPool to save memory in the future.
 * Until then, comment the code out since it is unused.
 */

/* ======   Debug   ====== */

/* =====   Buffer Pool   ===== */
/* a single Buffer Pool can be invoked from multiple threads in parallel */

type Buffer = struct {
	Fstart    uintptr
	Fcapacity size_t
}

/* note : no btultra2 variant for extDict nor dictMatchState,
 * because btultra2 is not meant to work with dictionaries
 * and is only specific for the first block (no prefix) */
/**** ended inlining compress/zstd_opt.c ****/
/**** start inlining compress/zstdmt_compress.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ======   Compiler specifics   ====== */

/* ======   Dependencies   ====== */
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/pool.h ****/
/**** skipping file: ../common/threading.h ****/
/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_ldm.h ****/
/**** skipping file: zstdmt_compress.h ****/

/* Guards code to support resizing the SeqPool.
 * We will want to resize the SeqPool to save memory in the future.
 * Until then, comment the code out since it is unused.
 */

/* ======   Debug   ====== */

/* =====   Buffer Pool   ===== */
/* a single Buffer Pool can be invoked from multiple threads in parallel */

type buffer_s = Buffer

var g_nullBuffer = Buffer{}

type ZSTDMT_bufferPool = struct {
	FpoolMutex    CRITICAL_SECTION
	FbufferSize   size_t
	FtotalBuffers uint32
	FnbBuffers    uint32
	FcMem         ZSTD_customMem
	Fbuffers      uintptr
}

type ZSTDMT_bufferPool_s = ZSTDMT_bufferPool

func ZSTDMT_freeBufferPool(tls *libc.TLS, bufPool uintptr) {
	var u uint32
	_ = u
	if !(bufPool != 0) {
		return
	} /* compatibility with free on NULL */
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers != 0 {
		u = uint32(0)
		for {
			if !(u < (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers) {
				break
			}
			ZSTD_customFree(tls, (*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(u)*16))).Fstart, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
			goto _1
		_1:
			;
			u = u + 1
		}
		ZSTD_customFree(tls, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
	}
	libc.XDeleteCriticalSection(tls, bufPool)
	ZSTD_customFree(tls, bufPool, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
}

func ZSTDMT_createBufferPool(tls *libc.TLS, maxNbBuffers uint32, cMem ZSTD_customMem) (r uintptr) {
	var bufPool uintptr
	_ = bufPool
	bufPool = ZSTD_customCalloc(tls, uint64(88), cMem)
	if bufPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	_ = libc.UintptrFromInt32(0)
	libc.XInitializeCriticalSection(tls, bufPool)
	if libc.Int32FromInt32(0) != 0 {
		ZSTD_customFree(tls, bufPool, cMem)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers = ZSTD_customCalloc(tls, uint64(maxNbBuffers)*uint64(16), cMem)
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers == libc.UintptrFromInt32(0) {
		ZSTDMT_freeBufferPool(tls, bufPool)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FbufferSize = uint64(libc.Int32FromInt32(64) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers = maxNbBuffers
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers = uint32(0)
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem = cMem
	return bufPool
}

// C documentation
//
//	/* only works at initialization, not during compression */
func ZSTDMT_sizeof_bufferPool(tls *libc.TLS, bufPool uintptr) (r size_t) {
	var arraySize, poolSize, totalBufferSize size_t
	var u uint32
	_, _, _, _ = arraySize, poolSize, totalBufferSize, u
	poolSize = uint64(88)
	arraySize = uint64((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers) * uint64(16)
	totalBufferSize = uint64(0)
	libc.XEnterCriticalSection(tls, bufPool)
	u = uint32(0)
	for {
		if !(u < (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers) {
			break
		}
		totalBufferSize = totalBufferSize + (*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(u)*16))).Fcapacity
		goto _1
	_1:
		;
		u = u + 1
	}
	libc.XLeaveCriticalSection(tls, bufPool)
	return poolSize + arraySize + totalBufferSize
}

// C documentation
//
//	/* ZSTDMT_setBufferSize() :
//	 * all future buffers provided by this buffer pool will have _at least_ this size
//	 * note : it's better for all buffers to have same size,
//	 * as they become freely interchangeable, reducing malloc/free usages and memory fragmentation */
func ZSTDMT_setBufferSize(tls *libc.TLS, bufPool uintptr, bSize size_t) {
	libc.XEnterCriticalSection(tls, bufPool)
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FbufferSize = bSize
	libc.XLeaveCriticalSection(tls, bufPool)
}

func ZSTDMT_expandBufferPool(tls *libc.TLS, srcBufPool uintptr, maxNbBuffers uint32) (r uintptr) {
	var bSize size_t
	var cMem ZSTD_customMem
	var newBufPool uintptr
	_, _, _ = bSize, cMem, newBufPool
	if srcBufPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(srcBufPool)).FtotalBuffers >= maxNbBuffers { /* good enough */
		return srcBufPool
	}
	/* need a larger buffer pool */
	cMem = (*ZSTDMT_bufferPool)(unsafe.Pointer(srcBufPool)).FcMem
	bSize = (*ZSTDMT_bufferPool)(unsafe.Pointer(srcBufPool)).FbufferSize
	ZSTDMT_freeBufferPool(tls, srcBufPool)
	newBufPool = ZSTDMT_createBufferPool(tls, maxNbBuffers, cMem)
	if newBufPool == libc.UintptrFromInt32(0) {
		return newBufPool
	}
	ZSTDMT_setBufferSize(tls, newBufPool, bSize)
	return newBufPool
	return r
}

// C documentation
//
//	/** ZSTDMT_getBuffer() :
//	 *  assumption : bufPool must be valid
//	 * @return : a buffer, with start pointer and size
//	 *  note: allocation may fail, in this case, start==NULL and size==0 */
func ZSTDMT_getBuffer(tls *libc.TLS, bufPool uintptr) (r Buffer) {
	var availBufferSize, bSize size_t
	var buf, buffer Buffer
	var start, v2 uintptr
	var v1 uint32
	var v3 uint64
	_, _, _, _, _, _, _, _ = availBufferSize, bSize, buf, buffer, start, v1, v2, v3
	bSize = (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FbufferSize
	libc.XEnterCriticalSection(tls, bufPool)
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers != 0 {
		v2 = bufPool + 52
		*(*uint32)(unsafe.Pointer(v2)) = *(*uint32)(unsafe.Pointer(v2)) - 1
		v1 = *(*uint32)(unsafe.Pointer(v2)) /* try to use an existing buffer */
		buf = *(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(v1)*16))
		availBufferSize = buf.Fcapacity
		*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers)*16)) = g_nullBuffer
		if libc.BoolInt32(availBufferSize >= bSize)&libc.BoolInt32(availBufferSize>>libc.Int32FromInt32(3) <= bSize) != 0 {
			/* large enough, but not too much */
			libc.XLeaveCriticalSection(tls, bufPool)
			return buf
		}
		/* size conditions not respected : scratch this buffer, create new one */
		ZSTD_customFree(tls, buf.Fstart, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
	}
	libc.XLeaveCriticalSection(tls, bufPool)
	/* create new buffer */
	start = ZSTD_customMalloc(tls, bSize, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
	buffer.Fstart = start /* note : start can be NULL if malloc fails ! */
	if start == libc.UintptrFromInt32(0) {
		v3 = uint64(0)
	} else {
		v3 = bSize
	}
	buffer.Fcapacity = v3
	if start == libc.UintptrFromInt32(0) {
	} else {
	}
	return buffer
	return r
}

// C documentation
//
//	/* store buffer for later re-use, up to pool capacity */
func ZSTDMT_releaseBuffer(tls *libc.TLS, bufPool uintptr, buf Buffer) {
	var v1 uint32
	var v2 uintptr
	_, _ = v1, v2
	if buf.Fstart == libc.UintptrFromInt32(0) {
		return
	} /* compatible with release on NULL */
	libc.XEnterCriticalSection(tls, bufPool)
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers < (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers {
		v2 = bufPool + 52
		v1 = *(*uint32)(unsafe.Pointer(v2))
		*(*uint32)(unsafe.Pointer(v2)) = *(*uint32)(unsafe.Pointer(v2)) + 1
		*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(v1)*16)) = buf /* stored for later use */
		libc.XLeaveCriticalSection(tls, bufPool)
		return
	}
	libc.XLeaveCriticalSection(tls, bufPool)
	/* Reached bufferPool capacity (note: should not happen) */
	ZSTD_customFree(tls, buf.Fstart, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
}

/* We need 2 output buffers per worker since each dstBuff must be flushed after it is released.
 * The 3 additional buffers are as follows:
 *   1 buffer for input loading
 *   1 buffer for "next input" when submitting current one
 *   1 buffer stuck in queue */

/* After a worker releases its rawSeqStore, it is immediately ready for reuse.
 * So we only need one seq buffer per worker. */

/* =====   Seq Pool Wrapper   ====== */

type ZSTDMT_seqPool = struct {
	FpoolMutex    CRITICAL_SECTION
	FbufferSize   size_t
	FtotalBuffers uint32
	FnbBuffers    uint32
	FcMem         ZSTD_customMem
	Fbuffers      uintptr
}

func ZSTDMT_sizeof_seqPool(tls *libc.TLS, seqPool uintptr) (r size_t) {
	return ZSTDMT_sizeof_bufferPool(tls, seqPool)
}

func bufferToSeq(tls *libc.TLS, buffer Buffer) (r RawSeqStore_t) {
	var seq RawSeqStore_t
	_ = seq
	seq = kNullRawSeqStore
	seq.Fseq = buffer.Fstart
	seq.Fcapacity = buffer.Fcapacity / uint64(12)
	return seq
}

func seqToBuffer(tls *libc.TLS, seq RawSeqStore_t) (r Buffer) {
	var buffer Buffer
	_ = buffer
	buffer.Fstart = seq.Fseq
	buffer.Fcapacity = seq.Fcapacity * uint64(12)
	return buffer
}

func ZSTDMT_getSeq(tls *libc.TLS, seqPool uintptr) (r RawSeqStore_t) {
	if (*ZSTDMT_seqPool)(unsafe.Pointer(seqPool)).FbufferSize == uint64(0) {
		return kNullRawSeqStore
	}
	return bufferToSeq(tls, ZSTDMT_getBuffer(tls, seqPool))
}

func ZSTDMT_releaseSeq(tls *libc.TLS, seqPool uintptr, seq RawSeqStore_t) {
	ZSTDMT_releaseBuffer(tls, seqPool, seqToBuffer(tls, seq))
}

func ZSTDMT_setNbSeq(tls *libc.TLS, seqPool uintptr, nbSeq size_t) {
	ZSTDMT_setBufferSize(tls, seqPool, nbSeq*uint64(12))
}

func ZSTDMT_createSeqPool(tls *libc.TLS, nbWorkers uint32, cMem ZSTD_customMem) (r uintptr) {
	var seqPool uintptr
	_ = seqPool
	seqPool = ZSTDMT_createBufferPool(tls, nbWorkers, cMem)
	if seqPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTDMT_setNbSeq(tls, seqPool, uint64(0))
	return seqPool
}

func ZSTDMT_freeSeqPool(tls *libc.TLS, seqPool uintptr) {
	ZSTDMT_freeBufferPool(tls, seqPool)
}

func ZSTDMT_expandSeqPool(tls *libc.TLS, pool uintptr, nbWorkers U32) (r uintptr) {
	return ZSTDMT_expandBufferPool(tls, pool, nbWorkers)
}

/* =====   CCtx Pool   ===== */
/* a single CCtx Pool can be invoked from multiple threads in parallel */

type ZSTDMT_CCtxPool = struct {
	FpoolMutex CRITICAL_SECTION
	FtotalCCtx int32
	FavailCCtx int32
	FcMem      ZSTD_customMem
	Fcctxs     uintptr
}

// C documentation
//
//	/* note : all CCtx borrowed from the pool must be reverted back to the pool _before_ freeing the pool */
func ZSTDMT_freeCCtxPool(tls *libc.TLS, pool uintptr) {
	var cid int32
	_ = cid
	if !(pool != 0) {
		return
	}
	libc.XDeleteCriticalSection(tls, pool)
	if (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs != 0 {
		cid = 0
		for {
			if !(cid < (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FtotalCCtx) {
				break
			}
			ZSTD_freeCCtx(tls, *(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs + uintptr(cid)*8)))
			goto _1
		_1:
			;
			cid = cid + 1
		} /* free compatible with NULL */
		ZSTD_customFree(tls, (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs, (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FcMem)
	}
	ZSTD_customFree(tls, pool, (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FcMem)
}

// C documentation
//
//	/* ZSTDMT_createCCtxPool() :
//	 * implies nbWorkers >= 1 , checked by caller ZSTDMT_createCCtx() */
func ZSTDMT_createCCtxPool(tls *libc.TLS, nbWorkers int32, cMem ZSTD_customMem) (r uintptr) {
	var cctxPool uintptr
	_ = cctxPool
	cctxPool = ZSTD_customCalloc(tls, uint64(80), cMem)
	if !(cctxPool != 0) {
		return libc.UintptrFromInt32(0)
	}
	_ = libc.UintptrFromInt32(0)
	libc.XInitializeCriticalSection(tls, cctxPool)
	if libc.Int32FromInt32(0) != 0 {
		ZSTD_customFree(tls, cctxPool, cMem)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FtotalCCtx = nbWorkers
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs = ZSTD_customCalloc(tls, uint64(nbWorkers)*uint64(8), cMem)
	if !((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs != 0) {
		ZSTDMT_freeCCtxPool(tls, cctxPool)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FcMem = cMem
	*(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs)) = ZSTD_createCCtx_advanced(tls, cMem)
	if !(*(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs)) != 0) {
		ZSTDMT_freeCCtxPool(tls, cctxPool)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx = int32(1) /* at least one cctx for single-thread mode */
	return cctxPool
}

func ZSTDMT_expandCCtxPool(tls *libc.TLS, srcPool uintptr, nbWorkers int32) (r uintptr) {
	var cMem ZSTD_customMem
	_ = cMem
	if srcPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	if nbWorkers <= (*ZSTDMT_CCtxPool)(unsafe.Pointer(srcPool)).FtotalCCtx {
		return srcPool
	} /* good enough */
	/* need a larger cctx pool */
	cMem = (*ZSTDMT_CCtxPool)(unsafe.Pointer(srcPool)).FcMem
	ZSTDMT_freeCCtxPool(tls, srcPool)
	return ZSTDMT_createCCtxPool(tls, nbWorkers, cMem)
	return r
}

// C documentation
//
//	/* only works during initialization phase, not during compression */
func ZSTDMT_sizeof_CCtxPool(tls *libc.TLS, cctxPool uintptr) (r size_t) {
	var arraySize, poolSize, totalCCtxSize size_t
	var nbWorkers, u uint32
	_, _, _, _, _ = arraySize, nbWorkers, poolSize, totalCCtxSize, u
	libc.XEnterCriticalSection(tls, cctxPool)
	nbWorkers = uint32((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FtotalCCtx)
	poolSize = uint64(80)
	arraySize = uint64((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FtotalCCtx) * uint64(8)
	totalCCtxSize = uint64(0)
	u = uint32(0)
	for {
		if !(u < nbWorkers) {
			break
		}
		totalCCtxSize = totalCCtxSize + ZSTD_sizeof_CCtx(tls, *(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs + uintptr(u)*8)))
		goto _1
	_1:
		;
		u = u + 1
	}
	libc.XLeaveCriticalSection(tls, cctxPool)
	return poolSize + arraySize + totalCCtxSize
	return r
}

func ZSTDMT_getCCtx(tls *libc.TLS, cctxPool uintptr) (r uintptr) {
	var cctx uintptr
	_ = cctx
	libc.XEnterCriticalSection(tls, cctxPool)
	if (*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx != 0 {
		(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx = (*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx - 1
		cctx = *(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs + uintptr((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx)*8))
		libc.XLeaveCriticalSection(tls, cctxPool)
		return cctx
	}
	libc.XLeaveCriticalSection(tls, cctxPool)
	return ZSTD_createCCtx_advanced(tls, (*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FcMem) /* note : can be NULL, when creation fails ! */
}

func ZSTDMT_releaseCCtx(tls *libc.TLS, pool uintptr, cctx uintptr) {
	var v1 int32
	var v2 uintptr
	_, _ = v1, v2
	if cctx == libc.UintptrFromInt32(0) {
		return
	} /* compatibility with release on NULL */
	libc.XEnterCriticalSection(tls, pool)
	if (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FavailCCtx < (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FtotalCCtx {
		v2 = pool + 44
		v1 = *(*int32)(unsafe.Pointer(v2))
		*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(v2)) + 1
		*(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs + uintptr(v1)*8)) = cctx
	} else {
		/* pool overflow : should not happen, since totalCCtx==nbWorkers */
		ZSTD_freeCCtx(tls, cctx)
	}
	libc.XLeaveCriticalSection(tls, pool)
}

/* ====   Serial State   ==== */

type Range = struct {
	Fstart uintptr
	Fsize  size_t
}

type SerialState = struct {
	Fmutex          CRITICAL_SECTION
	Fcond           CONDITION_VARIABLE
	Fparams         ZSTD_CCtx_params
	FldmState       ldmState_t
	FxxhState       XXH_NAMESPACEXXH64_state_t
	FnextJobID      uint32
	FldmWindowMutex CRITICAL_SECTION
	FldmWindowCond  CONDITION_VARIABLE
	FldmWindow      ZSTD_window_t
}

func ZSTDMT_serialState_reset(tls *libc.TLS, serialState uintptr, seqPool uintptr, _params ZSTD_CCtx_params, jobSize size_t, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e) (r int32) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var bucketLog, hashLog, prevBucketLog, v1 uint32
	var cMem ZSTD_customMem
	var dictEnd uintptr
	var hashSize, numBuckets size_t
	_, _, _, _, _, _, _, _ = bucketLog, cMem, dictEnd, hashLog, hashSize, numBuckets, prevBucketLog, v1
	/* Adjust parameters */
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		ZSTD_ldm_adjustParameters(tls, bp+96, bp+4)
	} else {
		libc.Xmemset(tls, bp+96, 0, libc.Uint64FromInt64(24))
	}
	(*SerialState)(unsafe.Pointer(serialState)).FnextJobID = uint32(0)
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams.FchecksumFlag != 0 {
		XXH_INLINE_XXH64_reset(tls, serialState+2384, uint64(0))
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		cMem = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcustomMem
		hashLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FhashLog
		hashSize = libc.Uint64FromInt32(1) << hashLog * uint64(8)
		bucketLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FhashLog - (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FbucketSizeLog
		prevBucketLog = (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FhashLog - (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FbucketSizeLog
		numBuckets = libc.Uint64FromInt32(1) << bucketLog
		/* Size the seq pool tables */
		ZSTDMT_setNbSeq(tls, seqPool, ZSTD_ldm_getMaxNbSeq(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams, jobSize))
		/* Reset the window */
		ZSTD_window_init(tls, serialState+272)
		/* Resize tables and output space if necessary. */
		if (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable == libc.UintptrFromInt32(0) || (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FhashLog < hashLog {
			ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable, cMem)
			(*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable = ZSTD_customMalloc(tls, hashSize, cMem)
		}
		if (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets == libc.UintptrFromInt32(0) || prevBucketLog < bucketLog {
			ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets, cMem)
			(*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets = ZSTD_customMalloc(tls, numBuckets, cMem)
		}
		if !((*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable != 0) || !((*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets != 0) {
			return int32(1)
		}
		/* Zero the tables */
		libc.Xmemset(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable, 0, hashSize)
		libc.Xmemset(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets, 0, numBuckets)
		/* Update window state and fill hash table with dict */
		(*SerialState)(unsafe.Pointer(serialState)).FldmState.FloadedDictEnd = uint32(0)
		if dictSize > uint64(0) {
			if dictContentType == int32(ZSTD_dct_rawContent) {
				dictEnd = dict + uintptr(dictSize)
				ZSTD_window_update(tls, serialState+272, dict, dictSize, 0)
				ZSTD_ldm_fillHashTable(tls, serialState+272, dict, dictEnd, bp+96)
				if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FforceWindow != 0 {
					v1 = uint32(0)
				} else {
					v1 = uint32(int64(dictEnd) - int64((*SerialState)(unsafe.Pointer(serialState)).FldmState.Fwindow.Fbase))
				}
				(*SerialState)(unsafe.Pointer(serialState)).FldmState.FloadedDictEnd = v1
			} else {
				/* don't even load anything */
			}
		}
		/* Initialize serialState's copy of ldmWindow. */
		(*SerialState)(unsafe.Pointer(serialState)).FldmWindow = (*SerialState)(unsafe.Pointer(serialState)).FldmState.Fwindow
	}
	(*SerialState)(unsafe.Pointer(serialState)).Fparams = *(*ZSTD_CCtx_params)(unsafe.Pointer(bp))
	(*SerialState)(unsafe.Pointer(serialState)).Fparams.FjobSize = uint64(uint32(jobSize))
	return 0
}

func ZSTDMT_serialState_init(tls *libc.TLS, serialState uintptr) (r int32) {
	var initError int32
	_ = initError
	initError = 0
	libc.Xmemset(tls, serialState, 0, libc.Uint64FromInt64(2568))
	_ = libc.UintptrFromInt32(0)
	libc.XInitializeCriticalSection(tls, serialState)
	initError = initError | libc.Int32FromInt32(0)
	_ = libc.UintptrFromInt32(0)
	InitializeConditionVariable(tls, serialState+40)
	initError = initError | libc.Int32FromInt32(0)
	_ = libc.UintptrFromInt32(0)
	libc.XInitializeCriticalSection(tls, serialState+2480)
	initError = initError | libc.Int32FromInt32(0)
	_ = libc.UintptrFromInt32(0)
	InitializeConditionVariable(tls, serialState+2520)
	initError = initError | libc.Int32FromInt32(0)
	return initError
}

func ZSTDMT_serialState_free(tls *libc.TLS, serialState uintptr) {
	var cMem ZSTD_customMem
	_ = cMem
	cMem = (*SerialState)(unsafe.Pointer(serialState)).Fparams.FcustomMem
	libc.XDeleteCriticalSection(tls, serialState)
	_ = serialState + 40
	libc.XDeleteCriticalSection(tls, serialState+2480)
	_ = serialState + 2520
	ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable, cMem)
	ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets, cMem)
}

func ZSTDMT_serialState_genSequences(tls *libc.TLS, serialState uintptr, seqStore uintptr, src Range, jobID uint32) {
	var error1 size_t
	_ = error1
	/* Wait for our turn */
	libc.XEnterCriticalSection(tls, serialState)
	for (*SerialState)(unsafe.Pointer(serialState)).FnextJobID < jobID {
		SleepConditionVariableCS(tls, serialState+40, serialState, uint32(INFINITE))
	}
	/* A future job may error and skip our job */
	if (*SerialState)(unsafe.Pointer(serialState)).FnextJobID == jobID {
		/* It is now our turn, do any processing necessary */
		if (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
			ZSTD_window_update(tls, serialState+272, src.Fstart, src.Fsize, 0)
			error1 = ZSTD_ldm_generateSequences(tls, serialState+272, seqStore, serialState+48+96, src.Fstart, src.Fsize)
			/* We provide a large enough buffer to never fail. */
			_ = error1
			/* Update ldmWindow to match the ldmState.window and signal the main
			 * thread if it is waiting for a buffer.
			 */
			libc.XEnterCriticalSection(tls, serialState+2480)
			(*SerialState)(unsafe.Pointer(serialState)).FldmWindow = (*SerialState)(unsafe.Pointer(serialState)).FldmState.Fwindow
			WakeConditionVariable(tls, serialState+2520)
			libc.XLeaveCriticalSection(tls, serialState+2480)
		}
		if (*SerialState)(unsafe.Pointer(serialState)).Fparams.FfParams.FchecksumFlag != 0 && src.Fsize > uint64(0) {
			XXH_INLINE_XXH64_update(tls, serialState+2384, src.Fstart, src.Fsize)
		}
	}
	/* Now it is the next jobs turn */
	(*SerialState)(unsafe.Pointer(serialState)).FnextJobID = (*SerialState)(unsafe.Pointer(serialState)).FnextJobID + 1
	WakeAllConditionVariable(tls, serialState+40)
	libc.XLeaveCriticalSection(tls, serialState)
}

func ZSTDMT_serialState_applySequences(tls *libc.TLS, serialState uintptr, jobCCtx uintptr, seqStore uintptr) {
	if (*RawSeqStore_t)(unsafe.Pointer(seqStore)).Fsize > uint64(0) {
		_ = serialState
		ZSTD_referenceExternalSequences(tls, jobCCtx, (*RawSeqStore_t)(unsafe.Pointer(seqStore)).Fseq, (*RawSeqStore_t)(unsafe.Pointer(seqStore)).Fsize)
	}
}

func ZSTDMT_serialState_ensureFinished(tls *libc.TLS, serialState uintptr, jobID uint32, cSize size_t) {
	libc.XEnterCriticalSection(tls, serialState)
	if (*SerialState)(unsafe.Pointer(serialState)).FnextJobID <= jobID {
		_ = cSize
		(*SerialState)(unsafe.Pointer(serialState)).FnextJobID = jobID + uint32(1)
		WakeAllConditionVariable(tls, serialState+40)
		libc.XEnterCriticalSection(tls, serialState+2480)
		ZSTD_window_clear(tls, serialState+2528)
		WakeConditionVariable(tls, serialState+2520)
		libc.XLeaveCriticalSection(tls, serialState+2480)
	}
	libc.XLeaveCriticalSection(tls, serialState)
}

/* ------------------------------------------ */
/* =====          Worker thread         ===== */
/* ------------------------------------------ */

var kNullRange = Range{}

type ZSTDMT_jobDescription = struct {
	Fconsumed            size_t
	FcSize               size_t
	Fjob_mutex           CRITICAL_SECTION
	Fjob_cond            CONDITION_VARIABLE
	FcctxPool            uintptr
	FbufPool             uintptr
	FseqPool             uintptr
	Fserial              uintptr
	FdstBuff             Buffer
	Fprefix              Range
	Fsrc                 Range
	FjobID               uint32
	FfirstJob            uint32
	FlastJob             uint32
	Fparams              ZSTD_CCtx_params
	Fcdict               uintptr
	FfullFrameSize       uint64
	FdstFlushed          size_t
	FframeChecksumNeeded uint32
}

// C documentation
//
//	/* ZSTDMT_compressionJob() is a POOL_function type */
func ZSTDMT_compressionJob(tls *libc.TLS, jobDescription uintptr) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var cSize, cSize1, chunkSize, err, forceWindowError, hSize, initError, initError1, lastBlockSize, lastBlockSize1, lastCBlockSize size_t
	var cctx, ip, job, oend, op, ostart uintptr
	var chunkNb, nbChunks int32
	var dstBuff Buffer
	var pledgedSrcSize U64
	var v1, v3 uint64
	var _ /* jobParams at bp+0 */ ZSTD_CCtx_params
	var _ /* rawSeqStore at bp+224 */ RawSeqStore_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cSize, cSize1, cctx, chunkNb, chunkSize, dstBuff, err, forceWindowError, hSize, initError, initError1, ip, job, lastBlockSize, lastBlockSize1, lastCBlockSize, nbChunks, oend, op, ostart, pledgedSrcSize, v1, v3
	job = jobDescription
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fparams /* do not modify job->params ! copy it, modify the copy */
	cctx = ZSTDMT_getCCtx(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcctxPool)
	*(*RawSeqStore_t)(unsafe.Pointer(bp + 224)) = ZSTDMT_getSeq(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FseqPool)
	dstBuff = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff
	lastCBlockSize = uint64(0)
	/* resources */
	if cctx == libc.UintptrFromInt32(0) {
		libc.XEnterCriticalSection(tls, job+16)
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = uint64(-int32(ZSTD_error_memory_allocation))
		libc.XLeaveCriticalSection(tls, job+16)
		goto _endJob
	}
	if dstBuff.Fstart == libc.UintptrFromInt32(0) { /* streaming job : doesn't provide a dstBuffer */
		dstBuff = ZSTDMT_getBuffer(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FbufPool)
		if dstBuff.Fstart == libc.UintptrFromInt32(0) {
			libc.XEnterCriticalSection(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = uint64(-int32(ZSTD_error_memory_allocation))
			libc.XLeaveCriticalSection(tls, job+16)
			goto _endJob
		}
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff = dstBuff /* this value can be read in ZSTDMT_flush, when it copies the whole job */
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) && (*(*RawSeqStore_t)(unsafe.Pointer(bp + 224))).Fseq == libc.UintptrFromInt32(0) {
		libc.XEnterCriticalSection(tls, job+16)
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = uint64(-int32(ZSTD_error_memory_allocation))
		libc.XLeaveCriticalSection(tls, job+16)
		goto _endJob
	}
	/* Don't compute the checksum for chunks, since we compute it externally,
	 * but write it in the header.
	 */
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FjobID != uint32(0) {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams.FchecksumFlag = 0
	}
	/* Don't run LDM for the chunks, since we handle it externally */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm = int32(ZSTD_ps_disable)
	/* Correct nbWorkers to 0. */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers = 0
	/* init */
	/* Perform serial step as early as possible */
	ZSTDMT_serialState_genSequences(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fserial, bp+224, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FjobID)
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fcdict != 0 {
		initError = ZSTD_compressBegin_advanced_internal(tls, cctx, libc.UintptrFromInt32(0), uint64(0), int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fcdict, bp, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfullFrameSize)
		/* only allowed for first job */
		if ZSTD_isError(tls, initError) != 0 {
			libc.XEnterCriticalSection(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = initError
			libc.XLeaveCriticalSection(tls, job+16)
			goto _endJob
		}
	} else {
		if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0 {
			v1 = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfullFrameSize
		} else {
			v1 = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize
		}
		pledgedSrcSize = v1
		forceWindowError = ZSTD_CCtxParams_setParameter(tls, bp, int32(ZSTD_c_experimentalParam3), libc.BoolInt32(!((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0)))
		if ZSTD_isError(tls, forceWindowError) != 0 {
			libc.XEnterCriticalSection(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = forceWindowError
			libc.XLeaveCriticalSection(tls, job+16)
			goto _endJob
		}
		if !((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0) {
			err = ZSTD_CCtxParams_setParameter(tls, bp, int32(ZSTD_c_experimentalParam15), 0)
			if ZSTD_isError(tls, err) != 0 {
				libc.XEnterCriticalSection(tls, job+16)
				(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = err
				libc.XLeaveCriticalSection(tls, job+16)
				goto _endJob
			}
		}
		initError1 = ZSTD_compressBegin_advanced_internal(tls, cctx, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fprefix.Fstart, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fprefix.Fsize, int32(ZSTD_dct_rawContent), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), bp, pledgedSrcSize)
		if ZSTD_isError(tls, initError1) != 0 {
			libc.XEnterCriticalSection(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = initError1
			libc.XLeaveCriticalSection(tls, job+16)
			goto _endJob
		}
	}
	/* External Sequences can only be applied after CCtx initialization */
	ZSTDMT_serialState_applySequences(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fserial, cctx, bp+224)
	if !((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0) { /* flush and overwrite frame header when it's not first job */
		hSize = ZSTD_compressContinue_public(tls, cctx, dstBuff.Fstart, dstBuff.Fcapacity, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fstart, uint64(0))
		if ZSTD_isError(tls, hSize) != 0 {
			libc.XEnterCriticalSection(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = hSize
			libc.XLeaveCriticalSection(tls, job+16)
			goto _endJob
		}
		ZSTD_invalidateRepCodes(tls, cctx)
	}
	/* compress the entire job by smaller chunks, for better granularity */
	chunkSize = uint64(libc.Int32FromInt32(4) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)))
	nbChunks = int32(((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize + (chunkSize - libc.Uint64FromInt32(1))) / chunkSize)
	ip = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fstart
	ostart = dstBuff.Fstart
	op = ostart
	oend = op + uintptr(dstBuff.Fcapacity)
	if uint64(8) > uint64(4) {
	} /* check overflow */
	chunkNb = int32(1)
	for {
		if !(chunkNb < nbChunks) {
			break
		}
		cSize = ZSTD_compressContinue_public(tls, cctx, op, uint64(int64(oend)-int64(op)), ip, chunkSize)
		if ZSTD_isError(tls, cSize) != 0 {
			libc.XEnterCriticalSection(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = cSize
			libc.XLeaveCriticalSection(tls, job+16)
			goto _endJob
		}
		ip = ip + uintptr(chunkSize)
		op = op + uintptr(cSize)
		/* stats */
		libc.XEnterCriticalSection(tls, job+16)
		*(*size_t)(unsafe.Pointer(job + 8)) += cSize
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fconsumed = chunkSize * uint64(chunkNb)
		WakeConditionVariable(tls, job+56) /* warns some more data is ready to be flushed */
		libc.XLeaveCriticalSection(tls, job+16)
		goto _2
	_2:
		;
		chunkNb = chunkNb + 1
	}
	/* last block */
	/* chunkSize must be power of 2 for mask==(chunkSize-1) to work */
	if libc.BoolUint32(nbChunks > libc.Int32FromInt32(0))|(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FlastJob != 0 {
		lastBlockSize1 = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize & (chunkSize - uint64(1))
		if libc.BoolInt32(lastBlockSize1 == uint64(0))&libc.BoolInt32((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize >= chunkSize) != 0 {
			v1 = chunkSize
		} else {
			v1 = lastBlockSize1
		}
		lastBlockSize = v1
		if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FlastJob != 0 {
			v3 = ZSTD_compressEnd_public(tls, cctx, op, uint64(int64(oend)-int64(op)), ip, lastBlockSize)
		} else {
			v3 = ZSTD_compressContinue_public(tls, cctx, op, uint64(int64(oend)-int64(op)), ip, lastBlockSize)
		}
		cSize1 = v3
		if ZSTD_isError(tls, cSize1) != 0 {
			libc.XEnterCriticalSection(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = cSize1
			libc.XLeaveCriticalSection(tls, job+16)
			goto _endJob
		}
		lastCBlockSize = cSize1
	}
	if !((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0) {
		/* Double check that we don't have an ext-dict, because then our
		 * repcode invalidation doesn't work.
		 */
	}
	ZSTD_CCtx_trace(tls, cctx, uint64(0))
	goto _endJob
_endJob:
	;
	ZSTDMT_serialState_ensureFinished(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fserial, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FjobID, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize)
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fprefix.Fsize > uint64(0) {
	}
	/* release resources */
	ZSTDMT_releaseSeq(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FseqPool, *(*RawSeqStore_t)(unsafe.Pointer(bp + 224)))
	ZSTDMT_releaseCCtx(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcctxPool, cctx)
	/* report */
	libc.XEnterCriticalSection(tls, job+16)
	if ZSTD_isError(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize) != 0 {
	}
	*(*size_t)(unsafe.Pointer(job + 8)) += lastCBlockSize
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fconsumed = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize /* when job->consumed == job->src.size , compression job is presumed completed */
	WakeConditionVariable(tls, job+56)
	libc.XLeaveCriticalSection(tls, job+16)
}

/* ------------------------------------------ */
/* =====   Multi-threaded compression   ===== */
/* ------------------------------------------ */

type InBuff_t = struct {
	Fprefix Range
	Fbuffer Buffer
	Ffilled size_t
}

type RoundBuff_t = struct {
	Fbuffer   uintptr
	Fcapacity size_t
	Fpos      size_t
}

var kNullRoundBuff = RoundBuff_t{}

/* Don't create chunks smaller than the zstd block size.
 * This stops us from regressing compression ratio too much,
 * and ensures our output fits in ZSTD_compressBound().
 *
 * If this is shrunk < ZSTD_BLOCKSIZELOG_MIN then
 * ZSTD_COMPRESSBOUND() will need to be updated.
 */

type RSyncState_t = struct {
	Fhash       U64
	FhitMask    U64
	FprimePower U64
}

func ZSTDMT_freeJobsTable(tls *libc.TLS, jobTable uintptr, nbJobs U32, cMem ZSTD_customMem) {
	var jobNb U32
	_ = jobNb
	if jobTable == libc.UintptrFromInt32(0) {
		return
	}
	jobNb = uint32(0)
	for {
		if !(jobNb < nbJobs) {
			break
		}
		libc.XDeleteCriticalSection(tls, jobTable+uintptr(jobNb)*416+16)
		_ = jobTable + uintptr(jobNb)*416 + 56
		goto _1
	_1:
		;
		jobNb = jobNb + 1
	}
	ZSTD_customFree(tls, jobTable, cMem)
}

// C documentation
//
//	/* ZSTDMT_allocJobsTable()
//	 * allocate and init a job table.
//	 * update *nbJobsPtr to next power of 2 value, as size of table */
func ZSTDMT_createJobsTable(tls *libc.TLS, nbJobsPtr uintptr, cMem ZSTD_customMem) (r uintptr) {
	var initError int32
	var jobNb, nbJobs, nbJobsLog2 U32
	var jobTable uintptr
	_, _, _, _, _ = initError, jobNb, jobTable, nbJobs, nbJobsLog2
	nbJobsLog2 = ZSTD_highbit32(tls, *(*U32)(unsafe.Pointer(nbJobsPtr))) + uint32(1)
	nbJobs = uint32(int32(1) << nbJobsLog2)
	jobTable = ZSTD_customCalloc(tls, uint64(nbJobs)*uint64(416), cMem)
	initError = 0
	if jobTable == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	*(*U32)(unsafe.Pointer(nbJobsPtr)) = nbJobs
	jobNb = uint32(0)
	for {
		if !(jobNb < nbJobs) {
			break
		}
		_ = libc.UintptrFromInt32(0)
		libc.XInitializeCriticalSection(tls, jobTable+uintptr(jobNb)*416+16)
		initError = initError | libc.Int32FromInt32(0)
		_ = libc.UintptrFromInt32(0)
		InitializeConditionVariable(tls, jobTable+uintptr(jobNb)*416+56)
		initError = initError | libc.Int32FromInt32(0)
		goto _1
	_1:
		;
		jobNb = jobNb + 1
	}
	if initError != 0 {
		ZSTDMT_freeJobsTable(tls, jobTable, nbJobs, cMem)
		return libc.UintptrFromInt32(0)
	}
	return jobTable
}

func ZSTDMT_expandJobsTable(tls *libc.TLS, mtctx uintptr, nbWorkers U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* nbJobs at bp+0 */ U32
	*(*U32)(unsafe.Pointer(bp)) = nbWorkers + uint32(2)
	if *(*U32)(unsafe.Pointer(bp)) > (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+uint32(1) { /* need more job capacity */
		ZSTDMT_freeJobsTable(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+uint32(1), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask = uint32(0)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs = ZSTDMT_createJobsTable(tls, bp, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs == libc.UintptrFromInt32(0) {
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		/* ensure nbJobs is a power of 2 */
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask = *(*U32)(unsafe.Pointer(bp)) - uint32(1)
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTDMT_CCtxParam_setNbWorkers():
//	 * Internal use only */
func ZSTDMT_CCtxParam_setNbWorkers(tls *libc.TLS, params uintptr, nbWorkers uint32) (r size_t) {
	return ZSTD_CCtxParams_setParameter(tls, params, int32(ZSTD_c_nbWorkers), int32(nbWorkers))
}

func ZSTDMT_createCCtx_advanced_internal(tls *libc.TLS, nbWorkers uint32, cMem ZSTD_customMem, pool uintptr) (r uintptr) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var initError int32
	var mtctx uintptr
	var v1 uint32
	var _ /* nbJobs at bp+0 */ U32
	_, _, _ = initError, mtctx, v1
	*(*U32)(unsafe.Pointer(bp)) = nbWorkers + uint32(2)
	if nbWorkers < uint32(1) {
		return libc.UintptrFromInt32(0)
	}
	if nbWorkers < uint32(libc.Int32FromInt32(256)) {
		v1 = nbWorkers
	} else {
		v1 = uint32(libc.Int32FromInt32(256))
	}
	nbWorkers = v1
	if libc.BoolInt32(cMem.FcustomAlloc != libc.UintptrFromInt32(0))^libc.BoolInt32(cMem.FcustomFree != libc.UintptrFromInt32(0)) != 0 {
		/* invalid custom allocator */
		return libc.UintptrFromInt32(0)
	}
	mtctx = ZSTD_customCalloc(tls, uint64(3040), cMem)
	if !(mtctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTDMT_CCtxParam_setNbWorkers(tls, mtctx+40, nbWorkers)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem = cMem
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(1)
	if pool != libc.UintptrFromInt32(0) {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory = pool
		libc.SetBitFieldPtr8Uint32(mtctx+3032, libc.Uint32FromInt32(1), 0, 0x1)
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory = POOL_create_advanced(tls, uint64(nbWorkers), uint64(0), cMem)
		libc.SetBitFieldPtr8Uint32(mtctx+3032, libc.Uint32FromInt32(0), 0, 0x1)
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs = ZSTDMT_createJobsTable(tls, bp, cMem)
	/* ensure nbJobs is a power of 2 */
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask = *(*U32)(unsafe.Pointer(bp)) - uint32(1)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool = ZSTDMT_createBufferPool(tls, uint32(2)*nbWorkers+uint32(3), cMem)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool = ZSTDMT_createCCtxPool(tls, int32(nbWorkers), cMem)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool = ZSTDMT_createSeqPool(tls, nbWorkers, cMem)
	initError = ZSTDMT_serialState_init(tls, mtctx+352)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff = kNullRoundBuff
	if libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool != 0))|initError != 0 {
		ZSTDMT_freeCCtx(tls, mtctx)
		return libc.UintptrFromInt32(0)
	}
	return mtctx
}

func ZSTDMT_createCCtx_advanced(tls *libc.TLS, nbWorkers uint32, cMem ZSTD_customMem, pool uintptr) (r uintptr) {
	return ZSTDMT_createCCtx_advanced_internal(tls, nbWorkers, cMem, pool)
}

// C documentation
//
//	/* ZSTDMT_releaseAllJobResources() :
//	 * note : ensure all workers are killed first ! */
func ZSTDMT_releaseAllJobResources(tls *libc.TLS, mtctx uintptr) {
	var cond CONDITION_VARIABLE
	var jobID uint32
	var mutex CRITICAL_SECTION
	_, _, _ = cond, jobID, mutex
	jobID = uint32(0)
	for {
		if !(jobID <= (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask) {
			break
		}
		/* Copy the mutex/cond out */
		mutex = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fjob_mutex
		cond = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fjob_cond
		ZSTDMT_releaseBuffer(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FdstBuff)
		/* Clear the job description, but keep the mutex/cond */
		libc.Xmemset(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*416, 0, libc.Uint64FromInt64(416))
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fjob_mutex = mutex
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fjob_cond = cond
		goto _1
	_1:
		;
		jobID = jobID + 1
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = g_nullBuffer
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(1)
}

func ZSTDMT_waitForAllJobsCompleted(tls *libc.TLS, mtctx uintptr) {
	var jobID uint32
	_ = jobID
	for (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		jobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
		libc.XEnterCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*416+16)
		for (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fconsumed < (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fsrc.Fsize {
			/* we want to block when waiting for data to flush */
			SleepConditionVariableCS(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*416+56, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*416+16, uint32(INFINITE))
		}
		libc.XLeaveCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*416+16)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID + 1
	}
}

func ZSTDMT_freeCCtx(tls *libc.TLS, mtctx uintptr) (r size_t) {
	if mtctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* compatible with free on NULL */
	if !(int32(uint32(*(*uint8)(unsafe.Pointer(mtctx + 3032))&0x1>>0)) != 0) {
		POOL_free(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory)
	} /* stop and free worker threads */
	ZSTDMT_releaseAllJobResources(tls, mtctx) /* release job resources into pools first */
	ZSTDMT_freeJobsTable(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+uint32(1), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
	ZSTDMT_freeBufferPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool)
	ZSTDMT_freeCCtxPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool)
	ZSTDMT_freeSeqPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool)
	ZSTDMT_serialState_free(tls, mtctx+352)
	ZSTD_freeCDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal)
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer != 0 {
		ZSTD_customFree(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
	}
	ZSTD_customFree(tls, mtctx, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
	return uint64(0)
}

func ZSTDMT_sizeof_CCtx(tls *libc.TLS, mtctx uintptr) (r size_t) {
	if mtctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* supports sizeof NULL */
	return uint64(3040) + POOL_sizeof(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory) + ZSTDMT_sizeof_bufferPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool) + uint64((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+libc.Uint32FromInt32(1))*uint64(416) + ZSTDMT_sizeof_CCtxPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool) + ZSTDMT_sizeof_seqPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool) + ZSTD_sizeof_CDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal) + (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity
}

// C documentation
//
//	/* ZSTDMT_resize() :
//	 * @return : error code if fails, 0 on success */
func ZSTDMT_resize(tls *libc.TLS, mtctx uintptr, nbWorkers uint32) (r size_t) {
	var err_code size_t
	_ = err_code
	if POOL_resize(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory, uint64(nbWorkers)) != 0 {
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	err_code = ZSTDMT_expandJobsTable(tls, mtctx, nbWorkers)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool = ZSTDMT_expandBufferPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, uint32(2)*nbWorkers+uint32(3))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool == libc.UintptrFromInt32(0) {
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool = ZSTDMT_expandCCtxPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool, int32(nbWorkers))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool == libc.UintptrFromInt32(0) {
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool = ZSTDMT_expandSeqPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool, nbWorkers)
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool == libc.UintptrFromInt32(0) {
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	ZSTDMT_CCtxParam_setNbWorkers(tls, mtctx+40, nbWorkers)
	return uint64(0)
}

// C documentation
//
//	/*! ZSTDMT_updateCParams_whileCompressing() :
//	 *  Updates a selected set of compression parameters, remaining compatible with currently active frame.
//	 *  New parameters will be applied to next compression job. */
func ZSTDMT_updateCParams_whileCompressing(tls *libc.TLS, mtctx uintptr, cctxParams uintptr) {
	var cParams ZSTD_compressionParameters
	var compressionLevel int32
	var saved_wlog U32
	_, _, _ = cParams, compressionLevel, saved_wlog
	saved_wlog = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcParams.FwindowLog /* Do not modify windowLog while compressing */
	compressionLevel = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcompressionLevel = compressionLevel
	cParams = ZSTD_getCParamsFromCCtxParams(tls, cctxParams, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), uint64(0), int32(ZSTD_cpm_noAttachDict))
	cParams.FwindowLog = saved_wlog
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcParams = cParams
}

// C documentation
//
//	/* ZSTDMT_getFrameProgression():
//	 * tells how much data has been consumed (input) and produced (output) for current frame.
//	 * able to count progression inside worker threads.
//	 * Note : mutex will be acquired during statistics collection inside workers. */
func ZSTDMT_getFrameProgression(tls *libc.TLS, mtctx uintptr) (r ZSTD_frameProgression) {
	var cResult, flushed, produced size_t
	var fps ZSTD_frameProgression
	var jobNb, lastJobNb, wJobID uint32
	var jobPtr uintptr
	var v1, v3 uint64
	_, _, _, _, _, _, _, _, _, _ = cResult, flushed, fps, jobNb, jobPtr, lastJobNb, produced, wJobID, v1, v3
	fps.Fingested = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fconsumed + (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
	fps.Fconsumed = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fconsumed
	v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fproduced
	fps.Fflushed = v1
	fps.Fproduced = v1
	fps.FcurrentJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID
	fps.FnbActiveWorkers = uint32(0)
	lastJobNb = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID + uint32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady)
	jobNb = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID
	for {
		if !(jobNb < lastJobNb) {
			break
		}
		wJobID = jobNb & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
		jobPtr = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416
		libc.XEnterCriticalSection(tls, jobPtr+16)
		cResult = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FcSize
		if ZSTD_isError(tls, cResult) != 0 {
			v1 = uint64(0)
		} else {
			v1 = cResult
		}
		produced = v1
		if ZSTD_isError(tls, cResult) != 0 {
			v3 = uint64(0)
		} else {
			v3 = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FdstFlushed
		}
		flushed = v3
		fps.Fingested += (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fsrc.Fsize
		fps.Fconsumed += (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fconsumed
		fps.Fproduced += produced
		fps.Fflushed += flushed
		fps.FnbActiveWorkers += libc.BoolUint32((*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fconsumed < (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fsrc.Fsize)
		libc.XLeaveCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+16)
		goto _2
	_2:
		;
		jobNb = jobNb + 1
	}
	return fps
}

func ZSTDMT_toFlushNow(tls *libc.TLS, mtctx uintptr) (r size_t) {
	var cResult, flushed, produced, toFlush size_t
	var jobID, wJobID uint32
	var jobPtr uintptr
	var v1, v2 uint64
	_, _, _, _, _, _, _, _, _ = cResult, flushed, jobID, jobPtr, produced, toFlush, wJobID, v1, v2
	jobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID
	if jobID == (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		return uint64(0)
	} /* no active job => nothing to flush */
	/* look into oldest non-fully-flushed job */
	wJobID = jobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
	jobPtr = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416
	libc.XEnterCriticalSection(tls, jobPtr+16)
	cResult = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FcSize
	if ZSTD_isError(tls, cResult) != 0 {
		v1 = uint64(0)
	} else {
		v1 = cResult
	}
	produced = v1
	if ZSTD_isError(tls, cResult) != 0 {
		v2 = uint64(0)
	} else {
		v2 = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FdstFlushed
	}
	flushed = v2
	toFlush = produced - flushed
	/* if toFlush==0, nothing is available to flush.
	 * However, jobID is expected to still be active:
	 * if jobID was already completed and fully flushed,
	 * ZSTDMT_flushProduced() should have already moved onto next job.
	 * Therefore, some input has not yet been consumed. */
	if toFlush == uint64(0) {
	}
	libc.XLeaveCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+16)
	return toFlush
}

/* ------------------------------------------ */
/* =====   Multi-threaded compression   ===== */
/* ------------------------------------------ */

func ZSTDMT_computeTargetJobLog(tls *libc.TLS, params uintptr) (r uint32) {
	var jobLog, v1 uint32
	var v4, v5 int32
	_, _, _, _ = jobLog, v1, v4, v5
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* In Long Range Mode, the windowLog is typically oversized.
		 * In which case, it's preferable to determine the jobSize
		 * based on cycleLog instead. */
		if uint32(libc.Int32FromInt32(21)) > ZSTD_cycleLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy)+uint32(3) {
			v1 = uint32(libc.Int32FromInt32(21))
		} else {
			v1 = ZSTD_cycleLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy) + uint32(3)
		}
		jobLog = v1
	} else {
		if uint32(libc.Int32FromInt32(20)) > (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog+uint32(2) {
			v1 = uint32(libc.Int32FromInt32(20))
		} else {
			v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog + uint32(2)
		}
		jobLog = v1
	}
	if MEM_32bits(tls) != 0 {
		v4 = int32(29)
	} else {
		v4 = int32(30)
	}
	if jobLog < uint32(v4) {
		v1 = jobLog
	} else {
		if MEM_32bits(tls) != 0 {
			v5 = int32(29)
		} else {
			v5 = int32(30)
		}
		v1 = uint32(v5)
	}
	return v1
}

func ZSTDMT_overlapLog_default(tls *libc.TLS, strat ZSTD_strategy) (r int32) {
	switch strat {
	case int32(ZSTD_btultra2):
		return int32(9)
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btopt):
		return int32(8)
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_lazy2):
		return int32(7)
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_dfast):
		fallthrough
	case int32(ZSTD_fast):
		fallthrough
	default:
	}
	return int32(6)
}

func ZSTDMT_overlapLog(tls *libc.TLS, ovlog int32, strat ZSTD_strategy) (r int32) {
	if ovlog == 0 {
		return ZSTDMT_overlapLog_default(tls, strat)
	}
	return ovlog
}

func ZSTDMT_computeOverlapSize(tls *libc.TLS, params uintptr) (r size_t) {
	var ovLog, overlapRLog int32
	var v1 uint32
	var v3 uint64
	_, _, _, _ = ovLog, overlapRLog, v1, v3
	overlapRLog = int32(9) - ZSTDMT_overlapLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FoverlapLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy)
	if overlapRLog >= int32(8) {
		v1 = uint32(0)
	} else {
		v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog - uint32(overlapRLog)
	}
	ovLog = int32(v1)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* In Long Range Mode, the windowLog is typically oversized.
		 * In which case, it's preferable to determine the jobSize
		 * based on chainLog instead.
		 * Then, ovLog becomes a fraction of the jobSize, rather than windowSize */
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog < ZSTDMT_computeTargetJobLog(tls, params)-uint32(2) {
			v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
		} else {
			v1 = ZSTDMT_computeTargetJobLog(tls, params) - uint32(2)
		}
		ovLog = int32(v1 - uint32(overlapRLog))
	}
	if ovLog == 0 {
		v3 = uint64(0)
	} else {
		v3 = libc.Uint64FromInt32(1) << ovLog
	}
	return v3
}

/* ====================================== */
/* =======      Streaming API     ======= */
/* ====================================== */

func ZSTDMT_initCStream_internal(tls *libc.TLS, mtctx uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, cdict uintptr, _params ZSTD_CCtx_params, pledgedSrcSize uint64) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var capacity, err_code, nbSlackBuffers, nbWorkers, sectionsSize, slackSize, windowSize size_t
	var jobSizeKB, rsyncBits U32
	var v1, v2 int32
	var v3 uint32
	var v5 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _ = capacity, err_code, jobSizeKB, nbSlackBuffers, nbWorkers, rsyncBits, sectionsSize, slackSize, windowSize, v1, v2, v3, v5
	/* params supposed partially fully validated at this point */
	/* either dict or cdict, not both */
	/* init */
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers != (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FnbWorkers {
		err_code = ZSTDMT_resize(tls, mtctx, uint32((*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers))
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize != uint64(0) && (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize < uint64(libc.Int32FromInt32(512)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize = uint64(libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	if MEM_32bits(tls) != 0 {
		v1 = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
	} else {
		v1 = libc.Int32FromInt32(1024) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize > uint64(v1) {
		if MEM_32bits(tls) != 0 {
			v2 = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		} else {
			v2 = libc.Int32FromInt32(1024) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		}
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize = uint64(v2)
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted == uint32(0) { /* previous compression not correctly finished */
		ZSTDMT_waitForAllJobsCompleted(tls, mtctx)
		ZSTDMT_releaseAllJobResources(tls, mtctx)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(1)
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams = *(*ZSTD_CCtx_params)(unsafe.Pointer(bp))
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeContentSize = pledgedSrcSize
	ZSTD_freeCDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal)
	if dict != 0 {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byCopy), dictContentType, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal == libc.UintptrFromInt32(0) {
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = libc.UintptrFromInt32(0)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = cdict
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize = ZSTDMT_computeOverlapSize(tls, bp)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize == uint64(0) {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize = uint64(1) << ZSTDMT_computeTargetJobLog(tls, bp)
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).Frsyncable != 0 {
		/* Aim for the targetsectionSize as the average job size. */
		jobSizeKB = uint32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize >> libc.Int32FromInt32(10))
		rsyncBits = ZSTD_highbit32(tls, jobSizeKB) + libc.Uint32FromInt32(10)
		/* We refuse to create jobs < RSYNC_MIN_BLOCK_SIZE bytes, so make sure our
		 * expected job size is at least 4x larger. */
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.Fhash = uint64(0)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FhitMask = uint64(1)<<rsyncBits - uint64(1)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FprimePower = ZSTD_rollingHash_primePower(tls, uint32(RSYNC_LENGTH))
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize
	} /* job size must be >= overlap size */
	ZSTDMT_setBufferSize(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, ZSTD_compressBound(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		v3 = uint32(1) << (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcParams.FwindowLog
	} else {
		v3 = uint32(0)
	}
	/* If ldm is enabled we need windowSize space. */
	windowSize = uint64(v3)
	/* Two buffers of slack, plus extra space for the overlap
	 * This is the minimum slack that LDM works with. One extra because
	 * flush might waste up to targetSectionSize-1 bytes. Another extra
	 * for the overlap (if > 0), then one to fill which doesn't overlap
	 * with the LDM window.
	 */
	nbSlackBuffers = uint64(int32(2) + libc.BoolInt32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize > uint64(0)))
	slackSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize * nbSlackBuffers
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FnbWorkers > int32(1) {
		v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FnbWorkers
	} else {
		v1 = int32(1)
	}
	/* Compute the total size, and always have enough slack */
	nbWorkers = uint64(v1)
	sectionsSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize * nbWorkers
	if windowSize > sectionsSize {
		v5 = windowSize
	} else {
		v5 = sectionsSize
	}
	capacity = v5 + slackSize
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity < capacity {
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer != 0 {
			ZSTD_customFree(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		}
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer = ZSTD_customMalloc(tls, capacity, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer == libc.UintptrFromInt32(0) {
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity = uint64(0)
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity = capacity
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = g_nullBuffer
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix = kNullRange
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fconsumed = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fproduced = uint64(0)
	/* update dictionary */
	ZSTD_freeCDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = libc.UintptrFromInt32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = libc.UintptrFromInt32(0)
	if dict != 0 {
		if dictContentType == int32(ZSTD_dct_rawContent) {
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart = dict
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fsize = dictSize
		} else {
			/* note : a loadPrefix becomes an internal CDict */
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byRef), dictContentType, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal
			if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal == libc.UintptrFromInt32(0) {
				return uint64(-int32(ZSTD_error_memory_allocation))
			}
		}
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = cdict
	}
	if ZSTDMT_serialState_reset(tls, mtctx+352, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp)), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize, dict, dictSize, dictContentType) != 0 {
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTDMT_writeLastEmptyBlock()
//	 * Write a single empty block with an end-of-frame to finish a frame.
//	 * Job must be created from streaming variant.
//	 * This function is always successful if expected conditions are fulfilled.
//	 */
func ZSTDMT_writeLastEmptyBlock(tls *libc.TLS, job uintptr) {
	/* last job is empty -> will be simplified into a last empty block */
	/* cannot be first job, as it also needs to create frame header */
	/* invoked from streaming variant only (otherwise, dstBuff might be user's output) */
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff = ZSTDMT_getBuffer(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FbufPool)
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff.Fstart == libc.UintptrFromInt32(0) {
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = uint64(-int32(ZSTD_error_memory_allocation))
		return
	}
	/* no buffer should ever be that small */
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc = kNullRange
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = ZSTD_writeLastEmptyBlock(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff.Fstart, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff.Fcapacity)
}

func ZSTDMT_createCompressionJob(tls *libc.TLS, mtctx uintptr, srcSize size_t, endOp ZSTD_EndDirective) (r size_t) {
	var endFrame int32
	var jobID uint32
	var newPrefixSize size_t
	var src, v1 uintptr
	var v2 uint64
	_, _, _, _, _, _ = endFrame, jobID, newPrefixSize, src, v1, v2
	jobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
	endFrame = libc.BoolInt32(endOp == int32(ZSTD_e_end))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID > (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID+(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask {
		return uint64(0)
	}
	if !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0) {
		src = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fsrc.Fstart = src
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fsrc.Fsize = srcSize
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fprefix = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fconsumed = uint64(0)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FcSize = uint64(0)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fparams = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID == uint32(0) {
			v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict
		} else {
			v1 = libc.UintptrFromInt32(0)
		}
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fcdict = v1
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FfullFrameSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeContentSize
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FdstBuff = g_nullBuffer
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FcctxPool = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FbufPool = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FseqPool = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).Fserial = mtctx + 352
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FjobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FfirstJob = libc.BoolUint32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID == libc.Uint32FromInt32(0))
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FlastJob = uint32(endFrame)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FframeChecksumNeeded = libc.BoolUint32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FfParams.FchecksumFlag != 0 && endFrame != 0 && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID > uint32(0))
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*416))).FdstFlushed = uint64(0)
		/* Update the round buffer pos and clear the input buffer to be reset */
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos += srcSize
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = g_nullBuffer
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
		/* Set the prefix for next job */
		if !(endFrame != 0) {
			if srcSize < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize {
				v2 = srcSize
			} else {
				v2 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize
			}
			newPrefixSize = v2
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart = src + uintptr(srcSize) - uintptr(newPrefixSize)
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fsize = newPrefixSize
		} else { /* endFrame==1 => no need for another input buffer */
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix = kNullRange
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded = uint32(endFrame)
			if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID == uint32(0) {
				/* single job exception : checksum is already calculated directly within worker thread */
				(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FfParams.FchecksumFlag = 0
			}
		}
		if srcSize == uint64(0) && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID > uint32(0) {
			/* only possible case : need to end the frame with an empty last block */
			ZSTDMT_writeLastEmptyBlock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*416)
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID + 1
			return uint64(0)
		}
	}
	if POOL_tryAdd(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory, __ccgo_fp(ZSTDMT_compressionJob), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*416) != 0 {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID + 1
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady = 0
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady = int32(1)
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTDMT_flushProduced() :
//	 *  flush whatever data has been produced but not yet flushed in current job.
//	 *  move to next job if current one is fully flushed.
//	 * `output` : `pos` will be updated with amount of data flushed .
//	 * `blockToFlush` : if >0, the function will block and wait if there is no data available to flush .
//	 * @return : amount of data remaining within internal buffer, 0 if no more, 1 if unknown but > 0, or an error code */
func ZSTDMT_flushProduced(tls *libc.TLS, mtctx uintptr, output uintptr, blockToFlush uint32, end ZSTD_EndDirective) (r size_t) {
	var cSize, srcConsumed, srcSize, toFlush size_t
	var checksum U32
	var wJobID uint32
	var v1 uint64
	_, _, _, _, _, _, _ = cSize, checksum, srcConsumed, srcSize, toFlush, wJobID, v1
	wJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
	libc.XEnterCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+16)
	if blockToFlush != 0 && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		for (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed == (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FcSize { /* nothing to flush */
			if (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fconsumed == (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fsrc.Fsize {
				break
			}
			SleepConditionVariableCS(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+56, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+16, uint32(INFINITE)) /* block when nothing to flush but some to come */
		}
	}
	/* try to flush something */
	cSize = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FcSize          /* shared */
	srcConsumed = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fconsumed /* shared */
	srcSize = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fsrc.Fsize    /* read-only, could be done after mutex lock, but no-declaration-after-statement */
	libc.XLeaveCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+16)
	if ZSTD_isError(tls, cSize) != 0 {
		ZSTDMT_waitForAllJobsCompleted(tls, mtctx)
		ZSTDMT_releaseAllJobResources(tls, mtctx)
		return cSize
	}
	/* add frame checksum if necessary (can only happen once) */
	if srcConsumed == srcSize && (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FframeChecksumNeeded != 0 {
		checksum = uint32(XXH_INLINE_XXH64_digest(tls, mtctx+352+2384))
		MEM_writeLE32(tls, (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstBuff.Fstart+uintptr((*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FcSize), checksum)
		cSize = cSize + uint64(4)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FcSize += uint64(4) /* can write this shared value, as worker is no longer active */
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FframeChecksumNeeded = uint32(0)
	}
	if cSize > uint64(0) {
		if cSize-(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed < (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize-(*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos {
			v1 = cSize - (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed
		} else {
			v1 = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize - (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
		} /* compression is ongoing or completed */
		toFlush = v1
		if toFlush > uint64(0) {
			libc.Xmemcpy(tls, (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst+uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos), (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstBuff.Fstart+uintptr((*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed), toFlush)
		}
		*(*size_t)(unsafe.Pointer(output + 16)) += toFlush
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed += toFlush                              /* can write : this value is only used by mtctx */
		if srcConsumed == srcSize && (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed == cSize { /* output buffer fully flushed => free this job position */
			ZSTDMT_releaseBuffer(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstBuff)
			(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstBuff = g_nullBuffer
			(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FcSize = uint64(0) /* ensure this job slot is considered "not started" in future check */
			*(*uint64)(unsafe.Pointer(mtctx + 2976)) += srcSize
			*(*uint64)(unsafe.Pointer(mtctx + 2984)) += cSize
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID + 1
		}
	}
	/* return value : how many bytes left in buffer ; fake it to 1 when unknown but >0 */
	if cSize > (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed {
		return cSize - (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).FdstFlushed
	}
	if srcSize > srcConsumed {
		return uint64(1)
	} /* current job not completely compressed */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		return uint64(1)
	} /* some more jobs ongoing */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0 {
		return uint64(1)
	} /* one job is ready to push, just not yet in the list */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled > uint64(0) {
		return uint64(1)
	} /* input is not empty, and still needs to be converted into a job */
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded /* all jobs are entirely flushed => if this one is last one, frame is completed */
	if end == int32(ZSTD_e_end) {
		return libc.BoolUint64(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded != 0))
	} /* for ZSTD_e_end, question becomes : is frame completed ? instead of : are internal buffers fully flushed ? */
	return uint64(0) /* internal buffers fully flushed */
}

// C documentation
//
//	/**
//	 * Returns the range of data used by the earliest job that is not yet complete.
//	 * If the data of the first job is broken up into two segments, we cover both
//	 * sections.
//	 */
func ZSTDMT_getInputDataInUse(tls *libc.TLS, mtctx uintptr) (r Range) {
	var consumed, nbJobs1stRoundMin, roundBuffCapacity size_t
	var firstJobID, jobID, lastJobID, wJobID uint32
	var range1 Range
	_, _, _, _, _, _, _, _ = consumed, firstJobID, jobID, lastJobID, nbJobs1stRoundMin, range1, roundBuffCapacity, wJobID
	firstJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID
	lastJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID
	/* no need to check during first round */
	roundBuffCapacity = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity
	nbJobs1stRoundMin = roundBuffCapacity / (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize
	if uint64(lastJobID) < nbJobs1stRoundMin {
		return kNullRange
	}
	jobID = firstJobID
	for {
		if !(jobID < lastJobID) {
			break
		}
		wJobID = jobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
		libc.XEnterCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+16)
		consumed = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fconsumed
		libc.XLeaveCriticalSection(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*416+16)
		if consumed < (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fsrc.Fsize {
			range1 = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fprefix
			if range1.Fsize == uint64(0) {
				/* Empty prefix */
				range1 = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*416))).Fsrc
			}
			/* Job source in multiple segments not supported yet */
			return range1
		}
		goto _1
	_1:
		;
		jobID = jobID + 1
	}
	return kNullRange
}

// C documentation
//
//	/**
//	 * Returns non-zero iff buffer and range overlap.
//	 */
func ZSTDMT_isOverlapped(tls *libc.TLS, buffer Buffer, range1 Range) (r int32) {
	var bufferEnd, bufferStart, rangeEnd, rangeStart uintptr
	_, _, _, _ = bufferEnd, bufferStart, rangeEnd, rangeStart
	bufferStart = buffer.Fstart
	rangeStart = range1.Fstart
	if rangeStart == libc.UintptrFromInt32(0) || bufferStart == libc.UintptrFromInt32(0) {
		return 0
	}
	bufferEnd = bufferStart + uintptr(buffer.Fcapacity)
	rangeEnd = rangeStart + uintptr(range1.Fsize)
	/* Empty ranges cannot overlap */
	if bufferStart == bufferEnd || rangeStart == rangeEnd {
		return 0
	}
	return libc.BoolInt32(bufferStart < rangeEnd && rangeStart < bufferEnd)
	return r
}

func ZSTDMT_doesOverlapWindow(tls *libc.TLS, buffer Buffer, window ZSTD_window_t) (r int32) {
	var extDict, prefix Range
	_, _ = extDict, prefix
	extDict.Fstart = window.FdictBase + uintptr(window.FlowLimit)
	extDict.Fsize = uint64(window.FdictLimit - window.FlowLimit)
	prefix.Fstart = window.Fbase + uintptr(window.FdictLimit)
	prefix.Fsize = uint64(int64(window.FnextSrc) - int64(window.Fbase+uintptr(window.FdictLimit)))
	return libc.BoolInt32(ZSTDMT_isOverlapped(tls, buffer, extDict) != 0 || ZSTDMT_isOverlapped(tls, buffer, prefix) != 0)
}

func ZSTDMT_waitForLdmComplete(tls *libc.TLS, mtctx uintptr, buffer Buffer) {
	var mutex uintptr
	_ = mutex
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		mutex = mtctx + 352 + 2480
		libc.XEnterCriticalSection(tls, mutex)
		for ZSTDMT_doesOverlapWindow(tls, buffer, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fserial.FldmWindow) != 0 {
			SleepConditionVariableCS(tls, mtctx+352+2520, mutex, uint32(INFINITE))
		}
		libc.XLeaveCriticalSection(tls, mutex)
	}
}

// C documentation
//
//	/**
//	 * Attempts to set the inBuff to the next section to fill.
//	 * If any part of the new section is still in use we give up.
//	 * Returns non-zero if the buffer is filled.
//	 */
func ZSTDMT_tryGetInputRange(tls *libc.TLS, mtctx uintptr) (r int32) {
	var buffer Buffer
	var inUse Range
	var prefixSize, spaceLeft, spaceNeeded size_t
	var start uintptr
	_, _, _, _, _, _ = buffer, inUse, prefixSize, spaceLeft, spaceNeeded, start
	inUse = ZSTDMT_getInputDataInUse(tls, mtctx)
	spaceLeft = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos
	spaceNeeded = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize
	if spaceLeft < spaceNeeded {
		/* ZSTD_invalidateRepCodes() doesn't work for extDict variants.
		 * Simply copy the prefix to the beginning in that case.
		 */
		start = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer
		prefixSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fsize
		buffer.Fstart = start
		buffer.Fcapacity = prefixSize
		if ZSTDMT_isOverlapped(tls, buffer, inUse) != 0 {
			return 0
		}
		ZSTDMT_waitForLdmComplete(tls, mtctx, buffer)
		libc.Xmemmove(tls, start, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart, prefixSize)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart = start
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos = prefixSize
	}
	buffer.Fstart = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer + uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos)
	buffer.Fcapacity = spaceNeeded
	if ZSTDMT_isOverlapped(tls, buffer, inUse) != 0 {
		return 0
	}
	ZSTDMT_waitForLdmComplete(tls, mtctx, buffer)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = buffer
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
	return int32(1)
}

type SyncPoint = struct {
	FtoLoad size_t
	Fflush  int32
}

// C documentation
//
//	/**
//	 * Searches through the input for a synchronization point. If one is found, we
//	 * will instruct the caller to flush, and return the number of bytes to load.
//	 * Otherwise, we will load as many bytes as possible and instruct the caller
//	 * to continue as normal.
//	 */
func findSynchronizationPoint(tls *libc.TLS, mtctx uintptr, input ZSTD_inBuffer) (r SyncPoint) {
	var hash, hitMask, primePower U64
	var istart, prev uintptr
	var pos size_t
	var syncPoint SyncPoint
	var toRemove BYTE
	var v1 uint64
	var v3 int32
	_, _, _, _, _, _, _, _, _, _ = hash, hitMask, istart, pos, prev, primePower, syncPoint, toRemove, v1, v3
	istart = input.Fsrc + uintptr(input.Fpos)
	primePower = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FprimePower
	hitMask = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FhitMask
	if input.Fsize-input.Fpos < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize-(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled {
		v1 = input.Fsize - input.Fpos
	} else {
		v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
	}
	syncPoint.FtoLoad = v1
	syncPoint.Fflush = 0
	if !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.Frsyncable != 0) {
		/* Rsync is disabled. */
		return syncPoint
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled+input.Fsize-input.Fpos < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		/* We don't emit synchronization points if it would produce too small blocks.
		 * We don't have enough input to find a synchronization point, so don't look.
		 */
		return syncPoint
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled+syncPoint.FtoLoad < uint64(RSYNC_LENGTH) {
		/* Not enough to compute the hash.
		 * We will miss any synchronization points in this RSYNC_LENGTH byte
		 * window. However, since it depends only in the internal buffers, if the
		 * state is already synchronized, we will remain synchronized.
		 * Additionally, the probability that we miss a synchronization point is
		 * low: RSYNC_LENGTH / targetSectionSize.
		 */
		return syncPoint
	}
	/* Initialize the loop variables. */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		/* We don't need to scan the first RSYNC_MIN_BLOCK_SIZE positions
		 * because they can't possibly be a sync point. So we can start
		 * part way through the input buffer.
		 */
		pos = uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
		if pos >= uint64(RSYNC_LENGTH) {
			prev = istart + uintptr(pos) - uintptr(RSYNC_LENGTH)
			hash = ZSTD_rollingHash_compute(tls, prev, uint64(RSYNC_LENGTH))
		} else {
			prev = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart + uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled) - uintptr(RSYNC_LENGTH)
			hash = ZSTD_rollingHash_compute(tls, prev+uintptr(pos), libc.Uint64FromInt32(RSYNC_LENGTH)-pos)
			hash = ZSTD_rollingHash_append(tls, hash, istart, pos)
		}
	} else {
		/* We have enough bytes buffered to initialize the hash,
		 * and have processed enough bytes to find a sync point.
		 * Start scanning at the beginning of the input.
		 */
		pos = uint64(0)
		prev = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart + uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled) - uintptr(RSYNC_LENGTH)
		hash = ZSTD_rollingHash_compute(tls, prev, uint64(RSYNC_LENGTH))
		if hash&hitMask == hitMask {
			/* We're already at a sync point so don't load any more until
			 * we're able to flush this sync point.
			 * This likely happened because the job table was full so we
			 * couldn't add our job.
			 */
			syncPoint.FtoLoad = uint64(0)
			syncPoint.Fflush = int32(1)
			return syncPoint
		}
	}
	/* Starting with the hash of the previous RSYNC_LENGTH bytes, roll
	 * through the input. If we hit a synchronization point, then cut the
	 * job off, and tell the compressor to flush the job. Otherwise, load
	 * all the bytes and continue as normal.
	 * If we go too long without a synchronization point (targetSectionSize)
	 * then a block will be emitted anyways, but this is okay, since if we
	 * are already synchronized we will remain synchronized.
	 */
	for {
		if !(pos < syncPoint.FtoLoad) {
			break
		}
		if pos < uint64(RSYNC_LENGTH) {
			v3 = int32(*(*BYTE)(unsafe.Pointer(prev + uintptr(pos))))
		} else {
			v3 = int32(*(*BYTE)(unsafe.Pointer(istart + uintptr(pos-uint64(RSYNC_LENGTH)))))
		}
		toRemove = uint8(v3)
		/* This assert is very expensive, and Debian compiles with asserts enabled.
		 * So disable it for now. We can get similar coverage by checking it at the
		 * beginning & end of the loop.
		 * assert(pos < RSYNC_LENGTH || ZSTD_rollingHash_compute(istart + pos - RSYNC_LENGTH, RSYNC_LENGTH) == hash);
		 */
		hash = ZSTD_rollingHash_rotate(tls, hash, toRemove, *(*BYTE)(unsafe.Pointer(istart + uintptr(pos))), primePower)
		if hash&hitMask == hitMask {
			syncPoint.FtoLoad = pos + uint64(1)
			syncPoint.Fflush = int32(1)
			pos = pos + 1 /* for assert */
			break
		}
		goto _2
	_2:
		;
		pos = pos + 1
	}
	return syncPoint
}

func ZSTDMT_nextInputSizeHint(tls *libc.TLS, mtctx uintptr) (r size_t) {
	var hintInSize size_t
	_ = hintInSize
	hintInSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
	if hintInSize == uint64(0) {
		hintInSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize
	}
	return hintInSize
}

// C documentation
//
//	/** ZSTDMT_compressStream_generic() :
//	 *  internal use only - exposed to be invoked from zstd_compress.c
//	 *  assumption : output and input are valid (pos <= size)
//	 * @return : minimum amount of data remaining to flush, 0 if none */
func ZSTDMT_compressStream_generic(tls *libc.TLS, mtctx uintptr, output uintptr, input uintptr, endOp ZSTD_EndDirective) (r size_t) {
	var err_code, jobSize, remainingToFlush size_t
	var forwardInputProgress uint32
	var syncPoint SyncPoint
	var v1 uint64
	_, _, _, _, _, _ = err_code, forwardInputProgress, jobSize, remainingToFlush, syncPoint, v1
	forwardInputProgress = uint32(0)
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded != 0 && endOp == int32(ZSTD_e_continue) {
		/* current frame being ended. Only flush/end are allowed */
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	/* fill input buffer */
	if !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0) && (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize > (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos { /* support NULL input */
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart == libc.UintptrFromInt32(0) {
			/* Can't fill an empty buffer */
			if !(ZSTDMT_tryGetInputRange(tls, mtctx) != 0) {
				/* It is only possible for this operation to fail if there are
				 * still compression jobs ongoing.
				 */
			} else {
			}
		}
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart != libc.UintptrFromInt32(0) {
			syncPoint = findSynchronizationPoint(tls, mtctx, *(*ZSTD_inBuffer)(unsafe.Pointer(input)))
			if syncPoint.Fflush != 0 && endOp == int32(ZSTD_e_continue) {
				endOp = int32(ZSTD_e_flush)
			}
			libc.Xmemcpy(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart+uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled), (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc+uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos), syncPoint.FtoLoad)
			*(*size_t)(unsafe.Pointer(input + 16)) += syncPoint.FtoLoad
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled += syncPoint.FtoLoad
			forwardInputProgress = libc.BoolUint32(syncPoint.FtoLoad > uint64(0))
		}
	}
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos < (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize && endOp == int32(ZSTD_e_end) {
		/* Can't end yet because the input is not fully consumed.
		 * We are in one of these cases:
		 * - mtctx->inBuff is NULL & empty: we couldn't get an input buffer so don't create a new job.
		 * - We filled the input buffer: flush this job but don't end the frame.
		 * - We hit a synchronization point: flush this job but don't end the frame.
		 */
		endOp = int32(ZSTD_e_flush)
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0 || (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled >= (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize || endOp != int32(ZSTD_e_continue) && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled > uint64(0) || endOp == int32(ZSTD_e_end) && !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded != 0) { /* must finish the frame with a zero-size block */
		jobSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
		err_code = ZSTDMT_createCompressionJob(tls, mtctx, jobSize, endOp)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
	}
	/* check for potential compressed data ready to be flushed */
	remainingToFlush = ZSTDMT_flushProduced(tls, mtctx, output, libc.BoolUint32(!(forwardInputProgress != 0)), endOp) /* block if there was no forward input progress */
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos < (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
		if remainingToFlush > uint64(libc.Int32FromInt32(1)) {
			v1 = remainingToFlush
		} else {
			v1 = uint64(libc.Int32FromInt32(1))
		}
		return v1
	} /* input not consumed : do not end flush yet */
	return remainingToFlush
	return r
}

/**** ended inlining compress/zstdmt_compress.c ****/

/**** start inlining decompress/huf_decompress.c ****/
/* ******************************************************************
 * huff0 huffman decoder,
 * part of Finite State Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Dependencies
****************************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Constants
****************************************************************/

/* **************************************************************
*  Macros
****************************************************************/

/* These two optional macros force the use one way or another of the two
 * Huffman decompression implementations. You can't force in both directions
 * at the same time.
 */

/* When DYNAMIC_BMI2 is enabled, fast decoders are only called when bmi2 is
 * supported at runtime, so we can add the BMI2 target attribute.
 * When it is disabled, we will still get BMI2 if it is enabled statically.
 */

/* **************************************************************
*  Error Management
****************************************************************/

/* **************************************************************
*  Byte alignment for workSpace management
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  BMI2 Variant Wrappers
//	****************************************************************/
type HUF_DecompressUsingDTableFn = uintptr

// C documentation
//
//	/*-***************************/
//	/*  generic DTableDesc       */
//	/*-***************************/
type DTableDesc = struct {
	FmaxTableLog BYTE
	FtableType   BYTE
	FtableLog    BYTE
	Freserved    BYTE
}

func HUF_getDTableDesc(tls *libc.TLS, table uintptr) (r DTableDesc) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* dtd at bp+0 */ DTableDesc
	libc.Xmemcpy(tls, bp, table, libc.Uint64FromInt64(4))
	return *(*DTableDesc)(unsafe.Pointer(bp))
}

func HUF_initFastDStream(tls *libc.TLS, ip uintptr) (r size_t) {
	var bitsConsumed, value size_t
	var lastByte BYTE
	var v1 uint32
	_, _, _, _ = bitsConsumed, lastByte, value, v1
	lastByte = *(*BYTE)(unsafe.Pointer(ip + 7))
	if lastByte != 0 {
		v1 = uint32(8) - ZSTD_highbit32(tls, uint32(lastByte))
	} else {
		v1 = uint32(0)
	}
	bitsConsumed = uint64(v1)
	value = MEM_readLEST(tls, ip) | uint64(1)
	return value << bitsConsumed
}

// C documentation
//
//	/**
//	 * The input/output arguments to the Huffman fast decoding loop:
//	 *
//	 * ip [in/out] - The input pointers, must be updated to reflect what is consumed.
//	 * op [in/out] - The output pointers, must be updated to reflect what is written.
//	 * bits [in/out] - The bitstream containers, must be updated to reflect the current state.
//	 * dt [in] - The decoding table.
//	 * ilowest [in] - The beginning of the valid range of the input. Decoders may read
//	 *                down to this pointer. It may be below iend[0].
//	 * oend [in] - The end of the output stream. op[3] must not cross oend.
//	 * iend [in] - The end of each input stream. ip[i] may cross iend[i],
//	 *             as long as it is above ilowest, but that indicates corruption.
//	 */
type HUF_DecompressFastArgs = struct {
	Fip      [4]uintptr
	Fop      [4]uintptr
	Fbits    [4]U64
	Fdt      uintptr
	Filowest uintptr
	Foend    uintptr
	Fiend    [4]uintptr
}

type HUF_DecompressFastLoopFn = uintptr

// C documentation
//
//	/**
//	 * Initializes args for the fast decoding loop.
//	 * @returns 1 on success
//	 *          0 if the fallback implementation should be used.
//	 *          Or an error code on failure.
//	 */
func HUF_DecompressFastArgs_init(tls *libc.TLS, args uintptr, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, DTable uintptr) (r size_t) {
	var dt, istart, oend uintptr
	var dtLog U32
	var length1, length2, length3, length4 size_t
	_, _, _, _, _, _, _, _ = dt, dtLog, istart, length1, length2, length3, length4, oend
	dt = DTable + uintptr(1)*4
	dtLog = uint32(HUF_getDTableDesc(tls, DTable).FtableLog)
	istart = src
	oend = ZSTD_maybeNullPtrAdd(tls, dst, int64(dstSize))
	/* The fast decoding loop assumes 64-bit little-endian.
	 * This condition is false on x32.
	 */
	if !(MEM_isLittleEndian(tls) != 0) || MEM_32bits(tls) != 0 {
		return uint64(0)
	}
	/* Avoid nullptr addition */
	if dstSize == uint64(0) {
		return uint64(0)
	}
	/* strict minimum : jump table + 1 byte per stream */
	if srcSize < uint64(10) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* Must have at least 8 bytes per stream because we don't handle initializing smaller bit containers.
	 * If table log is not correct at this point, fallback to the old decoder.
	 * On small inputs we don't have enough data to trigger the fast loop, so use the old decoder.
	 */
	if dtLog != uint32(HUF_DECODER_FAST_TABLELOG) {
		return uint64(0)
	}
	/* Read the jump table. */
	length1 = uint64(MEM_readLE16(tls, istart))
	length2 = uint64(MEM_readLE16(tls, istart+uintptr(2)))
	length3 = uint64(MEM_readLE16(tls, istart+uintptr(4)))
	length4 = srcSize - (length1 + length2 + length3 + uint64(6))
	*(*uintptr)(unsafe.Pointer(args + 120)) = istart + uintptr(6) /* jumpTable */
	*(*uintptr)(unsafe.Pointer(args + 120 + 1*8)) = *(*uintptr)(unsafe.Pointer(args + 120)) + uintptr(length1)
	*(*uintptr)(unsafe.Pointer(args + 120 + 2*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 1*8)) + uintptr(length2)
	*(*uintptr)(unsafe.Pointer(args + 120 + 3*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 2*8)) + uintptr(length3)
	/* HUF_initFastDStream() requires this, and this small of an input
	 * won't benefit from the ASM loop anyways.
	 */
	if length1 < uint64(8) || length2 < uint64(8) || length3 < uint64(8) || length4 < uint64(8) {
		return uint64(0)
	}
	if length4 > srcSize {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	/* ip[] contains the position that is currently loaded into bits[]. */
	*(*uintptr)(unsafe.Pointer(args)) = *(*uintptr)(unsafe.Pointer(args + 120 + 1*8)) - uintptr(8)
	*(*uintptr)(unsafe.Pointer(args + 1*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 2*8)) - uintptr(8)
	*(*uintptr)(unsafe.Pointer(args + 2*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 3*8)) - uintptr(8)
	*(*uintptr)(unsafe.Pointer(args + 3*8)) = src + uintptr(srcSize) - uintptr(8)
	/* op[] contains the output pointers. */
	*(*uintptr)(unsafe.Pointer(args + 32)) = dst
	*(*uintptr)(unsafe.Pointer(args + 32 + 1*8)) = *(*uintptr)(unsafe.Pointer(args + 32)) + uintptr((dstSize+uint64(3))/uint64(4))
	*(*uintptr)(unsafe.Pointer(args + 32 + 2*8)) = *(*uintptr)(unsafe.Pointer(args + 32 + 1*8)) + uintptr((dstSize+uint64(3))/uint64(4))
	*(*uintptr)(unsafe.Pointer(args + 32 + 3*8)) = *(*uintptr)(unsafe.Pointer(args + 32 + 2*8)) + uintptr((dstSize+uint64(3))/uint64(4))
	/* No point to call the ASM loop for tiny outputs. */
	if *(*uintptr)(unsafe.Pointer(args + 32 + 3*8)) >= oend {
		return uint64(0)
	}
	/* bits[] is the bit container.
	 * It is read from the MSB down to the LSB.
	 * It is shifted left as it is read, and zeros are
	 * shifted in. After the lowest valid bit a 1 is
	 * set, so that CountTrailingZeros(bits[]) can be used
	 * to count how many bits we've consumed.
	 */
	*(*U64)(unsafe.Pointer(args + 64)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args)))
	*(*U64)(unsafe.Pointer(args + 64 + 1*8)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args + 1*8)))
	*(*U64)(unsafe.Pointer(args + 64 + 2*8)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args + 2*8)))
	*(*U64)(unsafe.Pointer(args + 64 + 3*8)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args + 3*8)))
	/* The decoders must be sure to never read beyond ilowest.
	 * This is lower than iend[0], but allowing decoders to read
	 * down to ilowest can allow an extra iteration or two in the
	 * fast loop.
	 */
	(*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest = istart
	(*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Foend = oend
	(*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Fdt = dt
	return uint64(1)
}

func HUF_initRemainingDStream(tls *libc.TLS, bit uintptr, args uintptr, stream int32, segmentEnd uintptr) (r size_t) {
	/* Validate that we haven't overwritten. */
	if *(*uintptr)(unsafe.Pointer(args + 32 + uintptr(stream)*8)) > segmentEnd {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* Validate that we haven't read beyond iend[].
	 * Note that ip[] may be < iend[] because the MSB is
	 * the next bit to read, and we may have consumed 100%
	 * of the stream, so down to iend[i] - 8 is valid.
	 */
	if *(*uintptr)(unsafe.Pointer(args + uintptr(stream)*8)) < *(*uintptr)(unsafe.Pointer(args + 120 + uintptr(stream)*8))-uintptr(8) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* Construct the BIT_DStream_t. */
	(*BIT_DStream_t)(unsafe.Pointer(bit)).FbitContainer = MEM_readLEST(tls, *(*uintptr)(unsafe.Pointer(args + uintptr(stream)*8)))
	(*BIT_DStream_t)(unsafe.Pointer(bit)).FbitsConsumed = ZSTD_countTrailingZeros64(tls, *(*U64)(unsafe.Pointer(args + 64 + uintptr(stream)*8)))
	(*BIT_DStream_t)(unsafe.Pointer(bit)).Fstart = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest
	(*BIT_DStream_t)(unsafe.Pointer(bit)).FlimitPtr = (*BIT_DStream_t)(unsafe.Pointer(bit)).Fstart + uintptr(8)
	(*BIT_DStream_t)(unsafe.Pointer(bit)).Fptr = *(*uintptr)(unsafe.Pointer(args + uintptr(stream)*8))
	return uint64(0)
}

/* Calls X(N) for each stream 0, 1, 2, 3. */

/* Calls X(N, var) for each stream 0, 1, 2, 3. */

// C documentation
//
//	/*-***************************/
//	/*  single-symbol decoding   */
//	/*-***************************/
type HUF_DEltX1 = struct {
	FnbBits BYTE
	Fbyte1  BYTE
} /* single-symbol decoding */

// C documentation
//
//	/**
//	 * Packs 4 HUF_DEltX1 structs into a U64. This is used to lay down 4 entries at
//	 * a time.
//	 */
func HUF_DEltX1_set4(tls *libc.TLS, symbol BYTE, nbBits BYTE) (r U64) {
	var D4 U64
	_ = D4
	if MEM_isLittleEndian(tls) != 0 {
		D4 = uint64(int32(symbol)<<libc.Int32FromInt32(8) + int32(nbBits))
	} else {
		D4 = uint64(int32(symbol) + int32(nbBits)<<libc.Int32FromInt32(8))
	}
	D4 = D4 * uint64(0x0001000100010001)
	return D4
}

// C documentation
//
//	/**
//	 * Increase the tableLog to targetTableLog and rescales the stats.
//	 * If tableLog > targetTableLog this is a no-op.
//	 * @returns New tableLog
//	 */
func HUF_rescaleStats(tls *libc.TLS, huffWeight uintptr, rankVal uintptr, nbSymbols U32, tableLog U32, targetTableLog U32) (r U32) {
	var s, scale U32
	var v2 uintptr
	var v3 uint32
	_, _, _, _ = s, scale, v2, v3
	if tableLog > targetTableLog {
		return tableLog
	}
	if tableLog < targetTableLog {
		scale = targetTableLog - tableLog
		/* Increase the weight for all non-zero probability symbols by scale. */
		s = uint32(0)
		for {
			if !(s < nbSymbols) {
				break
			}
			v2 = huffWeight + uintptr(s)
			if int32(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(s)))) == 0 {
				v3 = uint32(0)
			} else {
				v3 = scale
			}
			*(*BYTE)(unsafe.Pointer(v2)) = BYTE(int32(*(*BYTE)(unsafe.Pointer(v2))) + int32(uint8(v3)))
			goto _1
		_1:
			;
			s = s + 1
		}
		/* Update rankVal to reflect the new weights.
		 * All weights except 0 get moved to weight + scale.
		 * Weights [1, scale] are empty.
		 */
		s = targetTableLog
		for {
			if !(s > scale) {
				break
			}
			*(*U32)(unsafe.Pointer(rankVal + uintptr(s)*4)) = *(*U32)(unsafe.Pointer(rankVal + uintptr(s-scale)*4))
			goto _4
		_4:
			;
			s = s - 1
		}
		s = scale
		for {
			if !(s > uint32(0)) {
				break
			}
			*(*U32)(unsafe.Pointer(rankVal + uintptr(s)*4)) = uint32(0)
			goto _5
		_5:
			;
			s = s - 1
		}
	}
	return targetTableLog
}

type HUF_ReadDTableX1_Workspace = struct {
	FrankVal    [13]U32
	FrankStart  [13]U32
	FstatsWksp  [219]U32
	Fsymbols    [256]BYTE
	FhuffWeight [256]BYTE
}

func HUF_readDTableX1_wksp(tls *libc.TLS, DTable uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var D, D1 HUF_DEltX1
	var D4, D41, D42 U64
	var curr, maxTableLog, nextRankStart, targetTableLog, w2, v5 U32
	var dt, dtPtr, wksp, v6 uintptr
	var iSize, w, w1 size_t
	var length, n, nLimit, rankStart, s, symbol, symbolCount, u, u1, uStart, unroll int32
	var nbBits BYTE
	var v1 uint32
	var _ /* dtd at bp+8 */ DTableDesc
	var _ /* nbSymbols at bp+4 */ U32
	var _ /* tableLog at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = D, D1, D4, D41, D42, curr, dt, dtPtr, iSize, length, maxTableLog, n, nLimit, nbBits, nextRankStart, rankStart, s, symbol, symbolCount, targetTableLog, u, u1, uStart, unroll, w, w1, w2, wksp, v1, v5, v6
	*(*U32)(unsafe.Pointer(bp)) = uint32(0)
	*(*U32)(unsafe.Pointer(bp + 4)) = uint32(0)
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	wksp = workSpace
	_ = libc.Uint64FromInt64(1)
	if uint64(1492) > wkspSize {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	_ = libc.Uint64FromInt64(1)
	/* ZSTD_memset(huffWeight, 0, sizeof(huffWeight)); */ /* is not necessary, even though some analyzer complain ... */
	iSize = HUF_readStats_wksp(tls, wksp+1236, uint64(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)), wksp, bp+4, bp, src, srcSize, wksp+104, uint64(876), flags)
	if ERR_isError(tls, iSize) != 0 {
		return iSize
	}
	/* Table header */
	*(*DTableDesc)(unsafe.Pointer(bp + 8)) = HUF_getDTableDesc(tls, DTable)
	maxTableLog = uint32(int32((*(*DTableDesc)(unsafe.Pointer(bp + 8))).FmaxTableLog) + int32(1))
	if maxTableLog < uint32(libc.Int32FromInt32(HUF_DECODER_FAST_TABLELOG)) {
		v1 = maxTableLog
	} else {
		v1 = uint32(libc.Int32FromInt32(HUF_DECODER_FAST_TABLELOG))
	}
	targetTableLog = v1
	*(*U32)(unsafe.Pointer(bp)) = HUF_rescaleStats(tls, wksp+1236, wksp, *(*U32)(unsafe.Pointer(bp + 4)), *(*U32)(unsafe.Pointer(bp)), targetTableLog)
	if *(*U32)(unsafe.Pointer(bp)) > uint32(int32((*(*DTableDesc)(unsafe.Pointer(bp + 8))).FmaxTableLog)+libc.Int32FromInt32(1)) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	} /* DTable too small, Huffman tree cannot fit in */
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableType = uint8(0)
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableLog = uint8(*(*U32)(unsafe.Pointer(bp)))
	libc.Xmemcpy(tls, DTable, bp+8, libc.Uint64FromInt64(4))
	/* Compute symbols and rankStart given rankVal:
	 *
	 * rankVal already contains the number of values of each weight.
	 *
	 * symbols contains the symbols ordered by weight. First are the rankVal[0]
	 * weight 0 symbols, followed by the rankVal[1] weight 1 symbols, and so on.
	 * symbols[0] is filled (but unused) to avoid a branch.
	 *
	 * rankStart contains the offset where each rank belongs in the DTable.
	 * rankStart[0] is not filled because there are no entries in the table for
	 * weight 0.
	 */
	nextRankStart = uint32(0)
	unroll = int32(4)
	nLimit = int32(*(*U32)(unsafe.Pointer(bp + 4))) - unroll + int32(1)
	n = 0
	for {
		if !(n < int32(*(*U32)(unsafe.Pointer(bp)))+int32(1)) {
			break
		}
		curr = nextRankStart
		nextRankStart = nextRankStart + *(*U32)(unsafe.Pointer(wksp + uintptr(n)*4))
		*(*U32)(unsafe.Pointer(wksp + 52 + uintptr(n)*4)) = curr
		goto _2
	_2:
		;
		n = n + 1
	}
	n = 0
	for {
		if !(n < nLimit) {
			break
		}
		u = 0
		for {
			if !(u < unroll) {
				break
			}
			w = uint64(*(*BYTE)(unsafe.Pointer(wksp + 1236 + uintptr(n+u))))
			v6 = wksp + 52 + uintptr(w)*4
			v5 = *(*U32)(unsafe.Pointer(v6))
			*(*U32)(unsafe.Pointer(v6)) = *(*U32)(unsafe.Pointer(v6)) + 1
			*(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(v5))) = uint8(n + u)
			goto _4
		_4:
			;
			u = u + 1
		}
		goto _3
	_3:
		;
		n = n + unroll
	}
	for {
		if !(n < int32(*(*U32)(unsafe.Pointer(bp + 4)))) {
			break
		}
		w1 = uint64(*(*BYTE)(unsafe.Pointer(wksp + 1236 + uintptr(n))))
		v6 = wksp + 52 + uintptr(w1)*4
		v5 = *(*U32)(unsafe.Pointer(v6))
		*(*U32)(unsafe.Pointer(v6)) = *(*U32)(unsafe.Pointer(v6)) + 1
		*(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(v5))) = uint8(n)
		goto _7
	_7:
		;
		n = n + 1
	}
	/* fill DTable
	 * We fill all entries of each weight in order.
	 * That way length is a constant for each iteration of the outer loop.
	 * We can switch based on the length to a different inner loop which is
	 * optimized for that particular case.
	 */
	symbol = int32(*(*U32)(unsafe.Pointer(wksp)))
	rankStart = 0
	w2 = uint32(1)
	for {
		if !(w2 < *(*U32)(unsafe.Pointer(bp))+uint32(1)) {
			break
		}
		symbolCount = int32(*(*U32)(unsafe.Pointer(wksp + uintptr(w2)*4)))
		length = int32(1) << w2 >> int32(1)
		uStart = rankStart
		nbBits = uint8(*(*U32)(unsafe.Pointer(bp)) + libc.Uint32FromInt32(1) - w2)
		switch length {
		case int32(1):
			goto _11
		case int32(2):
			goto _12
		case int32(4):
			goto _13
		case int32(8):
			goto _14
		default:
			goto _15
		}
		goto _16
	_11:
		;
		s = 0
	_19:
		;
		if !(s < symbolCount) {
			goto _17
		}
		D.Fbyte1 = *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s)))
		D.FnbBits = nbBits
		*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(uStart)*2)) = D
		uStart = uStart + int32(1)
		goto _18
	_18:
		;
		s = s + 1
		goto _19
		goto _17
	_17:
		;
		goto _16
	_12:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D1.Fbyte1 = *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s)))
			D1.FnbBits = nbBits
			*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(uStart+0)*2)) = D1
			*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(uStart+int32(1))*2)) = D1
			uStart = uStart + int32(2)
			goto _20
		_20:
			;
			s = s + 1
		}
		goto _16
	_13:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D4 = HUF_DEltX1_set4(tls, *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s))), nbBits)
			MEM_write64(tls, dt+uintptr(uStart)*2, D4)
			uStart = uStart + int32(4)
			goto _21
		_21:
			;
			s = s + 1
		}
		goto _16
	_14:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D41 = HUF_DEltX1_set4(tls, *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s))), nbBits)
			MEM_write64(tls, dt+uintptr(uStart)*2, D41)
			MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(4)*2, D41)
			uStart = uStart + int32(8)
			goto _22
		_22:
			;
			s = s + 1
		}
		goto _16
	_15:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D42 = HUF_DEltX1_set4(tls, *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s))), nbBits)
			u1 = 0
			for {
				if !(u1 < length) {
					break
				}
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(0)*2, D42)
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(4)*2, D42)
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(8)*2, D42)
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(12)*2, D42)
				goto _24
			_24:
				;
				u1 = u1 + int32(16)
			}
			uStart = uStart + length
			goto _23
		_23:
			;
			s = s + 1
		}
		goto _16
	_16:
		;
		symbol = symbol + symbolCount
		rankStart = rankStart + symbolCount*length
		goto _10
	_10:
		;
		w2 = w2 + 1
	}
	return iSize
}

func HUF_decodeSymbolX1(tls *libc.TLS, Dstream uintptr, dt uintptr, dtLog U32) (r BYTE) {
	var c BYTE
	var val size_t
	_, _ = c, val
	val = BIT_lookBitsFast(tls, Dstream, dtLog) /* note : dtLog >= 1 */
	c = (*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(val)*2))).Fbyte1
	BIT_skipBits(tls, Dstream, uint32((*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(val)*2))).FnbBits))
	return c
}

func HUF_decodeStreamX1(tls *libc.TLS, p uintptr, bitDPtr uintptr, pEnd uintptr, dt uintptr, dtLog U32) (r size_t) {
	var pStart, v1 uintptr
	_, _ = pStart, v1
	pStart = p
	/* up to 4 symbols at a time */
	if int64(pEnd)-int64(p) > int64(3) {
		for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd-uintptr(3)) != 0 {
			if MEM_64bits(tls) != 0 {
				v1 = p
				p = p + 1
				*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v1 = p
				p = p + 1
				*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v1 = p
				p = p + 1
				*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
			}
			v1 = p
			p = p + 1
			*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
		}
	} else {
		BIT_reloadDStream(tls, bitDPtr)
	}
	/* [0-3] symbols remaining */
	if MEM_32bits(tls) != 0 {
		for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd) != 0 {
			v1 = p
			p = p + 1
			*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
		}
	}
	/* no more data to retrieve from bitstream, no need to reload */
	for p < pEnd {
		v1 = p
		p = p + 1
		*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
	}
	return uint64(int64(pEnd) - int64(pStart))
}

func HUF_decompress1X1_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var _var_err__ size_t
	var dt, dtPtr, oend, op uintptr
	var dtLog U32
	var dtd DTableDesc
	var _ /* bitD at bp+0 */ BIT_DStream_t
	_, _, _, _, _, _, _ = _var_err__, dt, dtLog, dtPtr, dtd, oend, op
	op = dst
	oend = ZSTD_maybeNullPtrAdd(tls, op, int64(dstSize))
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	dtd = HUF_getDTableDesc(tls, DTable)
	dtLog = uint32(dtd.FtableLog)
	_var_err__ = BIT_initDStream(tls, bp, cSrc, cSrcSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	HUF_decodeStreamX1(tls, op, bp, oend, dt, dtLog)
	if !(BIT_endOfDStream(tls, bp) != 0) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	return dstSize
}

// C documentation
//
//	/* HUF_decompress4X1_usingDTable_internal_body():
//	 * Conditions :
//	 * @dstSize >= 6
//	 */
func HUF_decompress4X1_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(160)
	defer tls.Free(160)
	var _var_err__, _var_err__1, _var_err__2, _var_err__3, length1, length2, length3, length4, segmentSize size_t
	var dt, dtPtr, istart, istart1, istart2, istart3, istart4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart, v2 uintptr
	var dtLog, endCheck, endSignal U32
	var dtd DTableDesc
	var _ /* bitD1 at bp+0 */ BIT_DStream_t
	var _ /* bitD2 at bp+40 */ BIT_DStream_t
	var _ /* bitD3 at bp+80 */ BIT_DStream_t
	var _ /* bitD4 at bp+120 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _var_err__, _var_err__1, _var_err__2, _var_err__3, dt, dtLog, dtPtr, dtd, endCheck, endSignal, istart, istart1, istart2, istart3, istart4, length1, length2, length3, length4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart, segmentSize, v2
	/* Check */
	if cSrcSize < uint64(10) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* strict minimum : jump table + 1 byte per stream */
	if dstSize < uint64(6) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* stream 4-split doesn't work */
	istart = cSrc
	ostart = dst
	oend = ostart + uintptr(dstSize)
	olimit = oend - uintptr(3)
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	length1 = uint64(MEM_readLE16(tls, istart))
	length2 = uint64(MEM_readLE16(tls, istart+uintptr(2)))
	length3 = uint64(MEM_readLE16(tls, istart+uintptr(4)))
	length4 = cSrcSize - (length1 + length2 + length3 + uint64(6))
	istart1 = istart + uintptr(6) /* jumpTable */
	istart2 = istart1 + uintptr(length1)
	istart3 = istart2 + uintptr(length2)
	istart4 = istart3 + uintptr(length3)
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	opStart2 = ostart + uintptr(segmentSize)
	opStart3 = opStart2 + uintptr(segmentSize)
	opStart4 = opStart3 + uintptr(segmentSize)
	op1 = ostart
	op2 = opStart2
	op3 = opStart3
	op4 = opStart4
	dtd = HUF_getDTableDesc(tls, DTable)
	dtLog = uint32(dtd.FtableLog)
	endSignal = uint32(1)
	if length4 > cSrcSize {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	if opStart4 > oend {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	/* validated above */
	_var_err__ = BIT_initDStream(tls, bp, istart1, length1)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	_var_err__1 = BIT_initDStream(tls, bp+40, istart2, length2)
	if ERR_isError(tls, _var_err__1) != 0 {
		return _var_err__1
	}
	_var_err__2 = BIT_initDStream(tls, bp+80, istart3, length3)
	if ERR_isError(tls, _var_err__2) != 0 {
		return _var_err__2
	}
	_var_err__3 = BIT_initDStream(tls, bp+120, istart4, length4)
	if ERR_isError(tls, _var_err__3) != 0 {
		return _var_err__3
	}
	/* up to 16 symbols per loop (4 symbols per stream) in 64-bit mode */
	if uint64(int64(oend)-int64(op4)) >= uint64(8) {
		for {
			if !(endSignal&libc.BoolUint32(op4 < olimit) != 0) {
				break
			}
			if MEM_64bits(tls) != 0 {
				v2 = op1
				op1 = op1 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op2
				op2 = op2 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op3
				op3 = op3 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op4
				op4 = op4 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op1
				op1 = op1 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op2
				op2 = op2 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op3
				op3 = op3 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op4
				op4 = op4 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op1
				op1 = op1 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op2
				op2 = op2 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op3
				op3 = op3 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op4
				op4 = op4 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			}
			v2 = op1
			op1 = op1 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			v2 = op2
			op2 = op2 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			v2 = op3
			op3 = op3 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			v2 = op4
			op4 = op4 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp) == int32(BIT_DStream_unfinished))
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+40) == int32(BIT_DStream_unfinished))
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+80) == int32(BIT_DStream_unfinished))
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+120) == int32(BIT_DStream_unfinished))
			goto _1
		_1:
		}
	}
	/* check corruption */
	/* note : should not be necessary : op# advance in lock step, and we control op4.
	 *        but curiously, binary generated by gcc 7.2 & 7.3 with -mbmi2 runs faster when >=1 test is present */
	if op1 > opStart2 {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if op2 > opStart3 {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if op3 > opStart4 {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* note : op4 supposed already verified within main loop */
	/* finish bitStreams one by one */
	HUF_decodeStreamX1(tls, op1, bp, opStart2, dt, dtLog)
	HUF_decodeStreamX1(tls, op2, bp+40, opStart3, dt, dtLog)
	HUF_decodeStreamX1(tls, op3, bp+80, opStart4, dt, dtLog)
	HUF_decodeStreamX1(tls, op4, bp+120, oend, dt, dtLog)
	/* check */
	endCheck = BIT_endOfDStream(tls, bp) & BIT_endOfDStream(tls, bp+40) & BIT_endOfDStream(tls, bp+80) & BIT_endOfDStream(tls, bp+120)
	if !(endCheck != 0) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* decoded size */
	return dstSize
	return r
}

func HUF_decompress4X1_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_usingDTable_internal_fast_c_loop(tls *libc.TLS, args uintptr) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var ctz, ctz1, ctz2, ctz3, entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry3, entry4, entry5, entry6, entry7, entry8, entry9, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index3, index4, index5, index6, index7, index8, index9, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, stream int32
	var dtable, ilowest, oend, olimit uintptr
	var iiters, iters, oiters, symbols size_t
	var v3 uint64
	var _ /* bits at bp+0 */ [4]U64
	var _ /* ip at bp+32 */ [4]uintptr
	var _ /* op at bp+64 */ [4]uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = ctz, ctz1, ctz2, ctz3, dtable, entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry3, entry4, entry5, entry6, entry7, entry8, entry9, iiters, ilowest, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index3, index4, index5, index6, index7, index8, index9, iters, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, oend, oiters, olimit, stream, symbols, v3
	dtable = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Fdt
	oend = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Foend
	ilowest = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest
	/* Copy the arguments to local variables */
	libc.Xmemcpy(tls, bp, args+64, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+32, args, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+64, args+32, libc.Uint64FromInt64(32))
	for {
		/* Assert loop preconditions */
		stream = 0
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _2
		_2:
			;
			stream = stream + 1
		}
		/* Compute olimit */
		/* Each iteration produces 5 output symbols per stream */
		oiters = uint64(int64(oend)-int64((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)])) / uint64(5)
		/* Each iteration consumes up to 11 bits * 5 = 55 bits < 7 bytes
		 * per stream.
		 */
		iiters = uint64(int64((*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0])-int64(ilowest)) / uint64(7)
		if oiters < iiters {
			v3 = oiters
		} else {
			v3 = iiters
		}
		/* We can safely run iters iterations before running bounds checks */
		iters = v3
		symbols = iters * uint64(5)
		/* We can simply check that op[3] < olimit, instead of checking all
		 * of our bounds, since we can't hit the other bounds until we've run
		 * iters iterations, which only happens when op[3] == olimit.
		 */
		olimit = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + uintptr(symbols)
		/* Exit fast decoding loop once we reach the end. */
		if (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] == olimit {
			break
		}
		/* Exit the decoding loop if any input pointer has crossed the
		 * previous one. This indicates corruption, and a precondition
		 * to our loop is that ip[i] >= ip[0].
		 */
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			if (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream] < (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream-int32(1)] {
				goto _out
			}
			goto _4
		_4:
			;
			stream = stream + 1
		}
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _5
		_5:
			;
			stream = stream + 1
		}
		/* Manually unroll the loop because compilers don't consistently
		 * unroll the inner loops, which destroys performance.
		 */
		for cond := true; cond; cond = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] < olimit {
			/* Decode 5 symbols in each of the 4 streams */
			index = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= uint64(entry & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0])) = uint8(entry >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index1 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry1 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index1)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(entry1 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)])) = uint8(entry1 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index2 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry2 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index2)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(entry2 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)])) = uint8(entry2 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index3 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry3 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index3)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(entry3 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)])) = uint8(entry3 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index4 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry4 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index4)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= uint64(entry4 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 1)) = uint8(entry4 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index5 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry5 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index5)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(entry5 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 1)) = uint8(entry5 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index6 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry6 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index6)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(entry6 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 1)) = uint8(entry6 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index7 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry7 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index7)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(entry7 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 1)) = uint8(entry7 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index8 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry8 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index8)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= uint64(entry8 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 2)) = uint8(entry8 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index9 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry9 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index9)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(entry9 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 2)) = uint8(entry9 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index10 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry10 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index10)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(entry10 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 2)) = uint8(entry10 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index11 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry11 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index11)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(entry11 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 2)) = uint8(entry11 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index12 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry12 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index12)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= uint64(entry12 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 3)) = uint8(entry12 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index13 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry13 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index13)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(entry13 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 3)) = uint8(entry13 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index14 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry14 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index14)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(entry14 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 3)) = uint8(entry14 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index15 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry15 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index15)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(entry15 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 3)) = uint8(entry15 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index16 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry16 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index16)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= uint64(entry16 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 4)) = uint8(entry16 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index17 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry17 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index17)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(entry17 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 4)) = uint8(entry17 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index18 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry18 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index18)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(entry18 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 4)) = uint8(entry18 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index19 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry19 = int32(*(*U16)(unsafe.Pointer(dtable + uintptr(index19)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(entry19 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 4)) = uint8(entry19 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			/* Reload each of the 4 the bitstreams */
			ctz = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[0]))
			nbBits = ctz & int32(7)
			nbBytes = ctz >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32)) -= uintptr(nbBytes)
			(*(*[4]U64)(unsafe.Pointer(bp)))[0] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp)) <<= uint64(nbBits)
			ctz1 = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)]))
			nbBits1 = ctz1 & int32(7)
			nbBytes1 = ctz1 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 1*8)) -= uintptr(nbBytes1)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(1)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(nbBits1)
			ctz2 = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)]))
			nbBits2 = ctz2 & int32(7)
			nbBytes2 = ctz2 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 2*8)) -= uintptr(nbBytes2)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(2)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(nbBits2)
			ctz3 = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)]))
			nbBits3 = ctz3 & int32(7)
			nbBytes3 = ctz3 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 3*8)) -= uintptr(nbBytes3)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(3)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(nbBits3)
		}
		goto _1
	_1:
	}
	goto _out
_out:
	;
	/* Save the final values of each of the state variables back to args. */
	libc.Xmemcpy(tls, args+64, bp, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args, bp+32, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args+32, bp+64, libc.Uint64FromInt64(32))
}

// C documentation
//
//	/**
//	 * @returns @p dstSize on success (>= 6)
//	 *          0 if the fallback implementation should be used
//	 *          An error if an error occurred
//	 */
func HUF_decompress4X1_usingDTable_internal_fast(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, __ccgo_fp_loopFn HUF_DecompressFastLoopFn) (r size_t) {
	bp := tls.Alloc(192)
	defer tls.Free(192)
	var dt, ilowest, oend, segmentEnd uintptr
	var err_code, err_code1, ret, segmentSize size_t
	var i int32
	var _ /* args at bp+0 */ HUF_DecompressFastArgs
	var _ /* bit at bp+152 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _ = dt, err_code, err_code1, i, ilowest, oend, ret, segmentEnd, segmentSize
	dt = DTable + uintptr(1)*4
	ilowest = cSrc
	oend = ZSTD_maybeNullPtrAdd(tls, dst, int64(dstSize))
	ret = HUF_DecompressFastArgs_init(tls, bp, dst, dstSize, cSrc, cSrcSize, DTable)
	err_code = ret
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6721, 0)
		}
		return err_code
	}
	if ret == uint64(0) {
		return uint64(0)
	}
	(*(*func(*libc.TLS, uintptr))(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_loopFn})))(tls, bp)
	/* Our loop guarantees that ip[] >= ilowest and that we haven't
	 * overwritten any op[].
	 */
	_ = ilowest
	/* finish bit streams one by one. */
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	segmentEnd = dst
	i = 0
	for {
		if !(i < int32(4)) {
			break
		}
		if segmentSize <= uint64(int64(oend)-int64(segmentEnd)) {
			segmentEnd = segmentEnd + uintptr(segmentSize)
		} else {
			segmentEnd = oend
		}
		err_code1 = HUF_initRemainingDStream(tls, bp+152, bp, i, segmentEnd)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6751, 0)
			}
			return err_code1
		}
		/* Decompress and validate that we've produced exactly the expected length. */
		*(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) += uintptr(HUF_decodeStreamX1(tls, *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)), bp+152, segmentEnd, dt, uint32(HUF_DECODER_FAST_TABLELOG)))
		if *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) != segmentEnd {
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	/* decoded size */
	return dstSize
}

func HUF_decompress1X1_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X1_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X1_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_decompress1X1_usingDTable_internal_bmi2(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	return HUF_decompress1X1_usingDTable_internal_default(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var fallbackFn HUF_DecompressUsingDTableFn
	var loopFn HUF_DecompressFastLoopFn
	var ret size_t
	_, _, _ = fallbackFn, loopFn, ret
	fallbackFn = __ccgo_fp(HUF_decompress4X1_usingDTable_internal_default)
	loopFn = __ccgo_fp(HUF_decompress4X1_usingDTable_internal_fast_c_loop)
	if flags&int32(HUF_flags_bmi2) != 0 {
		fallbackFn = __ccgo_fp(HUF_decompress4X1_usingDTable_internal_bmi2)
	} else {
		return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	if libc.Bool(int32(HUF_ENABLE_FAST_DECODE) != 0) && !(flags&int32(HUF_flags_disableFast) != 0) {
		ret = HUF_decompress4X1_usingDTable_internal_fast(tls, dst, dstSize, cSrc, cSrcSize, DTable, loopFn)
		if ret != uint64(0) {
			return ret
		}
	}
	return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX1_wksp(tls, dctx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress4X1_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, dctx, flags)
}

/* *************************/
/* double-symbols decoding */
/* *************************/

type HUF_DEltX2 = struct {
	Fsequence U16
	FnbBits   BYTE
	Flength   BYTE
}

/* double-symbols decoding */
type sortedSymbol_t = struct {
	Fsymbol BYTE
}

type rankValCol_t = [13]U32

type rankVal_t = [12]rankValCol_t

// C documentation
//
//	/**
//	 * Constructs a HUF_DEltX2 in a U32.
//	 */
func HUF_buildDEltX2U32(tls *libc.TLS, symbol U32, nbBits U32, baseSeq U32, level int32) (r U32) {
	var seq U32
	var v1 uint32
	_, _ = seq, v1
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	if MEM_isLittleEndian(tls) != 0 {
		if level == int32(1) {
			v1 = symbol
		} else {
			v1 = baseSeq + symbol<<libc.Int32FromInt32(8)
		}
		seq = v1
		return seq + nbBits<<libc.Int32FromInt32(16) + uint32(level)<<libc.Int32FromInt32(24)
	} else {
		if level == int32(1) {
			v1 = symbol << libc.Int32FromInt32(8)
		} else {
			v1 = baseSeq<<libc.Int32FromInt32(8) + symbol
		}
		seq = v1
		return seq<<libc.Int32FromInt32(16) + nbBits<<libc.Int32FromInt32(8) + uint32(level)
	}
	return r
}

// C documentation
//
//	/**
//	 * Constructs a HUF_DEltX2.
//	 */
func HUF_buildDEltX2(tls *libc.TLS, symbol U32, nbBits U32, baseSeq U32, level int32) (r HUF_DEltX2) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* DElt at bp+0 */ HUF_DEltX2
	var _ /* val at bp+4 */ U32
	*(*U32)(unsafe.Pointer(bp + 4)) = HUF_buildDEltX2U32(tls, symbol, nbBits, baseSeq, level)
	_ = libc.Uint64FromInt64(1)
	libc.Xmemcpy(tls, bp, bp+4, libc.Uint64FromInt64(4))
	return *(*HUF_DEltX2)(unsafe.Pointer(bp))
}

// C documentation
//
//	/**
//	 * Constructs 2 HUF_DEltX2s and packs them into a U64.
//	 */
func HUF_buildDEltX2U64(tls *libc.TLS, symbol U32, nbBits U32, baseSeq U16, level int32) (r U64) {
	var DElt U32
	_ = DElt
	DElt = HUF_buildDEltX2U32(tls, symbol, nbBits, uint32(baseSeq), level)
	return uint64(DElt) + uint64(DElt)<<libc.Int32FromInt32(32)
}

// C documentation
//
//	/**
//	 * Fills the DTable rank with all the symbols from [begin, end) that are each
//	 * nbBits long.
//	 *
//	 * @param DTableRank The start of the rank in the DTable.
//	 * @param begin The first symbol to fill (inclusive).
//	 * @param end The last symbol to fill (exclusive).
//	 * @param nbBits Each symbol is nbBits long.
//	 * @param tableLog The table log.
//	 * @param baseSeq If level == 1 { 0 } else { the first level symbol }
//	 * @param level The level in the table. Must be 1 or 2.
//	 */
func HUF_fillDTableX2ForWeight(tls *libc.TLS, DTableRank uintptr, begin uintptr, end uintptr, nbBits U32, tableLog U32, baseSeq U16, level int32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var DElt, DElt1 HUF_DEltX2
	var DTableRankEnd, ptr, v10 uintptr
	var length U32
	var _ /* DEltX2 at bp+0 */ U64
	var _ /* DEltX2 at bp+16 */ U64
	var _ /* DEltX2 at bp+8 */ U64
	_, _, _, _, _, _ = DElt, DElt1, DTableRankEnd, length, ptr, v10
	length = uint32(1) << ((tableLog - nbBits) & uint32(0x1F))
	switch length {
	case uint32(1):
		goto _1
	case uint32(2):
		goto _2
	case uint32(4):
		goto _3
	case uint32(8):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
	ptr = begin
_9:
	;
	if !(ptr != end) {
		goto _7
	}
	DElt = HUF_buildDEltX2(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, uint32(baseSeq), level)
	v10 = DTableRank
	DTableRank += 4
	*(*HUF_DEltX2)(unsafe.Pointer(v10)) = DElt
	goto _8
_8:
	;
	ptr = ptr + 1
	goto _9
	goto _7
_7:
	;
	goto _6
_2:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		DElt1 = HUF_buildDEltX2(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, uint32(baseSeq), level)
		*(*HUF_DEltX2)(unsafe.Pointer(DTableRank)) = DElt1
		*(*HUF_DEltX2)(unsafe.Pointer(DTableRank + 1*4)) = DElt1
		DTableRank = DTableRank + uintptr(2)*4
		goto _11
	_11:
		;
		ptr = ptr + 1
	}
	goto _6
_3:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		*(*U64)(unsafe.Pointer(bp)) = HUF_buildDEltX2U64(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, baseSeq, level)
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(0)*4, bp, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(2)*4, bp, libc.Uint64FromInt64(8))
		DTableRank = DTableRank + uintptr(4)*4
		goto _12
	_12:
		;
		ptr = ptr + 1
	}
	goto _6
_4:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		*(*U64)(unsafe.Pointer(bp + 8)) = HUF_buildDEltX2U64(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, baseSeq, level)
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(0)*4, bp+8, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(2)*4, bp+8, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(4)*4, bp+8, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(6)*4, bp+8, libc.Uint64FromInt64(8))
		DTableRank = DTableRank + uintptr(8)*4
		goto _13
	_13:
		;
		ptr = ptr + 1
	}
	goto _6
_5:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		*(*U64)(unsafe.Pointer(bp + 16)) = HUF_buildDEltX2U64(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, baseSeq, level)
		DTableRankEnd = DTableRank + uintptr(length)*4
		for {
			if !(DTableRank != DTableRankEnd) {
				break
			}
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(0)*4, bp+16, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(2)*4, bp+16, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(4)*4, bp+16, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(6)*4, bp+16, libc.Uint64FromInt64(8))
			goto _15
		_15:
			;
			DTableRank = DTableRank + uintptr(8)*4
		}
		goto _14
	_14:
		;
		ptr = ptr + 1
	}
	goto _6
_6:
}

// C documentation
//
//	/* HUF_fillDTableX2Level2() :
//	 * `rankValOrigin` must be a table of at least (HUF_TABLELOG_MAX + 1) U32 */
func HUF_fillDTableX2Level2(tls *libc.TLS, DTable uintptr, targetLog U32, consumedBits U32, rankVal uintptr, minWeight int32, maxWeight1 int32, sortedSymbols uintptr, rankStart uintptr, nbBitsBaseline U32, baseSeq U16) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var begin, end, i, skipSize, w int32
	var length, nbBits, totalBits U32
	var _ /* DEltX2 at bp+0 */ U64
	_, _, _, _, _, _, _, _ = begin, end, i, length, nbBits, skipSize, totalBits, w
	/* Fill skipped values (all positions up to rankVal[minWeight]).
	 * These are positions only get a single symbol because the combined weight
	 * is too large.
	 */
	if minWeight > int32(1) {
		length = uint32(1) << ((targetLog - consumedBits) & uint32(0x1F))
		*(*U64)(unsafe.Pointer(bp)) = HUF_buildDEltX2U64(tls, uint32(baseSeq), consumedBits, uint16(0), int32(1))
		skipSize = int32(*(*U32)(unsafe.Pointer(rankVal + uintptr(minWeight)*4)))
		switch length {
		case uint32(2):
			libc.Xmemcpy(tls, DTable, bp, libc.Uint64FromInt64(8))
		case uint32(4):
			libc.Xmemcpy(tls, DTable+libc.UintptrFromInt32(0)*4, bp, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTable+libc.UintptrFromInt32(2)*4, bp, libc.Uint64FromInt64(8))
		default:
			i = 0
			for {
				if !(i < skipSize) {
					break
				}
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(0)*4, bp, libc.Uint64FromInt64(8))
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(2)*4, bp, libc.Uint64FromInt64(8))
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(4)*4, bp, libc.Uint64FromInt64(8))
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(6)*4, bp, libc.Uint64FromInt64(8))
				goto _1
			_1:
				;
				i = i + int32(8)
			}
		}
	}
	/* Fill each of the second level symbols by weight. */
	w = minWeight
	for {
		if !(w < maxWeight1) {
			break
		}
		begin = int32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w)*4)))
		end = int32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w+int32(1))*4)))
		nbBits = nbBitsBaseline - uint32(w)
		totalBits = nbBits + consumedBits
		HUF_fillDTableX2ForWeight(tls, DTable+uintptr(*(*U32)(unsafe.Pointer(rankVal + uintptr(w)*4)))*4, sortedSymbols+uintptr(begin), sortedSymbols+uintptr(end), totalBits, targetLog, baseSeq, int32(2))
		goto _2
	_2:
		;
		w = w + 1
	}
}

func HUF_fillDTableX2(tls *libc.TLS, DTable uintptr, targetLog U32, sortedList uintptr, rankStart uintptr, rankValOrigin uintptr, maxWeight U32, nbBitsBaseline U32) {
	var begin, end, minWeight, s, scaleLog, start, w, wEnd int32
	var length, minBits, nbBits U32
	var rankVal uintptr
	_, _, _, _, _, _, _, _, _, _, _, _ = begin, end, length, minBits, minWeight, nbBits, rankVal, s, scaleLog, start, w, wEnd
	rankVal = rankValOrigin
	scaleLog = int32(nbBitsBaseline - targetLog) /* note : targetLog >= srcLog, hence scaleLog <= 1 */
	minBits = nbBitsBaseline - maxWeight
	wEnd = int32(maxWeight) + int32(1)
	/* Fill DTable in order of weight. */
	w = int32(1)
	for {
		if !(w < wEnd) {
			break
		}
		begin = int32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w)*4)))
		end = int32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w+int32(1))*4)))
		nbBits = nbBitsBaseline - uint32(w)
		if targetLog-nbBits >= minBits {
			/* Enough room for a second symbol. */
			start = int32(*(*U32)(unsafe.Pointer(rankVal + uintptr(w)*4)))
			length = uint32(1) << ((targetLog - nbBits) & uint32(0x1F))
			minWeight = int32(nbBits + uint32(scaleLog))
			if minWeight < int32(1) {
				minWeight = int32(1)
			}
			/* Fill the DTable for every symbol of weight w.
			 * These symbols get at least 1 second symbol.
			 */
			s = begin
			for {
				if !(s != end) {
					break
				}
				HUF_fillDTableX2Level2(tls, DTable+uintptr(start)*4, targetLog, nbBits, rankValOrigin+uintptr(nbBits)*52, minWeight, wEnd, sortedList, rankStart, nbBitsBaseline, uint16((*(*sortedSymbol_t)(unsafe.Pointer(sortedList + uintptr(s)))).Fsymbol))
				start = int32(uint32(start) + length)
				goto _2
			_2:
				;
				s = s + 1
			}
		} else {
			/* Only a single symbol. */
			HUF_fillDTableX2ForWeight(tls, DTable+uintptr(*(*U32)(unsafe.Pointer(rankVal + uintptr(w)*4)))*4, sortedList+uintptr(begin), sortedList+uintptr(end), nbBits, targetLog, uint16(0), int32(1))
		}
		goto _1
	_1:
		;
		w = w + 1
	}
}

type HUF_ReadDTableX2_Workspace = struct {
	FrankVal      [12]rankValCol_t
	FrankStats    [13]U32
	FrankStart0   [15]U32
	FsortedSymbol [256]sortedSymbol_t
	FweightList   [256]BYTE
	FcalleeWksp   [219]U32
}

func HUF_readDTableX2_wksp(tls *libc.TLS, DTable uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r1 size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var consumed, curr, curr1, maxTableLog, maxW, minBits, nextRankStart, nextRankVal, r, s, w, w1, w2, w3, v4 U32
	var dt, dtPtr, rankStart, rankVal0, rankValPtr, wksp, v5 uintptr
	var iSize size_t
	var rescale int32
	var _ /* dtd at bp+8 */ DTableDesc
	var _ /* nbSymbols at bp+4 */ U32
	var _ /* tableLog at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = consumed, curr, curr1, dt, dtPtr, iSize, maxTableLog, maxW, minBits, nextRankStart, nextRankVal, r, rankStart, rankVal0, rankValPtr, rescale, s, w, w1, w2, w3, wksp, v4, v5
	*(*DTableDesc)(unsafe.Pointer(bp + 8)) = HUF_getDTableDesc(tls, DTable)
	maxTableLog = uint32((*(*DTableDesc)(unsafe.Pointer(bp + 8))).FmaxTableLog)
	dtPtr = DTable + uintptr(1)*4 /* force compiler to avoid strict-aliasing */
	dt = dtPtr
	wksp = workSpace
	if uint64(2124) > wkspSize {
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	rankStart = wksp + 676 + uintptr(1)*4
	libc.Xmemset(tls, wksp+624, 0, libc.Uint64FromInt64(52))
	libc.Xmemset(tls, wksp+676, 0, libc.Uint64FromInt64(60))
	_ = libc.Uint64FromInt64(1) /* if compiler fails here, assertion is wrong */
	if maxTableLog > uint32(HUF_TABLELOG_MAX) {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	}
	/* ZSTD_memset(weightList, 0, sizeof(weightList)); */ /* is not necessary, even though some analyzer complain ... */
	iSize = HUF_readStats_wksp(tls, wksp+992, uint64(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)), wksp+624, bp+4, bp, src, srcSize, wksp+1248, uint64(876), flags)
	if ERR_isError(tls, iSize) != 0 {
		return iSize
	}
	/* check result */
	if *(*U32)(unsafe.Pointer(bp)) > maxTableLog {
		return uint64(-int32(ZSTD_error_tableLog_tooLarge))
	} /* DTable can't fit code depth */
	if *(*U32)(unsafe.Pointer(bp)) <= uint32(HUF_DECODER_FAST_TABLELOG) && maxTableLog > uint32(HUF_DECODER_FAST_TABLELOG) {
		maxTableLog = uint32(HUF_DECODER_FAST_TABLELOG)
	}
	/* find maxWeight */
	maxW = *(*U32)(unsafe.Pointer(bp))
	for {
		if !(*(*U32)(unsafe.Pointer(wksp + 624 + uintptr(maxW)*4)) == uint32(0)) {
			break
		}
		goto _1
	_1:
		;
		maxW = maxW - 1
	} /* necessarily finds a solution before 0 */
	/* Get start index of each weight */
	nextRankStart = uint32(0)
	w = uint32(1)
	for {
		if !(w < maxW+uint32(1)) {
			break
		}
		curr = nextRankStart
		nextRankStart = nextRankStart + *(*U32)(unsafe.Pointer(wksp + 624 + uintptr(w)*4))
		*(*U32)(unsafe.Pointer(rankStart + uintptr(w)*4)) = curr
		goto _2
	_2:
		;
		w = w + 1
	}
	*(*U32)(unsafe.Pointer(rankStart)) = nextRankStart /* put all 0w symbols at the end of sorted list*/
	*(*U32)(unsafe.Pointer(rankStart + uintptr(maxW+uint32(1))*4)) = nextRankStart
	/* sort symbols by weight */
	s = uint32(0)
	for {
		if !(s < *(*U32)(unsafe.Pointer(bp + 4))) {
			break
		}
		w1 = uint32(*(*BYTE)(unsafe.Pointer(wksp + 992 + uintptr(s))))
		v5 = rankStart + uintptr(w1)*4
		v4 = *(*U32)(unsafe.Pointer(v5))
		*(*U32)(unsafe.Pointer(v5)) = *(*U32)(unsafe.Pointer(v5)) + 1
		r = v4
		(*(*sortedSymbol_t)(unsafe.Pointer(wksp + 736 + uintptr(r)))).Fsymbol = uint8(s)
		goto _3
	_3:
		;
		s = s + 1
	}
	*(*U32)(unsafe.Pointer(rankStart)) = uint32(0) /* forget 0w symbols; this is beginning of weight(1) */
	/* Build rankVal */
	rankVal0 = wksp
	rescale = int32(maxTableLog - *(*U32)(unsafe.Pointer(bp)) - uint32(1)) /* tableLog <= maxTableLog */
	nextRankVal = uint32(0)
	w2 = uint32(1)
	for {
		if !(w2 < maxW+uint32(1)) {
			break
		}
		curr1 = nextRankVal
		nextRankVal = nextRankVal + *(*U32)(unsafe.Pointer(wksp + 624 + uintptr(w2)*4))<<(w2+uint32(rescale))
		*(*U32)(unsafe.Pointer(rankVal0 + uintptr(w2)*4)) = curr1
		goto _6
	_6:
		;
		w2 = w2 + 1
	}
	minBits = *(*U32)(unsafe.Pointer(bp)) + uint32(1) - maxW
	consumed = minBits
	for {
		if !(consumed < maxTableLog-minBits+uint32(1)) {
			break
		}
		rankValPtr = wksp + uintptr(consumed)*52
		w3 = uint32(1)
		for {
			if !(w3 < maxW+uint32(1)) {
				break
			}
			*(*U32)(unsafe.Pointer(rankValPtr + uintptr(w3)*4)) = *(*U32)(unsafe.Pointer(rankVal0 + uintptr(w3)*4)) >> consumed
			goto _8
		_8:
			;
			w3 = w3 + 1
		}
		goto _7
	_7:
		;
		consumed = consumed + 1
	}
	HUF_fillDTableX2(tls, dt, maxTableLog, wksp+736, wksp+676, wksp, maxW, *(*U32)(unsafe.Pointer(bp))+uint32(1))
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableLog = uint8(maxTableLog)
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableType = uint8(1)
	libc.Xmemcpy(tls, DTable, bp+8, libc.Uint64FromInt64(4))
	return iSize
}

func HUF_decodeSymbolX2(tls *libc.TLS, op uintptr, DStream uintptr, dt uintptr, dtLog U32) (r U32) {
	var val size_t
	_ = val
	val = BIT_lookBitsFast(tls, DStream, dtLog) /* note : dtLog >= 1 */
	libc.Xmemcpy(tls, op, dt+uintptr(val)*4, uint64(libc.Int32FromInt32(2)))
	BIT_skipBits(tls, DStream, uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).FnbBits))
	return uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).Flength)
}

func HUF_decodeLastSymbolX2(tls *libc.TLS, op uintptr, DStream uintptr, dt uintptr, dtLog U32) (r U32) {
	var val size_t
	_ = val
	val = BIT_lookBitsFast(tls, DStream, dtLog) /* note : dtLog >= 1 */
	libc.Xmemcpy(tls, op, dt+uintptr(val)*4, uint64(libc.Int32FromInt32(1)))
	if int32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).Flength) == int32(1) {
		BIT_skipBits(tls, DStream, uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).FnbBits))
	} else {
		if uint64((*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed) < libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) {
			BIT_skipBits(tls, DStream, uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).FnbBits))
			if uint64((*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) {
				/* ugly hack; works only because it's the last symbol. Note : can't easily extract nbBits from just this symbol */
				(*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed = uint32(libc.Uint64FromInt64(8) * libc.Uint64FromInt32(8))
			}
		}
	}
	return uint32(1)
}

func HUF_decodeStreamX2(tls *libc.TLS, p uintptr, bitDPtr uintptr, pEnd uintptr, dt uintptr, dtLog U32) (r size_t) {
	var pStart uintptr
	_ = pStart
	pStart = p
	/* up to 8 symbols at a time */
	if uint64(int64(pEnd)-int64(p)) >= uint64(8) {
		if dtLog <= uint32(11) && MEM_64bits(tls) != 0 {
			/* up to 10 symbols at a time */
			for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd-uintptr(9)) != 0 {
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
			}
		} else {
			/* up to 8 symbols at a time */
			for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd-uintptr(libc.Uint64FromInt64(8)-libc.Uint64FromInt32(1))) != 0 {
				if MEM_64bits(tls) != 0 {
					p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				}
				if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
					p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				}
				if MEM_64bits(tls) != 0 {
					p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				}
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
			}
		}
	} else {
		BIT_reloadDStream(tls, bitDPtr)
	}
	/* closer to end : up to 2 symbols at a time */
	if uint64(int64(pEnd)-int64(p)) >= uint64(2) {
		for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p <= pEnd-uintptr(2)) != 0 {
			p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
		}
		for p <= pEnd-uintptr(2) {
			p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
		} /* no need to reload : reached the end of DStream */
	}
	if p < pEnd {
		p = p + uintptr(HUF_decodeLastSymbolX2(tls, p, bitDPtr, dt, dtLog))
	}
	return uint64(int64(p) - int64(pStart))
}

func HUF_decompress1X2_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var _var_err__ size_t
	var dt, dtPtr, oend, ostart uintptr
	var dtd DTableDesc
	var _ /* bitD at bp+0 */ BIT_DStream_t
	_, _, _, _, _, _ = _var_err__, dt, dtPtr, dtd, oend, ostart
	/* Init */
	_var_err__ = BIT_initDStream(tls, bp, cSrc, cSrcSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	/* decode */
	ostart = dst
	oend = ZSTD_maybeNullPtrAdd(tls, ostart, int64(dstSize))
	dtPtr = DTable + uintptr(1)*4 /* force compiler to not use strict-aliasing */
	dt = dtPtr
	dtd = HUF_getDTableDesc(tls, DTable)
	HUF_decodeStreamX2(tls, ostart, bp, oend, dt, uint32(dtd.FtableLog))
	/* check */
	if !(BIT_endOfDStream(tls, bp) != 0) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* decoded size */
	return dstSize
}

// C documentation
//
//	/* HUF_decompress4X2_usingDTable_internal_body():
//	 * Conditions:
//	 * @dstSize >= 6
//	 */
func HUF_decompress4X2_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(160)
	defer tls.Free(160)
	var _var_err__, _var_err__1, _var_err__2, _var_err__3, length1, length2, length3, length4, segmentSize size_t
	var dt, dtPtr, istart, istart1, istart2, istart3, istart4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart uintptr
	var dtLog, endCheck, endSignal U32
	var dtd DTableDesc
	var _ /* bitD1 at bp+0 */ BIT_DStream_t
	var _ /* bitD2 at bp+40 */ BIT_DStream_t
	var _ /* bitD3 at bp+80 */ BIT_DStream_t
	var _ /* bitD4 at bp+120 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _var_err__, _var_err__1, _var_err__2, _var_err__3, dt, dtLog, dtPtr, dtd, endCheck, endSignal, istart, istart1, istart2, istart3, istart4, length1, length2, length3, length4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart, segmentSize
	if cSrcSize < uint64(10) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* strict minimum : jump table + 1 byte per stream */
	if dstSize < uint64(6) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* stream 4-split doesn't work */
	istart = cSrc
	ostart = dst
	oend = ostart + uintptr(dstSize)
	olimit = oend - uintptr(libc.Uint64FromInt64(8)-libc.Uint64FromInt32(1))
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	length1 = uint64(MEM_readLE16(tls, istart))
	length2 = uint64(MEM_readLE16(tls, istart+uintptr(2)))
	length3 = uint64(MEM_readLE16(tls, istart+uintptr(4)))
	length4 = cSrcSize - (length1 + length2 + length3 + uint64(6))
	istart1 = istart + uintptr(6) /* jumpTable */
	istart2 = istart1 + uintptr(length1)
	istart3 = istart2 + uintptr(length2)
	istart4 = istart3 + uintptr(length3)
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	opStart2 = ostart + uintptr(segmentSize)
	opStart3 = opStart2 + uintptr(segmentSize)
	opStart4 = opStart3 + uintptr(segmentSize)
	op1 = ostart
	op2 = opStart2
	op3 = opStart3
	op4 = opStart4
	endSignal = uint32(1)
	dtd = HUF_getDTableDesc(tls, DTable)
	dtLog = uint32(dtd.FtableLog)
	if length4 > cSrcSize {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	if opStart4 > oend {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	_var_err__ = BIT_initDStream(tls, bp, istart1, length1)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	_var_err__1 = BIT_initDStream(tls, bp+40, istart2, length2)
	if ERR_isError(tls, _var_err__1) != 0 {
		return _var_err__1
	}
	_var_err__2 = BIT_initDStream(tls, bp+80, istart3, length3)
	if ERR_isError(tls, _var_err__2) != 0 {
		return _var_err__2
	}
	_var_err__3 = BIT_initDStream(tls, bp+120, istart4, length4)
	if ERR_isError(tls, _var_err__3) != 0 {
		return _var_err__3
	}
	/* 16-32 symbols per loop (4-8 symbols per stream) */
	if uint64(int64(oend)-int64(op4)) >= uint64(8) {
		for {
			if !(endSignal&libc.BoolUint32(op4 < olimit) != 0) {
				break
			}
			if MEM_64bits(tls) != 0 {
				op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			}
			op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			endSignal = uint32(int32(libc.BoolUint32(BIT_reloadDStreamFast(tls, bp) == int32(BIT_DStream_unfinished)) & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+40) == int32(BIT_DStream_unfinished)) & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+80) == int32(BIT_DStream_unfinished)) & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+120) == int32(BIT_DStream_unfinished))))
			goto _1
		_1:
		}
	}
	/* check corruption */
	if op1 > opStart2 {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if op2 > opStart3 {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if op3 > opStart4 {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* note : op4 already verified within main loop */
	/* finish bitStreams one by one */
	HUF_decodeStreamX2(tls, op1, bp, opStart2, dt, dtLog)
	HUF_decodeStreamX2(tls, op2, bp+40, opStart3, dt, dtLog)
	HUF_decodeStreamX2(tls, op3, bp+80, opStart4, dt, dtLog)
	HUF_decodeStreamX2(tls, op4, bp+120, oend, dt, dtLog)
	/* check */
	endCheck = BIT_endOfDStream(tls, bp) & BIT_endOfDStream(tls, bp+40) & BIT_endOfDStream(tls, bp+80) & BIT_endOfDStream(tls, bp+120)
	if !(endCheck != 0) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* decoded size */
	return dstSize
	return r
}

func HUF_decompress4X2_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X2_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X2_usingDTable_internal_fast_c_loop(tls *libc.TLS, args uintptr) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var ctz, ctz1, ctz2, ctz3, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index20, index21, index22, index23, index24, index3, index4, index5, index6, index7, index8, index9, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, stream int32
	var dtable, ilowest, olimit uintptr
	var entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry20, entry21, entry22, entry23, entry24, entry3, entry4, entry5, entry6, entry7, entry8, entry9 HUF_DEltX2
	var iters, oiters size_t
	var oend [4]uintptr
	var v4 uint64
	var _ /* bits at bp+0 */ [4]U64
	var _ /* ip at bp+32 */ [4]uintptr
	var _ /* op at bp+64 */ [4]uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = ctz, ctz1, ctz2, ctz3, dtable, entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry20, entry21, entry22, entry23, entry24, entry3, entry4, entry5, entry6, entry7, entry8, entry9, ilowest, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index20, index21, index22, index23, index24, index3, index4, index5, index6, index7, index8, index9, iters, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, oend, oiters, olimit, stream, v4
	dtable = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Fdt
	ilowest = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest
	/* Copy the arguments to local registers. */
	libc.Xmemcpy(tls, bp, args+64, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+32, args, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+64, args+32, libc.Uint64FromInt64(32))
	oend[0] = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)]
	oend[int32(1)] = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)]
	oend[int32(2)] = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)]
	oend[int32(3)] = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Foend
	for {
		/* Assert loop preconditions */
		stream = 0
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _2
		_2:
			;
			stream = stream + 1
		}
		/* Compute olimit */
		/* Each loop does 5 table lookups for each of the 4 streams.
		 * Each table lookup consumes up to 11 bits of input, and produces
		 * up to 2 bytes of output.
		 */
		/* We can consume up to 7 bytes of input per iteration per stream.
		 * We also know that each input pointer is >= ip[0]. So we can run
		 * iters loops before running out of input.
		 */
		iters = uint64(int64((*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0])-int64(ilowest)) / uint64(7)
		/* Each iteration can produce up to 10 bytes of output per stream.
		 * Each output stream my advance at different rates. So take the
		 * minimum number of safe iterations among all the output streams.
		 */
		stream = 0
		for {
			if !(stream < int32(4)) {
				break
			}
			oiters = uint64(int64(oend[stream])-int64((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[stream])) / uint64(10)
			if iters < oiters {
				v4 = iters
			} else {
				v4 = oiters
			}
			iters = v4
			goto _3
		_3:
			;
			stream = stream + 1
		}
		/* Each iteration produces at least 5 output symbols. So until
		 * op[3] crosses olimit, we know we haven't executed iters
		 * iterations yet. This saves us maintaining an iters counter,
		 * at the expense of computing the remaining # of iterations
		 * more frequently.
		 */
		olimit = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + uintptr(iters*libc.Uint64FromInt32(5))
		/* Exit the fast decoding loop once we reach the end. */
		if (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] == olimit {
			break
		}
		/* Exit the decoding loop if any input pointer has crossed the
		 * previous one. This indicates corruption, and a precondition
		 * to our loop is that ip[i] >= ip[0].
		 */
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			if (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream] < (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream-int32(1)] {
				goto _out
			}
			goto _5
		_5:
			;
			stream = stream + 1
		}
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _6
		_6:
			;
			stream = stream + 1
		}
		/* Manually unroll the loop because compilers don't consistently
		 * unroll the inner loops, which destroys performance.
		 */
		for cond := true; cond; cond = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] < olimit {
			/* Decode 5 symbols from each of the first 3 streams.
			 * The final stream will be decoded during the reload phase
			 * to reduce register pressure.
			 */
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= uint64(int32(entry.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index1 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry1 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index1)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry1.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(int32(entry1.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry1.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index2 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry2 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index2)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry2.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(int32(entry2.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry2.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index3 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry3 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index3)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry3.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry3.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry3.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index4 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry4 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index4)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry4.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= uint64(int32(entry4.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry4.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index5 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry5 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index5)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry5.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(int32(entry5.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry5.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index6 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry6 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index6)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry6.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(int32(entry6.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry6.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index7 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry7 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index7)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry7.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry7.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry7.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index8 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry8 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index8)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry8.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= uint64(int32(entry8.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry8.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index9 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry9 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index9)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry9.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(int32(entry9.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry9.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index10 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry10 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index10)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry10.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(int32(entry10.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry10.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index11 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry11 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index11)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry11.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry11.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry11.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index12 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry12 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index12)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry12.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= uint64(int32(entry12.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry12.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index13 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry13 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index13)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry13.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(int32(entry13.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry13.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index14 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry14 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index14)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry14.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(int32(entry14.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry14.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index15 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry15 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index15)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry15.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry15.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry15.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index16 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry16 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index16)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry16.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= uint64(int32(entry16.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry16.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index17 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry17 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index17)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry17.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(int32(entry17.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry17.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index18 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry18 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index18)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry18.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(int32(entry18.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry18.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index19 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry19 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index19)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry19.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry19.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry19.Flength)
			}
			/* Decode one symbol from the final stream */
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index20 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry20 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index20)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry20.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry20.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry20.Flength)
			}
			/* Decode 4 symbols from the final stream & reload bitstreams.
			 * The final stream is reloaded last, meaning that all 5 symbols
			 * are decoded from the final stream before it is reloaded.
			 */
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index21 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry21 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index21)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry21.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry21.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry21.Flength)
			}
			ctz = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[0]))
			nbBits = ctz & int32(7)
			nbBytes = ctz >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32)) -= uintptr(nbBytes)
			(*(*[4]U64)(unsafe.Pointer(bp)))[0] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp)) <<= uint64(nbBits)
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index22 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry22 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index22)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry22.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry22.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry22.Flength)
			}
			ctz1 = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)]))
			nbBits1 = ctz1 & int32(7)
			nbBytes1 = ctz1 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 1*8)) -= uintptr(nbBytes1)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(1)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= uint64(nbBits1)
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index23 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry23 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index23)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry23.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry23.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry23.Flength)
			}
			ctz2 = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)]))
			nbBits2 = ctz2 & int32(7)
			nbBytes2 = ctz2 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 2*8)) -= uintptr(nbBytes2)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(2)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= uint64(nbBits2)
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index24 = int32((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry24 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index24)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry24.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(int32(entry24.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry24.Flength)
			}
			ctz3 = int32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)]))
			nbBits3 = ctz3 & int32(7)
			nbBytes3 = ctz3 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 3*8)) -= uintptr(nbBytes3)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(3)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= uint64(nbBits3)
		}
		goto _1
	_1:
	}
	goto _out
_out:
	;
	/* Save the final values of each of the state variables back to args. */
	libc.Xmemcpy(tls, args+64, bp, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args, bp+32, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args+32, bp+64, libc.Uint64FromInt64(32))
}

func HUF_decompress4X2_usingDTable_internal_fast(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, __ccgo_fp_loopFn HUF_DecompressFastLoopFn) (r size_t) {
	bp := tls.Alloc(192)
	defer tls.Free(192)
	var dt, ilowest, oend, segmentEnd uintptr
	var err_code, err_code1, ret, segmentSize size_t
	var i int32
	var _ /* args at bp+0 */ HUF_DecompressFastArgs
	var _ /* bit at bp+152 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _ = dt, err_code, err_code1, i, ilowest, oend, ret, segmentEnd, segmentSize
	dt = DTable + uintptr(1)*4
	ilowest = cSrc
	oend = ZSTD_maybeNullPtrAdd(tls, dst, int64(dstSize))
	ret = HUF_DecompressFastArgs_init(tls, bp, dst, dstSize, cSrc, cSrcSize, DTable)
	err_code = ret
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6762, 0)
		}
		return err_code
	}
	if ret == uint64(0) {
		return uint64(0)
	}
	(*(*func(*libc.TLS, uintptr))(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_loopFn})))(tls, bp)
	/* note : op4 already verified within main loop */
	_ = ilowest
	/* finish bitStreams one by one */
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	segmentEnd = dst
	i = 0
	for {
		if !(i < int32(4)) {
			break
		}
		if segmentSize <= uint64(int64(oend)-int64(segmentEnd)) {
			segmentEnd = segmentEnd + uintptr(segmentSize)
		} else {
			segmentEnd = oend
		}
		err_code1 = HUF_initRemainingDStream(tls, bp+152, bp, i, segmentEnd)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6751, 0)
			}
			return err_code1
		}
		*(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) += uintptr(HUF_decodeStreamX2(tls, *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)), bp+152, segmentEnd, dt, uint32(HUF_DECODER_FAST_TABLELOG)))
		if *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) != segmentEnd {
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	/* decoded size */
	return dstSize
}

func HUF_decompress4X2_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var fallbackFn HUF_DecompressUsingDTableFn
	var loopFn HUF_DecompressFastLoopFn
	var ret size_t
	_, _, _ = fallbackFn, loopFn, ret
	fallbackFn = __ccgo_fp(HUF_decompress4X2_usingDTable_internal_default)
	loopFn = __ccgo_fp(HUF_decompress4X2_usingDTable_internal_fast_c_loop)
	if flags&int32(HUF_flags_bmi2) != 0 {
		fallbackFn = __ccgo_fp(HUF_decompress4X2_usingDTable_internal_bmi2)
	} else {
		return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	if libc.Bool(int32(HUF_ENABLE_FAST_DECODE) != 0) && !(flags&int32(HUF_flags_disableFast) != 0) {
		ret = HUF_decompress4X2_usingDTable_internal_fast(tls, dst, dstSize, cSrc, cSrcSize, DTable, loopFn)
		if ret != uint64(0) {
			return ret
		}
	}
	return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_decompress1X2_usingDTable_internal_bmi2(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	return HUF_decompress1X2_usingDTable_internal_default(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_DCtx_wksp(tls *libc.TLS, DCtx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX2_wksp(tls, DCtx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress1X2_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, DCtx, flags)
}

func HUF_decompress4X2_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX2_wksp(tls, dctx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress4X2_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, dctx, flags)
}

/* ***********************************/
/* Universal decompression selectors */
/* ***********************************/

type algo_time_t = struct {
	FtableTime     U32
	Fdecode256Time U32
}

var algoTime = [16][2]algo_time_t{
	0: {
		0: {},
		1: {
			FtableTime:     uint32(1),
			Fdecode256Time: uint32(1),
		},
	},
	1: {
		0: {},
		1: {
			FtableTime:     uint32(1),
			Fdecode256Time: uint32(1),
		},
	},
	2: {
		0: {
			FtableTime:     uint32(150),
			Fdecode256Time: uint32(216),
		},
		1: {
			FtableTime:     uint32(381),
			Fdecode256Time: uint32(119),
		},
	},
	3: {
		0: {
			FtableTime:     uint32(170),
			Fdecode256Time: uint32(205),
		},
		1: {
			FtableTime:     uint32(514),
			Fdecode256Time: uint32(112),
		},
	},
	4: {
		0: {
			FtableTime:     uint32(177),
			Fdecode256Time: uint32(199),
		},
		1: {
			FtableTime:     uint32(539),
			Fdecode256Time: uint32(110),
		},
	},
	5: {
		0: {
			FtableTime:     uint32(197),
			Fdecode256Time: uint32(194),
		},
		1: {
			FtableTime:     uint32(644),
			Fdecode256Time: uint32(107),
		},
	},
	6: {
		0: {
			FtableTime:     uint32(221),
			Fdecode256Time: uint32(192),
		},
		1: {
			FtableTime:     uint32(735),
			Fdecode256Time: uint32(107),
		},
	},
	7: {
		0: {
			FtableTime:     uint32(256),
			Fdecode256Time: uint32(189),
		},
		1: {
			FtableTime:     uint32(881),
			Fdecode256Time: uint32(106),
		},
	},
	8: {
		0: {
			FtableTime:     uint32(359),
			Fdecode256Time: uint32(188),
		},
		1: {
			FtableTime:     uint32(1167),
			Fdecode256Time: uint32(109),
		},
	},
	9: {
		0: {
			FtableTime:     uint32(582),
			Fdecode256Time: uint32(187),
		},
		1: {
			FtableTime:     uint32(1570),
			Fdecode256Time: uint32(114),
		},
	},
	10: {
		0: {
			FtableTime:     uint32(688),
			Fdecode256Time: uint32(187),
		},
		1: {
			FtableTime:     uint32(1712),
			Fdecode256Time: uint32(122),
		},
	},
	11: {
		0: {
			FtableTime:     uint32(825),
			Fdecode256Time: uint32(186),
		},
		1: {
			FtableTime:     uint32(1965),
			Fdecode256Time: uint32(136),
		},
	},
	12: {
		0: {
			FtableTime:     uint32(976),
			Fdecode256Time: uint32(185),
		},
		1: {
			FtableTime:     uint32(2131),
			Fdecode256Time: uint32(150),
		},
	},
	13: {
		0: {
			FtableTime:     uint32(1180),
			Fdecode256Time: uint32(186),
		},
		1: {
			FtableTime:     uint32(2070),
			Fdecode256Time: uint32(175),
		},
	},
	14: {
		0: {
			FtableTime:     uint32(1377),
			Fdecode256Time: uint32(185),
		},
		1: {
			FtableTime:     uint32(1731),
			Fdecode256Time: uint32(202),
		},
	},
	15: {
		0: {
			FtableTime:     uint32(1412),
			Fdecode256Time: uint32(185),
		},
		1: {
			FtableTime:     uint32(1695),
			Fdecode256Time: uint32(202),
		},
	},
}

// C documentation
//
//	/** HUF_selectDecoder() :
//	 *  Tells which decoder is likely to decode faster,
//	 *  based on a set of pre-computed metrics.
//	 * @return : 0==HUF_decompress4X1, 1==HUF_decompress4X2 .
//	 *  Assumption : 0 < dstSize <= 128 KB */
func HUF_selectDecoder(tls *libc.TLS, dstSize size_t, cSrcSize size_t) (r U32) {
	var D256, DTime0, DTime1, Q U32
	var v1 uint32
	_, _, _, _, _ = D256, DTime0, DTime1, Q, v1
	/* decoder timing evaluation */
	if cSrcSize >= dstSize {
		v1 = uint32(15)
	} else {
		v1 = uint32(cSrcSize * libc.Uint64FromInt32(16) / dstSize)
	}
	Q = v1 /* Q < 16 */
	D256 = uint32(dstSize >> libc.Int32FromInt32(8))
	DTime0 = (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16))).FtableTime + (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16))).Fdecode256Time*D256
	DTime1 = (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16 + 1*8))).FtableTime + (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16 + 1*8))).Fdecode256Time*D256
	DTime1 = DTime1 + DTime1>>int32(5) /* small advantage to algorithm using less memory, to reduce cache eviction */
	return libc.BoolUint32(DTime1 < DTime0)
	return r
}

func HUF_decompress1X_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var algoNb U32
	var v1 uint64
	_, _ = algoNb, v1
	/* validation checks */
	if dstSize == uint64(0) {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if cSrcSize > dstSize {
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* invalid */
	if cSrcSize == dstSize {
		libc.Xmemcpy(tls, dst, cSrc, dstSize)
		return dstSize
	} /* not compressed */
	if cSrcSize == uint64(1) {
		libc.Xmemset(tls, dst, int32(*(*BYTE)(unsafe.Pointer(cSrc))), dstSize)
		return dstSize
	} /* RLE */
	algoNb = HUF_selectDecoder(tls, dstSize, cSrcSize)
	if algoNb != 0 {
		v1 = HUF_decompress1X2_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	} else {
		v1 = HUF_decompress1X1_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	}
	return v1
	return r
}

func HUF_decompress1X_usingDTable(tls *libc.TLS, dst uintptr, maxDstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var dtd DTableDesc
	var v1 uint64
	_, _ = dtd, v1
	dtd = HUF_getDTableDesc(tls, DTable)
	if dtd.FtableType != 0 {
		v1 = HUF_decompress1X2_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	} else {
		v1 = HUF_decompress1X1_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	}
	return v1
}

func HUF_decompress1X1_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX1_wksp(tls, dctx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress1X1_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, dctx, flags)
}

func HUF_decompress4X_usingDTable(tls *libc.TLS, dst uintptr, maxDstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var dtd DTableDesc
	var v1 uint64
	_, _ = dtd, v1
	dtd = HUF_getDTableDesc(tls, DTable)
	if dtd.FtableType != 0 {
		v1 = HUF_decompress4X2_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	} else {
		v1 = HUF_decompress4X1_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	}
	return v1
}

func HUF_decompress4X_hufOnly_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var algoNb U32
	var v1 uint64
	_, _ = algoNb, v1
	/* validation checks */
	if dstSize == uint64(0) {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if cSrcSize == uint64(0) {
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	algoNb = HUF_selectDecoder(tls, dstSize, cSrcSize)
	if algoNb != 0 {
		v1 = HUF_decompress4X2_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	} else {
		v1 = HUF_decompress4X1_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	}
	return v1
	return r
}

/**** ended inlining decompress/huf_decompress.c ****/
/**** start inlining decompress/zstd_ddict.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* zstd_ddict.c :
 * concentrates all logic that needs to know the internals of ZSTD_DDict object */

/*-*******************************************************
*  Dependencies
*********************************************************/
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/cpu.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** start inlining zstd_decompress_internal.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* zstd_decompress_internal:
 * objects and definitions shared within lib/decompress modules */

/*-*******************************************************
 *  Dependencies
 *********************************************************/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/zstd_internal.h ****/

// C documentation
//
//	/*-*******************************************************
//	 *  Constants
//	 *********************************************************/
var LL_base = [36]U32{
	1:  uint32(1),
	2:  uint32(2),
	3:  uint32(3),
	4:  uint32(4),
	5:  uint32(5),
	6:  uint32(6),
	7:  uint32(7),
	8:  uint32(8),
	9:  uint32(9),
	10: uint32(10),
	11: uint32(11),
	12: uint32(12),
	13: uint32(13),
	14: uint32(14),
	15: uint32(15),
	16: uint32(16),
	17: uint32(18),
	18: uint32(20),
	19: uint32(22),
	20: uint32(24),
	21: uint32(28),
	22: uint32(32),
	23: uint32(40),
	24: uint32(48),
	25: uint32(64),
	26: uint32(0x80),
	27: uint32(0x100),
	28: uint32(0x200),
	29: uint32(0x400),
	30: uint32(0x800),
	31: uint32(0x1000),
	32: uint32(0x2000),
	33: uint32(0x4000),
	34: uint32(0x8000),
	35: uint32(0x10000),
}

var OF_base = [32]U32{
	1:  uint32(1),
	2:  uint32(1),
	3:  uint32(5),
	4:  uint32(0xD),
	5:  uint32(0x1D),
	6:  uint32(0x3D),
	7:  uint32(0x7D),
	8:  uint32(0xFD),
	9:  uint32(0x1FD),
	10: uint32(0x3FD),
	11: uint32(0x7FD),
	12: uint32(0xFFD),
	13: uint32(0x1FFD),
	14: uint32(0x3FFD),
	15: uint32(0x7FFD),
	16: uint32(0xFFFD),
	17: uint32(0x1FFFD),
	18: uint32(0x3FFFD),
	19: uint32(0x7FFFD),
	20: uint32(0xFFFFD),
	21: uint32(0x1FFFFD),
	22: uint32(0x3FFFFD),
	23: uint32(0x7FFFFD),
	24: uint32(0xFFFFFD),
	25: uint32(0x1FFFFFD),
	26: uint32(0x3FFFFFD),
	27: uint32(0x7FFFFFD),
	28: uint32(0xFFFFFFD),
	29: uint32(0x1FFFFFFD),
	30: uint32(0x3FFFFFFD),
	31: uint32(0x7FFFFFFD),
}

var OF_bits = [32]U8{
	1:  uint8(1),
	2:  uint8(2),
	3:  uint8(3),
	4:  uint8(4),
	5:  uint8(5),
	6:  uint8(6),
	7:  uint8(7),
	8:  uint8(8),
	9:  uint8(9),
	10: uint8(10),
	11: uint8(11),
	12: uint8(12),
	13: uint8(13),
	14: uint8(14),
	15: uint8(15),
	16: uint8(16),
	17: uint8(17),
	18: uint8(18),
	19: uint8(19),
	20: uint8(20),
	21: uint8(21),
	22: uint8(22),
	23: uint8(23),
	24: uint8(24),
	25: uint8(25),
	26: uint8(26),
	27: uint8(27),
	28: uint8(28),
	29: uint8(29),
	30: uint8(30),
	31: uint8(31),
}

var ML_base = [53]U32{
	0:  uint32(3),
	1:  uint32(4),
	2:  uint32(5),
	3:  uint32(6),
	4:  uint32(7),
	5:  uint32(8),
	6:  uint32(9),
	7:  uint32(10),
	8:  uint32(11),
	9:  uint32(12),
	10: uint32(13),
	11: uint32(14),
	12: uint32(15),
	13: uint32(16),
	14: uint32(17),
	15: uint32(18),
	16: uint32(19),
	17: uint32(20),
	18: uint32(21),
	19: uint32(22),
	20: uint32(23),
	21: uint32(24),
	22: uint32(25),
	23: uint32(26),
	24: uint32(27),
	25: uint32(28),
	26: uint32(29),
	27: uint32(30),
	28: uint32(31),
	29: uint32(32),
	30: uint32(33),
	31: uint32(34),
	32: uint32(35),
	33: uint32(37),
	34: uint32(39),
	35: uint32(41),
	36: uint32(43),
	37: uint32(47),
	38: uint32(51),
	39: uint32(59),
	40: uint32(67),
	41: uint32(83),
	42: uint32(99),
	43: uint32(0x83),
	44: uint32(0x103),
	45: uint32(0x203),
	46: uint32(0x403),
	47: uint32(0x803),
	48: uint32(0x1003),
	49: uint32(0x2003),
	50: uint32(0x4003),
	51: uint32(0x8003),
	52: uint32(0x10003),
}

// C documentation
//
//	/*-*******************************************************
//	 *  Decompression types
//	 *********************************************************/
type ZSTD_seqSymbol_header = struct {
	FfastMode U32
	FtableLog U32
}

type ZSTD_seqSymbol = struct {
	FnextState        U16
	FnbAdditionalBits BYTE
	FnbBits           BYTE
	FbaseValue        U32
}

type ZSTD_entropyDTables_t = struct {
	FLLTable   [513]ZSTD_seqSymbol
	FOFTable   [257]ZSTD_seqSymbol
	FMLTable   [513]ZSTD_seqSymbol
	FhufTable  [4097]HUF_DTable
	Frep       [3]U32
	Fworkspace [157]U32
}

type ZSTD_dStage = int32

type ZSTD_dStreamStage = int32

type ZSTD_dictUses_e = int32

// C documentation
//
//	/* Hashset for storing references to multiple ZSTD_DDict within ZSTD_DCtx */
type ZSTD_DDictHashSet = struct {
	FddictPtrTable     uintptr
	FddictPtrTableSize size_t
	FddictPtrCount     size_t
}

/* extra buffer, compensates when dst is not large enough to store litBuffer */

type ZSTD_litLocation_e = int32 /* typedef'd to ZSTD_DCtx within "zstd.h" */
func ZSTD_DCtx_get_bmi2(tls *libc.TLS, dctx uintptr) (r int32) {
	return (*ZSTD_DCtx_s)(unsafe.Pointer(dctx)).Fbmi2
}

/* typedef'd to ZSTD_DDict within "zstd.h" */

func ZSTD_DDict_dictContent(tls *libc.TLS, ddict uintptr) (r uintptr) {
	return (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent
}

func ZSTD_DDict_dictSize(tls *libc.TLS, ddict uintptr) (r size_t) {
	return (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize
}

func ZSTD_copyDDictParameters(tls *libc.TLS, dctx uintptr, ddict uintptr) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent + uintptr((*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	if (*ZSTD_DDict)(unsafe.Pointer(ddict)).FentropyPresent != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = uint32(1)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr = ddict + 24
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr = ddict + 24 + 6160
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr = ddict + 24 + 4104
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr = ddict + 24 + 10264
		*(*U32)(unsafe.Pointer(dctx + 32 + 26652)) = *(*U32)(unsafe.Pointer(ddict + 24 + 26652))
		*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + 1*4)) = *(*U32)(unsafe.Pointer(ddict + 24 + 26652 + 1*4))
		*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + 2*4)) = *(*U32)(unsafe.Pointer(ddict + 24 + 26652 + 2*4))
	} else {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = uint32(0)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(0)
	}
}

func ZSTD_loadEntropy_intoDDict(tls *libc.TLS, ddict uintptr, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var magic U32
	_ = magic
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID = uint32(0)
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FentropyPresent = uint32(0)
	if dictContentType == int32(ZSTD_dct_rawContent) {
		return uint64(0)
	}
	if (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize < uint64(8) {
		if dictContentType == int32(ZSTD_dct_fullDict) {
			return uint64(-int32(ZSTD_error_dictionary_corrupted))
		} /* only accept specified dictionaries */
		return uint64(0) /* pure content mode */
	}
	magic = MEM_readLE32(tls, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent)
	if magic != uint32(ZSTD_MAGIC_DICTIONARY) {
		if dictContentType == int32(ZSTD_dct_fullDict) {
			return uint64(-int32(ZSTD_error_dictionary_corrupted))
		} /* only accept specified dictionaries */
		return uint64(0) /* pure content mode */
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID = MEM_readLE32(tls, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent+uintptr(ZSTD_FRAMEIDSIZE))
	/* load entropy tables */
	if ZSTD_isError(tls, ZSTD_loadDEntropy(tls, ddict+24, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FentropyPresent = uint32(1)
	return uint64(0)
}

func ZSTD_initDDict_internal(tls *libc.TLS, ddict uintptr, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var err_code size_t
	var internalBuffer uintptr
	_, _ = err_code, internalBuffer
	if dictLoadMethod == int32(ZSTD_dlm_byRef) || !(dict != 0) || !(dictSize != 0) {
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer = libc.UintptrFromInt32(0)
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent = dict
		if !(dict != 0) {
			dictSize = uint64(0)
		}
	} else {
		internalBuffer = ZSTD_customMalloc(tls, dictSize, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FcMem)
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer = internalBuffer
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent = internalBuffer
		if !(internalBuffer != 0) {
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		libc.Xmemcpy(tls, internalBuffer, dict, dictSize)
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize = dictSize
	*(*HUF_DTable)(unsafe.Pointer(ddict + 24 + 10264)) = uint32(libc.Int32FromInt32(ZSTD_HUFFDTABLE_CAPACITY_LOG) * libc.Int32FromInt32(0x1000001)) /* cover both little and big endian */
	/* parse dictionary content */
	err_code = ZSTD_loadEntropy_intoDDict(tls, ddict, dictContentType)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return uint64(0)
}

func ZSTD_createDDict_advanced(tls *libc.TLS, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, customMem ZSTD_customMem) (r uintptr) {
	var ddict uintptr
	var initResult size_t
	_, _ = ddict, initResult
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	ddict = ZSTD_customMalloc(tls, uint64(27352), customMem)
	if ddict == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FcMem = customMem
	initResult = ZSTD_initDDict_internal(tls, ddict, dict, dictSize, dictLoadMethod, dictContentType)
	if ZSTD_isError(tls, initResult) != 0 {
		ZSTD_freeDDict(tls, ddict)
		return libc.UintptrFromInt32(0)
	}
	return ddict
	return r
}

// C documentation
//
//	/*! ZSTD_createDDict() :
//	*   Create a digested dictionary, to start decompression without startup delay.
//	*   `dict` content is copied inside DDict.
//	*   Consequently, `dict` can be released after `ZSTD_DDict` creation */
func ZSTD_createDDict(tls *libc.TLS, dict uintptr, dictSize size_t) (r uintptr) {
	var allocator ZSTD_customMem
	_ = allocator
	allocator = ZSTD_customMem{}
	return ZSTD_createDDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto), allocator)
}

// C documentation
//
//	/*! ZSTD_createDDict_byReference() :
//	 *  Create a digested dictionary, to start decompression without startup delay.
//	 *  Dictionary content is simply referenced, it will be accessed during decompression.
//	 *  Warning : dictBuffer must outlive DDict (DDict must be freed before dictBuffer) */
func ZSTD_createDDict_byReference(tls *libc.TLS, dictBuffer uintptr, dictSize size_t) (r uintptr) {
	var allocator ZSTD_customMem
	_ = allocator
	allocator = ZSTD_customMem{}
	return ZSTD_createDDict_advanced(tls, dictBuffer, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto), allocator)
}

func ZSTD_initStaticDDict(tls *libc.TLS, sBuffer uintptr, sBufferSize size_t, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r uintptr) {
	var ddict uintptr
	var neededSpace size_t
	var v1 uint64
	_, _, _ = ddict, neededSpace, v1
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = dictSize
	}
	neededSpace = uint64(27352) + v1
	ddict = sBuffer
	if uint64(sBuffer)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* 8-aligned */
	if sBufferSize < neededSpace {
		return libc.UintptrFromInt32(0)
	}
	if dictLoadMethod == int32(ZSTD_dlm_byCopy) {
		libc.Xmemcpy(tls, ddict+libc.UintptrFromInt32(1)*27352, dict, dictSize) /* local copy */
		dict = ddict + uintptr(1)*27352
	}
	if ZSTD_isError(tls, ZSTD_initDDict_internal(tls, ddict, dict, dictSize, int32(ZSTD_dlm_byRef), dictContentType)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	return ddict
}

func ZSTD_freeDDict(tls *libc.TLS, ddict uintptr) (r size_t) {
	var cMem ZSTD_customMem
	_ = cMem
	if ddict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	cMem = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FcMem
	ZSTD_customFree(tls, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer, cMem)
	ZSTD_customFree(tls, ddict, cMem)
	return uint64(0)
	return r
}

// C documentation
//
//	/*! ZSTD_estimateDDictSize() :
//	 *  Estimate amount of memory that will be needed to create a dictionary for decompression.
//	 *  Note : dictionary created by reference using ZSTD_dlm_byRef are smaller */
func ZSTD_estimateDDictSize(tls *libc.TLS, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e) (r size_t) {
	var v1 uint64
	_ = v1
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = dictSize
	}
	return uint64(27352) + v1
}

func ZSTD_sizeof_DDict(tls *libc.TLS, ddict uintptr) (r size_t) {
	var v1 uint64
	_ = v1
	if ddict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof on NULL */
	if (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer != 0 {
		v1 = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize
	} else {
		v1 = uint64(0)
	}
	return uint64(27352) + v1
}

// C documentation
//
//	/*! ZSTD_getDictID_fromDDict() :
//	 *  Provides the dictID of the dictionary loaded into `ddict`.
//	 *  If @return == 0, the dictionary is not conformant to Zstandard specification, or empty.
//	 *  Non-conformant dictionaries can still be loaded, but as content-only dictionaries. */
func ZSTD_getDictID_fromDDict(tls *libc.TLS, ddict uintptr) (r uint32) {
	if ddict == libc.UintptrFromInt32(0) {
		return uint32(0)
	}
	return (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID
}

/**** ended inlining decompress/zstd_ddict.c ****/
/**** start inlining decompress/zstd_decompress.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ***************************************************************
*  Tuning parameters
*****************************************************************/
/*!
 * HEAPMODE :
 * Select how default decompression function ZSTD_decompress() allocates its context,
 * on stack (0), or into heap (1, default; requires malloc()).
 * Note that functions with explicit context such as ZSTD_decompressDCtx() are unaffected.
 */

/*!
*  LEGACY_SUPPORT :
*  if set to 1+, ZSTD_decompress() can decode older formats (v0.1+)
 */

/*!
 *  MAXWINDOWSIZE_DEFAULT :
 *  maximum window size accepted by DStream __by default__.
 *  Frames requiring more memory will be rejected.
 *  It's possible to set a different limit using ZSTD_DCtx_setMaxWindowSize().
 */

/*!
 *  NO_FORWARD_PROGRESS_MAX :
 *  maximum allowed nb of calls to ZSTD_decompressStream()
 *  without any forward progress
 *  (defined as: no byte read from input, and no byte flushed to output)
 *  before triggering an error.
 */

/*-*******************************************************
*  Dependencies
*********************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/bits.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/xxhash.h ****/
/**** skipping file: zstd_decompress_internal.h ****/
/**** skipping file: zstd_ddict.h ****/
/**** start inlining zstd_decompress_block.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*******************************************************
 *  Dependencies
 *********************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: zstd_decompress_internal.h ****/

/* ===   Prototypes   === */

/* note: prototypes already published within `zstd.h` :
 * ZSTD_decompressBlock()
 */

/* note: prototypes already published within `zstd_internal.h` :
 * ZSTD_getcBlockSize()
 * ZSTD_decodeSeqHeaders()
 */

// C documentation
//
//	/* Streaming state is used to inform allocation of the literal buffer */
type streaming_operation = int32

const not_streaming = 0
const is_streaming = 1

/**** ended inlining zstd_decompress_block.h ****/

/*************************************
 * Multiple DDicts Hashset internals *
 *************************************/

// C documentation
//
//	/* Hash function to determine starting position of dict insertion within the table
//	 * Returns an index between [0, hashSet->ddictPtrTableSize]
//	 */
func ZSTD_DDictHashSet_getIndex(tls *libc.TLS, hashSet uintptr, _dictID U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*U32)(unsafe.Pointer(bp)) = _dictID
	var hash U64
	_ = hash
	hash = XXH_INLINE_XXH64(tls, bp, uint64(4), uint64(0))
	/* DDict ptr table size is a multiple of 2, use size - 1 as mask to get index within [0, hashSet->ddictPtrTableSize) */
	return hash & ((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize - uint64(1))
}

// C documentation
//
//	/* Adds DDict to a hashset without resizing it.
//	 * If inserting a DDict with a dictID that already exists in the set, replaces the one in the set.
//	 * Returns 0 if successful, or a zstd error code if something went wrong.
//	 */
func ZSTD_DDictHashSet_emplaceDDict(tls *libc.TLS, hashSet uintptr, ddict uintptr) (r size_t) {
	var dictID U32
	var idx, idxRangeMask size_t
	_, _, _ = dictID, idx, idxRangeMask
	dictID = ZSTD_getDictID_fromDDict(tls, ddict)
	idx = ZSTD_DDictHashSet_getIndex(tls, hashSet, dictID)
	idxRangeMask = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize - uint64(1)
	if (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount == (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6786, 0)
		}
		return uint64(-int32(ZSTD_error_GENERIC))
	}
	for *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8)) != libc.UintptrFromInt32(0) {
		/* Replace existing ddict if inserting ddict with same dictID */
		if ZSTD_getDictID_fromDDict(tls, *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8))) == dictID {
			*(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8)) = ddict
			return uint64(0)
		}
		idx = idx & idxRangeMask
		idx = idx + 1
	}
	*(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8)) = ddict
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount + 1
	return uint64(0)
}

// C documentation
//
//	/* Expands hash table by factor of DDICT_HASHSET_RESIZE_FACTOR and
//	 * rehashes all values, allocates new table, frees old table.
//	 * Returns 0 on success, otherwise a zstd error code.
//	 */
func ZSTD_DDictHashSet_expand(tls *libc.TLS, hashSet uintptr, customMem ZSTD_customMem) (r size_t) {
	var err_code, i, newTableSize, oldTableSize size_t
	var newTable, oldTable uintptr
	_, _, _, _, _, _ = err_code, i, newTable, newTableSize, oldTable, oldTableSize
	newTableSize = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize * uint64(DDICT_HASHSET_RESIZE_FACTOR)
	newTable = ZSTD_customCalloc(tls, uint64(8)*newTableSize, customMem)
	oldTable = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable
	oldTableSize = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize
	if !(newTable != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6804, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable = newTable
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize = newTableSize
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount = uint64(0)
	i = uint64(0)
	for {
		if !(i < oldTableSize) {
			break
		}
		if *(*uintptr)(unsafe.Pointer(oldTable + uintptr(i)*8)) != libc.UintptrFromInt32(0) {
			err_code = ZSTD_DDictHashSet_emplaceDDict(tls, hashSet, *(*uintptr)(unsafe.Pointer(oldTable + uintptr(i)*8)))
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code
			}
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	ZSTD_customFree(tls, oldTable, customMem)
	return uint64(0)
}

// C documentation
//
//	/* Fetches a DDict with the given dictID
//	 * Returns the ZSTD_DDict* with the requested dictID. If it doesn't exist, then returns NULL.
//	 */
func ZSTD_DDictHashSet_getDDict(tls *libc.TLS, hashSet uintptr, dictID U32) (r uintptr) {
	var currDictID, idx, idxRangeMask size_t
	_, _, _ = currDictID, idx, idxRangeMask
	idx = ZSTD_DDictHashSet_getIndex(tls, hashSet, dictID)
	idxRangeMask = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize - uint64(1)
	for {
		currDictID = uint64(ZSTD_getDictID_fromDDict(tls, *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8))))
		if currDictID == uint64(dictID) || currDictID == uint64(0) {
			/* currDictID == 0 implies a NULL ddict entry */
			break
		} else {
			idx = idx & idxRangeMask /* Goes to start of table when we reach the end */
			idx = idx + 1
		}
		goto _1
	_1:
	}
	return *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8))
}

// C documentation
//
//	/* Allocates space for and returns a ddict hash set
//	 * The hash set's ZSTD_DDict* table has all values automatically set to NULL to begin with.
//	 * Returns NULL if allocation failed.
//	 */
func ZSTD_createDDictHashSet(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var ret uintptr
	_ = ret
	ret = ZSTD_customMalloc(tls, uint64(24), customMem)
	if !(ret != 0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrTable = ZSTD_customCalloc(tls, libc.Uint64FromInt32(DDICT_HASHSET_TABLE_BASE_SIZE)*libc.Uint64FromInt64(8), customMem)
	if !((*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrTable != 0) {
		ZSTD_customFree(tls, ret, customMem)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrTableSize = uint64(DDICT_HASHSET_TABLE_BASE_SIZE)
	(*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrCount = uint64(0)
	return ret
}

// C documentation
//
//	/* Frees the table of ZSTD_DDict* within a hashset, then frees the hashset itself.
//	 * Note: The ZSTD_DDict* within the table are NOT freed.
//	 */
func ZSTD_freeDDictHashSet(tls *libc.TLS, hashSet uintptr, customMem ZSTD_customMem) {
	if hashSet != 0 && (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable != 0 {
		ZSTD_customFree(tls, (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable, customMem)
	}
	if hashSet != 0 {
		ZSTD_customFree(tls, hashSet, customMem)
	}
}

// C documentation
//
//	/* Public function: Adds a DDict into the ZSTD_DDictHashSet, possibly triggering a resize of the hash set.
//	 * Returns 0 on success, or a ZSTD error.
//	 */
func ZSTD_DDictHashSet_addDDict(tls *libc.TLS, hashSet uintptr, ddict uintptr, customMem ZSTD_customMem) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	if (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount*uint64(DDICT_HASHSET_MAX_LOAD_FACTOR_COUNT_MULT)/(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize*uint64(DDICT_HASHSET_MAX_LOAD_FACTOR_SIZE_MULT) != uint64(0) {
		err_code = ZSTD_DDictHashSet_expand(tls, hashSet, customMem)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
	}
	err_code1 = ZSTD_DDictHashSet_emplaceDDict(tls, hashSet, ddict)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/*-*************************************************************
//	*   Context management
//	***************************************************************/
func ZSTD_sizeof_DCtx(tls *libc.TLS, dctx uintptr) (r size_t) {
	if dctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof NULL */
	return uint64(95968) + ZSTD_sizeof_DDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal) + (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuffSize + (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBuffSize
}

func ZSTD_estimateDCtxSize(tls *libc.TLS) (r size_t) {
	return uint64(95968)
}

func ZSTD_startingInputLength(tls *libc.TLS, format ZSTD_format_e) (r size_t) {
	var startingInputLength size_t
	var v1 int32
	_, _ = startingInputLength, v1
	if format == int32(ZSTD_f_zstd1) {
		v1 = int32(5)
	} else {
		v1 = int32(1)
	}
	startingInputLength = uint64(v1)
	/* only supports formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless */
	return startingInputLength
}

func ZSTD_DCtx_resetParameters(tls *libc.TLS, dctx uintptr) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat = int32(ZSTD_f_zstd1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize = uint64(libc.Uint32FromInt32(1)<<libc.Int32FromInt32(ZSTD_WINDOWLOG_LIMIT_DEFAULT) + libc.Uint32FromInt32(1))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBufferMode = int32(ZSTD_bm_buffered)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum = int32(ZSTD_d_validateChecksum)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts = int32(ZSTD_rmd_refSingleDDict)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm = 0
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam = 0
}

func ZSTD_initDCtx_internal(tls *libc.TLS, dctx uintptr) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold = 0
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_dont_use)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuffSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBuffSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage = int32(zdss_init)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FnoForwardProgress = 0
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoversizedDuration = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = int32(1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fbmi2 = ZSTD_cpuSupportsBmi2(tls)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet = libc.UintptrFromInt32(0)
	ZSTD_DCtx_resetParameters(tls, dctx)
}

func ZSTD_initStaticDCtx(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	var dctx uintptr
	_ = dctx
	dctx = workspace
	if uint64(workspace)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* 8-aligned */
	if workspaceSize < uint64(95968) {
		return libc.UintptrFromInt32(0)
	} /* minimum size */
	ZSTD_initDCtx_internal(tls, dctx)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize = workspaceSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff = dctx + libc.UintptrFromInt32(1)*95968
	return dctx
}

func ZSTD_createDCtx_internal(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var dctx uintptr
	_ = dctx
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	dctx = ZSTD_customMalloc(tls, uint64(95968), customMem)
	if !(dctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem = customMem
	ZSTD_initDCtx_internal(tls, dctx)
	return dctx
	return r
}

func ZSTD_createDCtx_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, customMem)
}

func ZSTD_createDCtx(tls *libc.TLS) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, ZSTD_defaultCMem)
}

func ZSTD_clearDict(tls *libc.TLS, dctx uintptr) {
	ZSTD_freeDDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_dont_use)
}

func ZSTD_freeDCtx(tls *libc.TLS, dctx uintptr) (r size_t) {
	var cMem ZSTD_customMem
	_ = cMem
	if dctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6840, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	cMem = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem
	ZSTD_clearDict(tls, dctx)
	ZSTD_customFree(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff, cMem)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff = libc.UintptrFromInt32(0)
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet != 0 {
		ZSTD_freeDDictHashSet(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet, cMem)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet = libc.UintptrFromInt32(0)
	}
	ZSTD_customFree(tls, dctx, cMem)
	return uint64(0)
	return r
}

// C documentation
//
//	/* no longer useful */
func ZSTD_copyDCtx(tls *libc.TLS, dstDCtx uintptr, srcDCtx uintptr) {
	var toCopy size_t
	_ = toCopy
	toCopy = uint64(int64(dstDCtx+30240) - int64(dstDCtx))
	libc.Xmemcpy(tls, dstDCtx, srcDCtx, toCopy) /* no need to copy workspace */
}

// C documentation
//
//	/* Given a dctx with a digested frame params, re-selects the correct ZSTD_DDict based on
//	 * the requested dict ID from the frame. If there exists a reference to the correct ZSTD_DDict, then
//	 * accordingly sets the ddict to be used to decompress the frame.
//	 *
//	 * If no DDict is found, then no action is taken, and the ZSTD_DCtx::ddict remains as-is.
//	 *
//	 * ZSTD_d_refMultipleDDicts must be enabled for this function to be called.
//	 */
func ZSTD_DCtx_selectFrameDDict(tls *libc.TLS, dctx uintptr) {
	var frameDDict uintptr
	_ = frameDDict
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict != 0 {
		frameDDict = ZSTD_DDictHashSet_getDDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID)
		if frameDDict != 0 {
			ZSTD_clearDict(tls, dctx)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = frameDDict
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_indefinitely)
		}
	}
}

/*-*************************************************************
 *   Frame header decoding
 ***************************************************************/

// C documentation
//
//	/*! ZSTD_isFrame() :
//	 *  Tells if the content of `buffer` starts with a valid Frame Identifier.
//	 *  Note : Frame Identifier is 4 bytes. If `size < 4`, @return will always be 0.
//	 *  Note 2 : Legacy Frame Identifiers are considered valid only if Legacy Support is enabled.
//	 *  Note 3 : Skippable Frame Identifiers are considered valid. */
func ZSTD_isFrame(tls *libc.TLS, buffer uintptr, size size_t) (r uint32) {
	var magic U32
	_ = magic
	if size < uint64(ZSTD_FRAMEIDSIZE) {
		return uint32(0)
	}
	magic = MEM_readLE32(tls, buffer)
	if magic == uint32(ZSTD_MAGICNUMBER) {
		return uint32(1)
	}
	if magic&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
		return uint32(1)
	}
	return uint32(0)
}

// C documentation
//
//	/*! ZSTD_isSkippableFrame() :
//	 *  Tells if the content of `buffer` starts with a valid Frame Identifier for a skippable frame.
//	 *  Note : Frame Identifier is 4 bytes. If `size < 4`, @return will always be 0.
//	 */
func ZSTD_isSkippableFrame(tls *libc.TLS, buffer uintptr, size size_t) (r uint32) {
	var magic U32
	_ = magic
	if size < uint64(ZSTD_FRAMEIDSIZE) {
		return uint32(0)
	}
	magic = MEM_readLE32(tls, buffer)
	if magic&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
		return uint32(1)
	}
	return uint32(0)
}

// C documentation
//
//	/** ZSTD_frameHeaderSize_internal() :
//	 *  srcSize must be large enough to reach header size fields.
//	 *  note : only works for formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless.
//	 * @return : size of the Frame Header
//	 *           or an error code, which can be tested with ZSTD_isError() */
func ZSTD_frameHeaderSize_internal(tls *libc.TLS, src uintptr, srcSize size_t, format ZSTD_format_e) (r size_t) {
	var dictID, fcsId, singleSegment U32
	var fhd BYTE
	var minInputSize size_t
	_, _, _, _, _ = dictID, fcsId, fhd, minInputSize, singleSegment
	minInputSize = ZSTD_startingInputLength(tls, format)
	if srcSize < minInputSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	fhd = *(*BYTE)(unsafe.Pointer(src + uintptr(minInputSize-uint64(1))))
	dictID = uint32(int32(fhd) & int32(3))
	singleSegment = uint32(int32(fhd) >> int32(5) & int32(1))
	fcsId = uint32(int32(fhd) >> int32(6))
	return minInputSize + libc.BoolUint64(!(singleSegment != 0)) + ZSTD_did_fieldSize[dictID] + ZSTD_fcs_fieldSize[fcsId] + libc.BoolUint64(singleSegment != 0 && !(fcsId != 0))
	return r
}

// C documentation
//
//	/** ZSTD_frameHeaderSize() :
//	 *  srcSize must be >= ZSTD_frameHeaderSize_prefix.
//	 * @return : size of the Frame Header,
//	 *           or an error code (if srcSize is too small) */
func ZSTD_frameHeaderSize(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_frameHeaderSize_internal(tls, src, srcSize, int32(ZSTD_f_zstd1))
}

// C documentation
//
//	/** ZSTD_getFrameHeader_advanced() :
//	 *  decode Frame Header, or require larger `srcSize`.
//	 *  note : only works for formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless
//	 * @return : 0, `zfhPtr` is correctly filled,
//	 *          >0, `srcSize` is too small, value is wanted `srcSize` amount,
//	**           or an error code, which can be tested using ZSTD_isError() */
func ZSTD_getFrameHeader_advanced(tls *libc.TLS, zfhPtr uintptr, src uintptr, srcSize size_t, format ZSTD_format_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var checksumFlag, dictID, dictIDSizeCode, fcsID, singleSegment, windowLog U32
	var fhdByte, wlByte BYTE
	var fhsize, minInputSize, pos, toCopy, v2 size_t
	var frameContentSize, windowSize U64
	var ip uintptr
	var v1 uint64
	var _ /* hbuf at bp+0 */ [4]uint8
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = checksumFlag, dictID, dictIDSizeCode, fcsID, fhdByte, fhsize, frameContentSize, ip, minInputSize, pos, singleSegment, toCopy, windowLog, windowSize, wlByte, v1, v2
	ip = src
	minInputSize = ZSTD_startingInputLength(tls, format)
	if srcSize > uint64(0) {
		/* note : technically could be considered an assert(), since it's an invalid entry */
		if src == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6872, 0)
			}
			return uint64(-int32(ZSTD_error_GENERIC))
		}
	}
	if srcSize < minInputSize {
		if srcSize > uint64(0) && format != int32(ZSTD_f_zstd1_magicless) {
			if uint64(libc.Int32FromInt32(4)) < srcSize {
				v1 = uint64(libc.Int32FromInt32(4))
			} else {
				v1 = srcSize
			}
			/* when receiving less than @minInputSize bytes,
			 * control these bytes at least correspond to a supported magic number
			 * in order to error out early if they don't.
			**/
			toCopy = v1
			MEM_writeLE32(tls, bp, uint32(ZSTD_MAGICNUMBER))
			libc.Xmemcpy(tls, bp, src, toCopy)
			if MEM_readLE32(tls, bp) != uint32(ZSTD_MAGICNUMBER) {
				/* not a zstd frame : let's check if it's a skippable frame */
				MEM_writeLE32(tls, bp, uint32(ZSTD_MAGIC_SKIPPABLE_START))
				libc.Xmemcpy(tls, bp, src, toCopy)
				if MEM_readLE32(tls, bp)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) != uint32(ZSTD_MAGIC_SKIPPABLE_START) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+6917, 0)
					}
					return uint64(-int32(ZSTD_error_prefix_unknown))
				}
			}
		}
		return minInputSize
	}
	libc.Xmemset(tls, zfhPtr, 0, libc.Uint64FromInt64(48)) /* not strictly necessary, but static analyzers may not understand that zfhPtr will be read only if return value is zero, since they are 2 different signals */
	if format != int32(ZSTD_f_zstd1_magicless) && MEM_readLE32(tls, src) != uint32(ZSTD_MAGICNUMBER) {
		if MEM_readLE32(tls, src)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
			/* skippable frame */
			if srcSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) {
				return uint64(ZSTD_SKIPPABLEHEADERSIZE)
			} /* magic number + frame length */
			libc.Xmemset(tls, zfhPtr, 0, libc.Uint64FromInt64(48))
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeType = int32(ZSTD_skippableFrame)
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FdictID = MEM_readLE32(tls, src) - uint32(ZSTD_MAGIC_SKIPPABLE_START)
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FheaderSize = uint32(ZSTD_SKIPPABLEHEADERSIZE)
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeContentSize = uint64(MEM_readLE32(tls, src+uintptr(ZSTD_FRAMEIDSIZE)))
			return uint64(0)
		}
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_prefix_unknown))
	}
	/* ensure there is enough `srcSize` to fully read/decode frame header */
	fhsize = ZSTD_frameHeaderSize_internal(tls, src, srcSize, format)
	if srcSize < fhsize {
		return fhsize
	}
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FheaderSize = uint32(fhsize)
	fhdByte = *(*BYTE)(unsafe.Pointer(ip + uintptr(minInputSize-uint64(1))))
	pos = minInputSize
	dictIDSizeCode = uint32(int32(fhdByte) & int32(3))
	checksumFlag = uint32(int32(fhdByte) >> int32(2) & int32(1))
	singleSegment = uint32(int32(fhdByte) >> int32(5) & int32(1))
	fcsID = uint32(int32(fhdByte) >> int32(6))
	windowSize = uint64(0)
	dictID = uint32(0)
	frameContentSize = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	if int32(fhdByte)&int32(0x08) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6976, 0)
		}
		return uint64(-int32(ZSTD_error_frameParameter_unsupported))
	}
	if !(singleSegment != 0) {
		v2 = pos
		pos = pos + 1
		wlByte = *(*BYTE)(unsafe.Pointer(ip + uintptr(v2)))
		windowLog = uint32(int32(wlByte)>>int32(3) + int32(ZSTD_WINDOWLOG_ABSOLUTEMIN))
		if windowLog > uint32(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_frameParameter_windowTooLarge))
		}
		windowSize = libc.Uint64FromUint64(1) << windowLog
		windowSize = windowSize + windowSize>>libc.Int32FromInt32(3)*uint64(int32(wlByte)&libc.Int32FromInt32(7))
	}
	switch dictIDSizeCode {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
	case uint32(1):
		dictID = uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(pos))))
		pos = pos + 1
	case uint32(2):
		dictID = uint32(MEM_readLE16(tls, ip+uintptr(pos)))
		pos = pos + uint64(2)
	case uint32(3):
		dictID = MEM_readLE32(tls, ip+uintptr(pos))
		pos = pos + uint64(4)
		break
	}
	switch fcsID {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
		if singleSegment != 0 {
			frameContentSize = uint64(*(*BYTE)(unsafe.Pointer(ip + uintptr(pos))))
		}
	case uint32(1):
		frameContentSize = uint64(int32(MEM_readLE16(tls, ip+uintptr(pos))) + int32(256))
	case uint32(2):
		frameContentSize = uint64(MEM_readLE32(tls, ip+uintptr(pos)))
	case uint32(3):
		frameContentSize = MEM_readLE64(tls, ip+uintptr(pos))
		break
	}
	if singleSegment != 0 {
		windowSize = frameContentSize
	}
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeType = int32(ZSTD_frame)
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeContentSize = frameContentSize
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FwindowSize = windowSize
	if windowSize < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		v1 = windowSize
	} else {
		v1 = uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FblockSizeMax = uint32(v1)
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FdictID = dictID
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FchecksumFlag = checksumFlag
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_getFrameHeader() :
//	 *  decode Frame Header, or require larger `srcSize`.
//	 *  note : this function does not consume input, it only reads it.
//	 * @return : 0, `zfhPtr` is correctly filled,
//	 *          >0, `srcSize` is too small, value is wanted `srcSize` amount,
//	 *           or an error code, which can be tested using ZSTD_isError() */
func ZSTD_getFrameHeader(tls *libc.TLS, zfhPtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_getFrameHeader_advanced(tls, zfhPtr, src, srcSize, int32(ZSTD_f_zstd1))
}

// C documentation
//
//	/** ZSTD_getFrameContentSize() :
//	 *  compatible with legacy mode
//	 * @return : decompressed size of the single frame pointed to be `src` if known, otherwise
//	 *         - ZSTD_CONTENTSIZE_UNKNOWN if the size cannot be determined
//	 *         - ZSTD_CONTENTSIZE_ERROR if an error occurred (e.g. invalid magic number, srcSize too small) */
func ZSTD_getFrameContentSize(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var _ /* zfh at bp+0 */ ZSTD_FrameHeader
	if ZSTD_getFrameHeader(tls, bp, src, srcSize) != uint64(0) {
		return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
	}
	if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FframeType == int32(ZSTD_skippableFrame) {
		return uint64(0)
	} else {
		return (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FframeContentSize
	}
	return r
}

func readSkippableFrameSize(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	var sizeU32 U32
	var skippableHeaderSize, skippableSize size_t
	_, _, _ = sizeU32, skippableHeaderSize, skippableSize
	skippableHeaderSize = uint64(ZSTD_SKIPPABLEHEADERSIZE)
	if srcSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	sizeU32 = MEM_readLE32(tls, src+uintptr(ZSTD_FRAMEIDSIZE))
	if sizeU32+libc.Uint32FromInt32(ZSTD_SKIPPABLEHEADERSIZE) < sizeU32 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_frameParameter_unsupported))
	}
	skippableSize = skippableHeaderSize + uint64(sizeU32)
	if skippableSize > srcSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	return skippableSize
	return r
}

// C documentation
//
//	/*! ZSTD_readSkippableFrame() :
//	 * Retrieves content of a skippable frame, and writes it to dst buffer.
//	 *
//	 * The parameter magicVariant will receive the magicVariant that was supplied when the frame was written,
//	 * i.e. magicNumber - ZSTD_MAGIC_SKIPPABLE_START.  This can be NULL if the caller is not interested
//	 * in the magicVariant.
//	 *
//	 * Returns an error if destination buffer is not large enough, or if this is not a valid skippable frame.
//	 *
//	 * @return : number of bytes written or a ZSTD error.
//	 */
func ZSTD_readSkippableFrame(tls *libc.TLS, dst uintptr, dstCapacity size_t, magicVariant uintptr, src uintptr, srcSize size_t) (r size_t) {
	var magicNumber U32
	var skippableContentSize, skippableFrameSize size_t
	_, _, _ = magicNumber, skippableContentSize, skippableFrameSize
	if srcSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	magicNumber = MEM_readLE32(tls, src)
	skippableFrameSize = readSkippableFrameSize(tls, src, srcSize)
	skippableContentSize = skippableFrameSize - uint64(ZSTD_SKIPPABLEHEADERSIZE)
	/* check input validity */
	if !(ZSTD_isSkippableFrame(tls, src, srcSize) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_frameParameter_unsupported))
	}
	if skippableFrameSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) || skippableFrameSize > srcSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if skippableContentSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* deliver payload */
	if skippableContentSize > uint64(0) && dst != libc.UintptrFromInt32(0) {
		libc.Xmemcpy(tls, dst, src+libc.UintptrFromInt32(ZSTD_SKIPPABLEHEADERSIZE), skippableContentSize)
	}
	if magicVariant != libc.UintptrFromInt32(0) {
		*(*uint32)(unsafe.Pointer(magicVariant)) = magicNumber - uint32(ZSTD_MAGIC_SKIPPABLE_START)
	}
	return skippableContentSize
	return r
}

// C documentation
//
//	/** ZSTD_findDecompressedSize() :
//	 *  `srcSize` must be the exact length of some number of ZSTD compressed and/or
//	 *      skippable frames
//	 *  note: compatible with legacy mode
//	 * @return : decompressed size of the frames contained */
func ZSTD_findDecompressedSize(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	var fcs, totalDstSize uint64
	var frameSrcSize, skippableSize size_t
	var magicNumber U32
	_, _, _, _, _ = fcs, frameSrcSize, magicNumber, skippableSize, totalDstSize
	totalDstSize = uint64(0)
	for srcSize >= ZSTD_startingInputLength(tls, int32(ZSTD_f_zstd1)) {
		magicNumber = MEM_readLE32(tls, src)
		if magicNumber&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
			skippableSize = readSkippableFrameSize(tls, src, srcSize)
			if ZSTD_isError(tls, skippableSize) != 0 {
				return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
			}
			src = src + uintptr(skippableSize)
			srcSize = srcSize - skippableSize
			continue
		}
		fcs = ZSTD_getFrameContentSize(tls, src, srcSize)
		if fcs >= libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
			return fcs
		}
		if totalDstSize+fcs < totalDstSize {
			return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
		} /* check for overflow */
		totalDstSize = totalDstSize + fcs
		/* skip to next frame */
		frameSrcSize = ZSTD_findFrameCompressedSize(tls, src, srcSize)
		if ZSTD_isError(tls, frameSrcSize) != 0 {
			return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
		}
		src = src + uintptr(frameSrcSize)
		srcSize = srcSize - frameSrcSize
	} /* while (srcSize >= ZSTD_frameHeaderSize_prefix) */
	if srcSize != 0 {
		return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
	}
	return totalDstSize
}

// C documentation
//
//	/** ZSTD_getDecompressedSize() :
//	 *  compatible with legacy mode
//	 * @return : decompressed size if known, 0 otherwise
//	             note : 0 can mean any of the following :
//	                   - frame content is empty
//	                   - decompressed size field is not present in frame header
//	                   - frame header unknown / not supported
//	                   - frame header not complete (`srcSize` too small) */
func ZSTD_getDecompressedSize(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	var ret, v1 uint64
	_, _ = ret, v1
	ret = ZSTD_getFrameContentSize(tls, src, srcSize)
	_ = libc.Uint64FromInt64(1)
	if ret >= libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
		v1 = uint64(0)
	} else {
		v1 = ret
	}
	return v1
}

// C documentation
//
//	/** ZSTD_decodeFrameHeader() :
//	 * `headerSize` must be the size provided by ZSTD_frameHeaderSize().
//	 * If multiple DDict references are enabled, also will choose the correct DDict to use.
//	 * @return : 0 if success, or an error code, which can be tested using ZSTD_isError() */
func ZSTD_decodeFrameHeader(tls *libc.TLS, dctx uintptr, src uintptr, headerSize size_t) (r size_t) {
	var result size_t
	var v1 int32
	_, _ = result, v1
	result = ZSTD_getFrameHeader_advanced(tls, dctx+29928, src, headerSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat)
	if ZSTD_isError(tls, result) != 0 {
		return result
	} /* invalid header */
	if result > uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7004, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Reference DDict requested by frame if dctx references multiple ddicts */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts == int32(ZSTD_rmd_refMultipleDDicts) && (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet != 0 {
		ZSTD_DCtx_selectFrameDDict(tls, dctx)
	}
	/* Skip the dictID check in fuzzing mode, because it makes the search
	 * harder.
	 */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID != 0 && (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_wrong))
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 && !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum != 0) {
		v1 = int32(1)
	} else {
		v1 = 0
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum = uint32(v1)
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
		XXH_INLINE_XXH64_reset(tls, dctx+30008, uint64(0))
	}
	*(*U64)(unsafe.Pointer(dctx + 29976)) += headerSize
	return uint64(0)
}

func ZSTD_errorFrameSizeInfo(tls *libc.TLS, ret size_t) (r ZSTD_frameSizeInfo) {
	var frameSizeInfo ZSTD_frameSizeInfo
	_ = frameSizeInfo
	frameSizeInfo.FcompressedSize = ret
	frameSizeInfo.FdecompressedBound = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
	return frameSizeInfo
}

func ZSTD_findFrameSizeInfo(tls *libc.TLS, src uintptr, srcSize size_t, format ZSTD_format_e) (r ZSTD_frameSizeInfo) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var cBlockSize, nbBlocks, remainingSize, ret size_t
	var ip, ipstart uintptr
	var v1 uint64
	var _ /* blockProperties at bp+72 */ blockProperties_t
	var _ /* frameSizeInfo at bp+0 */ ZSTD_frameSizeInfo
	var _ /* zfh at bp+24 */ ZSTD_FrameHeader
	_, _, _, _, _, _, _ = cBlockSize, ip, ipstart, nbBlocks, remainingSize, ret, v1
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(24))
	if format == int32(ZSTD_f_zstd1) && srcSize >= uint64(ZSTD_SKIPPABLEHEADERSIZE) && MEM_readLE32(tls, src)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FcompressedSize = readSkippableFrameSize(tls, src, srcSize)
		return *(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))
	} else {
		ip = src
		ipstart = ip
		remainingSize = srcSize
		nbBlocks = uint64(0)
		/* Extract Frame Header */
		ret = ZSTD_getFrameHeader_advanced(tls, bp+24, src, srcSize, format)
		if ZSTD_isError(tls, ret) != 0 {
			return ZSTD_errorFrameSizeInfo(tls, ret)
		}
		if ret > uint64(0) {
			return ZSTD_errorFrameSizeInfo(tls, uint64(-int32(ZSTD_error_srcSize_wrong)))
		}
		ip = ip + uintptr((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FheaderSize)
		remainingSize = remainingSize - uint64((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FheaderSize)
		/* Iterate over each block */
		for int32(1) != 0 {
			cBlockSize = ZSTD_getcBlockSize(tls, ip, remainingSize, bp+72)
			if ZSTD_isError(tls, cBlockSize) != 0 {
				return ZSTD_errorFrameSizeInfo(tls, cBlockSize)
			}
			if ZSTD_blockHeaderSize+cBlockSize > remainingSize {
				return ZSTD_errorFrameSizeInfo(tls, uint64(-int32(ZSTD_error_srcSize_wrong)))
			}
			ip = ip + uintptr(ZSTD_blockHeaderSize+cBlockSize)
			remainingSize = remainingSize - (ZSTD_blockHeaderSize + cBlockSize)
			nbBlocks = nbBlocks + 1
			if (*(*blockProperties_t)(unsafe.Pointer(bp + 72))).FlastBlock != 0 {
				break
			}
		}
		/* Final frame content checksum */
		if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FchecksumFlag != 0 {
			if remainingSize < uint64(4) {
				return ZSTD_errorFrameSizeInfo(tls, uint64(-int32(ZSTD_error_srcSize_wrong)))
			}
			ip = ip + uintptr(4)
		}
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FnbBlocks = nbBlocks
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FcompressedSize = uint64(int64(ip) - int64(ipstart))
		if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
			v1 = (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FframeContentSize
		} else {
			v1 = nbBlocks * uint64((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FblockSizeMax)
		}
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FdecompressedBound = v1
		return *(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))
	}
	return r
}

func ZSTD_findFrameCompressedSize_advanced(tls *libc.TLS, src uintptr, srcSize size_t, format ZSTD_format_e) (r size_t) {
	var frameSizeInfo ZSTD_frameSizeInfo
	_ = frameSizeInfo
	frameSizeInfo = ZSTD_findFrameSizeInfo(tls, src, srcSize, format)
	return frameSizeInfo.FcompressedSize
}

// C documentation
//
//	/** ZSTD_findFrameCompressedSize() :
//	 * See docs in zstd.h
//	 * Note: compatible with legacy mode */
func ZSTD_findFrameCompressedSize(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_findFrameCompressedSize_advanced(tls, src, srcSize, int32(ZSTD_f_zstd1))
}

// C documentation
//
//	/** ZSTD_decompressBound() :
//	 *  compatible with legacy mode
//	 *  `src` must point to the start of a ZSTD frame or a skippable frame
//	 *  `srcSize` must be at least as large as the frame contained
//	 *  @return : the maximum decompressed size of the compressed source
//	 */
func ZSTD_decompressBound(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	var bound, decompressedBound uint64
	var compressedSize size_t
	var frameSizeInfo ZSTD_frameSizeInfo
	_, _, _, _ = bound, compressedSize, decompressedBound, frameSizeInfo
	bound = uint64(0)
	/* Iterate over each frame */
	for srcSize > uint64(0) {
		frameSizeInfo = ZSTD_findFrameSizeInfo(tls, src, srcSize, int32(ZSTD_f_zstd1))
		compressedSize = frameSizeInfo.FcompressedSize
		decompressedBound = frameSizeInfo.FdecompressedBound
		if ZSTD_isError(tls, compressedSize) != 0 || decompressedBound == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
			return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
		}
		src = src + uintptr(compressedSize)
		srcSize = srcSize - compressedSize
		bound = bound + decompressedBound
	}
	return bound
}

func ZSTD_decompressionMargin(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var compressedSize, err_code, margin size_t
	var decompressedBound uint64
	var frameSizeInfo ZSTD_frameSizeInfo
	var maxBlockSize, v2 uint32
	var v1 int32
	var _ /* zfh at bp+0 */ ZSTD_FrameHeader
	_, _, _, _, _, _, _, _ = compressedSize, decompressedBound, err_code, frameSizeInfo, margin, maxBlockSize, v1, v2
	margin = uint64(0)
	maxBlockSize = uint32(0)
	/* Iterate over each frame */
	for srcSize > uint64(0) {
		frameSizeInfo = ZSTD_findFrameSizeInfo(tls, src, srcSize, int32(ZSTD_f_zstd1))
		compressedSize = frameSizeInfo.FcompressedSize
		decompressedBound = frameSizeInfo.FdecompressedBound
		err_code = ZSTD_getFrameHeader(tls, bp, src, srcSize)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
		if ZSTD_isError(tls, compressedSize) != 0 || decompressedBound == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FframeType == int32(ZSTD_frame) {
			/* Add the frame header to our margin */
			margin = margin + uint64((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FheaderSize)
			/* Add the checksum to our margin */
			if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FchecksumFlag != 0 {
				v1 = int32(4)
			} else {
				v1 = 0
			}
			margin = margin + uint64(v1)
			/* Add 3 bytes per block */
			margin = margin + uint64(3)*frameSizeInfo.FnbBlocks
			/* Compute the max block size */
			if maxBlockSize > (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FblockSizeMax {
				v2 = maxBlockSize
			} else {
				v2 = (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FblockSizeMax
			}
			maxBlockSize = v2
		} else {
			/* Add the entire skippable frame size to our margin. */
			margin = margin + compressedSize
		}
		src = src + uintptr(compressedSize)
		srcSize = srcSize - compressedSize
	}
	/* Add the max block size back to the margin. */
	margin = margin + uint64(maxBlockSize)
	return margin
}

/*-*************************************************************
 *   Frame decoding
 ***************************************************************/

// C documentation
//
//	/** ZSTD_insertBlock() :
//	 *  insert `src` block into `dctx` history. Useful to track uncompressed blocks. */
func ZSTD_insertBlock(tls *libc.TLS, dctx uintptr, blockStart uintptr, blockSize size_t) (r size_t) {
	ZSTD_checkContinuity(tls, dctx, blockStart, blockSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = blockStart + uintptr(blockSize)
	return blockSize
}

func ZSTD_copyRawBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	if srcSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if dst == libc.UintptrFromInt32(0) {
		if srcSize == uint64(0) {
			return uint64(0)
		}
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstBuffer_null))
	}
	libc.Xmemmove(tls, dst, src, srcSize)
	return srcSize
}

func ZSTD_setRleBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, b BYTE, regenSize size_t) (r size_t) {
	if regenSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if dst == libc.UintptrFromInt32(0) {
		if regenSize == uint64(0) {
			return uint64(0)
		}
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstBuffer_null))
	}
	libc.Xmemset(tls, dst, int32(b), regenSize)
	return regenSize
}

func ZSTD_DCtx_trace_end(tls *libc.TLS, dctx uintptr, uncompressedSize U64, compressedSize U64, streaming int32) {
	_ = dctx
	_ = uncompressedSize
	_ = compressedSize
	_ = streaming
}

// C documentation
//
//	/*! ZSTD_decompressFrame() :
//	 * @dctx must be properly initialized
//	 *  will update *srcPtr and *srcSizePtr,
//	 *  to make *srcPtr progress by one frame. */
func ZSTD_decompressFrame(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, srcPtr uintptr, srcSizePtr uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cBlockSize, decodedSize, err_code, err_code1, frameHeaderSize, remainingSrcSize size_t
	var checkCalc, checkRead U32
	var ip, istart, oBlockEnd, oend, op, ostart, v1 uintptr
	var v2 int32
	var v4 uint32
	var _ /* blockProperties at bp+0 */ blockProperties_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cBlockSize, checkCalc, checkRead, decodedSize, err_code, err_code1, frameHeaderSize, ip, istart, oBlockEnd, oend, op, ostart, remainingSrcSize, v1, v2, v4
	istart = *(*uintptr)(unsafe.Pointer(srcPtr))
	ip = istart
	ostart = dst
	if dstCapacity != uint64(0) {
		v1 = ostart + uintptr(dstCapacity)
	} else {
		v1 = ostart
	}
	oend = v1
	op = ostart
	remainingSrcSize = *(*size_t)(unsafe.Pointer(srcSizePtr))
	/* check */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) {
		v2 = int32(6)
	} else {
		v2 = int32(2)
	}
	if remainingSrcSize < uint64(v2)+ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Frame Header */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) {
		v2 = int32(5)
	} else {
		v2 = int32(1)
	}
	frameHeaderSize = ZSTD_frameHeaderSize_internal(tls, ip, uint64(v2), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat)
	if ZSTD_isError(tls, frameHeaderSize) != 0 {
		return frameHeaderSize
	}
	if remainingSrcSize < frameHeaderSize+ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	err_code = ZSTD_decodeFrameHeader(tls, dctx, ip, frameHeaderSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ip = ip + uintptr(frameHeaderSize)
	remainingSrcSize = remainingSrcSize - frameHeaderSize
	/* Shrink the blockSizeMax if enabled */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam != 0 {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax < uint32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam) {
			v4 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax
		} else {
			v4 = uint32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam)
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax = v4
	}
	/* Loop on each block */
	for int32(1) != 0 {
		oBlockEnd = oend
		cBlockSize = ZSTD_getcBlockSize(tls, ip, remainingSrcSize, bp)
		if ZSTD_isError(tls, cBlockSize) != 0 {
			return cBlockSize
		}
		ip = ip + uintptr(ZSTD_blockHeaderSize)
		remainingSrcSize = remainingSrcSize - ZSTD_blockHeaderSize
		if cBlockSize > remainingSrcSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_srcSize_wrong))
		}
		if ip >= op && ip < oBlockEnd {
			/* We are decompressing in-place. Limit the output pointer so that we
			 * don't overwrite the block that we are currently reading. This will
			 * fail decompression if the input & output pointers aren't spaced
			 * far enough apart.
			 *
			 * This is important to set, even when the pointers are far enough
			 * apart, because ZSTD_decompressBlock_internal() can decide to store
			 * literals in the output buffer, after the block it is decompressing.
			 * Since we don't want anything to overwrite our input, we have to tell
			 * ZSTD_decompressBlock_internal to never write past ip.
			 *
			 * See ZSTD_allocateLiteralsBuffer() for reference.
			 */
			oBlockEnd = op + uintptr(int64(ip)-int64(op))
		}
		switch (*(*blockProperties_t)(unsafe.Pointer(bp))).FblockType {
		case int32(bt_compressed):
			goto _5
		case int32(bt_raw):
			goto _6
		case int32(bt_rle):
			goto _7
		default:
			goto _8
		case int32(bt_reserved):
			goto _9
		}
		goto _10
	_5:
		;
		decodedSize = ZSTD_decompressBlock_internal(tls, dctx, op, uint64(int64(oBlockEnd)-int64(op)), ip, cBlockSize, int32(not_streaming))
		goto _10
	_6:
		;
		/* Use oend instead of oBlockEnd because this function is safe to overlap. It uses memmove. */
		decodedSize = ZSTD_copyRawBlock(tls, op, uint64(int64(oend)-int64(op)), ip, cBlockSize)
		goto _10
	_7:
		;
		decodedSize = ZSTD_setRleBlock(tls, op, uint64(int64(oBlockEnd)-int64(op)), *(*BYTE)(unsafe.Pointer(ip)), uint64((*(*blockProperties_t)(unsafe.Pointer(bp))).ForigSize))
		goto _10
	_9:
		;
	_8:
		;
	_13:
		;
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7025, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
		goto _12
	_12:
		;
		if 0 != 0 {
			goto _13
		}
		goto _11
	_11:
		;
	_10:
		;
		err_code1 = decodedSize
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7044, 0)
			}
			return err_code1
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
			XXH_INLINE_XXH64_update(tls, dctx+30008, op, decodedSize)
		}
		if decodedSize != 0 { /* support dst = NULL,0 */
			op = op + uintptr(decodedSize)
		}
		ip = ip + uintptr(cBlockSize)
		remainingSrcSize = remainingSrcSize - cBlockSize
		if (*(*blockProperties_t)(unsafe.Pointer(bp))).FlastBlock != 0 {
			break
		}
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		if uint64(int64(op)-int64(ostart)) != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 { /* Frame content checksum verification */
		if remainingSrcSize < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_checksum_wrong))
		}
		if !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum != 0) {
			checkCalc = uint32(XXH_INLINE_XXH64_digest(tls, dctx+30008))
			checkRead = MEM_readLE32(tls, ip)
			if checkRead != checkCalc {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return uint64(-int32(ZSTD_error_checksum_wrong))
			}
		}
		ip = ip + uintptr(4)
		remainingSrcSize = remainingSrcSize - uint64(4)
	}
	ZSTD_DCtx_trace_end(tls, dctx, uint64(int64(op)-int64(ostart)), uint64(int64(ip)-int64(istart)), 0)
	/* Allow caller to get size read */
	*(*uintptr)(unsafe.Pointer(srcPtr)) = ip
	*(*size_t)(unsafe.Pointer(srcSizePtr)) = remainingSrcSize
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_decompressMultiFrame(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, _src uintptr, _srcSize size_t, dict uintptr, dictSize size_t, ddict uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _src
	*(*size_t)(unsafe.Pointer(bp + 8)) = _srcSize
	var dststart uintptr
	var err_code, err_code1, err_code2, res, skippableSize size_t
	var magicNumber U32
	var moreThan1Frame int32
	_, _, _, _, _, _, _, _ = dststart, err_code, err_code1, err_code2, magicNumber, moreThan1Frame, res, skippableSize
	dststart = dst
	moreThan1Frame = 0
	/* either dict or ddict set, not both */
	if ddict != 0 {
		dict = ZSTD_DDict_dictContent(tls, ddict)
		dictSize = ZSTD_DDict_dictSize(tls, ddict)
	}
	for *(*size_t)(unsafe.Pointer(bp + 8)) >= ZSTD_startingInputLength(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat) {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) && *(*size_t)(unsafe.Pointer(bp + 8)) >= uint64(4) {
			magicNumber = MEM_readLE32(tls, *(*uintptr)(unsafe.Pointer(bp)))
			if magicNumber&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
				/* skippable frame detected : skip it */
				skippableSize = readSkippableFrameSize(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*size_t)(unsafe.Pointer(bp + 8)))
				err_code = skippableSize
				if ERR_isError(tls, err_code) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+7072, 0)
					}
					return err_code
				}
				*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(skippableSize)
				*(*size_t)(unsafe.Pointer(bp + 8)) = *(*size_t)(unsafe.Pointer(bp + 8)) - skippableSize
				continue /* check next frame */
			}
		}
		if ddict != 0 {
			/* we were called from ZSTD_decompress_usingDDict */
			err_code1 = ZSTD_decompressBegin_usingDDict(tls, dctx, ddict)
			if ERR_isError(tls, err_code1) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code1
			}
		} else {
			/* this will initialize correctly with no dict if dict == NULL, so
			 * use this in all cases but ddict */
			err_code2 = ZSTD_decompressBegin_usingDict(tls, dctx, dict, dictSize)
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code2
			}
		}
		ZSTD_checkContinuity(tls, dctx, dst, dstCapacity)
		res = ZSTD_decompressFrame(tls, dctx, dst, dstCapacity, bp, bp+8)
		if ZSTD_getErrorCode(tls, res) == int32(ZSTD_error_prefix_unknown) && moreThan1Frame == int32(1) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7096, 0)
			}
			return uint64(-int32(ZSTD_error_srcSize_wrong))
		}
		if ZSTD_isError(tls, res) != 0 {
			return res
		}
		if res != uint64(0) {
			dst = dst + uintptr(res)
		}
		dstCapacity = dstCapacity - res
		moreThan1Frame = int32(1)
	} /* while (srcSize >= ZSTD_frameHeaderSize_prefix) */
	if *(*size_t)(unsafe.Pointer(bp + 8)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7453, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	return uint64(int64(dst) - int64(dststart))
}

func ZSTD_decompress_usingDict(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_decompressMultiFrame(tls, dctx, dst, dstCapacity, src, srcSize, dict, dictSize, libc.UintptrFromInt32(0))
}

func ZSTD_getDDict(tls *libc.TLS, dctx uintptr) (r uintptr) {
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses {
	default:
		fallthrough
	case int32(ZSTD_dont_use):
		ZSTD_clearDict(tls, dctx)
		return libc.UintptrFromInt32(0)
	case int32(ZSTD_use_indefinitely):
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict
	case int32(ZSTD_use_once):
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_dont_use)
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict
	}
	return r
}

func ZSTD_decompressDCtx(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_decompress_usingDDict(tls, dctx, dst, dstCapacity, src, srcSize, ZSTD_getDDict(tls, dctx))
}

func ZSTD_decompress(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var dctx uintptr
	var regenSize size_t
	_, _ = dctx, regenSize
	dctx = ZSTD_createDCtx_internal(tls, ZSTD_defaultCMem)
	if dctx == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	regenSize = ZSTD_decompressDCtx(tls, dctx, dst, dstCapacity, src, srcSize)
	ZSTD_freeDCtx(tls, dctx)
	return regenSize
}

// C documentation
//
//	/*-**************************************
//	*   Advanced Streaming Decompression API
//	*   Bufferless and synchronous
//	****************************************/
func ZSTD_nextSrcSizeToDecompress(tls *libc.TLS, dctx uintptr) (r size_t) {
	return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
}

// C documentation
//
//	/**
//	 * Similar to ZSTD_nextSrcSizeToDecompress(), but when a block input can be streamed, we
//	 * allow taking a partial block as the input. Currently only raw uncompressed blocks can
//	 * be streamed.
//	 *
//	 * For blocks that can be streamed, this allows us to reduce the latency until we produce
//	 * output, and avoid copying the input.
//	 *
//	 * @param inputSize - The total amount of input that the caller currently has.
//	 */
func ZSTD_nextSrcSizeToDecompressWithInputSize(tls *libc.TLS, dctx uintptr, inputSize size_t) (r size_t) {
	var v1, v2, v3 uint64
	_, _, _ = v1, v2, v3
	if !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_decompressBlock) || (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_decompressLastBlock)) {
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType != int32(bt_raw) {
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
	}
	if inputSize < (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected {
		v2 = inputSize
	} else {
		v2 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
	}
	if uint64(libc.Int32FromInt32(1)) > v2 {
		v1 = uint64(libc.Int32FromInt32(1))
	} else {
		if inputSize < (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected {
			v3 = inputSize
		} else {
			v3 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
		}
		v1 = v3
	}
	return v1
}

func ZSTD_nextInputType(tls *libc.TLS, dctx uintptr) (r ZSTD_nextInputType_e) {
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage {
	default: /* should not happen */
		fallthrough
	case int32(ZSTDds_getFrameHeaderSize):
		fallthrough
	case int32(ZSTDds_decodeFrameHeader):
		return int32(ZSTDnit_frameHeader)
	case int32(ZSTDds_decodeBlockHeader):
		return int32(ZSTDnit_blockHeader)
	case int32(ZSTDds_decompressBlock):
		return int32(ZSTDnit_block)
	case int32(ZSTDds_decompressLastBlock):
		return int32(ZSTDnit_lastBlock)
	case int32(ZSTDds_checkChecksum):
		return int32(ZSTDnit_checksum)
	case int32(ZSTDds_decodeSkippableHeader):
		fallthrough
	case int32(ZSTDds_skipFrame):
		return int32(ZSTDnit_skippableFrame)
	}
	return r
}

func ZSTD_isSkipFrame(tls *libc.TLS, dctx uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_skipFrame))
}

// C documentation
//
//	/** ZSTD_decompressContinue() :
//	 *  srcSize : must be the exact nb of bytes expected (see ZSTD_nextSrcSizeToDecompress())
//	 *  @return : nb of bytes generated into `dst` (necessarily <= `dstCapacity)
//	 *            or an error code, which can be tested using ZSTD_isError() */
func ZSTD_decompressContinue(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cBlockSize, err_code, err_code1, err_code2, rSize size_t
	var check32, h32 U32
	var v11 int32
	var _ /* bp at bp+0 */ blockProperties_t
	_, _, _, _, _, _, _, _ = cBlockSize, check32, err_code, err_code1, err_code2, h32, rSize, v11
	/* Sanity check */
	if srcSize != ZSTD_nextSrcSizeToDecompressWithInputSize(tls, dctx, srcSize) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7481, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	ZSTD_checkContinuity(tls, dctx, dst, dstCapacity)
	*(*U64)(unsafe.Pointer(dctx + 29976)) += srcSize
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage {
	case int32(ZSTDds_getFrameHeaderSize):
		goto _1
	case int32(ZSTDds_decodeFrameHeader):
		goto _2
	case int32(ZSTDds_decodeBlockHeader):
		goto _3
	case int32(ZSTDds_decompressBlock):
		goto _4
	case int32(ZSTDds_decompressLastBlock):
		goto _5
	case int32(ZSTDds_checkChecksum):
		goto _6
	case int32(ZSTDds_decodeSkippableHeader):
		goto _7
	case int32(ZSTDds_skipFrame):
		goto _8
	default:
		goto _9
	}
	goto _10
_1:
	;
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) { /* allows header */
		/* to read skippable magic number */
		if MEM_readLE32(tls, src)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) { /* skippable frame */
			libc.Xmemcpy(tls, dctx+95940, src, srcSize)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(ZSTD_SKIPPABLEHEADERSIZE) - srcSize /* remaining to load to get full skippable frame header */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeSkippableHeader)
			return uint64(0)
		}
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize = ZSTD_frameHeaderSize_internal(tls, src, srcSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat)
	if ZSTD_isError(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize) != 0 {
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize
	}
	libc.Xmemcpy(tls, dctx+95940, src, srcSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize - srcSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeFrameHeader)
	return uint64(0)
_2:
	;
	libc.Xmemcpy(tls, dctx+95940+uintptr((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize-srcSize), src, srcSize)
	err_code = ZSTD_decodeFrameHeader(tls, dctx, dctx+95940, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_blockHeaderSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeBlockHeader)
	return uint64(0)
_3:
	;
	cBlockSize = ZSTD_getcBlockSize(tls, src, ZSTD_blockHeaderSize, bp)
	if ZSTD_isError(tls, cBlockSize) != 0 {
		return cBlockSize
	}
	if cBlockSize > uint64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7493, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = cBlockSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType = (*(*blockProperties_t)(unsafe.Pointer(bp))).FblockType
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrleSize = uint64((*(*blockProperties_t)(unsafe.Pointer(bp))).ForigSize)
	if cBlockSize != 0 {
		if (*(*blockProperties_t)(unsafe.Pointer(bp))).FlastBlock != 0 {
			v11 = int32(ZSTDds_decompressLastBlock)
		} else {
			v11 = int32(ZSTDds_decompressBlock)
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = v11
		return uint64(0)
	}
	/* empty block */
	if (*(*blockProperties_t)(unsafe.Pointer(bp))).FlastBlock != 0 {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 {
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(4)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_checkChecksum)
		} else {
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* end of frame */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
		}
	} else {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_blockHeaderSize /* jump to next header */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeBlockHeader)
	}
	return uint64(0)
_5:
	;
_4:
	;
_14:
	;
	goto _13
_13:
	;
	if 0 != 0 {
		goto _14
	}
	goto _12
_12:
	;
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType {
	case int32(bt_compressed):
		goto _15
	case int32(bt_raw):
		goto _16
	case int32(bt_rle):
		goto _17
	default:
		goto _18
	case int32(bt_reserved):
		goto _19
	}
	goto _20
_15:
	;
_23:
	;
	goto _22
_22:
	;
	if 0 != 0 {
		goto _23
	}
	goto _21
_21:
	;
	rSize = ZSTD_decompressBlock_internal(tls, dctx, dst, dstCapacity, src, srcSize, int32(is_streaming))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* Streaming not supported */
	goto _20
_16:
	;
	rSize = ZSTD_copyRawBlock(tls, dst, dstCapacity, src, srcSize)
	err_code1 = rSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7520, 0)
		}
		return err_code1
	}
	*(*size_t)(unsafe.Pointer(dctx + 29920)) -= rSize
	goto _20
_17:
	;
	rSize = ZSTD_setRleBlock(tls, dst, dstCapacity, *(*BYTE)(unsafe.Pointer(src)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrleSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* Streaming not supported */
	goto _20
_19:
	; /* should never happen */
_18:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+7025, 0)
	}
	return uint64(-int32(ZSTD_error_corruption_detected))
_20:
	;
	err_code2 = rSize
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	if rSize > uint64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7545, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	*(*U64)(unsafe.Pointer(dctx + 29984)) += rSize
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
		XXH_INLINE_XXH64_update(tls, dctx+30008, dst, rSize)
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dst + uintptr(rSize)
	/* Stay on the same stage until we are finished streaming the block. */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected > uint64(0) {
		return rSize
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_decompressLastBlock) { /* end of frame */
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 { /* another round for frame checksum */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(4)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_checkChecksum)
		} else {
			ZSTD_DCtx_trace_end(tls, dctx, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprocessedCSize, int32(1))
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* ends here */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
		}
	} else {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeBlockHeader)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_blockHeaderSize
	}
	return rSize
_6:
	;
	/* guaranteed by dctx->expected */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
		h32 = uint32(XXH_INLINE_XXH64_digest(tls, dctx+30008))
		check32 = MEM_readLE32(tls, src)
		if check32 != h32 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_checksum_wrong))
		}
	}
	ZSTD_DCtx_trace_end(tls, dctx, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprocessedCSize, int32(1))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
	return uint64(0)
_7:
	;
	libc.Xmemcpy(tls, dctx+95940+uintptr(libc.Uint64FromInt32(ZSTD_SKIPPABLEHEADERSIZE)-srcSize), src, srcSize)    /* complete skippable header */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(MEM_readLE32(tls, dctx+95940+uintptr(ZSTD_FRAMEIDSIZE))) /* note : dctx->expected can grow seriously large, beyond local buffer size */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_skipFrame)
	return uint64(0)
_8:
	;
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
	return uint64(0)
_9:
	;
	/* impossible */
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1561, 0)
	}
	return uint64(-int32(ZSTD_error_GENERIC)) /* some compilers require default to do something */
_10:
	;
	return r
}

func ZSTD_refDictContent(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = dict - uintptr(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd)-int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = dict
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dict + uintptr(dictSize)
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_loadDEntropy() :
//	 *  dict : must point at beginning of a valid zstd dictionary.
//	 * @return : size of entropy tables read */
func ZSTD_loadDEntropy(tls *libc.TLS, entropy uintptr, dict uintptr, dictSize size_t) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var dictContentSize, hSize, litlengthHeaderSize, matchlengthHeaderSize, offcodeHeaderSize, workspaceSize size_t
	var dictEnd, dictPtr, workspace uintptr
	var i int32
	var rep U32
	var _ /* litlengthLog at bp+264 */ uint32
	var _ /* litlengthMaxValue at bp+260 */ uint32
	var _ /* litlengthNCount at bp+188 */ [36]int16
	var _ /* matchlengthLog at bp+184 */ uint32
	var _ /* matchlengthMaxValue at bp+180 */ uint32
	var _ /* matchlengthNCount at bp+72 */ [53]int16
	var _ /* offcodeLog at bp+68 */ uint32
	var _ /* offcodeMaxValue at bp+64 */ uint32
	var _ /* offcodeNCount at bp+0 */ [32]int16
	_, _, _, _, _, _, _, _, _, _, _ = dictContentSize, dictEnd, dictPtr, hSize, i, litlengthHeaderSize, matchlengthHeaderSize, offcodeHeaderSize, rep, workspace, workspaceSize
	dictPtr = dict
	dictEnd = dictPtr + uintptr(dictSize)
	if dictSize <= uint64(8) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7585, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	/* dict must be valid */
	dictPtr = dictPtr + uintptr(8) /* skip header = magic + dictID */
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	workspace = entropy /* use fse tables as temporary workspace; implies fse tables are grouped together */
	workspaceSize = libc.Uint64FromInt64(4104) + libc.Uint64FromInt64(2056) + libc.Uint64FromInt64(4104)
	hSize = HUF_readDTableX2_wksp(tls, entropy+10264, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)), workspace, workspaceSize, 0)
	if ERR_isError(tls, hSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	dictPtr = dictPtr + uintptr(hSize)
	*(*uint32)(unsafe.Pointer(bp + 64)) = uint32(MaxOff)
	offcodeHeaderSize = FSE_readNCount(tls, bp, bp+64, bp+68, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, offcodeHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 64)) > uint32(MaxOff) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 68)) > uint32(OffFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	ZSTD_buildFSETable(tls, entropy+4104, bp, *(*uint32)(unsafe.Pointer(bp + 64)), uintptr(unsafe.Pointer(&OF_base)), uintptr(unsafe.Pointer(&OF_bits)), *(*uint32)(unsafe.Pointer(bp + 68)), entropy+26664, uint64(628), 0)
	dictPtr = dictPtr + uintptr(offcodeHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 180)) = uint32(MaxML)
	matchlengthHeaderSize = FSE_readNCount(tls, bp+72, bp+180, bp+184, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, matchlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 180)) > uint32(MaxML) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 184)) > uint32(MLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	ZSTD_buildFSETable(tls, entropy+6160, bp+72, *(*uint32)(unsafe.Pointer(bp + 180)), uintptr(unsafe.Pointer(&ML_base)), uintptr(unsafe.Pointer(&ML_bits)), *(*uint32)(unsafe.Pointer(bp + 184)), entropy+26664, uint64(628), 0)
	dictPtr = dictPtr + uintptr(matchlengthHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 260)) = uint32(MaxLL)
	litlengthHeaderSize = FSE_readNCount(tls, bp+188, bp+260, bp+264, dictPtr, uint64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, litlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 260)) > uint32(MaxLL) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 264)) > uint32(LLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	ZSTD_buildFSETable(tls, entropy, bp+188, *(*uint32)(unsafe.Pointer(bp + 260)), uintptr(unsafe.Pointer(&LL_base)), uintptr(unsafe.Pointer(&LL_bits)), *(*uint32)(unsafe.Pointer(bp + 264)), entropy+26664, uint64(628), 0)
	dictPtr = dictPtr + uintptr(litlengthHeaderSize)
	if dictPtr+uintptr(12) > dictEnd {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	dictContentSize = uint64(int64(dictEnd) - int64(dictPtr+libc.UintptrFromInt32(12)))
	i = 0
	for {
		if !(i < int32(3)) {
			break
		}
		rep = MEM_readLE32(tls, dictPtr)
		dictPtr = dictPtr + uintptr(4)
		if rep == uint32(0) || uint64(rep) > dictContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dictionary_corrupted))
		}
		*(*U32)(unsafe.Pointer(entropy + 26652 + uintptr(i)*4)) = rep
		goto _1
	_1:
		;
		i = i + 1
	}
	return uint64(int64(dictPtr) - int64(dict))
}

func ZSTD_decompress_insertDictionary(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	var eSize size_t
	var magic, v1 U32
	_, _, _ = eSize, magic, v1
	if dictSize < uint64(8) {
		return ZSTD_refDictContent(tls, dctx, dict, dictSize)
	}
	magic = MEM_readLE32(tls, dict)
	if magic != uint32(ZSTD_MAGIC_DICTIONARY) {
		return ZSTD_refDictContent(tls, dctx, dict, dictSize) /* pure content mode */
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = MEM_readLE32(tls, dict+uintptr(ZSTD_FRAMEIDSIZE))
	/* load entropy tables */
	eSize = ZSTD_loadDEntropy(tls, dctx+32, dict, dictSize)
	if ZSTD_isError(tls, eSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	dict = dict + uintptr(eSize)
	dictSize = dictSize - eSize
	v1 = libc.Uint32FromInt32(1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = v1
	/* reference dictionary content */
	return ZSTD_refDictContent(tls, dctx, dict, dictSize)
}

func ZSTD_decompressBegin(tls *libc.TLS, dctx uintptr) (r size_t) {
	var v1 U32
	_ = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_startingInputLength(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat) /* dctx->format must be properly set */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprocessedCSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = libc.UintptrFromInt32(0)
	*(*HUF_DTable)(unsafe.Pointer(dctx + 32 + 10264)) = uint32(libc.Int32FromInt32(ZSTD_HUFFDTABLE_CAPACITY_LOG) * libc.Int32FromInt32(0x1000001)) /* cover both little and big endian */
	v1 = libc.Uint32FromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = uint32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType = int32(bt_reserved)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = int32(1)
	_ = libc.Uint64FromInt64(1)
	libc.Xmemcpy(tls, dctx+32+26652, uintptr(unsafe.Pointer(&repStartValue)), libc.Uint64FromInt64(12)) /* initial repcodes */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr = dctx + 32
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr = dctx + 32 + 6160
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr = dctx + 32 + 4104
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr = dctx + 32 + 10264
	return uint64(0)
}

func ZSTD_decompressBegin_usingDict(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_decompressBegin(tls, dctx)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if dict != 0 && dictSize != 0 {
		if ZSTD_isError(tls, ZSTD_decompress_insertDictionary(tls, dctx, dict, dictSize)) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dictionary_corrupted))
		}
	}
	return uint64(0)
}

/* ======   ZSTD_DDict   ====== */

func ZSTD_decompressBegin_usingDDict(tls *libc.TLS, dctx uintptr, ddict uintptr) (r size_t) {
	var dictEnd, dictStart uintptr
	var dictSize, err_code size_t
	_, _, _, _ = dictEnd, dictSize, dictStart, err_code
	if ddict != 0 {
		dictStart = ZSTD_DDict_dictContent(tls, ddict)
		dictSize = ZSTD_DDict_dictSize(tls, ddict)
		dictEnd = dictStart + uintptr(dictSize)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold = libc.BoolInt32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd != dictEnd)
	}
	err_code = ZSTD_decompressBegin(tls, dctx)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if ddict != 0 { /* NULL ddict is equivalent to no dictionary */
		ZSTD_copyDDictParameters(tls, dctx, ddict)
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_getDictID_fromDict() :
//	 *  Provides the dictID stored within dictionary.
//	 *  if @return == 0, the dictionary is not conformant with Zstandard specification.
//	 *  It can still be loaded, but as a content-only dictionary. */
func ZSTD_getDictID_fromDict(tls *libc.TLS, dict uintptr, dictSize size_t) (r uint32) {
	if dictSize < uint64(8) {
		return uint32(0)
	}
	if MEM_readLE32(tls, dict) != uint32(ZSTD_MAGIC_DICTIONARY) {
		return uint32(0)
	}
	return MEM_readLE32(tls, dict+uintptr(ZSTD_FRAMEIDSIZE))
}

// C documentation
//
//	/*! ZSTD_getDictID_fromFrame() :
//	 *  Provides the dictID required to decompress frame stored within `src`.
//	 *  If @return == 0, the dictID could not be decoded.
//	 *  This could for one of the following reasons :
//	 *  - The frame does not require a dictionary (most common case).
//	 *  - The frame was built with dictID intentionally removed.
//	 *    Needed dictionary is a hidden piece of information.
//	 *    Note : this use case also happens when using a non-conformant dictionary.
//	 *  - `srcSize` is too small, and as a result, frame header could not be decoded.
//	 *    Note : possible if `srcSize < ZSTD_FRAMEHEADERSIZE_MAX`.
//	 *  - This is not a Zstandard frame.
//	 *  When identifying the exact failure cause, it's possible to use
//	 *  ZSTD_getFrameHeader(), which will provide a more precise error code. */
func ZSTD_getDictID_fromFrame(tls *libc.TLS, src uintptr, srcSize size_t) (r uint32) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var hError size_t
	var _ /* zfp at bp+0 */ ZSTD_FrameHeader
	_ = hError
	*(*ZSTD_FrameHeader)(unsafe.Pointer(bp)) = ZSTD_FrameHeader{}
	hError = ZSTD_getFrameHeader(tls, bp, src, srcSize)
	if ZSTD_isError(tls, hError) != 0 {
		return uint32(0)
	}
	return (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FdictID
}

// C documentation
//
//	/*! ZSTD_decompress_usingDDict() :
//	*   Decompression using a pre-digested Dictionary
//	*   Use dictionary without significant overhead. */
func ZSTD_decompress_usingDDict(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, ddict uintptr) (r size_t) {
	/* pass content and size in case legacy frames are encountered */
	return ZSTD_decompressMultiFrame(tls, dctx, dst, dstCapacity, src, srcSize, libc.UintptrFromInt32(0), uint64(0), ddict)
}

/*=====================================
*   Streaming decompression
*====================================*/

func ZSTD_createDStream(tls *libc.TLS) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, ZSTD_defaultCMem)
}

func ZSTD_initStaticDStream(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	return ZSTD_initStaticDCtx(tls, workspace, workspaceSize)
}

func ZSTD_createDStream_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, customMem)
}

func ZSTD_freeDStream(tls *libc.TLS, zds uintptr) (r size_t) {
	return ZSTD_freeDCtx(tls, zds)
}

/* ***  Initialization  *** */

func ZSTD_DStreamInSize(tls *libc.TLS) (r size_t) {
	return uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) + ZSTD_blockHeaderSize
}

func ZSTD_DStreamOutSize(tls *libc.TLS) (r size_t) {
	return uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
}

func ZSTD_DCtx_loadDictionary_advanced(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r size_t) {
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearDict(tls, dctx)
	if dict != 0 && dictSize != uint64(0) {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal = ZSTD_createDDict_advanced(tls, dict, dictSize, dictLoadMethod, dictContentType, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem)
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1377, 0)
			}
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_indefinitely)
	}
	return uint64(0)
}

func ZSTD_DCtx_loadDictionary_byReference(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_DCtx_loadDictionary_advanced(tls, dctx, dict, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto))
}

func ZSTD_DCtx_loadDictionary(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_DCtx_loadDictionary_advanced(tls, dctx, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto))
}

func ZSTD_DCtx_refPrefix_advanced(tls *libc.TLS, dctx uintptr, prefix uintptr, prefixSize size_t, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_DCtx_loadDictionary_advanced(tls, dctx, prefix, prefixSize, int32(ZSTD_dlm_byRef), dictContentType)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_once)
	return uint64(0)
}

func ZSTD_DCtx_refPrefix(tls *libc.TLS, dctx uintptr, prefix uintptr, prefixSize size_t) (r size_t) {
	return ZSTD_DCtx_refPrefix_advanced(tls, dctx, prefix, prefixSize, int32(ZSTD_dct_rawContent))
}

// C documentation
//
//	/* ZSTD_initDStream_usingDict() :
//	 * return : expected size, aka ZSTD_startingInputLength().
//	 * this function cannot fail */
func ZSTD_initDStream_usingDict(tls *libc.TLS, zds uintptr, dict uintptr, dictSize size_t) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_DCtx_reset(tls, zds, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_DCtx_loadDictionary(tls, zds, dict, dictSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
}

// C documentation
//
//	/* note : this variant can't fail */
func ZSTD_initDStream(tls *libc.TLS, zds uintptr) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_DCtx_reset(tls, zds, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_DCtx_refDDict(tls, zds, libc.UintptrFromInt32(0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
}

// C documentation
//
//	/* ZSTD_initDStream_usingDDict() :
//	 * ddict will just be referenced, and must outlive decompression session
//	 * this function cannot fail */
func ZSTD_initDStream_usingDDict(tls *libc.TLS, dctx uintptr, ddict uintptr) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_DCtx_reset(tls, dctx, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_DCtx_refDDict(tls, dctx, ddict)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(dctx)).Fformat)
}

// C documentation
//
//	/* ZSTD_resetDStream() :
//	 * return : expected size, aka ZSTD_startingInputLength().
//	 * this function cannot fail */
func ZSTD_resetDStream(tls *libc.TLS, dctx uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_DCtx_reset(tls, dctx, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(dctx)).Fformat)
}

func ZSTD_DCtx_refDDict(tls *libc.TLS, dctx uintptr, ddict uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearDict(tls, dctx)
	if ddict != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = ddict
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_indefinitely)
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts == int32(ZSTD_rmd_refMultipleDDicts) {
			if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet == libc.UintptrFromInt32(0) {
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet = ZSTD_createDDictHashSet(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem)
				if !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet != 0) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+7603, 0)
					}
					return uint64(-int32(ZSTD_error_memory_allocation))
				}
			}
			/* Impossible: ddictSet cannot have been allocated if static dctx */
			err_code = ZSTD_DDictHashSet_addDDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet, ddict, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem)
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code
			}
		}
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_DCtx_setMaxWindowSize() :
//	 * note : no direct equivalence in ZSTD_DCtx_setParameter,
//	 * since this version sets windowSize, and the other sets windowLog */
func ZSTD_DCtx_setMaxWindowSize(tls *libc.TLS, dctx uintptr, maxWindowSize size_t) (r size_t) {
	var bounds ZSTD_bounds
	var max, min size_t
	_, _, _ = bounds, max, min
	bounds = ZSTD_dParam_getBounds(tls, int32(ZSTD_d_windowLogMax))
	min = libc.Uint64FromInt32(1) << bounds.FlowerBound
	max = libc.Uint64FromInt32(1) << bounds.FupperBound
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	if maxWindowSize < min {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if maxWindowSize > max {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize = maxWindowSize
	return uint64(0)
}

func ZSTD_DCtx_setFormat(tls *libc.TLS, dctx uintptr, format ZSTD_format_e) (r size_t) {
	return ZSTD_DCtx_setParameter(tls, dctx, int32(ZSTD_d_experimentalParam1), format)
}

func ZSTD_dParam_getBounds(tls *libc.TLS, dParam ZSTD_dParameter) (r ZSTD_bounds) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_bounds{}
	switch dParam {
	case int32(ZSTD_d_windowLogMax):
		bounds.FlowerBound = int32(ZSTD_WINDOWLOG_ABSOLUTEMIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)
		return bounds
	case int32(ZSTD_d_experimentalParam1):
		bounds.FlowerBound = int32(ZSTD_f_zstd1)
		bounds.FupperBound = int32(ZSTD_f_zstd1_magicless)
		_ = libc.Uint64FromInt64(1)
		return bounds
	case int32(ZSTD_d_experimentalParam2):
		bounds.FlowerBound = int32(ZSTD_bm_buffered)
		bounds.FupperBound = int32(ZSTD_bm_stable)
		return bounds
	case int32(ZSTD_d_experimentalParam3):
		bounds.FlowerBound = int32(ZSTD_d_validateChecksum)
		bounds.FupperBound = int32(ZSTD_d_ignoreChecksum)
		return bounds
	case int32(ZSTD_d_experimentalParam4):
		bounds.FlowerBound = int32(ZSTD_rmd_refSingleDDict)
		bounds.FupperBound = int32(ZSTD_rmd_refMultipleDDicts)
		return bounds
	case int32(ZSTD_d_experimentalParam5):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_d_experimentalParam6):
		bounds.FlowerBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(10)
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	default:
	}
	bounds.Ferror1 = uint64(-int32(ZSTD_error_parameter_unsupported))
	return bounds
}

// C documentation
//
//	/* ZSTD_dParam_withinBounds:
//	 * @return 1 if value is within dParam bounds,
//	 * 0 otherwise */
func ZSTD_dParam_withinBounds(tls *libc.TLS, dParam ZSTD_dParameter, value int32) (r int32) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_dParam_getBounds(tls, dParam)
	if ZSTD_isError(tls, bounds.Ferror1) != 0 {
		return 0
	}
	if value < bounds.FlowerBound {
		return 0
	}
	if value > bounds.FupperBound {
		return 0
	}
	return int32(1)
}

func ZSTD_DCtx_getParameter(tls *libc.TLS, dctx uintptr, param ZSTD_dParameter, value uintptr) (r size_t) {
	switch param {
	case int32(ZSTD_d_windowLogMax):
		*(*int32)(unsafe.Pointer(value)) = int32(ZSTD_highbit32(tls, uint32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize)))
		return uint64(0)
	case int32(ZSTD_d_experimentalParam1):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat
		return uint64(0)
	case int32(ZSTD_d_experimentalParam2):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBufferMode
		return uint64(0)
	case int32(ZSTD_d_experimentalParam3):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum
		return uint64(0)
	case int32(ZSTD_d_experimentalParam4):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts
		return uint64(0)
	case int32(ZSTD_d_experimentalParam5):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm
		return uint64(0)
	case int32(ZSTD_d_experimentalParam6):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam
		return uint64(0)
	default:
	}
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1319, 0)
	}
	return uint64(-int32(ZSTD_error_parameter_unsupported))
	return r
}

func ZSTD_DCtx_setParameter(tls *libc.TLS, dctx uintptr, dParam ZSTD_dParameter, value int32) (r size_t) {
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_stage_wrong))
	}
	switch dParam {
	case int32(ZSTD_d_windowLogMax):
		if value == 0 {
			value = int32(ZSTD_WINDOWLOG_LIMIT_DEFAULT)
		}
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_windowLogMax), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize = libc.Uint64FromInt32(1) << value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam1):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam1), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam2):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam2), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBufferMode = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam3):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam3), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam4):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam4), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize != uint64(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7643, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_unsupported))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam5):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam5), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm = libc.BoolInt32(value != 0)
		return uint64(0)
	case int32(ZSTD_d_experimentalParam6):
		if value != 0 {
			if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam6), value) != 0) {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return uint64(-int32(ZSTD_error_parameter_outOfBound))
			}
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam = value
		return uint64(0)
	default:
	}
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1319, 0)
	}
	return uint64(-int32(ZSTD_error_parameter_unsupported))
	return r
}

func ZSTD_DCtx_reset(tls *libc.TLS, dctx uintptr, reset ZSTD_ResetDirective) (r size_t) {
	if reset == int32(ZSTD_reset_session_only) || reset == int32(ZSTD_reset_session_and_parameters) {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage = int32(zdss_init)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FnoForwardProgress = 0
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = int32(1)
	}
	if reset == int32(ZSTD_reset_parameters) || reset == int32(ZSTD_reset_session_and_parameters) {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_stage_wrong))
		}
		ZSTD_clearDict(tls, dctx)
		ZSTD_DCtx_resetParameters(tls, dctx)
	}
	return uint64(0)
}

func ZSTD_sizeof_DStream(tls *libc.TLS, dctx uintptr) (r size_t) {
	return ZSTD_sizeof_DCtx(tls, dctx)
}

func ZSTD_decodingBufferSize_internal(tls *libc.TLS, windowSize uint64, frameContentSize uint64, blockSizeMax size_t) (r size_t) {
	var blockSize, minRBSize size_t
	var neededRBSize, neededSize, v1, v2, v3, v4 uint64
	_, _, _, _, _, _, _, _ = blockSize, minRBSize, neededRBSize, neededSize, v1, v2, v3, v4
	if windowSize < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		v2 = windowSize
	} else {
		v2 = uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	if v2 < blockSizeMax {
		if windowSize < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
			v3 = windowSize
		} else {
			v3 = uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
		}
		v1 = v3
	} else {
		v1 = blockSizeMax
	}
	blockSize = v1
	/* We need blockSize + WILDCOPY_OVERLENGTH worth of buffer so that if a block
	 * ends at windowSize + WILDCOPY_OVERLENGTH + 1 bytes, we can start writing
	 * the block at the beginning of the output buffer, and maintain a full window.
	 *
	 * We need another blockSize worth of buffer so that we can store split
	 * literals at the end of the block without overwriting the extDict window.
	 */
	neededRBSize = windowSize + blockSize*uint64(2) + uint64(libc.Int32FromInt32(WILDCOPY_OVERLENGTH)*libc.Int32FromInt32(2))
	if frameContentSize < neededRBSize {
		v4 = frameContentSize
	} else {
		v4 = neededRBSize
	}
	neededSize = v4
	minRBSize = neededSize
	if minRBSize != neededSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_frameParameter_windowTooLarge))
	}
	return minRBSize
}

func ZSTD_decodingBufferSize_min(tls *libc.TLS, windowSize uint64, frameContentSize uint64) (r size_t) {
	return ZSTD_decodingBufferSize_internal(tls, windowSize, frameContentSize, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)))
}

func ZSTD_estimateDStreamSize(tls *libc.TLS, windowSize size_t) (r size_t) {
	var blockSize, inBuffSize, outBuffSize size_t
	var v1 uint64
	_, _, _, _ = blockSize, inBuffSize, outBuffSize, v1
	if windowSize < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		v1 = windowSize
	} else {
		v1 = uint64(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	blockSize = v1
	inBuffSize = blockSize /* no block can be larger */
	outBuffSize = ZSTD_decodingBufferSize_min(tls, windowSize, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
	return ZSTD_estimateDCtxSize(tls) + inBuffSize + outBuffSize
}

func ZSTD_estimateDStreamSize_fromFrame(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var err size_t
	var windowSizeMax U32
	var _ /* zfh at bp+0 */ ZSTD_FrameHeader
	_, _ = err, windowSizeMax
	windowSizeMax = libc.Uint32FromUint32(1) << libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)
	err = ZSTD_getFrameHeader(tls, bp, src, srcSize)
	if ZSTD_isError(tls, err) != 0 {
		return err
	}
	if err > uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FwindowSize > uint64(windowSizeMax) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_frameParameter_windowTooLarge))
	}
	return ZSTD_estimateDStreamSize(tls, (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FwindowSize)
}

/* *****   Decompression   ***** */

func ZSTD_DCtx_isOverflow(tls *libc.TLS, zds uintptr, neededInBuffSize size_t, neededOutBuffSize size_t) (r int32) {
	return libc.BoolInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize+(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize >= (neededInBuffSize+neededOutBuffSize)*uint64(ZSTD_WORKSPACETOOLARGE_FACTOR))
}

func ZSTD_DCtx_updateOversizedDuration(tls *libc.TLS, zds uintptr, neededInBuffSize size_t, neededOutBuffSize size_t) {
	if ZSTD_DCtx_isOverflow(tls, zds, neededInBuffSize, neededOutBuffSize) != 0 {
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration + 1
	} else {
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration = uint64(0)
	}
}

func ZSTD_DCtx_isOversizedTooLong(tls *libc.TLS, zds uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration >= uint64(ZSTD_WORKSPACETOOLARGE_MAXDURATION))
}

// C documentation
//
//	/* Checks that the output buffer hasn't changed if ZSTD_obm_stable is used. */
func ZSTD_checkOutBuffer(tls *libc.TLS, zds uintptr, output uintptr) (r size_t) {
	var expect ZSTD_outBuffer
	_ = expect
	expect = (*ZSTD_DStream)(unsafe.Pointer(zds)).FexpectedOutBuffer
	/* No requirement when ZSTD_obm_stable is not enabled. */
	if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode != int32(ZSTD_bm_stable) {
		return uint64(0)
	}
	/* Any buffer is allowed in zdss_init, this must be the same for every other call until
	 * the context is reset.
	 */
	if (*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage == int32(zdss_init) {
		return uint64(0)
	}
	/* The buffer must match our expectation exactly. */
	if expect.Fdst == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst && expect.Fpos == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos && expect.Fsize == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
		return uint64(0)
	}
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+7689, 0)
	}
	return uint64(-int32(ZSTD_error_dstBuffer_wrong))
	return r
}

// C documentation
//
//	/* Calls ZSTD_decompressContinue() with the right parameters for ZSTD_decompressStream()
//	 * and updates the stage and the output buffer state. This call is extracted so it can be
//	 * used both when reading directly from the ZSTD_inBuffer, and in buffered input mode.
//	 * NOTE: You must break after calling this function since the streamStage is modified.
//	 */
func ZSTD_decompressContinueStream(tls *libc.TLS, zds uintptr, op uintptr, oend uintptr, src uintptr, srcSize size_t) (r size_t) {
	var decodedSize, decodedSize1, dstSize, dstSize1, err_code, err_code1 size_t
	var isSkipFrame int32
	var v1 uint64
	_, _, _, _, _, _, _, _ = decodedSize, decodedSize1, dstSize, dstSize1, err_code, err_code1, isSkipFrame, v1
	isSkipFrame = ZSTD_isSkipFrame(tls, zds)
	if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode == int32(ZSTD_bm_buffered) {
		if isSkipFrame != 0 {
			v1 = uint64(0)
		} else {
			v1 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize - (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart
		}
		dstSize = v1
		decodedSize = ZSTD_decompressContinue(tls, zds, (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuff+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart), dstSize, src, srcSize)
		err_code = decodedSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
		if !(decodedSize != 0) && !(isSkipFrame != 0) {
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
		} else {
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart + decodedSize
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_flush)
		}
	} else {
		if isSkipFrame != 0 {
			v1 = uint64(0)
		} else {
			v1 = uint64(int64(oend) - int64(*(*uintptr)(unsafe.Pointer(op))))
		}
		/* Write directly into the output buffer */
		dstSize1 = v1
		decodedSize1 = ZSTD_decompressContinue(tls, zds, *(*uintptr)(unsafe.Pointer(op)), dstSize1, src, srcSize)
		err_code1 = decodedSize1
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code1
		}
		*(*uintptr)(unsafe.Pointer(op)) += uintptr(decodedSize1)
		/* Flushing is not needed. */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
	}
	return uint64(0)
}

func ZSTD_decompressStream(tls *libc.TLS, zds uintptr, output uintptr, input uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var bufferSize, cSize, decompressedSize, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, flushedSize, hSize, loadedSize, neededInBuffSize, neededInSize, neededInSize1, neededOutBuffSize, nextSrcSizeHint, remainingInput, toFlushSize, toLoad, toLoad1, v15, v16, v17 size_t
	var dst, iend, ip, istart, oend, ostart, src, v1, v2, v3, v4 uintptr
	var isSkipFrame, tooLarge, tooSmall, v19, v20 int32
	var someMoreWork U32
	var v18 uint64
	var v23 uint32
	var _ /* op at bp+0 */ uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bufferSize, cSize, decompressedSize, dst, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, flushedSize, hSize, iend, ip, isSkipFrame, istart, loadedSize, neededInBuffSize, neededInSize, neededInSize1, neededOutBuffSize, nextSrcSizeHint, oend, ostart, remainingInput, someMoreWork, src, toFlushSize, toLoad, toLoad1, tooLarge, tooSmall, v1, v15, v16, v17, v18, v19, v2, v20, v23, v3, v4
	src = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos != uint64(0) {
		v1 = src + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos)
	} else {
		v1 = src
	}
	istart = v1
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize != uint64(0) {
		v2 = src + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize)
	} else {
		v2 = src
	}
	iend = v2
	ip = istart
	dst = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos != uint64(0) {
		v3 = dst + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos)
	} else {
		v3 = dst
	}
	ostart = v3
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize != uint64(0) {
		v4 = dst + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize)
	} else {
		v4 = dst
	}
	oend = v4
	*(*uintptr)(unsafe.Pointer(bp)) = ostart
	someMoreWork = uint32(1)
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos > (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7740, libc.VaList(bp+16, uint32((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos), uint32((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos > (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7777, libc.VaList(bp+16, uint32((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos), uint32((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize)))
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	err_code = ZSTD_checkOutBuffer(tls, zds, output)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	for someMoreWork != 0 {
		switch (*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage {
		case int32(zdss_init):
			goto _5
		case int32(zdss_loadHeader):
			goto _6
		case int32(zdss_read):
			goto _7
		case int32(zdss_load):
			goto _8
		case int32(zdss_flush):
			goto _9
		default:
			goto _10
		}
		goto _11
	_5:
		;
	_14:
		;
		goto _13
	_13:
		;
		if 0 != 0 {
			goto _14
		}
		goto _12
	_12:
		;
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_loadHeader)
		v17 = libc.Uint64FromInt32(0)
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd = v17
		v16 = v17
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart = v16
		v15 = v16
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos = v15
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize = v15
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte = uint32(0)
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FexpectedOutBuffer = *(*ZSTD_outBuffer)(unsafe.Pointer(output))
	_6:
		;
		hSize = ZSTD_getFrameHeader_advanced(tls, zds+29928, zds+95940, (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FrefMultipleDDicts != 0 && (*ZSTD_DStream)(unsafe.Pointer(zds)).FddictSet != 0 {
			ZSTD_DCtx_selectFrameDDict(tls, zds)
		}
		if ZSTD_isError(tls, hSize) != 0 {
			return hSize /* error */
		}
		if hSize != uint64(0) { /* need more input */
			toLoad = hSize - (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize /* if hSize!=0, hSize > zds->lhSize */
			remainingInput = uint64(int64(iend) - int64(ip))
			if toLoad > remainingInput { /* not enough input to load full header */
				if remainingInput > uint64(0) {
					libc.Xmemcpy(tls, zds+95940+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize), ip, remainingInput)
					*(*size_t)(unsafe.Pointer(zds + 30304)) += remainingInput
				}
				(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize
				/* check first few bytes */
				err_code1 = ZSTD_getFrameHeader_advanced(tls, zds+29928, zds+95940, (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
				if ERR_isError(tls, err_code1) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+7815, 0)
					}
					return err_code1
				}
				/* return hint input size */
				if (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat == int32(ZSTD_f_zstd1) {
					v19 = int32(6)
				} else {
					v19 = int32(2)
				}
				if uint64(v19) > hSize {
					if (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat == int32(ZSTD_f_zstd1) {
						v20 = int32(6)
					} else {
						v20 = int32(2)
					}
					v18 = uint64(v20)
				} else {
					v18 = hSize
				}
				return v18 - (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize + ZSTD_blockHeaderSize /* remaining header bytes + next block header */
			}
			libc.Xmemcpy(tls, zds+95940+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize), ip, toLoad)
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize = hSize
			ip = ip + uintptr(toLoad)
			goto _11
		}
		/* check for single-pass mode opportunity */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeType != int32(ZSTD_skippableFrame) && uint64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))) >= (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize {
			cSize = ZSTD_findFrameCompressedSize_advanced(tls, istart, uint64(int64(iend)-int64(istart)), (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
			if cSize <= uint64(int64(iend)-int64(istart)) {
				/* shortcut : using single-pass mode */
				decompressedSize = ZSTD_decompress_usingDDict(tls, zds, *(*uintptr)(unsafe.Pointer(bp)), uint64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))), istart, cSize, ZSTD_getDDict(tls, zds))
				if ZSTD_isError(tls, decompressedSize) != 0 {
					return decompressedSize
				}
				ip = istart + uintptr(cSize)
				if *(*uintptr)(unsafe.Pointer(bp)) != 0 {
					v1 = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(decompressedSize)
				} else {
					v1 = *(*uintptr)(unsafe.Pointer(bp))
				}
				*(*uintptr)(unsafe.Pointer(bp)) = v1 /* can occur if frameContentSize = 0 (empty frame) */
				(*ZSTD_DStream)(unsafe.Pointer(zds)).Fexpected = uint64(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_init)
				someMoreWork = uint32(0)
				goto _11
			}
		}
		/* Check output buffer is large enough for ZSTD_odm_stable. */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode == int32(ZSTD_bm_stable) && (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeType != int32(ZSTD_skippableFrame) && (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && uint64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))) < (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7850, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		/* Consume header (see ZSTDds_decodeFrameHeader) */
		err_code2 = ZSTD_decompressBegin_usingDDict(tls, zds, ZSTD_getDDict(tls, zds))
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code2
		}
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat == int32(ZSTD_f_zstd1) && MEM_readLE32(tls, zds+95940)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) { /* skippable frame */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fexpected = uint64(MEM_readLE32(tls, zds+95940+uintptr(ZSTD_FRAMEIDSIZE)))
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fstage = int32(ZSTDds_skipFrame)
		} else {
			err_code3 = ZSTD_decodeFrameHeader(tls, zds, zds+95940, (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize)
			if ERR_isError(tls, err_code3) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code3
			}
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fexpected = ZSTD_blockHeaderSize
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fstage = int32(ZSTDds_decodeBlockHeader)
		}
		/* control buffer memory usage */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize > uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_WINDOWLOG_ABSOLUTEMIN)) {
			v18 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize
		} else {
			v18 = uint64(libc.Uint32FromUint32(1) << libc.Int32FromInt32(ZSTD_WINDOWLOG_ABSOLUTEMIN))
		}
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize = v18
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize > (*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxWindowSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_frameParameter_windowTooLarge))
		}
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxBlockSizeParam != 0 {
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax < uint32((*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxBlockSizeParam) {
				v23 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax
			} else {
				v23 = uint32((*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxBlockSizeParam)
			}
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax = v23
		}
		/* Adapt buffer sizes to frame header instructions */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax > uint32(libc.Int32FromInt32(4)) {
			v23 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax
		} else {
			v23 = uint32(libc.Int32FromInt32(4))
		}
		neededInBuffSize = uint64(v23)
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode == int32(ZSTD_bm_buffered) {
			v18 = ZSTD_decodingBufferSize_internal(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize, uint64((*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax))
		} else {
			v18 = uint64(0)
		}
		neededOutBuffSize = v18
		ZSTD_DCtx_updateOversizedDuration(tls, zds, neededInBuffSize, neededOutBuffSize)
		tooSmall = libc.BoolInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize < neededInBuffSize || (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize < neededOutBuffSize)
		tooLarge = ZSTD_DCtx_isOversizedTooLong(tls, zds)
		if tooSmall != 0 || tooLarge != 0 {
			bufferSize = neededInBuffSize + neededOutBuffSize
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FstaticSize != 0 { /* static DCtx */
				/* controlled at init */
				if bufferSize > (*ZSTD_DStream)(unsafe.Pointer(zds)).FstaticSize-uint64(95968) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+1319, 0)
					}
					return uint64(-int32(ZSTD_error_memory_allocation))
				}
			} else {
				ZSTD_customFree(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff, (*ZSTD_DStream)(unsafe.Pointer(zds)).FcustomMem)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize = uint64(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize = uint64(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff = ZSTD_customMalloc(tls, bufferSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).FcustomMem)
				if (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff == libc.UintptrFromInt32(0) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+1319, 0)
					}
					return uint64(-int32(ZSTD_error_memory_allocation))
				}
			}
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize = neededInBuffSize
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuff = (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff + uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize)
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize = neededOutBuffSize
		}
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
	_7:
		;
		neededInSize = ZSTD_nextSrcSizeToDecompressWithInputSize(tls, zds, uint64(int64(iend)-int64(ip)))
		if neededInSize == uint64(0) { /* end of frame */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_init)
			someMoreWork = uint32(0)
			goto _11
		}
		if uint64(int64(iend)-int64(ip)) >= neededInSize { /* decode directly from src */
			err_code4 = ZSTD_decompressContinueStream(tls, zds, bp, oend, ip, neededInSize)
			if ERR_isError(tls, err_code4) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code4
			}
			ip = ip + uintptr(neededInSize)
			/* Function modifies the stage so we must break */
			goto _11
		}
		if ip == iend {
			someMoreWork = uint32(0)
			goto _11
		} /* no more input */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_load)
	_8:
		;
		neededInSize1 = ZSTD_nextSrcSizeToDecompress(tls, zds)
		toLoad1 = neededInSize1 - (*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos
		isSkipFrame = ZSTD_isSkipFrame(tls, zds)
		/* At this point we shouldn't be decompressing a block that we can stream. */
		if isSkipFrame != 0 {
			if toLoad1 < uint64(int64(iend)-int64(ip)) {
				v18 = toLoad1
			} else {
				v18 = uint64(int64(iend) - int64(ip))
			}
			loadedSize = v18
		} else {
			if toLoad1 > (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize-(*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+7905, 0)
				}
				return uint64(-int32(ZSTD_error_corruption_detected))
			}
			loadedSize = ZSTD_limitCopy(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos), toLoad1, ip, uint64(int64(iend)-int64(ip)))
		}
		if loadedSize != uint64(0) {
			/* ip may be NULL */
			ip = ip + uintptr(loadedSize)
			*(*size_t)(unsafe.Pointer(zds + 30256)) += loadedSize
		}
		if loadedSize < toLoad1 {
			someMoreWork = uint32(0)
			goto _11
		} /* not enough input, wait for more */
		/* decode loaded input */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos = uint64(0) /* input is consumed */
		err_code5 = ZSTD_decompressContinueStream(tls, zds, bp, oend, (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff, neededInSize1)
		if ERR_isError(tls, err_code5) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code5
		}
		/* Function modifies the stage so we must break */
		goto _11
	_9:
		;
		toFlushSize = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd - (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart
		flushedSize = ZSTD_limitCopy(tls, *(*uintptr)(unsafe.Pointer(bp)), uint64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))), (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuff+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart), toFlushSize)
		if *(*uintptr)(unsafe.Pointer(bp)) != 0 {
			v1 = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(flushedSize)
		} else {
			v1 = *(*uintptr)(unsafe.Pointer(bp))
		}
		*(*uintptr)(unsafe.Pointer(bp)) = v1
		*(*size_t)(unsafe.Pointer(zds + 30288)) += flushedSize
		if flushedSize == toFlushSize { /* flush completed */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize < (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize && (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart+uint64((*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax) > (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize {
				v15 = libc.Uint64FromInt32(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd = v15
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart = v15
			}
			goto _11
		}
		/* cannot complete flush */
		someMoreWork = uint32(0)
		goto _11
	_10:
		;
		/* impossible */
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1561, 0)
		}
		return uint64(-int32(ZSTD_error_GENERIC)) /* some compilers require default to do something */
	_11:
	}
	/* result */
	(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = uint64(int64(ip) - int64((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc))
	(*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos = uint64(int64(*(*uintptr)(unsafe.Pointer(bp))) - int64((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst))
	/* Update the expected output buffer for ZSTD_obm_stable. */
	(*ZSTD_DStream)(unsafe.Pointer(zds)).FexpectedOutBuffer = *(*ZSTD_outBuffer)(unsafe.Pointer(output))
	if ip == istart && *(*uintptr)(unsafe.Pointer(bp)) == ostart { /* no forward progress */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress = (*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress + 1
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress >= int32(ZSTD_NO_FORWARD_PROGRESS_MAX) {
			if *(*uintptr)(unsafe.Pointer(bp)) == oend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return uint64(-int32(ZSTD_error_noForwardProgress_destFull))
			}
			if ip == iend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return uint64(-int32(ZSTD_error_noForwardProgress_inputEmpty))
			}
		}
	} else {
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress = 0
	}
	nextSrcSizeHint = ZSTD_nextSrcSizeToDecompress(tls, zds)
	if !(nextSrcSizeHint != 0) { /* frame fully decoded */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd == (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart { /* output fully flushed */
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte != 0 {
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos >= (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
					/* can't release hostage (not present) */
					(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
					return uint64(1)
				}
				(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos + 1 /* release hostage */
			} /* zds->hostageByte */
			return uint64(0)
		} /* zds->outEnd == zds->outStart */
		if !((*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte != 0) { /* output not fully flushed; keep last byte as hostage; will be released when all output is flushed */
			(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos - 1 /* note : pos > 0, otherwise, impossible to finish reading last block */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte = uint32(1)
		}
		return uint64(1)
	} /* nextSrcSizeHint==0 */
	nextSrcSizeHint = nextSrcSizeHint + ZSTD_blockHeaderSize*libc.BoolUint64(ZSTD_nextInputType(tls, zds) == int32(ZSTDnit_block)) /* preload header of next block */
	nextSrcSizeHint = nextSrcSizeHint - (*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos                                                /* part already loaded*/
	return nextSrcSizeHint
	return r
}

func ZSTD_decompressStream_simpleArgs(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, dstPos uintptr, src uintptr, srcSize size_t, srcPos uintptr) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var cErr size_t
	var _ /* input at bp+24 */ ZSTD_inBuffer
	var _ /* output at bp+0 */ ZSTD_outBuffer
	_ = cErr
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fdst = dst
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fsize = dstCapacity
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos = *(*size_t)(unsafe.Pointer(dstPos))
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsrc = src
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsize = srcSize
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos = *(*size_t)(unsafe.Pointer(srcPos))
	cErr = ZSTD_decompressStream(tls, dctx, bp, bp+24)
	*(*size_t)(unsafe.Pointer(dstPos)) = (*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos
	*(*size_t)(unsafe.Pointer(srcPos)) = (*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos
	return cErr
	return r
}

/**** ended inlining decompress/zstd_decompress.c ****/
/**** start inlining decompress/zstd_decompress_block.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* zstd_decompress_block :
 * this module takes care of decompressing _compressed_ block */

/*-*******************************************************
*  Dependencies
*********************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/cpu.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: zstd_decompress_internal.h ****/
/**** skipping file: zstd_ddict.h ****/
/**** skipping file: zstd_decompress_block.h ****/
/**** skipping file: ../common/bits.h ****/

/*_*******************************************************
*  Macros
**********************************************************/

/* These two optional macros force the use one way or another of the two
 * ZSTD_decompressSequences implementations. You can't force in both directions
 * at the same time.
 */

// C documentation
//
//	/*_*******************************************************
//	*  Memory operations
//	**********************************************************/
func ZSTD_copy4(tls *libc.TLS, dst uintptr, src uintptr) {
	libc.Xmemcpy(tls, dst, src, uint64(libc.Int32FromInt32(4)))
}

/*-*************************************************************
 *   Block decoding
 ***************************************************************/

func ZSTD_blockSizeMax(tls *libc.TLS, dctx uintptr) (r size_t) {
	var blockSizeMax size_t
	var v1 uint32
	_, _ = blockSizeMax, v1
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression != 0 {
		v1 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax
	} else {
		v1 = uint32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	blockSizeMax = uint64(v1)
	return blockSizeMax
}

// C documentation
//
//	/*! ZSTD_getcBlockSize() :
//	 *  Provides the size of compressed block from block header `src` */
func ZSTD_getcBlockSize(tls *libc.TLS, src uintptr, srcSize size_t, bpPtr uintptr) (r size_t) {
	var cBlockHeader, cSize U32
	_, _ = cBlockHeader, cSize
	if srcSize < ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	cBlockHeader = MEM_readLE24(tls, src)
	cSize = cBlockHeader >> int32(3)
	(*blockProperties_t)(unsafe.Pointer(bpPtr)).FlastBlock = cBlockHeader & uint32(1)
	(*blockProperties_t)(unsafe.Pointer(bpPtr)).FblockType = int32(cBlockHeader >> libc.Int32FromInt32(1) & libc.Uint32FromInt32(3))
	(*blockProperties_t)(unsafe.Pointer(bpPtr)).ForigSize = cSize /* only useful for RLE */
	if (*blockProperties_t)(unsafe.Pointer(bpPtr)).FblockType == int32(bt_rle) {
		return uint64(1)
	}
	if (*blockProperties_t)(unsafe.Pointer(bpPtr)).FblockType == int32(bt_reserved) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	return uint64(cSize)
	return r
}

// C documentation
//
//	/* Allocate buffer for literals, either overlapping current dst, or split between dst and litExtraBuffer, or stored entirely within litExtraBuffer */
func ZSTD_allocateLiteralsBuffer(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, litSize size_t, streaming streaming_operation, expectedWriteSize size_t, splitImmediately uint32) {
	var blockSizeMax size_t
	_ = blockSizeMax
	blockSizeMax = ZSTD_blockSizeMax(tls, dctx)
	if streaming == int32(not_streaming) && dstCapacity > blockSizeMax+uint64(WILDCOPY_OVERLENGTH)+litSize+uint64(WILDCOPY_OVERLENGTH) {
		/* If we aren't streaming, we can just put the literals after the output
		 * of the current block. We don't need to worry about overwriting the
		 * extDict of our window, because it doesn't exist.
		 * So if we have space after the end of the block, just put it there.
		 */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dst + uintptr(blockSizeMax) + uintptr(WILDCOPY_OVERLENGTH)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer + uintptr(litSize)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_in_dst)
	} else {
		if litSize <= uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)) {
			/* Literals fit entirely within the extra buffer, put them there to avoid
			 * having to split the literals.
			 */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dctx + 30372
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer + uintptr(litSize)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
		} else {
			/* Literals must be split between the output block and the extra lit
			 * buffer. We fill the extra lit buffer with the tail of the literals,
			 * and put the rest of the literals at the end of the block, with
			 * WILDCOPY_OVERLENGTH of buffer room to allow for overreads.
			 * This MUST not write more than our maxBlockSize beyond dst, because in
			 * streaming mode, that could overwrite part of our extDict window.
			 */
			if splitImmediately != 0 {
				/* won't fit in litExtraBuffer, so it will be split between end of dst and extra buffer */
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dst + uintptr(expectedWriteSize) - uintptr(litSize) + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)) - uintptr(WILDCOPY_OVERLENGTH)
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer + uintptr(litSize) - uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
			} else {
				/* initially this will be stored entirely in dst during huffman decoding, it will partially be shifted to litExtraBuffer after */
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dst + uintptr(expectedWriteSize) - uintptr(litSize)
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = dst + uintptr(expectedWriteSize)
			}
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_split)
		}
	}
}

// C documentation
//
//	/*! ZSTD_decodeLiteralsBlock() :
//	 * Where it is possible to do so without being stomped by the output during decompression, the literals block will be stored
//	 * in the dstBuffer.  If there is room to do so, it will be stored in full in the excess dst space after where the current
//	 * block will be output.  Otherwise it will be stored at the end of the current dst blockspace, with a small portion being
//	 * stored in dctx->litExtraBuffer to help keep it "ahead" of the current output write.
//	 *
//	 * @return : nb of bytes read from src (< srcSize )
//	 *  note : symbol not declared but exposed for fullbench */
func ZSTD_decodeLiteralsBlock(tls *libc.TLS, dctx uintptr, src uintptr, srcSize size_t, dst uintptr, dstCapacity size_t, streaming streaming_operation) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _pos, _size, blockSizeMax, expectedWriteSize, expectedWriteSize1, expectedWriteSize2, hufSuccess, lhSize, lhSize1, lhSize2, litCSize, litSize, litSize1, litSize2 size_t
	var _ptr, istart uintptr
	var flags, v11, v12 int32
	var lhc, lhlCode, lhlCode1, lhlCode2, singleStream U32
	var litEncType SymbolEncodingType_e
	var v10 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _pos, _ptr, _size, blockSizeMax, expectedWriteSize, expectedWriteSize1, expectedWriteSize2, flags, hufSuccess, istart, lhSize, lhSize1, lhSize2, lhc, lhlCode, lhlCode1, lhlCode2, litCSize, litEncType, litSize, litSize1, litSize2, singleStream, v10, v11, v12
	if srcSize < uint64(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	istart = src
	litEncType = int32(*(*BYTE)(unsafe.Pointer(istart))) & libc.Int32FromInt32(3)
	blockSizeMax = ZSTD_blockSizeMax(tls, dctx)
	switch litEncType {
	case int32(set_repeat):
		goto _1
	case int32(set_compressed):
		goto _2
	case int32(set_basic):
		goto _3
	case int32(set_rle):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
_9:
	;
	goto _8
_8:
	;
	if 0 != 0 {
		goto _9
	}
	goto _7
_7:
	;
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy == uint32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
_2:
	;
	if srcSize < uint64(5) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7925, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	singleStream = uint32(0)
	lhlCode = uint32(int32(*(*BYTE)(unsafe.Pointer(istart))) >> int32(2) & int32(3))
	lhc = MEM_readLE32(tls, istart)
	if blockSizeMax < dstCapacity {
		v10 = blockSizeMax
	} else {
		v10 = dstCapacity
	}
	expectedWriteSize = v10
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		v11 = int32(HUF_flags_bmi2)
	} else {
		v11 = 0
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm != 0 {
		v12 = int32(HUF_flags_disableAsm)
	} else {
		v12 = 0
	}
	flags = 0 | v11 | v12
	switch lhlCode {
	case uint32(0):
		fallthrough
	case uint32(1):
		fallthrough
	default: /* note : default is impossible, since lhlCode into [0..3] */
		/* 2 - 2 - 10 - 10 */
		singleStream = libc.BoolUint32(!(lhlCode != 0))
		lhSize = uint64(3)
		litSize = uint64(lhc >> libc.Int32FromInt32(4) & uint32(0x3FF))
		litCSize = uint64(lhc >> libc.Int32FromInt32(14) & uint32(0x3FF))
	case uint32(2):
		/* 2 - 2 - 14 - 14 */
		lhSize = uint64(4)
		litSize = uint64(lhc >> libc.Int32FromInt32(4) & uint32(0x3FFF))
		litCSize = uint64(lhc >> int32(18))
	case uint32(3):
		/* 2 - 2 - 18 - 18 */
		lhSize = uint64(5)
		litSize = uint64(lhc >> libc.Int32FromInt32(4) & uint32(0x3FFFF))
		litCSize = uint64(lhc>>libc.Int32FromInt32(22)) + uint64(*(*BYTE)(unsafe.Pointer(istart + 4)))<<libc.Int32FromInt32(10)
		break
	}
	if litSize > uint64(0) && dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if litSize > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if !(singleStream != 0) {
		if litSize < uint64(MIN_LITERALS_FOR_4_STREAMS) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8007, libc.VaList(bp+8, litSize, int32(MIN_LITERALS_FOR_4_STREAMS)))
			}
			return uint64(-int32(ZSTD_error_literals_headerWrong))
		}
	}
	if litCSize+lhSize > srcSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if expectedWriteSize < litSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_allocateLiteralsBuffer(tls, dctx, dst, dstCapacity, litSize, streaming, expectedWriteSize, uint32(0))
	/* prefetch huffman table if cold */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold != 0 && litSize > uint64(768) {
		_ptr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr
		_size = libc.Uint64FromInt64(16388)
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+8, 0, int32(2)))
			goto _13
		_13:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
	}
	if litEncType == int32(set_repeat) {
		if singleStream != 0 {
			hufSuccess = HUF_decompress1X_usingDTable(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr, flags)
		} else {
			hufSuccess = HUF_decompress4X_usingDTable(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr, flags)
		}
	} else {
		if singleStream != 0 {
			hufSuccess = HUF_decompress1X1_DCtx_wksp(tls, dctx+32+10264, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, dctx+27324, uint64(2560), flags)
		} else {
			hufSuccess = HUF_decompress4X_hufOnly_wksp(tls, dctx+32+10264, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, dctx+27324, uint64(2560), flags)
		}
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		libc.Xmemcpy(tls, dctx+30372, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd-uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)), uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		libc.Xmemmove(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer+uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))-libc.UintptrFromInt32(WILDCOPY_OVERLENGTH), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize-uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		*(*uintptr)(unsafe.Pointer(dctx + 30352)) += uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16) - libc.Int32FromInt32(WILDCOPY_OVERLENGTH))
		*(*uintptr)(unsafe.Pointer(dctx + 30360)) -= uintptr(WILDCOPY_OVERLENGTH)
	}
	if ERR_isError(tls, hufSuccess) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = uint32(1)
	if litEncType == int32(set_compressed) {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr = dctx + 32 + 10264
	}
	return litCSize + lhSize
_3:
	;
	lhlCode1 = uint32(int32(*(*BYTE)(unsafe.Pointer(istart))) >> int32(2) & int32(3))
	if blockSizeMax < dstCapacity {
		v10 = blockSizeMax
	} else {
		v10 = dstCapacity
	}
	expectedWriteSize1 = v10
	switch lhlCode1 {
	case uint32(0):
		fallthrough
	case uint32(2):
		fallthrough
	default: /* note : default is impossible, since lhlCode into [0..3] */
		lhSize1 = uint64(1)
		litSize1 = uint64(int32(*(*BYTE)(unsafe.Pointer(istart))) >> int32(3))
	case uint32(1):
		lhSize1 = uint64(2)
		litSize1 = uint64(int32(MEM_readLE16(tls, istart)) >> int32(4))
	case uint32(3):
		lhSize1 = uint64(3)
		if srcSize < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8065, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		litSize1 = uint64(MEM_readLE24(tls, istart) >> int32(4))
		break
	}
	if litSize1 > uint64(0) && dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if litSize1 > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if expectedWriteSize1 < litSize1 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_allocateLiteralsBuffer(tls, dctx, dst, dstCapacity, litSize1, streaming, expectedWriteSize1, uint32(1))
	if lhSize1+litSize1+uint64(WILDCOPY_OVERLENGTH) > srcSize { /* risk reading beyond src buffer with wildcopy */
		if litSize1+lhSize1 > srcSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
			libc.Xmemcpy(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, istart+uintptr(lhSize1), litSize1-uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
			libc.Xmemcpy(tls, dctx+30372, istart+uintptr(lhSize1)+uintptr(litSize1)-uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)), uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		} else {
			libc.Xmemcpy(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, istart+uintptr(lhSize1), litSize1)
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize1
		return lhSize1 + litSize1
	}
	/* direct reference into compressed stream */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = istart + uintptr(lhSize1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr + uintptr(litSize1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
	return lhSize1 + litSize1
_4:
	;
	lhlCode2 = uint32(int32(*(*BYTE)(unsafe.Pointer(istart))) >> int32(2) & int32(3))
	if blockSizeMax < dstCapacity {
		v10 = blockSizeMax
	} else {
		v10 = dstCapacity
	}
	expectedWriteSize2 = v10
	switch lhlCode2 {
	case uint32(0):
		fallthrough
	case uint32(2):
		fallthrough
	default: /* note : default is impossible, since lhlCode into [0..3] */
		lhSize2 = uint64(1)
		litSize2 = uint64(int32(*(*BYTE)(unsafe.Pointer(istart))) >> int32(3))
	case uint32(1):
		lhSize2 = uint64(2)
		if srcSize < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8122, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		litSize2 = uint64(int32(MEM_readLE16(tls, istart)) >> int32(4))
	case uint32(3):
		lhSize2 = uint64(3)
		if srcSize < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8181, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		litSize2 = uint64(MEM_readLE24(tls, istart) >> int32(4))
		break
	}
	if litSize2 > uint64(0) && dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if litSize2 > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if expectedWriteSize2 < litSize2 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_allocateLiteralsBuffer(tls, dctx, dst, dstCapacity, litSize2, streaming, expectedWriteSize2, uint32(1))
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		libc.Xmemset(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, int32(*(*BYTE)(unsafe.Pointer(istart + uintptr(lhSize2)))), litSize2-uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		libc.Xmemset(tls, dctx+30372, int32(*(*BYTE)(unsafe.Pointer(istart + uintptr(lhSize2)))), uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
	} else {
		libc.Xmemset(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, int32(*(*BYTE)(unsafe.Pointer(istart + uintptr(lhSize2)))), litSize2)
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize2
	return lhSize2 + uint64(1)
_5:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+8240, 0)
	}
	return uint64(-int32(ZSTD_error_corruption_detected))
_6:
	;
	return r
}

func ZSTD_decodeLiteralsBlock_wrapper(tls *libc.TLS, dctx uintptr, src uintptr, srcSize size_t, dst uintptr, dstCapacity size_t) (r size_t) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = 0
	return ZSTD_decodeLiteralsBlock(tls, dctx, src, srcSize, dst, dstCapacity, int32(not_streaming))
}

/* Default FSE distribution tables.
 * These are pre-calculated FSE decoding tables using default distributions as defined in specification :
 * https://github.com/facebook/zstd/blob/release/doc/zstd_compression_format.md#default-distributions
 * They were generated programmatically with following method :
 * - start from default distributions, present in /lib/common/zstd_internal.h
 * - generate tables normally, using ZSTD_buildFSETable()
 * - printout the content of tables
 * - prettify output, report below, test with fuzzer to ensure it's correct */

// C documentation
//
//	/* Default FSE distribution table for Literal Lengths */
var LL_defaultDTable = [65]ZSTD_seqSymbol{
	0: {
		FnextState:        uint16(1),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(1),
		FbaseValue:        uint32(LL_DEFAULTNORMLOG),
	},
	1: {
		FnbBits: uint8(4),
	},
	2: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
	},
	3: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(1),
	},
	4: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(3),
	},
	5: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(4),
	},
	6: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	7: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	8: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	9: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	10: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(12),
	},
	11: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(14),
	},
	12: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16),
	},
	13: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(20),
	},
	14: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(22),
	},
	15: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(28),
	},
	16: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(32),
	},
	17: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(48),
	},
	18: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(64),
	},
	19: {
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(128),
	},
	20: {
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(256),
	},
	21: {
		FnbAdditionalBits: uint8(10),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(1024),
	},
	22: {
		FnbAdditionalBits: uint8(12),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(4096),
	},
	23: {
		FnextState: uint16(32),
		FnbBits:    uint8(4),
	},
	24: {
		FnbBits:    uint8(4),
		FbaseValue: uint32(1),
	},
	25: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(2),
	},
	26: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(4),
	},
	27: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(5),
	},
	28: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	29: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	30: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	31: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	32: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(13),
	},
	33: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16),
	},
	34: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(18),
	},
	35: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(22),
	},
	36: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(24),
	},
	37: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(32),
	},
	38: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(40),
	},
	39: {
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(64),
	},
	40: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(64),
	},
	41: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(128),
	},
	42: {
		FnbAdditionalBits: uint8(9),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(512),
	},
	43: {
		FnbAdditionalBits: uint8(11),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(2048),
	},
	44: {
		FnextState: uint16(48),
		FnbBits:    uint8(4),
	},
	45: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
		FbaseValue: uint32(1),
	},
	46: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(2),
	},
	47: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(3),
	},
	48: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(5),
	},
	49: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	50: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	51: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	52: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	53: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(12),
	},
	54: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(15),
	},
	55: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(18),
	},
	56: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(20),
	},
	57: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(24),
	},
	58: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(28),
	},
	59: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(40),
	},
	60: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(48),
	},
	61: {
		FnbAdditionalBits: uint8(16),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(65536),
	},
	62: {
		FnbAdditionalBits: uint8(15),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(32768),
	},
	63: {
		FnbAdditionalBits: uint8(14),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(16384),
	},
	64: {
		FnbAdditionalBits: uint8(13),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(8192),
	},
} /* LL_defaultDTable */

// C documentation
//
//	/* Default FSE distribution table for Offset Codes */
var OF_defaultDTable = [33]ZSTD_seqSymbol{
	0: {
		FnextState:        uint16(1),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(1),
		FbaseValue:        uint32(OF_DEFAULTNORMLOG),
	},
	1: {
		FnbBits: uint8(5),
	},
	2: {
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(61),
	},
	3: {
		FnbAdditionalBits: uint8(9),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(509),
	},
	4: {
		FnbAdditionalBits: uint8(15),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(32765),
	},
	5: {
		FnbAdditionalBits: uint8(21),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(2097149),
	},
	6: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(5),
	},
	7: {
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(125),
	},
	8: {
		FnbAdditionalBits: uint8(12),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(4093),
	},
	9: {
		FnbAdditionalBits: uint8(18),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(262141),
	},
	10: {
		FnbAdditionalBits: uint8(23),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(8388605),
	},
	11: {
		FnbAdditionalBits: uint8(5),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(29),
	},
	12: {
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(253),
	},
	13: {
		FnbAdditionalBits: uint8(14),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16381),
	},
	14: {
		FnbAdditionalBits: uint8(20),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1048573),
	},
	15: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1),
	},
	16: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(125),
	},
	17: {
		FnbAdditionalBits: uint8(11),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(2045),
	},
	18: {
		FnbAdditionalBits: uint8(17),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(131069),
	},
	19: {
		FnbAdditionalBits: uint8(22),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(4194301),
	},
	20: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(13),
	},
	21: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(253),
	},
	22: {
		FnbAdditionalBits: uint8(13),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(8189),
	},
	23: {
		FnbAdditionalBits: uint8(19),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(524285),
	},
	24: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1),
	},
	25: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(61),
	},
	26: {
		FnbAdditionalBits: uint8(10),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1021),
	},
	27: {
		FnbAdditionalBits: uint8(16),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(65533),
	},
	28: {
		FnbAdditionalBits: uint8(28),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(268435453),
	},
	29: {
		FnbAdditionalBits: uint8(27),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(134217725),
	},
	30: {
		FnbAdditionalBits: uint8(26),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(67108861),
	},
	31: {
		FnbAdditionalBits: uint8(25),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(33554429),
	},
	32: {
		FnbAdditionalBits: uint8(24),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16777213),
	},
} /* OF_defaultDTable */

// C documentation
//
//	/* Default FSE distribution table for Match Lengths */
var ML_defaultDTable = [65]ZSTD_seqSymbol{
	0: {
		FnextState:        uint16(1),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(1),
		FbaseValue:        uint32(ML_DEFAULTNORMLOG),
	},
	1: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(3),
	},
	2: {
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	3: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(5),
	},
	4: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	5: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	6: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	7: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	8: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(13),
	},
	9: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(16),
	},
	10: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(19),
	},
	11: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(22),
	},
	12: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(25),
	},
	13: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(28),
	},
	14: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(31),
	},
	15: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(34),
	},
	16: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(37),
	},
	17: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(41),
	},
	18: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(47),
	},
	19: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(59),
	},
	20: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(83),
	},
	21: {
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(131),
	},
	22: {
		FnbAdditionalBits: uint8(9),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(515),
	},
	23: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	24: {
		FnbBits:    uint8(4),
		FbaseValue: uint32(5),
	},
	25: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	26: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	27: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	28: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	29: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(12),
	},
	30: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(15),
	},
	31: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(18),
	},
	32: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(21),
	},
	33: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(24),
	},
	34: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(27),
	},
	35: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(30),
	},
	36: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(33),
	},
	37: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(35),
	},
	38: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(39),
	},
	39: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(43),
	},
	40: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(51),
	},
	41: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(67),
	},
	42: {
		FnbAdditionalBits: uint8(5),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(99),
	},
	43: {
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(259),
	},
	44: {
		FnextState: uint16(32),
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	45: {
		FnextState: uint16(48),
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	46: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
		FbaseValue: uint32(5),
	},
	47: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	48: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	49: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	50: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	51: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(14),
	},
	52: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(17),
	},
	53: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(20),
	},
	54: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(23),
	},
	55: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(26),
	},
	56: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(29),
	},
	57: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(32),
	},
	58: {
		FnbAdditionalBits: uint8(16),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(65539),
	},
	59: {
		FnbAdditionalBits: uint8(15),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(32771),
	},
	60: {
		FnbAdditionalBits: uint8(14),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(16387),
	},
	61: {
		FnbAdditionalBits: uint8(13),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(8195),
	},
	62: {
		FnbAdditionalBits: uint8(12),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(4099),
	},
	63: {
		FnbAdditionalBits: uint8(11),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(2051),
	},
	64: {
		FnbAdditionalBits: uint8(10),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(1027),
	},
} /* ML_defaultDTable */

func ZSTD_buildSeqTable_rle(tls *libc.TLS, dt uintptr, baseValue U32, nbAddBits U8) {
	var DTableH, cell, ptr uintptr
	_, _, _ = DTableH, cell, ptr
	ptr = dt
	DTableH = ptr
	cell = dt + uintptr(1)*8
	(*ZSTD_seqSymbol_header)(unsafe.Pointer(DTableH)).FtableLog = uint32(0)
	(*ZSTD_seqSymbol_header)(unsafe.Pointer(DTableH)).FfastMode = uint32(0)
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FnbBits = uint8(0)
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FnextState = uint16(0)
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FnbAdditionalBits = nbAddBits
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FbaseValue = baseValue
}

// C documentation
//
//	/* ZSTD_buildFSETable() :
//	 * generate FSE decoding table for one symbol (ll, ml or off)
//	 * cannot fail if input is valid =>
//	 * all inputs are presumed validated at this stage */
func ZSTD_buildFSETable_body(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var add, sv U64
	var highThreshold, maxSV1, nextState, position1, s, s1, s3, step1, symbol, tableMask1, tableSize, u1, v2 U32
	var i, i1, n, n1 int32
	var largeLimit S16
	var pos, position, s2, step, tableMask, u, uPosition, unroll size_t
	var spread, symbolNext, tableDecode, v11 uintptr
	var v10 U16
	var _ /* DTableH at bp+0 */ ZSTD_seqSymbol_header
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = add, highThreshold, i, i1, largeLimit, maxSV1, n, n1, nextState, pos, position, position1, s, s1, s2, s3, spread, step, step1, sv, symbol, symbolNext, tableDecode, tableMask, tableMask1, tableSize, u, u1, uPosition, unroll, v10, v11, v2
	tableDecode = dt + uintptr(1)*8
	maxSV1 = maxSymbolValue + uint32(1)
	tableSize = uint32(int32(1) << tableLog)
	symbolNext = wksp
	spread = symbolNext + uintptr(libc.Int32FromInt32(MaxML))*2 + libc.UintptrFromInt32(1)*2
	highThreshold = tableSize - uint32(1)
	/* Sanity Checks */
	_ = wkspSize
	/* Init, lay down lowprob symbols */
	(*(*ZSTD_seqSymbol_header)(unsafe.Pointer(bp))).FtableLog = tableLog
	(*(*ZSTD_seqSymbol_header)(unsafe.Pointer(bp))).FfastMode = uint32(1)
	largeLimit = int16(libc.Int32FromInt32(1) << (tableLog - libc.Uint32FromInt32(1)))
	s = uint32(0)
	for {
		if !(s < maxSV1) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) == -int32(1) {
			v2 = highThreshold
			highThreshold = highThreshold - 1
			(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(v2)*8))).FbaseValue = s
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = uint16(1)
		} else {
			if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) >= int32(largeLimit) {
				(*(*ZSTD_seqSymbol_header)(unsafe.Pointer(bp))).FfastMode = uint32(0)
			}
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = uint16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)))
		}
		goto _1
	_1:
		;
		s = s + 1
	}
	libc.Xmemcpy(tls, dt, bp, libc.Uint64FromInt64(8))
	/* Spread symbols */
	/* Specialized symbol spreading for the case when there are
	 * no low probability (-1 count) symbols. When compressing
	 * small blocks we avoid low probability symbols to hit this
	 * case, since header decoding speed matters more.
	 */
	if highThreshold == tableSize-uint32(1) {
		tableMask = uint64(tableSize - uint32(1))
		step = uint64(tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3))
		/* First lay down the symbols in order.
		 * We use a uint64_t to lay down 8 bytes at a time. This reduces branch
		 * misses since small blocks generally have small table logs, so nearly
		 * all symbols have counts <= 8. We ensure we have 8 bytes at the end of
		 * our buffer to handle the over-write.
		 */
		add = uint64(0x0101010101010101)
		pos = uint64(0)
		sv = uint64(0)
		s1 = uint32(0)
		for {
			if !(s1 < maxSV1) {
				break
			}
			n = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s1)*2)))
			MEM_write64(tls, spread+uintptr(pos), sv)
			i = int32(8)
			for {
				if !(i < n) {
					break
				}
				MEM_write64(tls, spread+uintptr(pos)+uintptr(i), sv)
				goto _4
			_4:
				;
				i = i + int32(8)
			}
			pos = pos + uint64(n)
			goto _3
		_3:
			;
			s1 = s1 + 1
			sv = sv + add
		}
		/* Now we spread those positions across the table.
		 * The benefit of doing it in two stages is that we avoid the
		 * variable size inner loop, which caused lots of branch misses.
		 * Now we can run through all the positions without any branch misses.
		 * We unroll the loop twice, since that is what empirically worked best.
		 */
		position = uint64(0)
		unroll = uint64(2)
		/* FSE_MIN_TABLELOG is 5 */
		s2 = uint64(0)
		for {
			if !(s2 < uint64(tableSize)) {
				break
			}
			u = uint64(0)
			for {
				if !(u < unroll) {
					break
				}
				uPosition = (position + u*step) & tableMask
				(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(uPosition)*8))).FbaseValue = uint32(*(*BYTE)(unsafe.Pointer(spread + uintptr(s2+u))))
				goto _6
			_6:
				;
				u = u + 1
			}
			position = (position + unroll*step) & tableMask
			goto _5
		_5:
			;
			s2 = s2 + unroll
		}
	} else {
		tableMask1 = tableSize - uint32(1)
		step1 = tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3)
		position1 = uint32(0)
		s3 = uint32(0)
		for {
			if !(s3 < maxSV1) {
				break
			}
			n1 = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))
			i1 = 0
			for {
				if !(i1 < n1) {
					break
				}
				(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(position1)*8))).FbaseValue = s3
				position1 = (position1 + step1) & tableMask1
				for libc.BoolInt32(position1 > highThreshold) != 0 {
					position1 = (position1 + step1) & tableMask1
				} /* lowprob area */
				goto _8
			_8:
				;
				i1 = i1 + 1
			}
			goto _7
		_7:
			;
			s3 = s3 + 1
		}
		/* position must reach all cells once, otherwise normalizedCounter is incorrect */
	}
	/* Build Decoding table */
	u1 = uint32(0)
	for {
		if !(u1 < tableSize) {
			break
		}
		symbol = (*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FbaseValue
		v11 = symbolNext + uintptr(symbol)*2
		v10 = *(*U16)(unsafe.Pointer(v11))
		*(*U16)(unsafe.Pointer(v11)) = *(*U16)(unsafe.Pointer(v11)) + 1
		nextState = uint32(v10)
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnbBits = uint8(tableLog - ZSTD_highbit32(tls, nextState))
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnextState = uint16(nextState<<(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnbBits - tableSize)
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnbAdditionalBits = *(*U8)(unsafe.Pointer(nbAdditionalBits + uintptr(symbol)))
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FbaseValue = *(*U32)(unsafe.Pointer(baseValue + uintptr(symbol)*4))
		goto _9
	_9:
		;
		u1 = u1 + 1
	}
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func ZSTD_buildFSETable_body_default(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t) {
	ZSTD_buildFSETable_body(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
}

func ZSTD_buildFSETable_body_bmi2(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t) {
	ZSTD_buildFSETable_body(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
}

func ZSTD_buildFSETable(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t, bmi2 int32) {
	if bmi2 != 0 {
		ZSTD_buildFSETable_body_bmi2(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
		return
	}
	_ = bmi2
	ZSTD_buildFSETable_body_default(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
}

// C documentation
//
//	/*! ZSTD_buildSeqTable() :
//	 * @return : nb bytes read from src,
//	 *           or an error code if it fails */
func ZSTD_buildSeqTable(tls *libc.TLS, DTableSpace uintptr, DTablePtr uintptr, type1 SymbolEncodingType_e, _max uint32, maxLog U32, src uintptr, srcSize size_t, baseValue uintptr, nbAdditionalBits uintptr, defaultTable uintptr, flagRepeatTable U32, ddictIsCold int32, nbSeq int32, wksp uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	*(*uint32)(unsafe.Pointer(bp)) = _max
	var _pos, _size, headerSize, pSize size_t
	var _ptr, pStart uintptr
	var baseline, symbol U32
	var nbBits U8
	var _ /* norm at bp+8 */ [53]S16
	var _ /* tableLog at bp+4 */ uint32
	_, _, _, _, _, _, _, _, _ = _pos, _ptr, _size, baseline, headerSize, nbBits, pSize, pStart, symbol
	switch type1 {
	case int32(set_rle):
		goto _1
	case int32(set_basic):
		goto _2
	case int32(set_repeat):
		goto _3
	case int32(set_compressed):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
_9:
	;
	if !(srcSize != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	goto _8
_8:
	;
	if 0 != 0 {
		goto _9
	}
	goto _7
_7:
	;
	if uint32(*(*BYTE)(unsafe.Pointer(src))) > *(*uint32)(unsafe.Pointer(bp)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	symbol = uint32(*(*BYTE)(unsafe.Pointer(src)))
	baseline = *(*U32)(unsafe.Pointer(baseValue + uintptr(symbol)*4))
	nbBits = *(*U8)(unsafe.Pointer(nbAdditionalBits + uintptr(symbol)))
	ZSTD_buildSeqTable_rle(tls, DTableSpace, baseline, nbBits)
	*(*uintptr)(unsafe.Pointer(DTablePtr)) = DTableSpace
	return uint64(1)
_2:
	;
	*(*uintptr)(unsafe.Pointer(DTablePtr)) = defaultTable
	return uint64(0)
_3:
	;
	if !(flagRepeatTable != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* prefetch FSE table if used */
	if ddictIsCold != 0 && nbSeq > int32(24) {
		pStart = *(*uintptr)(unsafe.Pointer(DTablePtr))
		pSize = uint64(8) * uint64(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)<<maxLog)
		_ptr = pStart
		_size = pSize
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+128, 0, int32(2)))
			goto _10
		_10:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
	}
	return uint64(0)
_4:
	;
	headerSize = FSE_readNCount(tls, bp+8, bp, bp+4, src, srcSize)
	if ERR_isError(tls, headerSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	if *(*uint32)(unsafe.Pointer(bp + 4)) > maxLog {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	ZSTD_buildFSETable(tls, DTableSpace, bp+8, *(*uint32)(unsafe.Pointer(bp)), baseValue, nbAdditionalBits, *(*uint32)(unsafe.Pointer(bp + 4)), wksp, wkspSize, bmi2)
	*(*uintptr)(unsafe.Pointer(DTablePtr)) = DTableSpace
	return headerSize
_5:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+8240, 0)
	}
	return uint64(-int32(ZSTD_error_GENERIC))
_6:
	;
	return r
}

func ZSTD_decodeSeqHeaders(tls *libc.TLS, dctx uintptr, nbSeqPtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	var LLtype, MLtype, OFtype SymbolEncodingType_e
	var iend, ip, istart, v1 uintptr
	var llhSize, mlhSize, ofhSize size_t
	var nbSeq int32
	_, _, _, _, _, _, _, _, _, _, _ = LLtype, MLtype, OFtype, iend, ip, istart, llhSize, mlhSize, nbSeq, ofhSize, v1
	istart = src
	iend = istart + uintptr(srcSize)
	ip = istart
	/* check */
	if srcSize < uint64(MIN_SEQUENCES_SIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* SeqHead */
	v1 = ip
	ip = ip + 1
	nbSeq = int32(*(*BYTE)(unsafe.Pointer(v1)))
	if nbSeq > int32(0x7F) {
		if nbSeq == int32(0xFF) {
			if ip+uintptr(2) > iend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return uint64(-int32(ZSTD_error_srcSize_wrong))
			}
			nbSeq = int32(MEM_readLE16(tls, ip)) + int32(LONGNBSEQ)
			ip = ip + uintptr(2)
		} else {
			if ip >= iend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return uint64(-int32(ZSTD_error_srcSize_wrong))
			}
			v1 = ip
			ip = ip + 1
			nbSeq = (nbSeq-int32(0x80))<<int32(8) + int32(*(*BYTE)(unsafe.Pointer(v1)))
		}
	}
	*(*int32)(unsafe.Pointer(nbSeqPtr)) = nbSeq
	if nbSeq == 0 {
		/* No sequence : section ends immediately */
		if ip != iend {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8251, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		return uint64(int64(ip) - int64(istart))
	}
	/* FSE table descriptors */
	if ip+uintptr(1) > iend {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	} /* minimum possible size: 1 byte for symbol encoding types */
	if int32(*(*BYTE)(unsafe.Pointer(ip)))&int32(3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	} /* The last field, Reserved, must be all-zeroes. */
	LLtype = int32(*(*BYTE)(unsafe.Pointer(ip))) >> libc.Int32FromInt32(6)
	OFtype = int32(*(*BYTE)(unsafe.Pointer(ip))) >> libc.Int32FromInt32(4) & libc.Int32FromInt32(3)
	MLtype = int32(*(*BYTE)(unsafe.Pointer(ip))) >> libc.Int32FromInt32(2) & libc.Int32FromInt32(3)
	ip = ip + 1
	/* Build DTables */
	llhSize = ZSTD_buildSeqTable(tls, dctx+32, dctx, LLtype, uint32(MaxLL), uint32(LLFSELog), ip, uint64(int64(iend)-int64(ip)), uintptr(unsafe.Pointer(&LL_base)), uintptr(unsafe.Pointer(&LL_bits)), uintptr(unsafe.Pointer(&LL_defaultDTable)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold, nbSeq, dctx+27324, uint64(2560), ZSTD_DCtx_get_bmi2(tls, dctx))
	if ZSTD_isError(tls, llhSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8300, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	ip = ip + uintptr(llhSize)
	ofhSize = ZSTD_buildSeqTable(tls, dctx+32+4104, dctx+16, OFtype, uint32(MaxOff), uint32(OffFSELog), ip, uint64(int64(iend)-int64(ip)), uintptr(unsafe.Pointer(&OF_base)), uintptr(unsafe.Pointer(&OF_bits)), uintptr(unsafe.Pointer(&OF_defaultDTable)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold, nbSeq, dctx+27324, uint64(2560), ZSTD_DCtx_get_bmi2(tls, dctx))
	if ZSTD_isError(tls, ofhSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8300, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	ip = ip + uintptr(ofhSize)
	mlhSize = ZSTD_buildSeqTable(tls, dctx+32+6160, dctx+8, MLtype, uint32(MaxML), uint32(MLFSELog), ip, uint64(int64(iend)-int64(ip)), uintptr(unsafe.Pointer(&ML_base)), uintptr(unsafe.Pointer(&ML_bits)), uintptr(unsafe.Pointer(&ML_defaultDTable)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold, nbSeq, dctx+27324, uint64(2560), ZSTD_DCtx_get_bmi2(tls, dctx))
	if ZSTD_isError(tls, mlhSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8300, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	ip = ip + uintptr(mlhSize)
	return uint64(int64(ip) - int64(istart))
}

type seq_t = struct {
	FlitLength   size_t
	FmatchLength size_t
	Foffset      size_t
}

type ZSTD_fseState = struct {
	Fstate size_t
	Ftable uintptr
}

type seqState_t = struct {
	FDStream    BIT_DStream_t
	FstateLL    ZSTD_fseState
	FstateOffb  ZSTD_fseState
	FstateML    ZSTD_fseState
	FprevOffset [3]size_t
}

// C documentation
//
//	/*! ZSTD_overlapCopy8() :
//	 *  Copies 8 bytes from ip to op and updates op and ip where ip <= op.
//	 *  If the offset is < 8 then the offset is spread to at least 8 bytes.
//	 *
//	 *  Precondition: *ip <= *op
//	 *  Postcondition: *op - *op >= 8
//	 */
func ZSTD_overlapCopy8(tls *libc.TLS, op uintptr, ip uintptr, offset size_t) {
	var sub2 int32
	_ = sub2
	if offset < uint64(8) {
		sub2 = dec64table[offset]
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)))) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip))))
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)) + 1)) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip)) + 1))
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)) + 2)) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip)) + 2))
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)) + 3)) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip)) + 3))
		*(*uintptr)(unsafe.Pointer(ip)) += uintptr(dec32table[offset])
		ZSTD_copy4(tls, *(*uintptr)(unsafe.Pointer(op))+uintptr(4), *(*uintptr)(unsafe.Pointer(ip)))
		*(*uintptr)(unsafe.Pointer(ip)) -= uintptr(sub2)
	} else {
		ZSTD_copy8(tls, *(*uintptr)(unsafe.Pointer(op)), *(*uintptr)(unsafe.Pointer(ip)))
	}
	*(*uintptr)(unsafe.Pointer(ip)) += uintptr(8)
	*(*uintptr)(unsafe.Pointer(op)) += uintptr(8)
}

/* close range match, overlap */
var dec32table = [8]U32{
	1: uint32(1),
	2: uint32(2),
	3: uint32(1),
	4: uint32(4),
	5: uint32(4),
	6: uint32(4),
	7: uint32(4),
} /* added */

var dec64table = [8]int32{
	0: int32(8),
	1: int32(8),
	2: int32(8),
	3: int32(7),
	4: int32(8),
	5: int32(9),
	6: int32(10),
	7: int32(11),
} /* subtracted */

// C documentation
//
//	/*! ZSTD_safecopy() :
//	 *  Specialized version of memcpy() that is allowed to READ up to WILDCOPY_OVERLENGTH past the input buffer
//	 *  and write up to 16 bytes past oend_w (op >= oend_w is allowed).
//	 *  This function is only called in the uncommon case where the sequence is near the end of the block. It
//	 *  should be fast for a single long sequence, but can be slow for several short sequences.
//	 *
//	 *  @param ovtype controls the overlap detection
//	 *         - ZSTD_no_overlap: The source and destination are guaranteed to be at least WILDCOPY_VECLEN bytes apart.
//	 *         - ZSTD_overlap_src_before_dst: The src and dst may overlap and may be any distance apart.
//	 *           The src buffer must be before the dst buffer.
//	 */
func ZSTD_safecopy(tls *libc.TLS, _op uintptr, oend_w uintptr, _ip uintptr, length ptrdiff_t, ovtype ZSTD_overlap_e) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _op
	*(*uintptr)(unsafe.Pointer(bp + 8)) = _ip
	var diff ptrdiff_t
	var oend, v1, v2 uintptr
	_, _, _, _ = diff, oend, v1, v2
	diff = int64(*(*uintptr)(unsafe.Pointer(bp))) - int64(*(*uintptr)(unsafe.Pointer(bp + 8)))
	oend = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(length)
	if length < int64(8) {
		/* Handle short lengths. */
		for *(*uintptr)(unsafe.Pointer(bp)) < oend {
			v1 = *(*uintptr)(unsafe.Pointer(bp))
			*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + 1
			v2 = *(*uintptr)(unsafe.Pointer(bp + 8))
			*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp + 8)) + 1
			*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
		}
		return
	}
	if ovtype == int32(ZSTD_overlap_src_before_dst) {
		/* Copy 8 bytes and ensure the offset >= 8 when there can be overlap. */
		ZSTD_overlapCopy8(tls, bp, bp+8, uint64(diff))
		length = length - int64(8)
	}
	if oend <= oend_w {
		/* No risk of overwrite. */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), length, ovtype)
		return
	}
	if *(*uintptr)(unsafe.Pointer(bp)) <= oend_w {
		/* Wildcopy until we get close to the end. */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), int64(oend_w)-int64(*(*uintptr)(unsafe.Pointer(bp))), ovtype)
		*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp + 8)) + uintptr(int64(oend_w)-int64(*(*uintptr)(unsafe.Pointer(bp))))
		*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(int64(oend_w)-int64(*(*uintptr)(unsafe.Pointer(bp))))
	}
	/* Handle the leftovers. */
	for *(*uintptr)(unsafe.Pointer(bp)) < oend {
		v1 = *(*uintptr)(unsafe.Pointer(bp))
		*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + 1
		v2 = *(*uintptr)(unsafe.Pointer(bp + 8))
		*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp + 8)) + 1
		*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
	}
}

// C documentation
//
//	/* ZSTD_safecopyDstBeforeSrc():
//	 * This version allows overlap with dst before src, or handles the non-overlap case with dst after src
//	 * Kept separate from more common ZSTD_safecopy case to avoid performance impact to the safecopy common case */
func ZSTD_safecopyDstBeforeSrc(tls *libc.TLS, op uintptr, ip uintptr, length ptrdiff_t) {
	var diff ptrdiff_t
	var oend, v1, v2 uintptr
	_, _, _, _ = diff, oend, v1, v2
	diff = int64(op) - int64(ip)
	oend = op + uintptr(length)
	if length < int64(8) || diff > int64(-int32(8)) {
		/* Handle short lengths, close overlaps, and dst not before src. */
		for op < oend {
			v1 = op
			op = op + 1
			v2 = ip
			ip = ip + 1
			*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
		}
		return
	}
	if op <= oend-uintptr(WILDCOPY_OVERLENGTH) && diff < int64(-int32(WILDCOPY_VECLEN)) {
		ZSTD_wildcopy(tls, op, ip, int64(oend-uintptr(WILDCOPY_OVERLENGTH))-int64(op), int32(ZSTD_no_overlap))
		ip = ip + uintptr(int64(oend-uintptr(WILDCOPY_OVERLENGTH))-int64(op))
		op = op + uintptr(int64(oend-uintptr(WILDCOPY_OVERLENGTH))-int64(op))
	}
	/* Handle the leftovers. */
	for op < oend {
		v1 = op
		op = op + 1
		v2 = ip
		ip = ip + 1
		*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
	}
}

// C documentation
//
//	/* ZSTD_execSequenceEnd():
//	 * This version handles cases that are near the end of the output buffer. It requires
//	 * more careful checks to make sure there is no overflow. By separating out these hard
//	 * and unlikely cases, we can speed up the common cases.
//	 *
//	 * NOTE: This function needs to be fast for a single long sequence, but doesn't need
//	 * to be optimized for many small sequences, since those fall into ZSTD_execSequence().
//	 */
func ZSTD_execSequenceEnd(tls *libc.TLS, op uintptr, oend uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	var iLitEnd, match, oLitEnd, oend_w uintptr
	var length1, sequenceLength size_t
	_, _, _, _, _, _ = iLitEnd, length1, match, oLitEnd, oend_w, sequenceLength
	oLitEnd = op + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	match = oLitEnd - uintptr(sequence.Foffset)
	oend_w = oend - uintptr(WILDCOPY_OVERLENGTH)
	/* bounds checks : careful of address space overflow in 32-bit mode */
	if sequenceLength > uint64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8326, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if sequence.FlitLength > uint64(int64(litLimit)-int64(*(*uintptr)(unsafe.Pointer(litPtr)))) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8363, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* copy literals */
	ZSTD_safecopy(tls, op, oend_w, *(*uintptr)(unsafe.Pointer(litPtr)), int64(sequence.FlitLength), int32(ZSTD_no_overlap))
	op = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd
	/* copy Match */
	if sequence.Foffset > uint64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix */
		if sequence.Foffset > uint64(int64(oLitEnd)-int64(virtualStart)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		match = dictEnd - uintptr(int64(prefixStart)-int64(match))
		if match+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, match, sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = uint64(int64(dictEnd) - int64(match))
		libc.Xmemmove(tls, oLitEnd, match, length1)
		op = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		match = prefixStart
	}
	ZSTD_safecopy(tls, op, oend_w, match, int64(sequence.FmatchLength), int32(ZSTD_overlap_src_before_dst))
	return sequenceLength
}

// C documentation
//
//	/* ZSTD_execSequenceEndSplitLitBuffer():
//	 * This version is intended to be used during instances where the litBuffer is still split.  It is kept separate to avoid performance impact for the good case.
//	 */
func ZSTD_execSequenceEndSplitLitBuffer(tls *libc.TLS, op uintptr, oend uintptr, oend_w uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	var iLitEnd, match, oLitEnd uintptr
	var length1, sequenceLength size_t
	_, _, _, _, _ = iLitEnd, length1, match, oLitEnd, sequenceLength
	oLitEnd = op + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	match = oLitEnd - uintptr(sequence.Foffset)
	/* bounds checks : careful of address space overflow in 32-bit mode */
	if sequenceLength > uint64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8326, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if sequence.FlitLength > uint64(int64(litLimit)-int64(*(*uintptr)(unsafe.Pointer(litPtr)))) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8363, 0)
		}
		return uint64(-int32(ZSTD_error_corruption_detected))
	}
	/* copy literals */
	if op > *(*uintptr)(unsafe.Pointer(litPtr)) && op < *(*uintptr)(unsafe.Pointer(litPtr))+uintptr(sequence.FlitLength) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8397, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(litPtr)), int64(sequence.FlitLength))
	op = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd
	/* copy Match */
	if sequence.Foffset > uint64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix */
		if sequence.Foffset > uint64(int64(oLitEnd)-int64(virtualStart)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		match = dictEnd - uintptr(int64(prefixStart)-int64(match))
		if match+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, match, sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = uint64(int64(dictEnd) - int64(match))
		libc.Xmemmove(tls, oLitEnd, match, length1)
		op = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		match = prefixStart
	}
	ZSTD_safecopy(tls, op, oend_w, match, int64(sequence.FmatchLength), int32(ZSTD_overlap_src_before_dst))
	return sequenceLength
}

func ZSTD_execSequence(tls *libc.TLS, _op uintptr, oend uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _op
	var iLitEnd, oLitEnd, oMatchEnd, oend_w uintptr
	var length1, sequenceLength size_t
	var _ /* match at bp+8 */ uintptr
	_, _, _, _, _, _ = iLitEnd, length1, oLitEnd, oMatchEnd, oend_w, sequenceLength
	oLitEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	oMatchEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequenceLength) /* risk : address space overflow (32-bits) */
	oend_w = oend - uintptr(WILDCOPY_OVERLENGTH)                          /* risk : address space underflow on oend=NULL */
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	*(*uintptr)(unsafe.Pointer(bp + 8)) = oLitEnd - uintptr(sequence.Foffset)
	/* Handle edge cases in a slow path:
	 *   - Read beyond end of literals
	 *   - Match end is within WILDCOPY_OVERLIMIT of oend
	 *   - 32-bit mode and the match length overflows
	 */
	if libc.BoolInt32(iLitEnd > litLimit || oMatchEnd > oend_w || MEM_32bits(tls) != 0 && uint64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))) < sequenceLength+uint64(WILDCOPY_OVERLENGTH)) != 0 {
		return ZSTD_execSequenceEnd(tls, *(*uintptr)(unsafe.Pointer(bp)), oend, sequence, litPtr, litLimit, prefixStart, virtualStart, dictEnd)
	}
	/* Assumptions (everything else goes into ZSTD_execSequenceEnd()) */
	/* Copy Literals:
	 * Split out litLength <= 16 since it is nearly always true. +1.6% on gcc-9.
	 * We likely don't need the full 32-byte wildcopy.
	 */
	ZSTD_copy16(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(litPtr)))
	if libc.BoolInt32(sequence.FlitLength > libc.Uint64FromInt32(16)) != 0 {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp))+uintptr(16), *(*uintptr)(unsafe.Pointer(litPtr))+uintptr(16), int64(sequence.FlitLength-uint64(16)), int32(ZSTD_no_overlap))
	}
	*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd /* update for next sequence */
	/* Copy Match */
	if sequence.Foffset > uint64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix -> go into extDict */
		if libc.BoolInt32(sequence.Foffset > uint64(int64(oLitEnd)-int64(virtualStart))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		*(*uintptr)(unsafe.Pointer(bp + 8)) = dictEnd + uintptr(int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(prefixStart))
		if *(*uintptr)(unsafe.Pointer(bp + 8))+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = uint64(int64(dictEnd) - int64(*(*uintptr)(unsafe.Pointer(bp + 8))))
		libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), length1)
		*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		*(*uintptr)(unsafe.Pointer(bp + 8)) = prefixStart
	}
	/* Match within prefix of 1 or more bytes */
	/* Nearly all offsets are >= WILDCOPY_VECLEN bytes, which means we can use wildcopy
	 * without overlap checking.
	 */
	if libc.BoolInt32(sequence.Foffset >= libc.Uint64FromInt32(WILDCOPY_VECLEN)) != 0 {
		/* We bet on a full wildcopy for matches, since we expect matches to be
		 * longer than literals (in general). In silesia, ~10% of matches are longer
		 * than 16 bytes.
		 */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), int64(sequence.FmatchLength), int32(ZSTD_no_overlap))
		return sequenceLength
	}
	/* Copy 8 bytes and spread the offset to be >= 8. */
	ZSTD_overlapCopy8(tls, bp, bp+8, sequence.Foffset)
	/* If the match length is > 8 bytes, then continue with the wildcopy. */
	if sequence.FmatchLength > uint64(8) {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), int64(sequence.FmatchLength)-int64(8), int32(ZSTD_overlap_src_before_dst))
	}
	return sequenceLength
}

func ZSTD_execSequenceSplitLitBuffer(tls *libc.TLS, _op uintptr, oend uintptr, oend_w uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _op
	var iLitEnd, oLitEnd, oMatchEnd uintptr
	var length1, sequenceLength size_t
	var _ /* match at bp+8 */ uintptr
	_, _, _, _, _ = iLitEnd, length1, oLitEnd, oMatchEnd, sequenceLength
	oLitEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	oMatchEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequenceLength) /* risk : address space overflow (32-bits) */
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	*(*uintptr)(unsafe.Pointer(bp + 8)) = oLitEnd - uintptr(sequence.Foffset)
	/* Handle edge cases in a slow path:
	 *   - Read beyond end of literals
	 *   - Match end is within WILDCOPY_OVERLIMIT of oend
	 *   - 32-bit mode and the match length overflows
	 */
	if libc.BoolInt32(iLitEnd > litLimit || oMatchEnd > oend_w || MEM_32bits(tls) != 0 && uint64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))) < sequenceLength+uint64(WILDCOPY_OVERLENGTH)) != 0 {
		return ZSTD_execSequenceEndSplitLitBuffer(tls, *(*uintptr)(unsafe.Pointer(bp)), oend, oend_w, sequence, litPtr, litLimit, prefixStart, virtualStart, dictEnd)
	}
	/* Assumptions (everything else goes into ZSTD_execSequenceEnd()) */
	/* Copy Literals:
	 * Split out litLength <= 16 since it is nearly always true. +1.6% on gcc-9.
	 * We likely don't need the full 32-byte wildcopy.
	 */
	ZSTD_copy16(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(litPtr)))
	if libc.BoolInt32(sequence.FlitLength > libc.Uint64FromInt32(16)) != 0 {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp))+uintptr(16), *(*uintptr)(unsafe.Pointer(litPtr))+uintptr(16), int64(sequence.FlitLength-uint64(16)), int32(ZSTD_no_overlap))
	}
	*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd /* update for next sequence */
	/* Copy Match */
	if sequence.Foffset > uint64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix -> go into extDict */
		if libc.BoolInt32(sequence.Foffset > uint64(int64(oLitEnd)-int64(virtualStart))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		*(*uintptr)(unsafe.Pointer(bp + 8)) = dictEnd + uintptr(int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(prefixStart))
		if *(*uintptr)(unsafe.Pointer(bp + 8))+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = uint64(int64(dictEnd) - int64(*(*uintptr)(unsafe.Pointer(bp + 8))))
		libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), length1)
		*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		*(*uintptr)(unsafe.Pointer(bp + 8)) = prefixStart
	}
	/* Match within prefix of 1 or more bytes */
	/* Nearly all offsets are >= WILDCOPY_VECLEN bytes, which means we can use wildcopy
	 * without overlap checking.
	 */
	if libc.BoolInt32(sequence.Foffset >= libc.Uint64FromInt32(WILDCOPY_VECLEN)) != 0 {
		/* We bet on a full wildcopy for matches, since we expect matches to be
		 * longer than literals (in general). In silesia, ~10% of matches are longer
		 * than 16 bytes.
		 */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), int64(sequence.FmatchLength), int32(ZSTD_no_overlap))
		return sequenceLength
	}
	/* Copy 8 bytes and spread the offset to be >= 8. */
	ZSTD_overlapCopy8(tls, bp, bp+8, sequence.Foffset)
	/* If the match length is > 8 bytes, then continue with the wildcopy. */
	if sequence.FmatchLength > uint64(8) {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), int64(sequence.FmatchLength)-int64(8), int32(ZSTD_overlap_src_before_dst))
	}
	return sequenceLength
}

func ZSTD_initFseState(tls *libc.TLS, DStatePtr uintptr, bitD uintptr, dt uintptr) {
	var DTableH, ptr uintptr
	_, _ = DTableH, ptr
	ptr = dt
	DTableH = ptr
	(*ZSTD_fseState)(unsafe.Pointer(DStatePtr)).Fstate = BIT_readBits(tls, bitD, (*ZSTD_seqSymbol_header)(unsafe.Pointer(DTableH)).FtableLog)
	BIT_reloadDStream(tls, bitD)
	(*ZSTD_fseState)(unsafe.Pointer(DStatePtr)).Ftable = dt + uintptr(1)*8
}

func ZSTD_updateFseStateWithDInfo(tls *libc.TLS, DStatePtr uintptr, bitD uintptr, nextState U16, nbBits U32) {
	var lowBits size_t
	_ = lowBits
	lowBits = BIT_readBits(tls, bitD, nbBits)
	(*ZSTD_fseState)(unsafe.Pointer(DStatePtr)).Fstate = uint64(nextState) + lowBits
}

/* We need to add at most (ZSTD_WINDOWLOG_MAX_32 - 1) bits to read the maximum
 * offset bits. But we can only read at most STREAM_ACCUMULATOR_MIN_32
 * bits before reloading. This value is the maximum number of bytes we read
 * after reloading when we are decoding long offsets.
 */

type ZSTD_longOffset_e = int32

const ZSTD_lo_isRegularOffset = 0
const ZSTD_lo_isLongOffset = 1

// C documentation
//
//	/**
//	 * ZSTD_decodeSequence():
//	 * @p longOffsets : tells the decoder to reload more bit while decoding large offsets
//	 *                  only used in 32-bit mode
//	 * @return : Sequence (litL + matchL + offset)
//	 */
func ZSTD_decodeSequence(tls *libc.TLS, seqState uintptr, longOffsets ZSTD_longOffset_e, isLastSeq int32) (r seq_t) {
	var extraBits, ll0, llnbBits, mlnbBits, ofBase, ofnbBits U32
	var llBits, mlBits, ofBits, totalBits BYTE
	var llDInfo, mlDInfo, ofDInfo uintptr
	var llNext, mlNext, ofNext U16
	var offset, temp, v2 size_t
	var seq seq_t
	var v1 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = extraBits, ll0, llBits, llDInfo, llNext, llnbBits, mlBits, mlDInfo, mlNext, mlnbBits, ofBase, ofBits, ofDInfo, ofNext, offset, ofnbBits, seq, temp, totalBits, v1, v2
	/*
	 * ZSTD_seqSymbol is a 64 bits wide structure.
	 * It can be loaded in one operation
	 * and its fields extracted by simply shifting or bit-extracting on aarch64.
	 * GCC doesn't recognize this and generates more unnecessary ldr/ldrb/ldrh
	 * operations that cause performance drop. This can be avoided by using this
	 * ZSTD_memcpy hack.
	 */
	llDInfo = (*seqState_t)(unsafe.Pointer(seqState)).FstateLL.Ftable + uintptr((*seqState_t)(unsafe.Pointer(seqState)).FstateLL.Fstate)*8
	mlDInfo = (*seqState_t)(unsafe.Pointer(seqState)).FstateML.Ftable + uintptr((*seqState_t)(unsafe.Pointer(seqState)).FstateML.Fstate)*8
	ofDInfo = (*seqState_t)(unsafe.Pointer(seqState)).FstateOffb.Ftable + uintptr((*seqState_t)(unsafe.Pointer(seqState)).FstateOffb.Fstate)*8
	seq.FmatchLength = uint64((*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FbaseValue)
	seq.FlitLength = uint64((*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FbaseValue)
	ofBase = (*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FbaseValue
	llBits = (*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FnbAdditionalBits
	mlBits = (*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FnbAdditionalBits
	ofBits = (*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FnbAdditionalBits
	totalBits = uint8(int32(llBits) + int32(mlBits) + int32(ofBits))
	llNext = (*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FnextState
	mlNext = (*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FnextState
	ofNext = (*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FnextState
	llnbBits = uint32((*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FnbBits)
	mlnbBits = uint32((*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FnbBits)
	ofnbBits = uint32((*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FnbBits)
	/*
	 * As gcc has better branch and block analyzers, sometimes it is only
	 * valuable to mark likeliness for clang, it gives around 3-4% of
	 * performance.
	 */
	/* sequence */
	if int32(ofBits) > int32(1) {
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		if MEM_32bits(tls) != 0 && longOffsets != 0 && int32(ofBits) >= int32(STREAM_ACCUMULATOR_MIN_32) {
			/* Always read extra bits, this keeps the logic simple,
			 * avoids branches, and avoids accidentally reading 0 bits.
			 */
			extraBits = uint32(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_32) - libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_32))
			offset = uint64(ofBase) + BIT_readBitsFast(tls, seqState, uint32(ofBits)-extraBits)<<extraBits
			BIT_reloadDStream(tls, seqState)
			offset = offset + BIT_readBitsFast(tls, seqState, extraBits)
		} else {
			offset = uint64(ofBase) + BIT_readBitsFast(tls, seqState, uint32(ofBits)) /* <=  (ZSTD_WINDOWLOG_MAX-1) bits */
			if MEM_32bits(tls) != 0 {
				BIT_reloadDStream(tls, seqState)
			}
		}
		*(*size_t)(unsafe.Pointer(seqState + 88 + 2*8)) = *(*size_t)(unsafe.Pointer(seqState + 88 + 1*8))
		*(*size_t)(unsafe.Pointer(seqState + 88 + 1*8)) = *(*size_t)(unsafe.Pointer(seqState + 88))
		*(*size_t)(unsafe.Pointer(seqState + 88)) = offset
	} else {
		ll0 = libc.BoolUint32((*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FbaseValue == libc.Uint32FromInt32(0))
		if libc.BoolInt32(int32(ofBits) == libc.Int32FromInt32(0)) != 0 {
			offset = *(*size_t)(unsafe.Pointer(seqState + 88 + uintptr(ll0)*8))
			*(*size_t)(unsafe.Pointer(seqState + 88 + 1*8)) = *(*size_t)(unsafe.Pointer(seqState + 88 + libc.BoolUintptr(!(ll0 != 0))*8))
			*(*size_t)(unsafe.Pointer(seqState + 88)) = offset
		} else {
			offset = uint64(ofBase+ll0) + BIT_readBitsFast(tls, seqState, uint32(1))
			if offset == uint64(3) {
				v1 = *(*size_t)(unsafe.Pointer(seqState + 88)) - uint64(1)
			} else {
				v1 = *(*size_t)(unsafe.Pointer(seqState + 88 + uintptr(offset)*8))
			}
			temp = v1
			temp = temp - libc.BoolUint64(!(temp != 0)) /* 0 is not valid: input corrupted => force offset to -1 => corruption detected at execSequence */
			if offset != uint64(1) {
				*(*size_t)(unsafe.Pointer(seqState + 88 + 2*8)) = *(*size_t)(unsafe.Pointer(seqState + 88 + 1*8))
			}
			*(*size_t)(unsafe.Pointer(seqState + 88 + 1*8)) = *(*size_t)(unsafe.Pointer(seqState + 88))
			v2 = temp
			offset = v2
			*(*size_t)(unsafe.Pointer(seqState + 88)) = v2
		}
	}
	seq.Foffset = offset
	if int32(mlBits) > 0 {
		seq.FmatchLength += BIT_readBitsFast(tls, seqState, uint32(mlBits))
	}
	if MEM_32bits(tls) != 0 && int32(mlBits)+int32(llBits) >= libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_32)-(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_32)-libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_32)) {
		BIT_reloadDStream(tls, seqState)
	}
	if MEM_64bits(tls) != 0 && libc.BoolInt32(int32(totalBits) >= libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_64)-(libc.Int32FromInt32(LLFSELog)+libc.Int32FromInt32(MLFSELog)+libc.Int32FromInt32(OffFSELog))) != 0 {
		BIT_reloadDStream(tls, seqState)
	}
	/* Ensure there are enough bits to read the rest of data in 64-bit mode. */
	_ = libc.Uint64FromInt64(1)
	if int32(llBits) > 0 {
		seq.FlitLength += BIT_readBitsFast(tls, seqState, uint32(llBits))
	}
	if MEM_32bits(tls) != 0 {
		BIT_reloadDStream(tls, seqState)
	}
	if !(isLastSeq != 0) {
		/* don't update FSE state for last Sequence */
		ZSTD_updateFseStateWithDInfo(tls, seqState+40, seqState, llNext, llnbBits) /* <=  9 bits */
		ZSTD_updateFseStateWithDInfo(tls, seqState+72, seqState, mlNext, mlnbBits) /* <=  9 bits */
		if MEM_32bits(tls) != 0 {
			BIT_reloadDStream(tls, seqState)
		} /* <= 18 bits */
		ZSTD_updateFseStateWithDInfo(tls, seqState+56, seqState, ofNext, ofnbBits) /* <=  8 bits */
		BIT_reloadDStream(tls, seqState)
	}
	return seq
}

func ZSTD_decompressSequences_bodySplitLitBuffer(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	bp := tls.Alloc(128)
	defer tls.Free(128)
	var dictEnd, iend, ip, litBufferEnd, oend, op, ostart, prefixStart, vBase uintptr
	var i, i1 U32
	var lastLLSize, lastLLSize1, leftoverLit, oneSeqSize, oneSeqSize1, oneSeqSize2 size_t
	var sequence, sequence1 seq_t
	var _ /* litPtr at bp+0 */ uintptr
	var _ /* seqState at bp+8 */ seqState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dictEnd, i, i1, iend, ip, lastLLSize, lastLLSize1, leftoverLit, litBufferEnd, oend, oneSeqSize, oneSeqSize1, oneSeqSize2, op, ostart, prefixStart, sequence, sequence1, vBase
	ip = seqStart
	iend = ip + uintptr(seqSize)
	ostart = dst
	oend = ZSTD_maybeNullPtrAdd(tls, ostart, int64(maxDstSize))
	op = ostart
	*(*uintptr)(unsafe.Pointer(bp)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr
	litBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd
	prefixStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart
	vBase = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart
	dictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	/* Literals are split between internal buffer & output buffer */
	if nbSeq != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		i = uint32(0)
		for {
			if !(i < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i)*8)) = uint64(*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i)*4)))
			goto _1
		_1:
			;
			i = i + 1
		}
		if ERR_isError(tls, BIT_initDStream(tls, bp+8, ip, uint64(int64(iend)-int64(ip)))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		ZSTD_initFseState(tls, bp+8+40, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr)
		ZSTD_initFseState(tls, bp+8+56, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr)
		ZSTD_initFseState(tls, bp+8+72, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr)
		_ = libc.Uint64FromInt64(1)
		/* decompress without overrunning litPtr begins */
		sequence = seq_t{} /* some static analyzer believe that @sequence is not initialized (it necessarily is, since for(;;) loop as at least one iteration) */
		/* Align the decompression loop to 32 + 16 bytes.
		 *
		 * zstd compiled with gcc-9 on an Intel i9-9900k shows 10% decompression
		 * speed swings based on the alignment of the decompression loop. This
		 * performance swing is caused by parts of the decompression loop falling
		 * out of the DSB. The entire decompression loop should fit in the DSB,
		 * when it can't we get much worse performance. You can measure if you've
		 * hit the good case or the bad case with this perf command for some
		 * compressed file test.zst:
		 *
		 *   perf stat -e cycles -e instructions -e idq.all_dsb_cycles_any_uops                 *             -e idq.all_mite_cycles_any_uops -- ./zstd -tq test.zst
		 *
		 * If you see most cycles served out of the MITE you've hit the bad case.
		 * If you see most cycles served out of the DSB you've hit the good case.
		 * If it is pretty even then you may be in an okay case.
		 *
		 * This issue has been reproduced on the following CPUs:
		 *   - Kabylake: Macbook Pro (15-inch, 2019) 2.4 GHz Intel Core i9
		 *               Use Instruments->Counters to get DSB/MITE cycles.
		 *               I never got performance swings, but I was able to
		 *               go from the good case of mostly DSB to half of the
		 *               cycles served from MITE.
		 *   - Coffeelake: Intel i9-9900k
		 *   - Coffeelake: Intel i7-9700k
		 *
		 * I haven't been able to reproduce the instability or DSB misses on any
		 * of the following CPUS:
		 *   - Haswell
		 *   - Broadwell: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GH
		 *   - Skylake
		 *
		 * Alignment is done for each of the three major decompression loops:
		 *   - ZSTD_decompressSequences_bodySplitLitBuffer - presplit section of the literal buffer
		 *   - ZSTD_decompressSequences_bodySplitLitBuffer - postsplit section of the literal buffer
		 *   - ZSTD_decompressSequences_body
		 * Alignment choices are made to minimize large swings on bad cases and influence on performance
		 * from changes external to this code, rather than to overoptimize on the current commit.
		 *
		 * If you are seeing performance stability this script can help test.
		 * It tests on 4 commits in zstd where I saw performance change.
		 *
		 *   https://gist.github.com/terrelln/9889fc06a423fd5ca6e99351564473f4
		 */
		/* Handle the initial state where litBuffer is currently split between dst and litExtraBuffer */
		for {
			if !(nbSeq != 0) {
				break
			}
			sequence = ZSTD_decodeSequence(tls, bp+8, isLongOffset, libc.BoolInt32(nbSeq == int32(1)))
			if *(*uintptr)(unsafe.Pointer(bp))+uintptr(sequence.FlitLength) > (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd {
				break
			}
			oneSeqSize = ZSTD_execSequenceSplitLitBuffer(tls, op, oend, *(*uintptr)(unsafe.Pointer(bp))+uintptr(sequence.FlitLength)-uintptr(WILDCOPY_OVERLENGTH), sequence, bp, litBufferEnd, prefixStart, vBase, dictEnd)
			if int32(ZSTD_isError(tls, oneSeqSize)) != 0 {
				return oneSeqSize
			}
			op = op + uintptr(oneSeqSize)
			goto _2
		_2:
			;
			nbSeq = nbSeq - 1
		}
		/* If there are more sequences, they will need to read literals from litExtraBuffer; copy over the remainder from dst and update litPtr and litEnd */
		if nbSeq > 0 {
			leftoverLit = uint64(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
			if leftoverLit != 0 {
				if leftoverLit > uint64(int64(oend)-int64(op)) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+8456, 0)
					}
					return uint64(-int32(ZSTD_error_dstSize_tooSmall))
				}
				ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(bp)), int64(leftoverLit))
				sequence.FlitLength -= leftoverLit
				op = op + uintptr(leftoverLit)
			}
			*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
			litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
			oneSeqSize1 = ZSTD_execSequence(tls, op, oend, sequence, bp, litBufferEnd, prefixStart, vBase, dictEnd)
			if int32(ZSTD_isError(tls, oneSeqSize1)) != 0 {
				return oneSeqSize1
			}
			op = op + uintptr(oneSeqSize1)
			nbSeq = nbSeq - 1
		}
		if nbSeq > 0 {
			/* there is remaining lit from extra buffer */
			for {
				if !(nbSeq != 0) {
					break
				}
				sequence1 = ZSTD_decodeSequence(tls, bp+8, isLongOffset, libc.BoolInt32(nbSeq == int32(1)))
				oneSeqSize2 = ZSTD_execSequence(tls, op, oend, sequence1, bp, litBufferEnd, prefixStart, vBase, dictEnd)
				if int32(ZSTD_isError(tls, oneSeqSize2)) != 0 {
					return oneSeqSize2
				}
				op = op + uintptr(oneSeqSize2)
				goto _3
			_3:
				;
				nbSeq = nbSeq - 1
			}
		}
		/* check if reached exact end */
		if nbSeq != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		if !(BIT_endOfDStream(tls, bp+8) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		/* save reps for next block */
		i1 = uint32(0)
		for {
			if !(i1 < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i1)*4)) = uint32(*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i1)*8)))
			goto _4
		_4:
			;
			i1 = i1 + 1
		}
	}
	/* last literal segment */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		/* split hasn't been reached yet, first get dst then copy litExtraBuffer */
		lastLLSize = uint64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
		if lastLLSize > uint64(int64(oend)-int64(op)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		if op != libc.UintptrFromInt32(0) {
			libc.Xmemmove(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize)
			op = op + uintptr(lastLLSize)
		}
		*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
		litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
	}
	/* copy last literals from internal buffer */
	lastLLSize1 = uint64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
	if lastLLSize1 > uint64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if op != libc.UintptrFromInt32(0) {
		libc.Xmemcpy(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize1)
		op = op + uintptr(lastLLSize1)
	}
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_decompressSequences_body(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	bp := tls.Alloc(128)
	defer tls.Free(128)
	var dictEnd, iend, ip, litEnd, oend, op, ostart, prefixStart, vBase, v1 uintptr
	var i, i1 U32
	var lastLLSize, oneSeqSize size_t
	var sequence seq_t
	var _ /* litPtr at bp+0 */ uintptr
	var _ /* seqState at bp+8 */ seqState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dictEnd, i, i1, iend, ip, lastLLSize, litEnd, oend, oneSeqSize, op, ostart, prefixStart, sequence, vBase, v1
	ip = seqStart
	iend = ip + uintptr(seqSize)
	ostart = dst
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_not_in_dst) {
		v1 = ZSTD_maybeNullPtrAdd(tls, ostart, int64(maxDstSize))
	} else {
		v1 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	}
	oend = v1
	op = ostart
	*(*uintptr)(unsafe.Pointer(bp)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr
	litEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize)
	prefixStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart
	vBase = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart
	dictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	/* Regen sequences */
	if nbSeq != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		i = uint32(0)
		for {
			if !(i < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i)*8)) = uint64(*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i)*4)))
			goto _2
		_2:
			;
			i = i + 1
		}
		if ERR_isError(tls, BIT_initDStream(tls, bp+8, ip, uint64(int64(iend)-int64(ip)))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		ZSTD_initFseState(tls, bp+8+40, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr)
		ZSTD_initFseState(tls, bp+8+56, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr)
		ZSTD_initFseState(tls, bp+8+72, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr)
		for {
			if !(nbSeq != 0) {
				break
			}
			sequence = ZSTD_decodeSequence(tls, bp+8, isLongOffset, libc.BoolInt32(nbSeq == int32(1)))
			oneSeqSize = ZSTD_execSequence(tls, op, oend, sequence, bp, litEnd, prefixStart, vBase, dictEnd)
			if int32(ZSTD_isError(tls, oneSeqSize)) != 0 {
				return oneSeqSize
			}
			op = op + uintptr(oneSeqSize)
			goto _3
		_3:
			;
			nbSeq = nbSeq - 1
		}
		/* check if reached exact end */
		if !(BIT_endOfDStream(tls, bp+8) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		/* save reps for next block */
		i1 = uint32(0)
		for {
			if !(i1 < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i1)*4)) = uint32(*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i1)*8)))
			goto _4
		_4:
			;
			i1 = i1 + 1
		}
	}
	/* last literal segment */
	lastLLSize = uint64(int64(litEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
	if lastLLSize > uint64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if op != libc.UintptrFromInt32(0) {
		libc.Xmemcpy(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize)
		op = op + uintptr(lastLLSize)
	}
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_decompressSequences_default(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesSplitLitBuffer_default(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_bodySplitLitBuffer(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_prefetchMatch(tls *libc.TLS, prefetchPos size_t, sequence seq_t, prefixStart uintptr, dictEnd uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var match, matchBase, v1 uintptr
	_, _, _ = match, matchBase, v1
	prefetchPos = prefetchPos + sequence.FlitLength
	if sequence.Foffset > prefetchPos {
		v1 = dictEnd
	} else {
		v1 = prefixStart
	}
	matchBase = v1
	/* note : this operation can overflow when seq.offset is really too large, which can only happen when input is corrupted.
	 * No consequence though : memory address is only used for prefetching, not for dereferencing */
	match = ZSTD_wrappedPtrSub(tls, ZSTD_wrappedPtrAdd(tls, matchBase, int64(prefetchPos)), int64(sequence.Foffset))
	libc.X__builtin_prefetch(tls, match, libc.VaList(bp+8, 0, int32(3)))
	libc.X__builtin_prefetch(tls, match+libc.UintptrFromInt32(CACHELINE_SIZE), libc.VaList(bp+8, 0, int32(3))) /* note : it's safe to invoke PREFETCH() on any memory address, including invalid ones */
	return prefetchPos + sequence.FmatchLength
}

// C documentation
//
//	/* This decoding function employs prefetching
//	 * to reduce latency impact of cache misses.
//	 * It's generally employed when block contains a significant portion of long-distance matches
//	 * or when coupled with a "cold" dictionary */
func ZSTD_decompressSequencesLong_body(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	bp := tls.Alloc(320)
	defer tls.Free(320)
	var dictEnd, dictStart, iend, ip, litBufferEnd, oend, op, ostart, prefixStart, sequence2, v1 uintptr
	var i, seqAdvance, seqNb, v2 int32
	var i1 U32
	var lastLLSize, lastLLSize1, leftoverLit, leftoverLit1, oneSeqSize, oneSeqSize1, oneSeqSize2, oneSeqSize3, prefetchPos size_t
	var sequence, sequence1 seq_t
	var v6 uint64
	var _ /* litPtr at bp+0 */ uintptr
	var _ /* seqState at bp+200 */ seqState_t
	var _ /* sequences at bp+8 */ [8]seq_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dictEnd, dictStart, i, i1, iend, ip, lastLLSize, lastLLSize1, leftoverLit, leftoverLit1, litBufferEnd, oend, oneSeqSize, oneSeqSize1, oneSeqSize2, oneSeqSize3, op, ostart, prefetchPos, prefixStart, seqAdvance, seqNb, sequence, sequence1, sequence2, v1, v2, v6
	ip = seqStart
	iend = ip + uintptr(seqSize)
	ostart = dst
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_in_dst) {
		v1 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	} else {
		v1 = ZSTD_maybeNullPtrAdd(tls, ostart, int64(maxDstSize))
	}
	oend = v1
	op = ostart
	*(*uintptr)(unsafe.Pointer(bp)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr
	litBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd
	prefixStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart
	dictStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart
	dictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	/* Regen sequences */
	if nbSeq != 0 {
		if nbSeq < int32(STORED_SEQS) {
			v2 = nbSeq
		} else {
			v2 = int32(STORED_SEQS)
		}
		seqAdvance = v2
		prefetchPos = uint64(int64(op) - int64(prefixStart)) /* track position relative to prefixStart */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		i = 0
		for {
			if !(i < int32(ZSTD_REP_NUM)) {
				break
			}
			*(*size_t)(unsafe.Pointer(bp + 200 + 88 + uintptr(i)*8)) = uint64(*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i)*4)))
			goto _3
		_3:
			;
			i = i + 1
		}
		if ERR_isError(tls, BIT_initDStream(tls, bp+200, ip, uint64(int64(iend)-int64(ip)))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		ZSTD_initFseState(tls, bp+200+40, bp+200, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr)
		ZSTD_initFseState(tls, bp+200+56, bp+200, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr)
		ZSTD_initFseState(tls, bp+200+72, bp+200, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr)
		/* prepare in advance */
		seqNb = 0
		for {
			if !(seqNb < seqAdvance) {
				break
			}
			sequence = ZSTD_decodeSequence(tls, bp+200, isLongOffset, libc.BoolInt32(seqNb == nbSeq-int32(1)))
			prefetchPos = ZSTD_prefetchMatch(tls, prefetchPos, sequence, prefixStart, dictEnd)
			(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[seqNb] = sequence
			goto _4
		_4:
			;
			seqNb = seqNb + 1
		}
		/* decompress without stomping litBuffer */
		for {
			if !(seqNb < nbSeq) {
				break
			}
			sequence1 = ZSTD_decodeSequence(tls, bp+200, isLongOffset, libc.BoolInt32(seqNb == nbSeq-int32(1)))
			if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) && *(*uintptr)(unsafe.Pointer(bp))+uintptr((*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))].FlitLength) > (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd {
				/* lit buffer is reaching split point, empty out the first buffer and transition to litExtraBuffer */
				leftoverLit = uint64(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
				if leftoverLit != 0 {
					if leftoverLit > uint64(int64(oend)-int64(op)) {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+8456, 0)
						}
						return uint64(-int32(ZSTD_error_dstSize_tooSmall))
					}
					ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(bp)), int64(leftoverLit))
					(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))].FlitLength -= leftoverLit
					op = op + uintptr(leftoverLit)
				}
				*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
				litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
				oneSeqSize = ZSTD_execSequence(tls, op, oend, (*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))], bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				if ZSTD_isError(tls, oneSeqSize) != 0 {
					return oneSeqSize
				}
				prefetchPos = ZSTD_prefetchMatch(tls, prefetchPos, sequence1, prefixStart, dictEnd)
				(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[seqNb&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))] = sequence1
				op = op + uintptr(oneSeqSize)
			} else {
				if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
					v6 = ZSTD_execSequenceSplitLitBuffer(tls, op, oend, *(*uintptr)(unsafe.Pointer(bp))+uintptr((*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))].FlitLength)-uintptr(WILDCOPY_OVERLENGTH), (*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))], bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				} else {
					v6 = ZSTD_execSequence(tls, op, oend, (*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))], bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				}
				/* lit buffer is either wholly contained in first or second split, or not split at all*/
				oneSeqSize1 = v6
				if ZSTD_isError(tls, oneSeqSize1) != 0 {
					return oneSeqSize1
				}
				prefetchPos = ZSTD_prefetchMatch(tls, prefetchPos, sequence1, prefixStart, dictEnd)
				(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[seqNb&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))] = sequence1
				op = op + uintptr(oneSeqSize1)
			}
			goto _5
		_5:
			;
			seqNb = seqNb + 1
		}
		if !(BIT_endOfDStream(tls, bp+200) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_corruption_detected))
		}
		/* finish queue */
		seqNb = seqNb - seqAdvance
		for {
			if !(seqNb < nbSeq) {
				break
			}
			sequence2 = bp + 8 + uintptr(seqNb&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1)))*24
			if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) && *(*uintptr)(unsafe.Pointer(bp))+uintptr((*seq_t)(unsafe.Pointer(sequence2)).FlitLength) > (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd {
				leftoverLit1 = uint64(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
				if leftoverLit1 != 0 {
					if leftoverLit1 > uint64(int64(oend)-int64(op)) {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+8456, 0)
						}
						return uint64(-int32(ZSTD_error_dstSize_tooSmall))
					}
					ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(bp)), int64(leftoverLit1))
					*(*size_t)(unsafe.Pointer(sequence2)) -= leftoverLit1
					op = op + uintptr(leftoverLit1)
				}
				*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
				litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
				oneSeqSize2 = ZSTD_execSequence(tls, op, oend, *(*seq_t)(unsafe.Pointer(sequence2)), bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				if ZSTD_isError(tls, oneSeqSize2) != 0 {
					return oneSeqSize2
				}
				op = op + uintptr(oneSeqSize2)
			} else {
				if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
					v6 = ZSTD_execSequenceSplitLitBuffer(tls, op, oend, *(*uintptr)(unsafe.Pointer(bp))+uintptr((*seq_t)(unsafe.Pointer(sequence2)).FlitLength)-uintptr(WILDCOPY_OVERLENGTH), *(*seq_t)(unsafe.Pointer(sequence2)), bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				} else {
					v6 = ZSTD_execSequence(tls, op, oend, *(*seq_t)(unsafe.Pointer(sequence2)), bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				}
				oneSeqSize3 = v6
				if ZSTD_isError(tls, oneSeqSize3) != 0 {
					return oneSeqSize3
				}
				op = op + uintptr(oneSeqSize3)
			}
			goto _7
		_7:
			;
			seqNb = seqNb + 1
		}
		/* save reps for next block */
		i1 = uint32(0)
		for {
			if !(i1 < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i1)*4)) = uint32(*(*size_t)(unsafe.Pointer(bp + 200 + 88 + uintptr(i1)*8)))
			goto _9
		_9:
			;
			i1 = i1 + 1
		}
	}
	/* last literal segment */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) { /* first deplete literal buffer in dst, then copy litExtraBuffer */
		lastLLSize = uint64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
		if lastLLSize > uint64(int64(oend)-int64(op)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		if op != libc.UintptrFromInt32(0) {
			libc.Xmemmove(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize)
			op = op + uintptr(lastLLSize)
		}
		*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
		litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
	}
	lastLLSize1 = uint64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
	if lastLLSize1 > uint64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if op != libc.UintptrFromInt32(0) {
		libc.Xmemmove(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize1)
		op = op + uintptr(lastLLSize1)
	}
	return uint64(int64(op) - int64(ostart))
}

func ZSTD_decompressSequencesLong_default(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequencesLong_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequences_bmi2(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesSplitLitBuffer_bmi2(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_bodySplitLitBuffer(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesLong_bmi2(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequencesLong_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequences(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		return ZSTD_decompressSequences_bmi2(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
	}
	return ZSTD_decompressSequences_default(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesSplitLitBuffer(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		return ZSTD_decompressSequencesSplitLitBuffer_bmi2(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
	}
	return ZSTD_decompressSequencesSplitLitBuffer_default(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

// C documentation
//
//	/* ZSTD_decompressSequencesLong() :
//	 * decompression function triggered when a minimum share of offsets is considered "long",
//	 * aka out of cache.
//	 * note : "long" definition seems overloaded here, sometimes meaning "wider than bitstream register", and sometimes meaning "farther than memory cache distance".
//	 * This function will try to mitigate main memory latency through the use of prefetching */
func ZSTD_decompressSequencesLong(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		return ZSTD_decompressSequencesLong_bmi2(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
	}
	return ZSTD_decompressSequencesLong_default(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

// C documentation
//
//	/**
//	 * @returns The total size of the history referenceable by zstd, including
//	 * both the prefix and the extDict. At @p op any offset larger than this
//	 * is invalid.
//	 */
func ZSTD_totalHistorySize(tls *libc.TLS, op uintptr, virtualStart uintptr) (r size_t) {
	return uint64(int64(op) - int64(virtualStart))
}

type ZSTD_OffsetInfo = struct {
	FlongOffsetShare     uint32
	FmaxNbAdditionalBits uint32
}

// C documentation
//
//	/* ZSTD_getOffsetInfo() :
//	 * condition : offTable must be valid
//	 * @return : "share" of long offsets (arbitrarily defined as > (1<<23))
//	 *           compared to maximum possible of (1<<OffFSELog),
//	 *           as well as the maximum number additional bits required.
//	 */
func ZSTD_getOffsetInfo(tls *libc.TLS, offTable uintptr, nbSeq int32) (r ZSTD_OffsetInfo) {
	var info ZSTD_OffsetInfo
	var max, tableLog, u U32
	var ptr, table uintptr
	var v2 uint32
	_, _, _, _, _, _, _ = info, max, ptr, table, tableLog, u, v2
	info = ZSTD_OffsetInfo{}
	/* If nbSeq == 0, then the offTable is uninitialized, but we have
	 * no sequences, so both values should be 0.
	 */
	if nbSeq != 0 {
		ptr = offTable
		tableLog = (*(*ZSTD_seqSymbol_header)(unsafe.Pointer(ptr))).FtableLog
		table = offTable + uintptr(1)*8
		max = uint32(int32(1) << tableLog)
		/* max not too large */
		u = uint32(0)
		for {
			if !(u < max) {
				break
			}
			if info.FmaxNbAdditionalBits > uint32((*(*ZSTD_seqSymbol)(unsafe.Pointer(table + uintptr(u)*8))).FnbAdditionalBits) {
				v2 = info.FmaxNbAdditionalBits
			} else {
				v2 = uint32((*(*ZSTD_seqSymbol)(unsafe.Pointer(table + uintptr(u)*8))).FnbAdditionalBits)
			}
			info.FmaxNbAdditionalBits = v2
			if int32((*(*ZSTD_seqSymbol)(unsafe.Pointer(table + uintptr(u)*8))).FnbAdditionalBits) > int32(22) {
				info.FlongOffsetShare += uint32(1)
			}
			goto _1
		_1:
			;
			u = u + 1
		}
		info.FlongOffsetShare <<= uint32(OffFSELog) - tableLog /* scale to OffFSELog */
	}
	return info
}

// C documentation
//
//	/**
//	 * @returns The maximum offset we can decode in one read of our bitstream, without
//	 * reloading more bits in the middle of the offset bits read. Any offsets larger
//	 * than this must use the long offset decoder.
//	 */
func ZSTD_maxShortOffset(tls *libc.TLS) (r size_t) {
	var maxOffbase, maxOffset size_t
	var v1 int32
	_, _, _ = maxOffbase, maxOffset, v1
	if MEM_64bits(tls) != 0 {
		/* We can decode any offset without reloading bits.
		 * This might change if the max window size grows.
		 */
		_ = libc.Uint64FromInt64(1)
		return uint64(-libc.Int32FromInt32(1))
	} else {
		if MEM_32bits(tls) != 0 {
			v1 = int32(STREAM_ACCUMULATOR_MIN_32)
		} else {
			v1 = int32(STREAM_ACCUMULATOR_MIN_64)
		}
		/* The maximum offBase is (1 << (STREAM_ACCUMULATOR_MIN + 1)) - 1.
		 * This offBase would require STREAM_ACCUMULATOR_MIN extra bits.
		 * Then we have to subtract ZSTD_REP_NUM to get the maximum possible offset.
		 */
		maxOffbase = libc.Uint64FromInt32(1)<<(uint32(v1)+libc.Uint32FromInt32(1)) - uint64(1)
		maxOffset = maxOffbase - uint64(ZSTD_REP_NUM)
		return maxOffset
	}
	return r
}

func ZSTD_decompressBlock_internal(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, streaming streaming_operation) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var blockSizeMax, litCSize, seqHSize, totalHistorySize size_t
	var info ZSTD_OffsetInfo
	var ip uintptr
	var isLongOffset ZSTD_longOffset_e
	var minShare U32
	var usePrefetchDecoder, v2 int32
	var v1 uint64
	var v3 bool
	var _ /* nbSeq at bp+0 */ int32
	_, _, _, _, _, _, _, _, _, _, _, _ = blockSizeMax, info, ip, isLongOffset, litCSize, minShare, seqHSize, totalHistorySize, usePrefetchDecoder, v1, v2, v3 /* blockType == blockCompressed */
	ip = src
	/* Note : the wording of the specification
	 * allows compressed block to be sized exactly ZSTD_blockSizeMax(dctx).
	 * This generally does not happen, as it makes little sense,
	 * since an uncompressed block would feature same size and have no decompression cost.
	 * Also, note that decoder from reference libzstd before < v1.5.4
	 * would consider this edge case as an error.
	 * As a consequence, avoid generating compressed blocks of size ZSTD_blockSizeMax(dctx)
	 * for broader compatibility with the deployed ecosystem of zstd decoders */
	if srcSize > ZSTD_blockSizeMax(tls, dctx) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Decode literals section */
	litCSize = ZSTD_decodeLiteralsBlock(tls, dctx, src, srcSize, dst, dstCapacity, streaming)
	if ZSTD_isError(tls, litCSize) != 0 {
		return litCSize
	}
	ip = ip + uintptr(litCSize)
	srcSize = srcSize - litCSize
	/* Build Decoding Tables */
	if dstCapacity < ZSTD_blockSizeMax(tls, dctx) {
		v1 = dstCapacity
	} else {
		v1 = ZSTD_blockSizeMax(tls, dctx)
	}
	/* Compute the maximum block size, which must also work when !frame and fParams are unset.
	 * Additionally, take the min with dstCapacity to ensure that the totalHistorySize fits in a size_t.
	 */
	blockSizeMax = v1
	totalHistorySize = ZSTD_totalHistorySize(tls, ZSTD_maybeNullPtrAdd(tls, dst, int64(blockSizeMax)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart)
	/* isLongOffset must be true if there are long offsets.
	 * Offsets are long if they are larger than ZSTD_maxShortOffset().
	 * We don't expect that to be the case in 64-bit mode.
	 *
	 * We check here to see if our history is large enough to allow long offsets.
	 * If it isn't, then we can't possible have (valid) long offsets. If the offset
	 * is invalid, then it is okay to read it incorrectly.
	 *
	 * If isLongOffsets is true, then we will later check our decoding table to see
	 * if it is even possible to generate long offsets.
	 */
	isLongOffset = libc.BoolInt32(MEM_32bits(tls) != 0 && totalHistorySize > ZSTD_maxShortOffset(tls))
	/* These macros control at build-time which decompressor implementation
	 * we use. If neither is defined, we do some inspection and dispatch at
	 * runtime.
	 */
	usePrefetchDecoder = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold
	seqHSize = ZSTD_decodeSeqHeaders(tls, dctx, bp, ip, srcSize)
	if ZSTD_isError(tls, seqHSize) != 0 {
		return seqHSize
	}
	ip = ip + uintptr(seqHSize)
	srcSize = srcSize - seqHSize
	if (dst == libc.UintptrFromInt32(0) || dstCapacity == uint64(0)) && *(*int32)(unsafe.Pointer(bp)) > 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if MEM_64bits(tls) != 0 && libc.Bool(uint64(8) == uint64(8)) && uint64(-libc.Int32FromInt32(1))-uint64(dst) < uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8496, 0)
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* If we could potentially have long offsets, or we might want to use the prefetch decoder,
	 * compute information about the share of long offsets, and the maximum nbAdditionalBits.
	 * NOTE: could probably use a larger nbSeq limit
	 */
	if isLongOffset != 0 || !(usePrefetchDecoder != 0) && totalHistorySize > uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) && *(*int32)(unsafe.Pointer(bp)) > int32(8) {
		info = ZSTD_getOffsetInfo(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr, *(*int32)(unsafe.Pointer(bp)))
		if v3 = isLongOffset != 0; v3 {
			if MEM_32bits(tls) != 0 {
				v2 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v2 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
		}
		if v3 && info.FmaxNbAdditionalBits <= uint32(v2) {
			/* If isLongOffset, but the maximum number of additional bits that we see in our table is small
			 * enough, then we know it is impossible to have too long an offset in this block, so we can
			 * use the regular offset decoder.
			 */
			isLongOffset = int32(ZSTD_lo_isRegularOffset)
		}
		if !(usePrefetchDecoder != 0) {
			if MEM_64bits(tls) != 0 {
				v2 = int32(7)
			} else {
				v2 = int32(20)
			}
			minShare = uint32(v2) /* heuristic values, correspond to 2.73% and 7.81% */
			usePrefetchDecoder = libc.BoolInt32(info.FlongOffsetShare >= minShare)
		}
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold = 0
	if usePrefetchDecoder != 0 {
		return ZSTD_decompressSequencesLong(tls, dctx, dst, dstCapacity, ip, srcSize, *(*int32)(unsafe.Pointer(bp)), isLongOffset)
	}
	/* else */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		return ZSTD_decompressSequencesSplitLitBuffer(tls, dctx, dst, dstCapacity, ip, srcSize, *(*int32)(unsafe.Pointer(bp)), isLongOffset)
	} else {
		return ZSTD_decompressSequences(tls, dctx, dst, dstCapacity, ip, srcSize, *(*int32)(unsafe.Pointer(bp)), isLongOffset)
	}
	return r
}

func ZSTD_checkContinuity(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t) {
	if dst != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd && dstSize > uint64(0) { /* not contiguous */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = dst - uintptr(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd)-int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart))
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = dst
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dst
	}
}

func ZSTD_decompressBlock_deprecated(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var dSize, err_code size_t
	_, _ = dSize, err_code
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = 0
	ZSTD_checkContinuity(tls, dctx, dst, dstCapacity)
	dSize = ZSTD_decompressBlock_internal(tls, dctx, dst, dstCapacity, src, srcSize, int32(not_streaming))
	err_code = dSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dst + uintptr(dSize)
	return dSize
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_decompressBlock_deprecated() */
func ZSTD_decompressBlock(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_decompressBlock_deprecated(tls, dctx, dst, dstCapacity, src, srcSize)
}

type _iobuf = struct {
	F_ptr      uintptr
	F_cnt      int32
	F_base     uintptr
	F_flag     int32
	F_file     int32
	F_charbuf  int32
	F_bufsiz   int32
	F_tmpfname uintptr
}

type FILE = struct {
	F_ptr      uintptr
	F_cnt      int32
	F_base     uintptr
	F_flag     int32
	F_file     int32
	F_charbuf  int32
	F_bufsiz   int32
	F_tmpfname uintptr
}

type fpos_t = int64

type __timeb32 = struct {
	Ftime     __time32_t
	Fmillitm  uint16
	Ftimezone int16
	Fdstflag  int16
}

type timeb = struct {
	Ftime     time_t
	Fmillitm  uint16
	Ftimezone int16
	Fdstflag  int16
}

type __timeb64 = struct {
	Ftime     __time64_t
	Fmillitm  uint16
	Ftimezone int16
	Fdstflag  int16
}

type _timespec32 = struct {
	Ftv_sec  __time32_t
	Ftv_nsec int32
}

type _timespec64 = struct {
	Ftv_sec  __time64_t
	Ftv_nsec int32
}

type clock_t = int32

type tm = struct {
	Ftm_sec   int32
	Ftm_min   int32
	Ftm_hour  int32
	Ftm_mday  int32
	Ftm_mon   int32
	Ftm_year  int32
	Ftm_wday  int32
	Ftm_yday  int32
	Ftm_isdst int32
}

type timeval = struct {
	Ftv_sec  int32
	Ftv_usec int32
}

type timezone1 = struct {
	Ftz_minuteswest int32
	Ftz_dsttime     int32
}

type clockid_t = int32

type ZDICT_params_t = struct {
	FcompressionLevel  int32
	FnotificationLevel uint32
	FdictID            uint32
}

/* This can be overridden externally to hide static symbols. */

/* ====================================================================================
 * The definitions in this section are considered experimental.
 * They should never be used with a dynamic library, as they may change in the future.
 * They are provided for advanced usages.
 * Use them only in association with static linking.
 * ==================================================================================== */

/* Deprecated: Remove in v1.6.0 */

// C documentation
//
//	/*! ZDICT_cover_params_t:
//	 *  k and d are the only required parameters.
//	 *  For others, value 0 means default.
//	 */
type ZDICT_cover_params_t = struct {
	Fk                       uint32
	Fd                       uint32
	Fsteps                   uint32
	FnbThreads               uint32
	FsplitPoint              float64
	FshrinkDict              uint32
	FshrinkDictMaxRegression uint32
	FzParams                 ZDICT_params_t
}

type ZDICT_fastCover_params_t = struct {
	Fk                       uint32
	Fd                       uint32
	Ff                       uint32
	Fsteps                   uint32
	FnbThreads               uint32
	FsplitPoint              float64
	Faccel                   uint32
	FshrinkDict              uint32
	FshrinkDictMaxRegression uint32
	FzParams                 ZDICT_params_t
}

type ZDICT_legacy_params_t = struct {
	FselectivityLevel uint32
	FzParams          ZDICT_params_t
}

/**** ended inlining ../zdict.h ****/
/**** start inlining cover.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/threading.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../zdict.h ****/

// C documentation
//
//	/**
//	 * COVER_best_t is used for two purposes:
//	 * 1. Synchronizing threads.
//	 * 2. Saving the best parameters and dictionary.
//	 *
//	 * All of the methods except COVER_best_init() are thread safe if zstd is
//	 * compiled with multithreaded support.
//	 */
type COVER_best_t = struct {
	Fmutex          CRITICAL_SECTION
	Fcond           CONDITION_VARIABLE
	FliveJobs       size_t
	Fdict           uintptr
	FdictSize       size_t
	Fparameters     ZDICT_cover_params_t
	FcompressedSize size_t
}

/**** ended inlining ../zdict.h ****/
/**** start inlining cover.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/threading.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../zdict.h ****/

// C documentation
//
//	/**
//	 * COVER_best_t is used for two purposes:
//	 * 1. Synchronizing threads.
//	 * 2. Saving the best parameters and dictionary.
//	 *
//	 * All of the methods except COVER_best_init() are thread safe if zstd is
//	 * compiled with multithreaded support.
//	 */
type COVER_best_s = COVER_best_t

// C documentation
//
//	/**
//	 * A segment is a range in the source as well as the score of the segment.
//	 */
type COVER_segment_t = struct {
	Fbegin U32
	Fend   U32
	Fscore U32
}

// C documentation
//
//	/**
//	 *Number of epochs and size of each epoch.
//	 */
type COVER_epoch_info_t = struct {
	Fnum  U32
	Fsize U32
}

// C documentation
//
//	/**
//	 * Struct used for the dictionary selection function.
//	 */
type COVER_dictSelection_t = struct {
	FdictContent         uintptr
	FdictSize            size_t
	FtotalCompressedSize size_t
}

// C documentation
//
//	/**
//	 * Struct used for the dictionary selection function.
//	 */
type COVER_dictSelection = COVER_dictSelection_t

/**** ended inlining cover.h ****/

/*-*************************************
*  Constants
***************************************/
/**
* There are 32bit indexes used to ref samples, so limit samples size to 4GB
* on 64bit builds.
* For 32bit builds we choose 1 GB.
* Most 32bit platforms have 2GB user-mode addressable space and we allocate a large
* contiguous buffer, so 1GB is already a high limit.
 */

// C documentation
//
//	/*-*************************************
//	*  Console display
//	***************************************/
var g_displayLevel = int32(0)

var g_refreshRate = int32(libc.Int32FromInt32(CLOCKS_PER_SEC) * libc.Int32FromInt32(15) / libc.Int32FromInt32(100))
var g_time = int32(0)

/*-*************************************
* Hash table
***************************************
* A small specialized hash map for storing activeDmers.
* The map does not resize, so if it becomes full it will loop forever.
* Thus, the map must be large enough to store every value.
* The map implements linear probing and keeps its load less than 0.5.
 */

type COVER_map_pair_t = struct {
	Fkey   U32
	Fvalue U32
}

/*-*************************************
* Hash table
***************************************
* A small specialized hash map for storing activeDmers.
* The map does not resize, so if it becomes full it will loop forever.
* Thus, the map must be large enough to store every value.
* The map implements linear probing and keeps its load less than 0.5.
 */

type COVER_map_pair_t_s = COVER_map_pair_t

type COVER_map_t = struct {
	Fdata     uintptr
	FsizeLog  U32
	Fsize     U32
	FsizeMask U32
}

type COVER_map_s = COVER_map_t

// C documentation
//
//	/**
//	 * Clear the map.
//	 */
func COVER_map_clear(tls *libc.TLS, map1 uintptr) {
	libc.Xmemset(tls, (*COVER_map_t)(unsafe.Pointer(map1)).Fdata, int32(uint32(-libc.Int32FromInt32(1))), uint64((*COVER_map_t)(unsafe.Pointer(map1)).Fsize)*uint64(8))
}

// C documentation
//
//	/**
//	 * Initializes a map of the given size.
//	 * Returns 1 on success and 0 on failure.
//	 * The map must be destroyed with COVER_map_destroy().
//	 * The map is only guaranteed to be large enough to hold size elements.
//	 */
func COVER_map_init(tls *libc.TLS, map1 uintptr, size U32) (r int32) {
	(*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog = ZSTD_highbit32(tls, size) + uint32(2)
	(*COVER_map_t)(unsafe.Pointer(map1)).Fsize = libc.Uint32FromInt32(1) << (*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog
	(*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask = (*COVER_map_t)(unsafe.Pointer(map1)).Fsize - uint32(1)
	(*COVER_map_t)(unsafe.Pointer(map1)).Fdata = libc.Xmalloc(tls, uint64((*COVER_map_t)(unsafe.Pointer(map1)).Fsize)*uint64(8))
	if !((*COVER_map_t)(unsafe.Pointer(map1)).Fdata != 0) {
		(*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog = uint32(0)
		(*COVER_map_t)(unsafe.Pointer(map1)).Fsize = uint32(0)
		return 0
	}
	COVER_map_clear(tls, map1)
	return int32(1)
}

// C documentation
//
//	/**
//	 * Internal hash function
//	 */
var COVER_prime4bytes = uint32(2654435761)

func COVER_map_hash(tls *libc.TLS, map1 uintptr, key U32) (r U32) {
	return key * COVER_prime4bytes >> (uint32(32) - (*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog)
}

// C documentation
//
//	/**
//	 * Helper function that returns the index that a key should be placed into.
//	 */
func COVER_map_index(tls *libc.TLS, map1 uintptr, key U32) (r U32) {
	var hash, i U32
	var pos uintptr
	_, _, _ = hash, i, pos
	hash = COVER_map_hash(tls, map1, key)
	i = hash
	for {
		pos = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(i)*8
		if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue == uint32(-libc.Int32FromInt32(1)) {
			return i
		}
		if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey == key {
			return i
		}
		goto _1
	_1:
		;
		i = (i + uint32(1)) & (*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask
	}
	return r
}

// C documentation
//
//	/**
//	 * Returns the pointer to the value for key.
//	 * If key is not in the map, it is inserted and the value is set to 0.
//	 * The map must not be full.
//	 */
func COVER_map_at(tls *libc.TLS, map1 uintptr, key U32) (r uintptr) {
	var pos uintptr
	_ = pos
	pos = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(COVER_map_index(tls, map1, key))*8
	if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue == uint32(-libc.Int32FromInt32(1)) {
		(*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey = key
		(*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue = uint32(0)
	}
	return pos + 4
}

// C documentation
//
//	/**
//	 * Deletes key from the map if present.
//	 */
func COVER_map_remove(tls *libc.TLS, map1 uintptr, key U32) {
	var del, pos uintptr
	var i, shift U32
	_, _, _, _ = del, i, pos, shift
	i = COVER_map_index(tls, map1, key)
	del = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(i)*8
	shift = uint32(1)
	if (*COVER_map_pair_t)(unsafe.Pointer(del)).Fvalue == uint32(-libc.Int32FromInt32(1)) {
		return
	}
	i = (i + uint32(1)) & (*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask
	for {
		pos = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(i)*8
		/* If the position is empty we are done */
		if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue == uint32(-libc.Int32FromInt32(1)) {
			(*COVER_map_pair_t)(unsafe.Pointer(del)).Fvalue = uint32(-libc.Int32FromInt32(1))
			return
		}
		/* If pos can be moved to del do so */
		if (i-COVER_map_hash(tls, map1, (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey))&(*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask >= shift {
			(*COVER_map_pair_t)(unsafe.Pointer(del)).Fkey = (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey
			(*COVER_map_pair_t)(unsafe.Pointer(del)).Fvalue = (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue
			del = pos
			shift = uint32(1)
		} else {
			shift = shift + 1
		}
		goto _1
	_1:
		;
		i = (i + uint32(1)) & (*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask
	}
}

// C documentation
//
//	/**
//	 * Destroys a map that is inited with COVER_map_init().
//	 */
func COVER_map_destroy(tls *libc.TLS, map1 uintptr) {
	if (*COVER_map_t)(unsafe.Pointer(map1)).Fdata != 0 {
		libc.Xfree(tls, (*COVER_map_t)(unsafe.Pointer(map1)).Fdata)
	}
	(*COVER_map_t)(unsafe.Pointer(map1)).Fdata = libc.UintptrFromInt32(0)
	(*COVER_map_t)(unsafe.Pointer(map1)).Fsize = uint32(0)
}

/*-*************************************
* Context
***************************************/

type COVER_ctx_t = struct {
	Fsamples        uintptr
	Foffsets        uintptr
	FsamplesSizes   uintptr
	FnbSamples      size_t
	FnbTrainSamples size_t
	FnbTestSamples  size_t
	Fsuffix         uintptr
	FsuffixSize     size_t
	Ffreqs          uintptr
	FdmerAt         uintptr
	Fd              uint32
}

// C documentation
//
//	/* C90 only offers qsort() that needs a global context. */
var g_coverCtx = libc.UintptrFromInt32(0)

/*-*************************************
*  Helper functions
***************************************/

// C documentation
//
//	/**
//	 * Returns the sum of the sample sizes.
//	 */
func COVER_sum(tls *libc.TLS, samplesSizes uintptr, nbSamples uint32) (r size_t) {
	var i uint32
	var sum size_t
	_, _ = i, sum
	sum = uint64(0)
	i = uint32(0)
	for {
		if !(i < nbSamples) {
			break
		}
		sum = sum + *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8))
		goto _1
	_1:
		;
		i = i + 1
	}
	return sum
}

// C documentation
//
//	/**
//	 * Returns -1 if the dmer at lp is less than the dmer at rp.
//	 * Return 0 if the dmers at lp and rp are equal.
//	 * Returns 1 if the dmer at lp is greater than the dmer at rp.
//	 */
func COVER_cmp(tls *libc.TLS, ctx uintptr, lp uintptr, rp uintptr) (r int32) {
	var lhs, rhs U32
	_, _ = lhs, rhs
	lhs = *(*U32)(unsafe.Pointer(lp))
	rhs = *(*U32)(unsafe.Pointer(rp))
	return libc.Xmemcmp(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(lhs), (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(rhs), uint64((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd))
}

// C documentation
//
//	/**
//	 * Faster version for d <= 8.
//	 */
func COVER_cmp8(tls *libc.TLS, ctx uintptr, lp uintptr, rp uintptr) (r int32) {
	var lhs, mask, rhs U64
	var v1 uint64
	_, _, _, _ = lhs, mask, rhs, v1
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd == uint32(8) {
		v1 = uint64(-libc.Int32FromInt32(1))
	} else {
		v1 = libc.Uint64FromInt32(1)<<(libc.Uint32FromInt32(8)*(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd) - uint64(1)
	}
	mask = v1
	lhs = MEM_readLE64(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(*(*U32)(unsafe.Pointer(lp)))) & mask
	rhs = MEM_readLE64(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(*(*U32)(unsafe.Pointer(rp)))) & mask
	if lhs < rhs {
		return -int32(1)
	}
	return libc.BoolInt32(lhs > rhs)
}

// C documentation
//
//	/**
//	 * Same as COVER_cmp() except ties are broken by pointer value
//	 */
func COVER_strict_cmp(tls *libc.TLS, lp uintptr, rp uintptr) (r int32) {
	var result, v1 int32
	_, _ = result, v1
	result = COVER_cmp(tls, g_coverCtx, lp, rp)
	if result == 0 {
		if lp < rp {
			v1 = -int32(1)
		} else {
			v1 = int32(1)
		}
		result = v1
	}
	return result
}

// C documentation
//
//	/**
//	 * Faster version for d <= 8.
//	 */
func COVER_strict_cmp8(tls *libc.TLS, lp uintptr, rp uintptr) (r int32) {
	var result, v1 int32
	_, _ = result, v1
	result = COVER_cmp8(tls, g_coverCtx, lp, rp)
	if result == 0 {
		if lp < rp {
			v1 = -int32(1)
		} else {
			v1 = int32(1)
		}
		result = v1
	}
	return result
}

// C documentation
//
//	/**
//	 * Abstract away divergence of qsort_r() parameters.
//	 * Hopefully when C11 become the norm, we will be able
//	 * to clean it up.
//	 */
func stableSort(tls *libc.TLS, ctx uintptr) {
	var v1 uintptr
	_ = v1
	g_coverCtx = ctx
	/* TODO(cavalcanti): implement a reentrant qsort() when is not available. */
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd <= uint32(8) {
		v1 = __ccgo_fp(COVER_strict_cmp8)
	} else {
		v1 = __ccgo_fp(COVER_strict_cmp)
	}
	libc.Xqsort(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize, uint64(4), v1)
}

// C documentation
//
//	/**
//	 * Returns the first pointer in [first, last) whose element does not compare
//	 * less than value.  If no such element exists it returns last.
//	 */
func COVER_lower_bound(tls *libc.TLS, first uintptr, last uintptr, value size_t) (r uintptr) {
	var count, step size_t
	var ptr, v1 uintptr
	_, _, _, _ = count, ptr, step, v1
	count = uint64((int64(last) - int64(first)) / 8)
	for count != uint64(0) {
		step = count / uint64(2)
		ptr = first
		ptr = ptr + uintptr(step)*8
		if *(*size_t)(unsafe.Pointer(ptr)) < value {
			ptr += 8
			v1 = ptr
			first = v1
			count = count - (step + uint64(1))
		} else {
			count = step
		}
	}
	return first
}

// C documentation
//
//	/**
//	 * Generic groupBy function.
//	 * Groups an array sorted by cmp into groups with equivalent values.
//	 * Calls grp for each group.
//	 */
func COVER_groupBy(tls *libc.TLS, data uintptr, count size_t, size size_t, ctx uintptr, __ccgo_fp_cmp uintptr, __ccgo_fp_grp uintptr) {
	var grpEnd, ptr uintptr
	var num size_t
	_, _, _ = grpEnd, num, ptr
	ptr = data
	num = uint64(0)
	for num < count {
		grpEnd = ptr + uintptr(size)
		num = num + 1
		for num < count && (*(*func(*libc.TLS, uintptr, uintptr, uintptr) int32)(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_cmp})))(tls, ctx, ptr, grpEnd) == 0 {
			grpEnd = grpEnd + uintptr(size)
			num = num + 1
		}
		(*(*func(*libc.TLS, uintptr, uintptr, uintptr))(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_grp})))(tls, ctx, ptr, grpEnd)
		ptr = grpEnd
	}
}

/*-*************************************
*  Cover functions
***************************************/

// C documentation
//
//	/**
//	 * Called on each group of positions with the same dmer.
//	 * Counts the frequency of each dmer and saves it in the suffix array.
//	 * Fills `ctx->dmerAt`.
//	 */
func COVER_group(tls *libc.TLS, ctx uintptr, group uintptr, groupEnd uintptr) {
	var curOffsetPtr, grpEnd, grpPtr, offsetsEnd, sampleEndPtr uintptr
	var curSampleEnd size_t
	var dmerId, freq U32
	_, _, _, _, _, _, _, _ = curOffsetPtr, curSampleEnd, dmerId, freq, grpEnd, grpPtr, offsetsEnd, sampleEndPtr
	/* The group consists of all the positions with the same first d bytes. */
	grpPtr = group
	grpEnd = groupEnd
	/* The dmerId is how we will reference this dmer.
	 * This allows us to map the whole dmer space to a much smaller space, the
	 * size of the suffix array.
	 */
	dmerId = uint32((int64(grpPtr) - int64((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix)) / 4)
	/* Count the number of samples this dmer shows up in */
	freq = uint32(0)
	/* Details */
	curOffsetPtr = (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets
	offsetsEnd = (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr((*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples)*8
	/* Once *grpPtr >= curSampleEnd this occurrence of the dmer is in a
	 * different sample than the last.
	 */
	curSampleEnd = *(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets))
	for {
		if !(grpPtr != grpEnd) {
			break
		}
		/* Save the dmerId for this position so we can get back to it. */
		*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(*(*U32)(unsafe.Pointer(grpPtr)))*4)) = dmerId
		/* Dictionaries only help for the first reference to the dmer.
		 * After that zstd can reference the match from the previous reference.
		 * So only count each dmer once for each sample it is in.
		 */
		if uint64(*(*U32)(unsafe.Pointer(grpPtr))) < curSampleEnd {
			goto _1
		}
		freq = freq + uint32(1)
		/* Binary search to find the end of the sample *grpPtr is in.
		 * In the common case that grpPtr + 1 == grpEnd we can skip the binary
		 * search because the loop is over.
		 */
		if grpPtr+uintptr(1)*4 != grpEnd {
			sampleEndPtr = COVER_lower_bound(tls, curOffsetPtr, offsetsEnd, uint64(*(*U32)(unsafe.Pointer(grpPtr))))
			curSampleEnd = *(*size_t)(unsafe.Pointer(sampleEndPtr))
			curOffsetPtr = sampleEndPtr + uintptr(1)*8
		}
		goto _1
	_1:
		;
		grpPtr += 4
	}
	/* At this point we are never going to look at this segment of the suffix
	 * array again.  We take advantage of this fact to save memory.
	 * We store the frequency of the dmer in the first position of the group,
	 * which is dmerId.
	 */
	*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix + uintptr(dmerId)*4)) = freq
}

// C documentation
//
//	/**
//	 * Selects the best segment in an epoch.
//	 * Segments of are scored according to the function:
//	 *
//	 * Let F(d) be the frequency of dmer d.
//	 * Let S_i be the dmer at position i of segment S which has length k.
//	 *
//	 *     Score(S) = F(S_1) + F(S_2) + ... + F(S_{k-d+1})
//	 *
//	 * Once the dmer d is in the dictionary we set F(d) = 0.
//	 */
func COVER_selectSegment(tls *libc.TLS, ctx uintptr, freqs uintptr, activeDmers uintptr, begin U32, end U32, parameters ZDICT_cover_params_t) (r COVER_segment_t) {
	var activeSegment, bestSegment COVER_segment_t
	var d, delDmer, dmersInK, freq, k, newBegin, newDmer, newEnd, pos, pos1 U32
	var delDmerOcc, newDmerOcc uintptr
	var v2 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = activeSegment, bestSegment, d, delDmer, delDmerOcc, dmersInK, freq, k, newBegin, newDmer, newDmerOcc, newEnd, pos, pos1, v2
	/* Constants */
	k = parameters.Fk
	d = parameters.Fd
	dmersInK = k - d + uint32(1)
	/* Try each segment (activeSegment) and save the best (bestSegment) */
	bestSegment = COVER_segment_t{}
	/* Reset the activeDmers in the segment */
	COVER_map_clear(tls, activeDmers)
	/* The activeSegment starts at the beginning of the epoch. */
	activeSegment.Fbegin = begin
	activeSegment.Fend = begin
	activeSegment.Fscore = uint32(0)
	/* Slide the activeSegment through the whole epoch.
	 * Save the best segment in bestSegment.
	 */
	for activeSegment.Fend < end {
		/* The dmerId for the dmer at the next position */
		newDmer = *(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(activeSegment.Fend)*4))
		/* The entry in activeDmers for this dmerId */
		newDmerOcc = COVER_map_at(tls, activeDmers, newDmer)
		/* If the dmer isn't already present in the segment add its score. */
		if *(*U32)(unsafe.Pointer(newDmerOcc)) == uint32(0) {
			/* The paper suggest using the L-0.5 norm, but experiments show that it
			 * doesn't help.
			 */
			activeSegment.Fscore += *(*U32)(unsafe.Pointer(freqs + uintptr(newDmer)*4))
		}
		/* Add the dmer to the segment */
		activeSegment.Fend += uint32(1)
		*(*U32)(unsafe.Pointer(newDmerOcc)) += uint32(1)
		/* If the window is now too large, drop the first position */
		if activeSegment.Fend-activeSegment.Fbegin == dmersInK+uint32(1) {
			delDmer = *(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(activeSegment.Fbegin)*4))
			delDmerOcc = COVER_map_at(tls, activeDmers, delDmer)
			activeSegment.Fbegin += uint32(1)
			*(*U32)(unsafe.Pointer(delDmerOcc)) -= uint32(1)
			/* If this is the last occurrence of the dmer, subtract its score */
			if *(*U32)(unsafe.Pointer(delDmerOcc)) == uint32(0) {
				COVER_map_remove(tls, activeDmers, delDmer)
				activeSegment.Fscore -= *(*U32)(unsafe.Pointer(freqs + uintptr(delDmer)*4))
			}
		}
		/* If this segment is the best so far save it */
		if activeSegment.Fscore > bestSegment.Fscore {
			bestSegment = activeSegment
		}
	}
	/* Trim off the zero frequency head and tail from the segment. */
	newBegin = bestSegment.Fend
	newEnd = bestSegment.Fbegin
	pos = bestSegment.Fbegin
	for {
		if !(pos != bestSegment.Fend) {
			break
		}
		freq = *(*U32)(unsafe.Pointer(freqs + uintptr(*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(pos)*4)))*4))
		if freq != uint32(0) {
			if newBegin < pos {
				v2 = newBegin
			} else {
				v2 = pos
			}
			newBegin = v2
			newEnd = pos + uint32(1)
		}
		goto _1
	_1:
		;
		pos = pos + 1
	}
	bestSegment.Fbegin = newBegin
	bestSegment.Fend = newEnd
	pos1 = bestSegment.Fbegin
	for {
		if !(pos1 != bestSegment.Fend) {
			break
		}
		*(*U32)(unsafe.Pointer(freqs + uintptr(*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(pos1)*4)))*4)) = uint32(0)
		goto _3
	_3:
		;
		pos1 = pos1 + 1
	}
	return bestSegment
}

// C documentation
//
//	/**
//	 * Check the validity of the parameters.
//	 * Returns non-zero if the parameters are valid and 0 otherwise.
//	 */
func COVER_checkParameters(tls *libc.TLS, parameters ZDICT_cover_params_t, maxDictSize size_t) (r int32) {
	/* k and d are required parameters */
	if parameters.Fd == uint32(0) || parameters.Fk == uint32(0) {
		return 0
	}
	/* k <= maxDictSize */
	if uint64(parameters.Fk) > maxDictSize {
		return 0
	}
	/* d <= k */
	if parameters.Fd > parameters.Fk {
		return 0
	}
	/* 0 < splitPoint <= 1 */
	if parameters.FsplitPoint <= libc.Float64FromInt32(0) || parameters.FsplitPoint > libc.Float64FromInt32(1) {
		return 0
	}
	return int32(1)
}

// C documentation
//
//	/**
//	 * Clean up a context initialized with `COVER_ctx_init()`.
//	 */
func COVER_ctx_destroy(tls *libc.TLS, ctx uintptr) {
	if !(ctx != 0) {
		return
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix = libc.UintptrFromInt32(0)
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.UintptrFromInt32(0)
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt = libc.UintptrFromInt32(0)
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.UintptrFromInt32(0)
	}
}

// C documentation
//
//	/**
//	 * Prepare a context for dictionary building.
//	 * The context is only dependent on the parameter `d` and can be used multiple
//	 * times.
//	 * Returns 0 on success or error code on error.
//	 * The context must be destroyed with `COVER_ctx_destroy()`.
//	 */
func COVER_ctx_init(tls *libc.TLS, ctx uintptr, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, d uint32, splitPoint float64) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var i, i1 U32
	var nbTestSamples, nbTrainSamples, v1, v2 uint32
	var samples, v9 uintptr
	var testSamplesSize, totalSamplesSize, trainingSamplesSize size_t
	var v3, v4, v5 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = i, i1, nbTestSamples, nbTrainSamples, samples, testSamplesSize, totalSamplesSize, trainingSamplesSize, v1, v2, v3, v4, v5, v9
	samples = samplesBuffer
	totalSamplesSize = COVER_sum(tls, samplesSizes, nbSamples)
	if splitPoint < float64(1) {
		v1 = uint32(float64(float64(nbSamples) * splitPoint))
	} else {
		v1 = nbSamples
	}
	/* Split samples into testing and training sets */
	nbTrainSamples = v1
	if splitPoint < float64(1) {
		v2 = nbSamples - nbTrainSamples
	} else {
		v2 = nbSamples
	}
	nbTestSamples = v2
	if splitPoint < float64(1) {
		v3 = COVER_sum(tls, samplesSizes, nbTrainSamples)
	} else {
		v3 = totalSamplesSize
	}
	trainingSamplesSize = v3
	if splitPoint < float64(1) {
		v4 = COVER_sum(tls, samplesSizes+uintptr(nbTrainSamples)*8, nbTestSamples)
	} else {
		v4 = totalSamplesSize
	}
	testSamplesSize = v4
	/* Checks */
	if uint64(d) > libc.Uint64FromInt64(8) {
		v5 = uint64(d)
	} else {
		v5 = libc.Uint64FromInt64(8)
	}
	if totalSamplesSize < v5 || totalSamplesSize >= uint64(uint32(-libc.Int32FromInt32(1))) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8508, libc.VaList(bp+8, uint32(totalSamplesSize>>libc.Int32FromInt32(20)), uint32(-libc.Int32FromInt32(1))>>libc.Int32FromInt32(20)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there are at least 5 training samples */
	if nbTrainSamples < uint32(5) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8572, libc.VaList(bp+8, nbTrainSamples))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there's testing sample */
	if nbTestSamples < uint32(1) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8627, libc.VaList(bp+8, nbTestSamples))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Zero the context */
	libc.Xmemset(tls, ctx, 0, uint64(88))
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8681, libc.VaList(bp+8, nbTrainSamples, uint32(trainingSamplesSize)))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8722, libc.VaList(bp+8, nbTestSamples, uint32(testSamplesSize)))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples = samples
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes = samplesSizes
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples = uint64(nbSamples)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples = uint64(nbTrainSamples)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTestSamples = uint64(nbTestSamples)
	/* Partial suffix array */
	if uint64(d) > libc.Uint64FromInt64(8) {
		v3 = uint64(d)
	} else {
		v3 = libc.Uint64FromInt64(8)
	}
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize = trainingSamplesSize - v3 + uint64(1)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix = libc.Xmalloc(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	/* Maps index to the dmerID */
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt = libc.Xmalloc(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	/* The offsets of each file */
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.Xmalloc(tls, uint64(nbSamples+libc.Uint32FromInt32(1))*uint64(8))
	if !((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix != 0) || !((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt != 0) || !((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8762, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		COVER_ctx_destroy(tls, ctx)
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.UintptrFromInt32(0)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd = d
	/* Fill offsets from the samplesSizes */
	*(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)) = uint64(0)
	i = uint32(1)
	for {
		if !(i <= nbSamples) {
			break
		}
		*(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i)*8)) = *(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i-uint32(1))*8)) + *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i-uint32(1))*8))
		goto _7
	_7:
		;
		i = i + 1
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8798, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	i1 = uint32(0)
	for {
		if !(uint64(i1) < (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize) {
			break
		}
		*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix + uintptr(i1)*4)) = i1
		goto _8
	_8:
		;
		i1 = i1 + 1
	}
	stableSort(tls, ctx)
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8833, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	/* For each dmer group (group of positions with the same first d bytes):
	 * 1. For each position we set dmerAt[position] = dmerID.  The dmerID is
	 *    (groupBeginPtr - suffix).  This allows us to go from position to
	 *    dmerID so we can look up values in freq.
	 * 2. We calculate how many samples the dmer occurs in and save it in
	 *    freqs[dmerId].
	 */
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd <= uint32(8) {
		v9 = __ccgo_fp(COVER_cmp8)
	} else {
		v9 = __ccgo_fp(COVER_cmp)
	}
	COVER_groupBy(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize, uint64(4), ctx, v9, __ccgo_fp(COVER_group))
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix = libc.UintptrFromInt32(0)
	return uint64(0)
}

func COVER_warnOnSmallCorpus(tls *libc.TLS, maxDictSize size_t, nbDmers size_t, displayLevel int32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var ratio float64
	_ = ratio
	ratio = float64(nbDmers) / float64(maxDictSize)
	if ratio >= libc.Float64FromInt32(10) {
		return
	}
	if displayLevel >= int32(1) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8856, libc.VaList(bp+8, uint32(maxDictSize), uint32(nbDmers), ratio))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
}

func COVER_computeEpochs(tls *libc.TLS, maxDictSize U32, nbDmers U32, k U32, passes U32) (r COVER_epoch_info_t) {
	var epochs COVER_epoch_info_t
	var minEpochSize U32
	var v1 uint32
	_, _, _ = epochs, minEpochSize, v1
	minEpochSize = k * uint32(10)
	if uint32(libc.Int32FromInt32(1)) > maxDictSize/k/passes {
		v1 = uint32(libc.Int32FromInt32(1))
	} else {
		v1 = maxDictSize / k / passes
	}
	epochs.Fnum = v1
	epochs.Fsize = nbDmers / epochs.Fnum
	if epochs.Fsize >= minEpochSize {
		return epochs
	}
	if minEpochSize < nbDmers {
		v1 = minEpochSize
	} else {
		v1 = nbDmers
	}
	epochs.Fsize = v1
	epochs.Fnum = nbDmers / epochs.Fsize
	return epochs
}

// C documentation
//
//	/**
//	 * Given the prepared context build the dictionary.
//	 */
func COVER_buildDictionary(tls *libc.TLS, ctx uintptr, freqs uintptr, activeDmers uintptr, dictBuffer uintptr, dictBufferCapacity size_t, parameters ZDICT_cover_params_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var dict uintptr
	var epoch, maxZeroScoreRun, segmentSize, tail, zeroScoreRun, v5 size_t
	var epochBegin, epochEnd U32
	var epochs COVER_epoch_info_t
	var segment COVER_segment_t
	var v1, v2, v3 uint32
	var v6 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dict, epoch, epochBegin, epochEnd, epochs, maxZeroScoreRun, segment, segmentSize, tail, zeroScoreRun, v1, v2, v3, v5, v6
	dict = dictBuffer
	tail = dictBufferCapacity
	/* Divide the data into epochs. We will select one segment from each epoch. */
	epochs = COVER_computeEpochs(tls, uint32(dictBufferCapacity), uint32((*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize), parameters.Fk, uint32(4))
	if uint32(libc.Int32FromInt32(100)) < epochs.Fnum>>libc.Int32FromInt32(3) {
		v2 = uint32(libc.Int32FromInt32(100))
	} else {
		v2 = epochs.Fnum >> libc.Int32FromInt32(3)
	}
	if uint32(libc.Int32FromInt32(10)) > v2 {
		v1 = uint32(libc.Int32FromInt32(10))
	} else {
		if uint32(libc.Int32FromInt32(100)) < epochs.Fnum>>libc.Int32FromInt32(3) {
			v3 = uint32(libc.Int32FromInt32(100))
		} else {
			v3 = epochs.Fnum >> libc.Int32FromInt32(3)
		}
		v1 = v3
	}
	maxZeroScoreRun = uint64(v1)
	zeroScoreRun = uint64(0)
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9136, libc.VaList(bp+8, epochs.Fnum, epochs.Fsize))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	/* Loop through the epochs until there are no more segments or the dictionary
	 * is full.
	 */
	epoch = uint64(0)
	for {
		if !(tail > uint64(0)) {
			break
		}
		epochBegin = uint32(epoch * uint64(epochs.Fsize))
		epochEnd = epochBegin + epochs.Fsize
		/* Select a segment */
		segment = COVER_selectSegment(tls, ctx, freqs, activeDmers, epochBegin, epochEnd, parameters)
		/* If the segment covers no dmers, then we are out of content.
		 * There may be new content in other epochs, for continue for some time.
		 */
		if segment.Fscore == uint32(0) {
			zeroScoreRun = zeroScoreRun + 1
			v5 = zeroScoreRun
			if v5 >= maxZeroScoreRun {
				break
			}
			goto _4
		}
		zeroScoreRun = uint64(0)
		/* Trim the segment if necessary and if it is too small then we are done */
		if uint64(segment.Fend-segment.Fbegin+parameters.Fd-libc.Uint32FromInt32(1)) < tail {
			v6 = uint64(segment.Fend - segment.Fbegin + parameters.Fd - libc.Uint32FromInt32(1))
		} else {
			v6 = tail
		}
		segmentSize = v6
		if segmentSize < uint64(parameters.Fd) {
			break
		}
		/* We fill the dictionary from the back to allow the best segments to be
		 * referenced with the smallest offsets.
		 */
		tail = tail - segmentSize
		libc.Xmemcpy(tls, dict+uintptr(tail), (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(segment.Fbegin), segmentSize)
		if g_displayLevel >= int32(2) {
			if clock(tls)-g_time > g_refreshRate || g_displayLevel >= int32(4) {
				g_time = clock(tls)
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9180, libc.VaList(bp+8, uint32((dictBufferCapacity-tail)*libc.Uint64FromInt32(100)/dictBufferCapacity)))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
		}
		goto _4
	_4:
		;
		epoch = (epoch + uint64(1)) % uint64(epochs.Fnum)
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9193, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	return tail
}

func ZDICT_trainFromBuffer_cover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters ZDICT_cover_params_t) (r size_t) {
	bp := tls.Alloc(128)
	defer tls.Free(128)
	var dict uintptr
	var dictionarySize, initVal, tail size_t
	var _ /* activeDmers at bp+88 */ COVER_map_t
	var _ /* ctx at bp+0 */ COVER_ctx_t
	_, _, _, _ = dict, dictionarySize, initVal, tail
	dict = dictBuffer
	parameters.FsplitPoint = float64(1)
	/* Initialize global data */
	g_displayLevel = int32(parameters.FzParams.FnotificationLevel)
	/* Checks */
	if !(COVER_checkParameters(tls, parameters, dictBufferCapacity) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9200, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9228, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9269, libc.VaList(bp+120, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* Initialize context and activeDmers */
	initVal = COVER_ctx_init(tls, bp, samplesBuffer, samplesSizes, nbSamples, parameters.Fd, parameters.FsplitPoint)
	if ZSTD_isError(tls, initVal) != 0 {
		return initVal
	}
	COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*COVER_ctx_t)(unsafe.Pointer(bp))).FsuffixSize, g_displayLevel)
	if !(COVER_map_init(tls, bp+88, parameters.Fk-parameters.Fd+uint32(1)) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9309, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		COVER_ctx_destroy(tls, bp)
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9353, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	tail = COVER_buildDictionary(tls, bp, (*(*COVER_ctx_t)(unsafe.Pointer(bp))).Ffreqs, bp+88, dictBuffer, dictBufferCapacity, parameters)
	dictionarySize = ZDICT_finalizeDictionary(tls, dict, dictBufferCapacity, dict+uintptr(tail), dictBufferCapacity-tail, samplesBuffer, samplesSizes, nbSamples, parameters.FzParams)
	if !(ZSTD_isError(tls, dictionarySize) != 0) {
		if g_displayLevel >= int32(2) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9374, libc.VaList(bp+120, uint32(dictionarySize)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
	}
	COVER_ctx_destroy(tls, bp)
	COVER_map_destroy(tls, bp+88)
	return dictionarySize
	return r
}

func COVER_checkTotalCompressedSize(tls *libc.TLS, parameters ZDICT_cover_params_t, samplesSizes uintptr, samples uintptr, offsets uintptr, nbTrainSamples size_t, nbSamples size_t, dict uintptr, dictBufferCapacity size_t) (r size_t) {
	var cctx, cdict, dst uintptr
	var dstCapacity, i, maxSampleSize, size, totalCompressedSize size_t
	var v1 uint64
	_, _, _, _, _, _, _, _, _ = cctx, cdict, dst, dstCapacity, i, maxSampleSize, size, totalCompressedSize, v1
	totalCompressedSize = uint64(-int32(ZSTD_error_GENERIC))
	/* Allocate dst with enough space to compress the maximum sized sample */
	maxSampleSize = uint64(0)
	if parameters.FsplitPoint < float64(1) {
		v1 = nbTrainSamples
	} else {
		v1 = uint64(0)
	}
	i = v1
	for {
		if !(i < nbSamples) {
			break
		}
		if *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8)) > maxSampleSize {
			v1 = *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8))
		} else {
			v1 = maxSampleSize
		}
		maxSampleSize = v1
		goto _2
	_2:
		;
		i = i + 1
	}
	dstCapacity = ZSTD_compressBound(tls, maxSampleSize)
	dst = libc.Xmalloc(tls, dstCapacity)
	/* Create the cctx and cdict */
	cctx = ZSTD_createCCtx(tls)
	cdict = ZSTD_createCDict(tls, dict, dictBufferCapacity, parameters.FzParams.FcompressionLevel)
	if !(dst != 0) || !(cctx != 0) || !(cdict != 0) {
		goto _compressCleanup
	}
	/* Compress each sample and sum their sizes (or error) */
	totalCompressedSize = dictBufferCapacity
	if parameters.FsplitPoint < float64(1) {
		v1 = nbTrainSamples
	} else {
		v1 = uint64(0)
	}
	i = v1
	for {
		if !(i < nbSamples) {
			break
		}
		size = ZSTD_compress_usingCDict(tls, cctx, dst, dstCapacity, samples+uintptr(*(*size_t)(unsafe.Pointer(offsets + uintptr(i)*8))), *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8)), cdict)
		if ZSTD_isError(tls, size) != 0 {
			totalCompressedSize = size
			goto _compressCleanup
		}
		totalCompressedSize = totalCompressedSize + size
		goto _5
	_5:
		;
		i = i + 1
	}
	goto _compressCleanup
_compressCleanup:
	;
	ZSTD_freeCCtx(tls, cctx)
	ZSTD_freeCDict(tls, cdict)
	if dst != 0 {
		libc.Xfree(tls, dst)
	}
	return totalCompressedSize
}

// C documentation
//
//	/**
//	 * Initialize the `COVER_best_t`.
//	 */
func COVER_best_init(tls *libc.TLS, best uintptr) {
	if best == libc.UintptrFromInt32(0) {
		return
	} /* compatible with init on NULL */
	_ = libc.UintptrFromInt32(0)
	libc.XInitializeCriticalSection(tls, best)
	_ = libc.Int32FromInt32(0)
	_ = libc.UintptrFromInt32(0)
	InitializeConditionVariable(tls, best+40)
	_ = libc.Int32FromInt32(0)
	(*COVER_best_t)(unsafe.Pointer(best)).FliveJobs = uint64(0)
	(*COVER_best_t)(unsafe.Pointer(best)).Fdict = libc.UintptrFromInt32(0)
	(*COVER_best_t)(unsafe.Pointer(best)).FdictSize = uint64(0)
	(*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize = uint64(-libc.Int32FromInt32(1))
	libc.Xmemset(tls, best+72, 0, uint64(48))
}

// C documentation
//
//	/**
//	 * Wait until liveJobs == 0.
//	 */
func COVER_best_wait(tls *libc.TLS, best uintptr) {
	if !(best != 0) {
		return
	}
	libc.XEnterCriticalSection(tls, best)
	for (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs != uint64(0) {
		SleepConditionVariableCS(tls, best+40, best, uint32(INFINITE))
	}
	libc.XLeaveCriticalSection(tls, best)
}

// C documentation
//
//	/**
//	 * Call COVER_best_wait() and then destroy the COVER_best_t.
//	 */
func COVER_best_destroy(tls *libc.TLS, best uintptr) {
	if !(best != 0) {
		return
	}
	COVER_best_wait(tls, best)
	if (*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0 {
		libc.Xfree(tls, (*COVER_best_t)(unsafe.Pointer(best)).Fdict)
	}
	libc.XDeleteCriticalSection(tls, best)
	_ = best + 40
}

// C documentation
//
//	/**
//	 * Called when a thread is about to be launched.
//	 * Increments liveJobs.
//	 */
func COVER_best_start(tls *libc.TLS, best uintptr) {
	if !(best != 0) {
		return
	}
	libc.XEnterCriticalSection(tls, best)
	(*COVER_best_t)(unsafe.Pointer(best)).FliveJobs = (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs + 1
	libc.XLeaveCriticalSection(tls, best)
}

// C documentation
//
//	/**
//	 * Called when a thread finishes executing, both on error or success.
//	 * Decrements liveJobs and signals any waiting threads if liveJobs == 0.
//	 * If this dictionary is the best so far save it and its parameters.
//	 */
func COVER_best_finish(tls *libc.TLS, best uintptr, parameters ZDICT_cover_params_t, selection COVER_dictSelection_t) {
	var compressedSize, dictSize, liveJobs size_t
	var dict uintptr
	_, _, _, _ = compressedSize, dict, dictSize, liveJobs
	dict = selection.FdictContent
	compressedSize = selection.FtotalCompressedSize
	dictSize = selection.FdictSize
	if !(best != 0) {
		return
	}
	libc.XEnterCriticalSection(tls, best)
	(*COVER_best_t)(unsafe.Pointer(best)).FliveJobs = (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs - 1
	liveJobs = (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs
	/* If the new dictionary is better */
	if compressedSize < (*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize {
		/* Allocate space if necessary */
		if !((*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0) || (*COVER_best_t)(unsafe.Pointer(best)).FdictSize < dictSize {
			if (*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0 {
				libc.Xfree(tls, (*COVER_best_t)(unsafe.Pointer(best)).Fdict)
			}
			(*COVER_best_t)(unsafe.Pointer(best)).Fdict = libc.Xmalloc(tls, dictSize)
			if !((*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0) {
				(*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize = uint64(-int32(ZSTD_error_GENERIC))
				(*COVER_best_t)(unsafe.Pointer(best)).FdictSize = uint64(0)
				WakeConditionVariable(tls, best+40)
				libc.XLeaveCriticalSection(tls, best)
				return
			}
		}
		/* Save the dictionary, parameters, and size */
		if dict != 0 {
			libc.Xmemcpy(tls, (*COVER_best_t)(unsafe.Pointer(best)).Fdict, dict, dictSize)
			(*COVER_best_t)(unsafe.Pointer(best)).FdictSize = dictSize
			(*COVER_best_t)(unsafe.Pointer(best)).Fparameters = parameters
			(*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize = compressedSize
		}
	}
	if liveJobs == uint64(0) {
		WakeAllConditionVariable(tls, best+40)
	}
	libc.XLeaveCriticalSection(tls, best)
}

func setDictSelection(tls *libc.TLS, buf uintptr, s size_t, csz size_t) (r COVER_dictSelection_t) {
	var ds COVER_dictSelection_t
	_ = ds
	ds.FdictContent = buf
	ds.FdictSize = s
	ds.FtotalCompressedSize = csz
	return ds
}

func COVER_dictSelectionError(tls *libc.TLS, error1 size_t) (r COVER_dictSelection_t) {
	return setDictSelection(tls, libc.UintptrFromInt32(0), uint64(0), error1)
}

func COVER_dictSelectionIsError(tls *libc.TLS, selection COVER_dictSelection_t) (r uint32) {
	return libc.BoolUint32(ZSTD_isError(tls, selection.FtotalCompressedSize) != 0 || !(selection.FdictContent != 0))
}

func COVER_dictSelectionFree(tls *libc.TLS, selection COVER_dictSelection_t) {
	libc.Xfree(tls, selection.FdictContent)
}

func COVER_selectDict(tls *libc.TLS, customDictContent uintptr, dictBufferCapacity size_t, dictContentSize size_t, samplesBuffer uintptr, samplesSizes uintptr, nbFinalizeSamples uint32, nbCheckSamples size_t, nbSamples size_t, params ZDICT_cover_params_t, offsets uintptr, totalCompressedSize size_t) (r COVER_dictSelection_t) {
	var candidateDictBuffer, customDictContentEnd, largestDictbuffer uintptr
	var largestCompressed, largestDict size_t
	var regressionTolerance float64
	_, _, _, _, _, _ = candidateDictBuffer, customDictContentEnd, largestCompressed, largestDict, largestDictbuffer, regressionTolerance
	largestDict = uint64(0)
	largestCompressed = uint64(0)
	customDictContentEnd = customDictContent + uintptr(dictContentSize)
	largestDictbuffer = libc.Xmalloc(tls, dictBufferCapacity)
	candidateDictBuffer = libc.Xmalloc(tls, dictBufferCapacity)
	regressionTolerance = float64(params.FshrinkDictMaxRegression)/float64(100) + float64(1)
	if !(largestDictbuffer != 0) || !(candidateDictBuffer != 0) {
		libc.Xfree(tls, largestDictbuffer)
		libc.Xfree(tls, candidateDictBuffer)
		return COVER_dictSelectionError(tls, dictContentSize)
	}
	/* Initial dictionary size and compressed size */
	libc.Xmemcpy(tls, largestDictbuffer, customDictContent, dictContentSize)
	dictContentSize = ZDICT_finalizeDictionary(tls, largestDictbuffer, dictBufferCapacity, customDictContent, dictContentSize, samplesBuffer, samplesSizes, nbFinalizeSamples, params.FzParams)
	if ZDICT_isError(tls, dictContentSize) != 0 {
		libc.Xfree(tls, largestDictbuffer)
		libc.Xfree(tls, candidateDictBuffer)
		return COVER_dictSelectionError(tls, dictContentSize)
	}
	totalCompressedSize = COVER_checkTotalCompressedSize(tls, params, samplesSizes, samplesBuffer, offsets, nbCheckSamples, nbSamples, largestDictbuffer, dictContentSize)
	if ZSTD_isError(tls, totalCompressedSize) != 0 {
		libc.Xfree(tls, largestDictbuffer)
		libc.Xfree(tls, candidateDictBuffer)
		return COVER_dictSelectionError(tls, totalCompressedSize)
	}
	if params.FshrinkDict == uint32(0) {
		libc.Xfree(tls, candidateDictBuffer)
		return setDictSelection(tls, largestDictbuffer, dictContentSize, totalCompressedSize)
	}
	largestDict = dictContentSize
	largestCompressed = totalCompressedSize
	dictContentSize = uint64(ZDICT_DICTSIZE_MIN)
	/* Largest dict is initially at least ZDICT_DICTSIZE_MIN */
	for dictContentSize < largestDict {
		libc.Xmemcpy(tls, candidateDictBuffer, largestDictbuffer, largestDict)
		dictContentSize = ZDICT_finalizeDictionary(tls, candidateDictBuffer, dictBufferCapacity, customDictContentEnd-uintptr(dictContentSize), dictContentSize, samplesBuffer, samplesSizes, nbFinalizeSamples, params.FzParams)
		if ZDICT_isError(tls, dictContentSize) != 0 {
			libc.Xfree(tls, largestDictbuffer)
			libc.Xfree(tls, candidateDictBuffer)
			return COVER_dictSelectionError(tls, dictContentSize)
		}
		totalCompressedSize = COVER_checkTotalCompressedSize(tls, params, samplesSizes, samplesBuffer, offsets, nbCheckSamples, nbSamples, candidateDictBuffer, dictContentSize)
		if ZSTD_isError(tls, totalCompressedSize) != 0 {
			libc.Xfree(tls, largestDictbuffer)
			libc.Xfree(tls, candidateDictBuffer)
			return COVER_dictSelectionError(tls, totalCompressedSize)
		}
		if float64(totalCompressedSize) <= float64(float64(largestCompressed)*regressionTolerance) {
			libc.Xfree(tls, largestDictbuffer)
			return setDictSelection(tls, candidateDictBuffer, dictContentSize, totalCompressedSize)
		}
		dictContentSize = dictContentSize * uint64(2)
	}
	dictContentSize = largestDict
	totalCompressedSize = largestCompressed
	libc.Xfree(tls, candidateDictBuffer)
	return setDictSelection(tls, largestDictbuffer, dictContentSize, totalCompressedSize)
}

// C documentation
//
//	/**
//	 * Parameters for COVER_tryParameters().
//	 */
type COVER_tryParameters_data_t = struct {
	Fctx                uintptr
	Fbest               uintptr
	FdictBufferCapacity size_t
	Fparameters         ZDICT_cover_params_t
}

// C documentation
//
//	/**
//	 * Parameters for COVER_tryParameters().
//	 */
type COVER_tryParameters_data_s = COVER_tryParameters_data_t

// C documentation
//
//	/**
//	 * Tries a set of parameters and updates the COVER_best_t with the results.
//	 * This function is thread safe if zstd is compiled with multithreaded support.
//	 * It takes its parameters as an *OWNING* opaque pointer to support threading.
//	 */
func COVER_tryParameters(tls *libc.TLS, opaque uintptr) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var ctx, data, dict, freqs uintptr
	var dictBufferCapacity, tail, totalCompressedSize size_t
	var parameters ZDICT_cover_params_t
	var selection COVER_dictSelection_t
	var _ /* activeDmers at bp+0 */ COVER_map_t
	_, _, _, _, _, _, _, _, _ = ctx, data, dict, dictBufferCapacity, freqs, parameters, selection, tail, totalCompressedSize
	/* Save parameters as local variables */
	data = opaque
	ctx = (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx
	parameters = (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters
	dictBufferCapacity = (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity
	totalCompressedSize = uint64(-int32(ZSTD_error_GENERIC))
	dict = libc.Xmalloc(tls, dictBufferCapacity)
	selection = COVER_dictSelectionError(tls, uint64(-int32(ZSTD_error_GENERIC)))
	freqs = libc.Xmalloc(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	if !(COVER_map_init(tls, bp, parameters.Fk-parameters.Fd+uint32(1)) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9309, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	if !(dict != 0) || !(freqs != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9409, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	/* Copy the frequencies because we need to modify them */
	libc.Xmemcpy(tls, freqs, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	/* Build the dictionary */
	tail = COVER_buildDictionary(tls, ctx, freqs, bp, dict, dictBufferCapacity, parameters)
	selection = COVER_selectDict(tls, dict+uintptr(tail), dictBufferCapacity, dictBufferCapacity-tail, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes, uint32((*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples), (*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples, parameters, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets, totalCompressedSize)
	if COVER_dictSelectionIsError(tls, selection) != 0 {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9452, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	goto _cleanup
_cleanup:
	;
	libc.Xfree(tls, dict)
	COVER_best_finish(tls, (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest, parameters, selection)
	libc.Xfree(tls, data)
	COVER_map_destroy(tls, bp)
	COVER_dictSelectionFree(tls, selection)
	libc.Xfree(tls, freqs)
}

func ZDICT_optimizeTrainFromBuffer_cover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters uintptr) (r size_t) {
	bp := tls.Alloc(240)
	defer tls.Free(240)
	var compressedSize, dictSize, initVal size_t
	var d, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, shrinkDict, v2, v3, v4, v5, v6, v7 uint32
	var data, pool uintptr
	var displayLevel, warned, v8 int32
	var splitPoint, v1 float64
	var _ /* best at bp+0 */ COVER_best_t
	var _ /* ctx at bp+128 */ COVER_ctx_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = compressedSize, d, data, dictSize, displayLevel, initVal, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, pool, shrinkDict, splitPoint, warned, v1, v2, v3, v4, v5, v6, v7, v8
	/* constants */
	nbThreads = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FnbThreads
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FsplitPoint <= float64(0) {
		v1 = float64(1)
	} else {
		v1 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FsplitPoint
	}
	splitPoint = v1
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v2 = uint32(6)
	} else {
		v2 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMinD = v2
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v3 = uint32(8)
	} else {
		v3 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMaxD = v3
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v4 = uint32(50)
	} else {
		v4 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMinK = v4
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v5 = uint32(2000)
	} else {
		v5 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMaxK = v5
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fsteps == uint32(0) {
		v6 = uint32(40)
	} else {
		v6 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fsteps
	}
	kSteps = v6
	if (kMaxK-kMinK)/kSteps > uint32(libc.Int32FromInt32(1)) {
		v7 = (kMaxK - kMinK) / kSteps
	} else {
		v7 = uint32(libc.Int32FromInt32(1))
	}
	kStepSize = v7
	kIterations = (uint32(1) + (kMaxD-kMinD)/uint32(2)) * (uint32(1) + (kMaxK-kMinK)/kStepSize)
	shrinkDict = uint32(0)
	/* Local variables */
	displayLevel = int32((*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FzParams.FnotificationLevel)
	iteration = uint32(1)
	pool = libc.UintptrFromInt32(0)
	warned = 0
	/* Checks */
	if splitPoint <= libc.Float64FromInt32(0) || splitPoint > libc.Float64FromInt32(1) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9481, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if kMinK < kMaxD || kMaxK < kMinK {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9481, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9228, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9269, libc.VaList(bp+224, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbThreads > uint32(1) {
		pool = POOL_create(tls, uint64(nbThreads), uint64(1))
		if !(pool != 0) {
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
	}
	/* Initialization */
	COVER_best_init(tls, bp)
	/* Turn down global display level to clean up display at level 2 and below */
	if displayLevel == 0 {
		v8 = 0
	} else {
		v8 = displayLevel - int32(1)
	}
	g_displayLevel = v8
	/* Loop through d first because each new value needs a new context */
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9503, libc.VaList(bp+224, kIterations))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	d = kMinD
	for {
		if !(d <= kMaxD) {
			break
		}
		if displayLevel >= int32(3) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9543, libc.VaList(bp+224, d))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		initVal = COVER_ctx_init(tls, bp+128, samplesBuffer, samplesSizes, nbSamples, d, splitPoint)
		if ZSTD_isError(tls, initVal) != 0 {
			if displayLevel >= int32(1) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9549, 0)
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			COVER_best_destroy(tls, bp)
			POOL_free(tls, pool)
			return initVal
		}
		if !(warned != 0) {
			COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*COVER_ctx_t)(unsafe.Pointer(bp + 128))).FsuffixSize, displayLevel)
			warned = int32(1)
		}
		/* Loop through k reusing the same context */
		k = kMinK
		for {
			if !(k <= kMaxK) {
				break
			}
			/* Prepare the arguments */
			data = libc.Xmalloc(tls, uint64(72))
			if displayLevel >= int32(3) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9579, libc.VaList(bp+224, k))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			if !(data != 0) {
				if displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9585, 0)
					libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
				}
				COVER_best_destroy(tls, bp)
				COVER_ctx_destroy(tls, bp+128)
				POOL_free(tls, pool)
				return uint64(-int32(ZSTD_error_memory_allocation))
			}
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx = bp + 128
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest = bp
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity = dictBufferCapacity
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters = *(*ZDICT_cover_params_t)(unsafe.Pointer(parameters))
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fk = k
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fd = d
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FsplitPoint = splitPoint
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fsteps = kSteps
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FshrinkDict = shrinkDict
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FzParams.FnotificationLevel = uint32(g_displayLevel)
			/* Check the parameters */
			if !(COVER_checkParameters(tls, (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters, dictBufferCapacity) != 0) {
				if g_displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9200, 0)
					libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
				}
				libc.Xfree(tls, data)
				goto _10
			}
			/* Call the function and pass ownership of data to it */
			COVER_best_start(tls, bp)
			if pool != 0 {
				POOL_add(tls, pool, __ccgo_fp(COVER_tryParameters), data)
			} else {
				COVER_tryParameters(tls, data)
			}
			/* Print status */
			if displayLevel >= int32(2) {
				if clock(tls)-g_time > g_refreshRate || displayLevel >= int32(4) {
					g_time = clock(tls)
					libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9180, libc.VaList(bp+224, iteration*libc.Uint32FromInt32(100)/kIterations))
					libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
				}
			}
			iteration = iteration + 1
			goto _10
		_10:
			;
			k = k + kStepSize
		}
		COVER_best_wait(tls, bp)
		COVER_ctx_destroy(tls, bp+128)
		goto _9
	_9:
		;
		d = d + uint32(2)
	}
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9193, libc.VaList(bp+224, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	/* Fill the output buffer and parameters with output of the best parameters */
	dictSize = (*(*COVER_best_t)(unsafe.Pointer(bp))).FdictSize
	if ZSTD_isError(tls, (*(*COVER_best_t)(unsafe.Pointer(bp))).FcompressedSize) != 0 {
		compressedSize = (*(*COVER_best_t)(unsafe.Pointer(bp))).FcompressedSize
		COVER_best_destroy(tls, bp)
		POOL_free(tls, pool)
		return compressedSize
	}
	*(*ZDICT_cover_params_t)(unsafe.Pointer(parameters)) = (*(*COVER_best_t)(unsafe.Pointer(bp))).Fparameters
	libc.Xmemcpy(tls, dictBuffer, (*(*COVER_best_t)(unsafe.Pointer(bp))).Fdict, dictSize)
	COVER_best_destroy(tls, bp)
	POOL_free(tls, pool)
	return dictSize
	return r
}

/**** ended inlining divsufsort.h ****/

/*- Constants -*/
/* minstacksize = log(SS_BLOCKSIZE) / log(3) * 2 */

/*- Macros -*/

/*- Private Functions -*/

var lg_table = [256]int32{
	0:   -int32(1),
	2:   int32(1),
	3:   int32(1),
	4:   int32(2),
	5:   int32(2),
	6:   int32(2),
	7:   int32(2),
	8:   int32(3),
	9:   int32(3),
	10:  int32(3),
	11:  int32(3),
	12:  int32(3),
	13:  int32(3),
	14:  int32(3),
	15:  int32(3),
	16:  int32(4),
	17:  int32(4),
	18:  int32(4),
	19:  int32(4),
	20:  int32(4),
	21:  int32(4),
	22:  int32(4),
	23:  int32(4),
	24:  int32(4),
	25:  int32(4),
	26:  int32(4),
	27:  int32(4),
	28:  int32(4),
	29:  int32(4),
	30:  int32(4),
	31:  int32(4),
	32:  int32(5),
	33:  int32(5),
	34:  int32(5),
	35:  int32(5),
	36:  int32(5),
	37:  int32(5),
	38:  int32(5),
	39:  int32(5),
	40:  int32(5),
	41:  int32(5),
	42:  int32(5),
	43:  int32(5),
	44:  int32(5),
	45:  int32(5),
	46:  int32(5),
	47:  int32(5),
	48:  int32(5),
	49:  int32(5),
	50:  int32(5),
	51:  int32(5),
	52:  int32(5),
	53:  int32(5),
	54:  int32(5),
	55:  int32(5),
	56:  int32(5),
	57:  int32(5),
	58:  int32(5),
	59:  int32(5),
	60:  int32(5),
	61:  int32(5),
	62:  int32(5),
	63:  int32(5),
	64:  int32(6),
	65:  int32(6),
	66:  int32(6),
	67:  int32(6),
	68:  int32(6),
	69:  int32(6),
	70:  int32(6),
	71:  int32(6),
	72:  int32(6),
	73:  int32(6),
	74:  int32(6),
	75:  int32(6),
	76:  int32(6),
	77:  int32(6),
	78:  int32(6),
	79:  int32(6),
	80:  int32(6),
	81:  int32(6),
	82:  int32(6),
	83:  int32(6),
	84:  int32(6),
	85:  int32(6),
	86:  int32(6),
	87:  int32(6),
	88:  int32(6),
	89:  int32(6),
	90:  int32(6),
	91:  int32(6),
	92:  int32(6),
	93:  int32(6),
	94:  int32(6),
	95:  int32(6),
	96:  int32(6),
	97:  int32(6),
	98:  int32(6),
	99:  int32(6),
	100: int32(6),
	101: int32(6),
	102: int32(6),
	103: int32(6),
	104: int32(6),
	105: int32(6),
	106: int32(6),
	107: int32(6),
	108: int32(6),
	109: int32(6),
	110: int32(6),
	111: int32(6),
	112: int32(6),
	113: int32(6),
	114: int32(6),
	115: int32(6),
	116: int32(6),
	117: int32(6),
	118: int32(6),
	119: int32(6),
	120: int32(6),
	121: int32(6),
	122: int32(6),
	123: int32(6),
	124: int32(6),
	125: int32(6),
	126: int32(6),
	127: int32(6),
	128: int32(7),
	129: int32(7),
	130: int32(7),
	131: int32(7),
	132: int32(7),
	133: int32(7),
	134: int32(7),
	135: int32(7),
	136: int32(7),
	137: int32(7),
	138: int32(7),
	139: int32(7),
	140: int32(7),
	141: int32(7),
	142: int32(7),
	143: int32(7),
	144: int32(7),
	145: int32(7),
	146: int32(7),
	147: int32(7),
	148: int32(7),
	149: int32(7),
	150: int32(7),
	151: int32(7),
	152: int32(7),
	153: int32(7),
	154: int32(7),
	155: int32(7),
	156: int32(7),
	157: int32(7),
	158: int32(7),
	159: int32(7),
	160: int32(7),
	161: int32(7),
	162: int32(7),
	163: int32(7),
	164: int32(7),
	165: int32(7),
	166: int32(7),
	167: int32(7),
	168: int32(7),
	169: int32(7),
	170: int32(7),
	171: int32(7),
	172: int32(7),
	173: int32(7),
	174: int32(7),
	175: int32(7),
	176: int32(7),
	177: int32(7),
	178: int32(7),
	179: int32(7),
	180: int32(7),
	181: int32(7),
	182: int32(7),
	183: int32(7),
	184: int32(7),
	185: int32(7),
	186: int32(7),
	187: int32(7),
	188: int32(7),
	189: int32(7),
	190: int32(7),
	191: int32(7),
	192: int32(7),
	193: int32(7),
	194: int32(7),
	195: int32(7),
	196: int32(7),
	197: int32(7),
	198: int32(7),
	199: int32(7),
	200: int32(7),
	201: int32(7),
	202: int32(7),
	203: int32(7),
	204: int32(7),
	205: int32(7),
	206: int32(7),
	207: int32(7),
	208: int32(7),
	209: int32(7),
	210: int32(7),
	211: int32(7),
	212: int32(7),
	213: int32(7),
	214: int32(7),
	215: int32(7),
	216: int32(7),
	217: int32(7),
	218: int32(7),
	219: int32(7),
	220: int32(7),
	221: int32(7),
	222: int32(7),
	223: int32(7),
	224: int32(7),
	225: int32(7),
	226: int32(7),
	227: int32(7),
	228: int32(7),
	229: int32(7),
	230: int32(7),
	231: int32(7),
	232: int32(7),
	233: int32(7),
	234: int32(7),
	235: int32(7),
	236: int32(7),
	237: int32(7),
	238: int32(7),
	239: int32(7),
	240: int32(7),
	241: int32(7),
	242: int32(7),
	243: int32(7),
	244: int32(7),
	245: int32(7),
	246: int32(7),
	247: int32(7),
	248: int32(7),
	249: int32(7),
	250: int32(7),
	251: int32(7),
	252: int32(7),
	253: int32(7),
	254: int32(7),
	255: int32(7),
}

func ss_ilg(tls *libc.TLS, n int32) (r int32) {
	var v1 int32
	_ = v1
	if n&int32(0xff00) != 0 {
		v1 = int32(8) + lg_table[n>>int32(8)&int32(0xff)]
	} else {
		v1 = 0 + lg_table[n>>0&int32(0xff)]
	}
	return v1
}

var sqq_table = [256]int32{
	1:   int32(16),
	2:   int32(22),
	3:   int32(27),
	4:   int32(32),
	5:   int32(35),
	6:   int32(39),
	7:   int32(42),
	8:   int32(45),
	9:   int32(48),
	10:  int32(50),
	11:  int32(53),
	12:  int32(55),
	13:  int32(57),
	14:  int32(59),
	15:  int32(61),
	16:  int32(64),
	17:  int32(65),
	18:  int32(67),
	19:  int32(69),
	20:  int32(71),
	21:  int32(73),
	22:  int32(75),
	23:  int32(76),
	24:  int32(78),
	25:  int32(80),
	26:  int32(81),
	27:  int32(83),
	28:  int32(84),
	29:  int32(86),
	30:  int32(87),
	31:  int32(89),
	32:  int32(90),
	33:  int32(91),
	34:  int32(93),
	35:  int32(94),
	36:  int32(96),
	37:  int32(97),
	38:  int32(98),
	39:  int32(99),
	40:  int32(101),
	41:  int32(102),
	42:  int32(103),
	43:  int32(104),
	44:  int32(106),
	45:  int32(107),
	46:  int32(108),
	47:  int32(109),
	48:  int32(110),
	49:  int32(112),
	50:  int32(113),
	51:  int32(114),
	52:  int32(115),
	53:  int32(116),
	54:  int32(117),
	55:  int32(118),
	56:  int32(119),
	57:  int32(120),
	58:  int32(121),
	59:  int32(122),
	60:  int32(123),
	61:  int32(124),
	62:  int32(125),
	63:  int32(126),
	64:  int32(128),
	65:  int32(128),
	66:  int32(129),
	67:  int32(130),
	68:  int32(131),
	69:  int32(132),
	70:  int32(133),
	71:  int32(134),
	72:  int32(135),
	73:  int32(136),
	74:  int32(137),
	75:  int32(138),
	76:  int32(139),
	77:  int32(140),
	78:  int32(141),
	79:  int32(142),
	80:  int32(143),
	81:  int32(144),
	82:  int32(144),
	83:  int32(145),
	84:  int32(146),
	85:  int32(147),
	86:  int32(148),
	87:  int32(149),
	88:  int32(150),
	89:  int32(150),
	90:  int32(151),
	91:  int32(152),
	92:  int32(153),
	93:  int32(154),
	94:  int32(155),
	95:  int32(155),
	96:  int32(156),
	97:  int32(157),
	98:  int32(158),
	99:  int32(159),
	100: int32(160),
	101: int32(160),
	102: int32(161),
	103: int32(162),
	104: int32(163),
	105: int32(163),
	106: int32(164),
	107: int32(165),
	108: int32(166),
	109: int32(167),
	110: int32(167),
	111: int32(168),
	112: int32(169),
	113: int32(170),
	114: int32(170),
	115: int32(171),
	116: int32(172),
	117: int32(173),
	118: int32(173),
	119: int32(174),
	120: int32(175),
	121: int32(176),
	122: int32(176),
	123: int32(177),
	124: int32(178),
	125: int32(178),
	126: int32(179),
	127: int32(180),
	128: int32(181),
	129: int32(181),
	130: int32(182),
	131: int32(183),
	132: int32(183),
	133: int32(184),
	134: int32(185),
	135: int32(185),
	136: int32(186),
	137: int32(187),
	138: int32(187),
	139: int32(188),
	140: int32(189),
	141: int32(189),
	142: int32(190),
	143: int32(191),
	144: int32(192),
	145: int32(192),
	146: int32(193),
	147: int32(193),
	148: int32(194),
	149: int32(195),
	150: int32(195),
	151: int32(196),
	152: int32(197),
	153: int32(197),
	154: int32(198),
	155: int32(199),
	156: int32(199),
	157: int32(200),
	158: int32(201),
	159: int32(201),
	160: int32(202),
	161: int32(203),
	162: int32(203),
	163: int32(204),
	164: int32(204),
	165: int32(205),
	166: int32(206),
	167: int32(206),
	168: int32(207),
	169: int32(208),
	170: int32(208),
	171: int32(209),
	172: int32(209),
	173: int32(210),
	174: int32(211),
	175: int32(211),
	176: int32(212),
	177: int32(212),
	178: int32(213),
	179: int32(214),
	180: int32(214),
	181: int32(215),
	182: int32(215),
	183: int32(216),
	184: int32(217),
	185: int32(217),
	186: int32(218),
	187: int32(218),
	188: int32(219),
	189: int32(219),
	190: int32(220),
	191: int32(221),
	192: int32(221),
	193: int32(222),
	194: int32(222),
	195: int32(223),
	196: int32(224),
	197: int32(224),
	198: int32(225),
	199: int32(225),
	200: int32(226),
	201: int32(226),
	202: int32(227),
	203: int32(227),
	204: int32(228),
	205: int32(229),
	206: int32(229),
	207: int32(230),
	208: int32(230),
	209: int32(231),
	210: int32(231),
	211: int32(232),
	212: int32(232),
	213: int32(233),
	214: int32(234),
	215: int32(234),
	216: int32(235),
	217: int32(235),
	218: int32(236),
	219: int32(236),
	220: int32(237),
	221: int32(237),
	222: int32(238),
	223: int32(238),
	224: int32(239),
	225: int32(240),
	226: int32(240),
	227: int32(241),
	228: int32(241),
	229: int32(242),
	230: int32(242),
	231: int32(243),
	232: int32(243),
	233: int32(244),
	234: int32(244),
	235: int32(245),
	236: int32(245),
	237: int32(246),
	238: int32(246),
	239: int32(247),
	240: int32(247),
	241: int32(248),
	242: int32(248),
	243: int32(249),
	244: int32(249),
	245: int32(250),
	246: int32(250),
	247: int32(251),
	248: int32(251),
	249: int32(252),
	250: int32(252),
	251: int32(253),
	252: int32(253),
	253: int32(254),
	254: int32(254),
	255: int32(255),
}

func ss_isqrt(tls *libc.TLS, x int32) (r int32) {
	var e, y, v1, v2, v3 int32
	_, _, _, _, _ = e, y, v1, v2, v3
	if x >= libc.Int32FromInt32(SS_BLOCKSIZE)*libc.Int32FromInt32(SS_BLOCKSIZE) {
		return int32(SS_BLOCKSIZE)
	}
	if uint32(x)&uint32(0xffff0000) != 0 {
		if uint32(x)&uint32(0xff000000) != 0 {
			v2 = int32(24) + lg_table[x>>int32(24)&int32(0xff)]
		} else {
			v2 = int32(16) + lg_table[x>>int32(16)&int32(0xff)]
		}
		v1 = v2
	} else {
		if x&int32(0x0000ff00) != 0 {
			v3 = int32(8) + lg_table[x>>int32(8)&int32(0xff)]
		} else {
			v3 = 0 + lg_table[x>>0&int32(0xff)]
		}
		v1 = v3
	}
	e = v1
	if e >= int32(16) {
		y = sqq_table[x>>(e-int32(6)-e&int32(1))] << (e>>int32(1) - int32(7))
		if e >= int32(24) {
			y = (y + int32(1) + x/y) >> int32(1)
		}
		y = (y + int32(1) + x/y) >> int32(1)
	} else {
		if e >= int32(8) {
			y = sqq_table[x>>(e-int32(6)-e&int32(1))]>>(libc.Int32FromInt32(7)-e>>libc.Int32FromInt32(1)) + int32(1)
		} else {
			return sqq_table[x] >> int32(4)
		}
	}
	if x < y*y {
		v1 = y - int32(1)
	} else {
		v1 = y
	}
	return v1
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Compares two suffixes. */
func ss_compare(tls *libc.TLS, T uintptr, p1 uintptr, p2 uintptr, depth int32) (r int32) {
	var U1, U1n, U2, U2n uintptr
	var v2, v3, v4 int32
	_, _, _, _, _, _, _ = U1, U1n, U2, U2n, v2, v3, v4
	U1 = T + uintptr(depth) + uintptr(*(*int32)(unsafe.Pointer(p1)))
	U2 = T + uintptr(depth) + uintptr(*(*int32)(unsafe.Pointer(p2)))
	U1n = T + uintptr(*(*int32)(unsafe.Pointer(p1 + libc.UintptrFromInt32(1)*4))) + uintptr(2)
	U2n = T + uintptr(*(*int32)(unsafe.Pointer(p2 + libc.UintptrFromInt32(1)*4))) + libc.UintptrFromInt32(2)
	for {
		if !(U1 < U1n && U2 < U2n && int32(*(*uint8)(unsafe.Pointer(U1))) == int32(*(*uint8)(unsafe.Pointer(U2)))) {
			break
		}
		goto _1
	_1:
		;
		U1 = U1 + 1
		U2 = U2 + 1
	}
	if U1 < U1n {
		if U2 < U2n {
			v3 = int32(*(*uint8)(unsafe.Pointer(U1))) - int32(*(*uint8)(unsafe.Pointer(U2)))
		} else {
			v3 = int32(1)
		}
		v2 = v3
	} else {
		if U2 < U2n {
			v4 = -int32(1)
		} else {
			v4 = 0
		}
		v2 = v4
	}
	return v2
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Insertionsort for small size groups */
func ss_insertionsort(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, last uintptr, depth int32) {
	var i, j, v4 uintptr
	var r, t, v3 int32
	_, _, _, _, _, _ = i, j, r, t, v3, v4
	i = last - uintptr(2)*4
	for {
		if !(first <= i) {
			break
		}
		t = *(*int32)(unsafe.Pointer(i))
		j = i + libc.UintptrFromInt32(1)*4
		for {
			v3 = ss_compare(tls, T, PA+uintptr(t)*4, PA+uintptr(*(*int32)(unsafe.Pointer(j)))*4, depth)
			r = v3
			if !(0 < v3) {
				break
			}
			for {
				*(*int32)(unsafe.Pointer(j - libc.UintptrFromInt32(1)*4)) = *(*int32)(unsafe.Pointer(j))
				goto _5
			_5:
				;
				j += 4
				v4 = j
				if !(v4 < last && *(*int32)(unsafe.Pointer(j)) < 0) {
					break
				}
			}
			if last <= j {
				break
			}
			goto _2
		_2:
		}
		if r == 0 {
			*(*int32)(unsafe.Pointer(j)) = ^*(*int32)(unsafe.Pointer(j))
		}
		*(*int32)(unsafe.Pointer(j - libc.UintptrFromInt32(1)*4)) = t
		goto _1
	_1:
		;
		i -= 4
	}
}

/*---------------------------------------------------------------------------*/

func ss_fixdown(tls *libc.TLS, Td uintptr, PA uintptr, SA uintptr, i int32, size int32) {
	var c, d, e, j, k, v, v2, v3, v4 int32
	_, _, _, _, _, _, _, _, _ = c, d, e, j, k, v, v2, v3, v4
	v = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
	c = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(v)*4))))))
	for {
		v2 = libc.Int32FromInt32(2)*i + libc.Int32FromInt32(1)
		j = v2
		if !(v2 < size) {
			break
		}
		v4 = j
		j = j + 1
		v3 = v4
		k = v3
		d = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(v3)*4)))*4))))))
		v2 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(j)*4)))*4))))))
		e = v2
		if d < v2 {
			k = j
			d = e
		}
		if d <= c {
			break
		}
		goto _1
	_1:
		;
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(k)*4))
		i = k
	}
	*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = v
}

// C documentation
//
//	/* Simple top-down heapsort. */
func ss_heapsort(tls *libc.TLS, Td uintptr, PA uintptr, SA uintptr, size int32) {
	var i, m, t int32
	_, _, _ = i, m, t
	m = size
	if size%int32(2) == 0 {
		m = m - 1
		if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)))*4)))))) < int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)))*4)))))) {
			t = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)) = t
		}
	}
	i = m/int32(2) - int32(1)
	for {
		if !(0 <= i) {
			break
		}
		ss_fixdown(tls, Td, PA, SA, i, m)
		goto _1
	_1:
		;
		i = i - 1
	}
	if size%int32(2) == 0 {
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
		*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = t
		ss_fixdown(tls, Td, PA, SA, 0, m)
	}
	i = m - int32(1)
	for {
		if !(0 < i) {
			break
		}
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
		ss_fixdown(tls, Td, PA, SA, 0, i)
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = t
		goto _2
	_2:
		;
		i = i - 1
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Returns the median of three elements. */
func ss_median3(tls *libc.TLS, Td uintptr, PA uintptr, v1 uintptr, v2 uintptr, v3 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) {
		t = v1
		v1 = v2
		v2 = t
	}
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
		if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
			return v1
		} else {
			return v3
		}
	}
	return v2
}

// C documentation
//
//	/* Returns the median of five elements. */
func ss_median5(tls *libc.TLS, Td uintptr, PA uintptr, v1 uintptr, v2 uintptr, v3 uintptr, v4 uintptr, v5 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
		t = v2
		v2 = v3
		v3 = t
	}
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v5)))*4)))))) {
		t = v4
		v4 = v5
		v5 = t
	}
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) {
		t = v2
		v2 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
		t = v1
		v1 = v3
		v3 = t
	}
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) {
		t = v1
		v1 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) > int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) {
		return v4
	}
	return v3
}

// C documentation
//
//	/* Returns the pivot element. */
func ss_pivot(tls *libc.TLS, Td uintptr, PA uintptr, first uintptr, last uintptr) (r uintptr) {
	var middle uintptr
	var t int32
	_, _ = middle, t
	t = int32((int64(last) - int64(first)) / 4)
	middle = first + uintptr(t/int32(2))*4
	if t <= int32(512) {
		if t <= int32(32) {
			return ss_median3(tls, Td, PA, first, middle, last-uintptr(1)*4)
		} else {
			t = t >> int32(2)
			return ss_median5(tls, Td, PA, first, first+uintptr(t)*4, middle, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
		}
	}
	t = t >> int32(3)
	first = ss_median3(tls, Td, PA, first, first+uintptr(t)*4, first+uintptr(t<<libc.Int32FromInt32(1))*4)
	middle = ss_median3(tls, Td, PA, middle-uintptr(t)*4, middle, middle+uintptr(t)*4)
	last = ss_median3(tls, Td, PA, last-uintptr(1)*4-uintptr(t<<libc.Int32FromInt32(1))*4, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
	return ss_median3(tls, Td, PA, first, middle, last)
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Binary partition for substrings. */
func ss_partition(tls *libc.TLS, PA uintptr, first uintptr, last uintptr, depth int32) (r uintptr) {
	var a, b, v3 uintptr
	var t int32
	_, _, _, _ = a, b, t, v3
	a = first - uintptr(1)*4
	b = last
	for {
		for {
			a += 4
			v3 = a
			if !(v3 < b && *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))+depth >= *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a))+int32(1))*4))+int32(1)) {
				break
			}
			*(*int32)(unsafe.Pointer(a)) = ^*(*int32)(unsafe.Pointer(a))
			goto _2
		_2:
		}
		for {
			b -= 4
			v3 = b
			if !(a < v3 && *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))+depth < *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b))+int32(1))*4))+int32(1)) {
				break
			}
			goto _4
		_4:
		}
		if b <= a {
			break
		}
		t = ^*(*int32)(unsafe.Pointer(b))
		*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
		*(*int32)(unsafe.Pointer(a)) = t
		goto _1
	_1:
	}
	if first < a {
		*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
	}
	return a
}

// C documentation
//
//	/* Multikey introsort for medium size groups. */
func ss_mintrosort(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, last uintptr, depth int32) {
	var Td, a, b, c, d, e, f, v12, v15 uintptr
	var limit, s, ssize, t, v, x, v3, v4 int32
	var stack [16]struct {
		Fa uintptr
		Fb uintptr
		Fc int32
		Fd int32
	}
	var v2 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = Td, a, b, c, d, e, f, limit, s, ssize, stack, t, v, x, v12, v15, v2, v3, v4
	x = 0
	ssize = 0
	limit = ss_ilg(tls, int32((int64(last)-int64(first))/4))
	for {
		if (int64(last)-int64(first))/4 <= int64(libc.Int32FromInt32(SS_INSERTIONSORT_THRESHOLD)) {
			if int64(1) < (int64(last)-int64(first))/4 {
				ss_insertionsort(tls, T, PA, first, last, depth)
			}
			if v2 = !!(libc.Int32FromInt32(0) <= ssize); !v2 {
				libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(48858))
			}
			_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v3 = ssize
			first = stack[v3].Fa
			last = stack[ssize].Fb
			depth = stack[ssize].Fc
			limit = stack[ssize].Fd
			goto _1
		}
		Td = T + uintptr(depth)
		v3 = limit
		limit = limit - 1
		if v3 == 0 {
			ss_heapsort(tls, Td, PA, first, int32((int64(last)-int64(first))/4))
		}
		if limit < 0 {
			a = first + uintptr(1)*4
			v = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first)))*4))))))
			for {
				if !(a < last) {
					break
				}
				v3 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))))))
				x = v3
				if v3 != v {
					if int64(1) < (int64(a)-int64(first))/4 {
						break
					}
					v = x
					first = a
				}
				goto _5
			_5:
				;
				a += 4
			}
			if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first)))*4))-int32(1))))) < v {
				first = ss_partition(tls, PA, first, a, depth)
			}
			if (int64(a)-int64(first))/4 <= (int64(last)-int64(a))/4 {
				if int64(1) < (int64(a)-int64(first))/4 {
					if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48877))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = a
					stack[ssize].Fb = last
					stack[ssize].Fc = depth
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = -libc.Int32FromInt32(1)
					last = a
					depth = depth + int32(1)
					limit = ss_ilg(tls, int32((int64(a)-int64(first))/4))
				} else {
					first = a
					limit = -libc.Int32FromInt32(1)
				}
			} else {
				if int64(1) < (int64(last)-int64(a))/4 {
					if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48884))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = first
					stack[ssize].Fb = a
					stack[ssize].Fc = depth + int32(1)
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = ss_ilg(tls, int32((int64(a)-int64(first))/4))
					first = a
					limit = -libc.Int32FromInt32(1)
				} else {
					last = a
					depth = depth + int32(1)
					limit = ss_ilg(tls, int32((int64(a)-int64(first))/4))
				}
			}
			goto _1
		}
		/* choose pivot */
		a = ss_pivot(tls, Td, PA, first, last)
		v = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))))))
		t = *(*int32)(unsafe.Pointer(first))
		*(*int32)(unsafe.Pointer(first)) = *(*int32)(unsafe.Pointer(a))
		*(*int32)(unsafe.Pointer(a)) = t
		/* partition */
		b = first
		for {
			b += 4
			v12 = b
			if v2 = v12 < last; v2 {
				v3 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))))))
				x = v3
			}
			if !(v2 && v3 == v) {
				break
			}
			goto _11
		_11:
		}
		v12 = b
		a = v12
		if v12 < last && x < v {
			for {
				b += 4
				v15 = b
				if v2 = v15 < last; v2 {
					v3 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))))))
					x = v3
				}
				if !(v2 && v3 <= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
					*(*int32)(unsafe.Pointer(a)) = t
					a += 4
				}
				goto _16
			_16:
			}
		}
		c = last
		for {
			c -= 4
			v12 = c
			if v2 = b < v12; v2 {
				v3 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4))))))
				x = v3
			}
			if !(v2 && v3 == v) {
				break
			}
			goto _20
		_20:
		}
		v12 = c
		d = v12
		if b < v12 && x > v {
			for {
				c -= 4
				v15 = c
				if v2 = b < v15; v2 {
					v3 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4))))))
					x = v3
				}
				if !(v2 && v3 >= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(c))
					*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
					*(*int32)(unsafe.Pointer(d)) = t
					d -= 4
				}
				goto _25
			_25:
			}
		}
		for {
			if !(b < c) {
				break
			}
			t = *(*int32)(unsafe.Pointer(b))
			*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(c))
			*(*int32)(unsafe.Pointer(c)) = t
			for {
				b += 4
				v12 = b
				if v2 = v12 < c; v2 {
					v3 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))))))
					x = v3
				}
				if !(v2 && v3 <= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
					*(*int32)(unsafe.Pointer(a)) = t
					a += 4
				}
				goto _30
			_30:
			}
			for {
				c -= 4
				v12 = c
				if v2 = b < v12; v2 {
					v3 = int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4))))))
					x = v3
				}
				if !(v2 && v3 >= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(c))
					*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
					*(*int32)(unsafe.Pointer(d)) = t
					d -= 4
				}
				goto _34
			_34:
			}
			goto _29
		_29:
		}
		if a <= d {
			c = b - uintptr(1)*4
			v3 = int32((int64(a) - int64(first)) / 4)
			s = v3
			v4 = int32((int64(b) - int64(a)) / 4)
			t = v4
			if v3 > v4 {
				s = t
			}
			e = first
			f = b - uintptr(s)*4
			for {
				if !(0 < s) {
					break
				}
				t = *(*int32)(unsafe.Pointer(e))
				*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
				*(*int32)(unsafe.Pointer(f)) = t
				goto _40
			_40:
				;
				s = s - 1
				e += 4
				f += 4
			}
			v3 = int32((int64(d) - int64(c)) / 4)
			s = v3
			v4 = int32((int64(last)-int64(d))/4 - libc.Int64FromInt32(1))
			t = v4
			if v3 > v4 {
				s = t
			}
			e = b
			f = last - uintptr(s)*4
			for {
				if !(0 < s) {
					break
				}
				t = *(*int32)(unsafe.Pointer(e))
				*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
				*(*int32)(unsafe.Pointer(f)) = t
				goto _43
			_43:
				;
				s = s - 1
				e += 4
				f += 4
			}
			a = first + uintptr((int64(b)-int64(a))/4)*4
			c = last - uintptr((int64(d)-int64(c))/4)*4
			if v <= int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))-int32(1))))) {
				v12 = a
			} else {
				v12 = ss_partition(tls, PA, a, c, depth)
			}
			b = v12
			if (int64(a)-int64(first))/4 <= (int64(last)-int64(c))/4 {
				if (int64(last)-int64(c))/4 <= (int64(c)-int64(b))/4 {
					if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48934))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = b
					stack[ssize].Fb = c
					stack[ssize].Fc = depth + int32(1)
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48935))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = c
					stack[ssize].Fb = last
					stack[ssize].Fc = depth
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = limit
					last = a
				} else {
					if (int64(a)-int64(first))/4 <= (int64(c)-int64(b))/4 {
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48938))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = c
						stack[ssize].Fb = last
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48939))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = b
						stack[ssize].Fb = c
						stack[ssize].Fc = depth + int32(1)
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
						last = a
					} else {
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48942))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = c
						stack[ssize].Fb = last
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48943))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = first
						stack[ssize].Fb = a
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						first = b
						last = c
						depth = depth + int32(1)
						limit = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					}
				}
			} else {
				if (int64(a)-int64(first))/4 <= (int64(c)-int64(b))/4 {
					if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48948))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = b
					stack[ssize].Fb = c
					stack[ssize].Fc = depth + int32(1)
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48949))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = first
					stack[ssize].Fb = a
					stack[ssize].Fc = depth
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = limit
					first = c
				} else {
					if (int64(last)-int64(c))/4 <= (int64(c)-int64(b))/4 {
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48952))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = first
						stack[ssize].Fb = a
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48953))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = b
						stack[ssize].Fb = c
						stack[ssize].Fc = depth + int32(1)
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
						first = c
					} else {
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48956))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = first
						stack[ssize].Fb = a
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = !!(ssize < libc.Int32FromInt32(SS_MISORT_STACKSIZE)); !v2 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(48957))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = c
						stack[ssize].Fb = last
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						first = b
						last = c
						depth = depth + int32(1)
						limit = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					}
				}
			}
		} else {
			limit = limit + int32(1)
			if int32(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first)))*4))-int32(1))))) < v {
				first = ss_partition(tls, PA, first, last, depth)
				limit = ss_ilg(tls, int32((int64(last)-int64(first))/4))
			}
			depth = depth + int32(1)
		}
		goto _1
	_1:
	}
}

/*---------------------------------------------------------------------------*/

func ss_blockswap(tls *libc.TLS, a uintptr, b uintptr, n int32) {
	var t int32
	_ = t
	for {
		if !(0 < n) {
			break
		}
		t = *(*int32)(unsafe.Pointer(a))
		*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
		*(*int32)(unsafe.Pointer(b)) = t
		goto _1
	_1:
		;
		n = n - 1
		a += 4
		b += 4
	}
}

func ss_rotate(tls *libc.TLS, first uintptr, middle uintptr, last uintptr) {
	var a, b, v2, v3 uintptr
	var l, r, t int32
	_, _, _, _, _, _, _ = a, b, l, r, t, v2, v3
	l = int32((int64(middle) - int64(first)) / 4)
	r = int32((int64(last) - int64(middle)) / 4)
	for {
		if !(0 < l && 0 < r) {
			break
		}
		if l == r {
			ss_blockswap(tls, first, middle, l)
			break
		}
		if l < r {
			a = last - uintptr(1)*4
			b = middle - libc.UintptrFromInt32(1)*4
			t = *(*int32)(unsafe.Pointer(a))
			for cond := true; cond; cond = int32(1) != 0 {
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
				v3 = b
				b -= 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if b < first {
					*(*int32)(unsafe.Pointer(a)) = t
					last = a
					r = r - (l + int32(1))
					if r <= l {
						break
					}
					a = a - uintptr(1)*4
					b = middle - libc.UintptrFromInt32(1)*4
					t = *(*int32)(unsafe.Pointer(a))
				}
			}
		} else {
			a = first
			b = middle
			t = *(*int32)(unsafe.Pointer(a))
			for cond := true; cond; cond = int32(1) != 0 {
				v2 = a
				a += 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
				v3 = b
				b += 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if last <= b {
					*(*int32)(unsafe.Pointer(a)) = t
					first = a + uintptr(1)*4
					l = l - (r + int32(1))
					if l <= r {
						break
					}
					a = a + uintptr(1)*4
					b = middle
					t = *(*int32)(unsafe.Pointer(a))
				}
			}
		}
		goto _1
	_1:
	}
}

/*---------------------------------------------------------------------------*/

func ss_inplacemerge(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, depth int32) {
	var a, b, p, v4 uintptr
	var half, len1, q, r, x, v3 int32
	_, _, _, _, _, _, _, _, _, _ = a, b, half, len1, p, q, r, x, v3, v4
	for {
		if *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) < 0 {
			x = int32(1)
			p = PA + uintptr(^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)))*4
		} else {
			x = 0
			p = PA + uintptr(*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)))*4
		}
		a = first
		len1 = int32((int64(middle) - int64(first)) / 4)
		half = len1 >> int32(1)
		r = -libc.Int32FromInt32(1)
		for {
			if !(0 < len1) {
				break
			}
			b = a + uintptr(half)*4
			if 0 <= *(*int32)(unsafe.Pointer(b)) {
				v3 = *(*int32)(unsafe.Pointer(b))
			} else {
				v3 = ^*(*int32)(unsafe.Pointer(b))
			}
			q = ss_compare(tls, T, PA+uintptr(v3)*4, p, depth)
			if q < 0 {
				a = b + uintptr(1)*4
				half = half - (len1&int32(1) ^ int32(1))
			} else {
				r = q
			}
			goto _2
		_2:
			;
			len1 = half
			half = half >> int32(1)
		}
		if a < middle {
			if r == 0 {
				*(*int32)(unsafe.Pointer(a)) = ^*(*int32)(unsafe.Pointer(a))
			}
			ss_rotate(tls, a, middle, last)
			last = last - uintptr((int64(middle)-int64(a))/4)*4
			middle = a
			if first == middle {
				break
			}
		}
		last -= 4
		if x != 0 {
			for {
				last -= 4
				v4 = last
				if !(*(*int32)(unsafe.Pointer(v4)) < 0) {
					break
				}
			}
		}
		if middle == last {
			break
		}
		goto _1
	_1:
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Merge-forward with internal buffer. */
func ss_mergeforward(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, buf uintptr, depth int32) {
	var a, b, bufend, c, v2, v3 uintptr
	var r, t int32
	_, _, _, _, _, _, _, _ = a, b, bufend, c, r, t, v2, v3
	bufend = buf + uintptr((int64(middle)-int64(first))/4)*4 - uintptr(1)*4
	ss_blockswap(tls, buf, first, int32((int64(middle)-int64(first))/4))
	v2 = first
	a = v2
	t = *(*int32)(unsafe.Pointer(v2))
	b = buf
	c = middle
	for {
		r = ss_compare(tls, T, PA+uintptr(*(*int32)(unsafe.Pointer(b)))*4, PA+uintptr(*(*int32)(unsafe.Pointer(c)))*4, depth)
		if r < 0 {
			for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
				v2 = a
				a += 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
				if bufend <= b {
					*(*int32)(unsafe.Pointer(bufend)) = t
					return
				}
				v2 = b
				b += 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
			}
		} else {
			if r > 0 {
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
					v2 = a
					a += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
					v3 = c
					c += 4
					*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					if last <= c {
						for b < bufend {
							v2 = a
							a += 4
							*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
							v3 = b
							b += 4
							*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
						}
						*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
						*(*int32)(unsafe.Pointer(b)) = t
						return
					}
				}
			} else {
				*(*int32)(unsafe.Pointer(c)) = ^*(*int32)(unsafe.Pointer(c))
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
					v2 = a
					a += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
					if bufend <= b {
						*(*int32)(unsafe.Pointer(bufend)) = t
						return
					}
					v2 = b
					b += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
				}
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
					v2 = a
					a += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
					v3 = c
					c += 4
					*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					if last <= c {
						for b < bufend {
							v2 = a
							a += 4
							*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
							v3 = b
							b += 4
							*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
						}
						*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
						*(*int32)(unsafe.Pointer(b)) = t
						return
					}
				}
			}
		}
		goto _1
	_1:
	}
}

// C documentation
//
//	/* Merge-backward with internal buffer. */
func ss_mergebackward(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, buf uintptr, depth int32) {
	var a, b, bufend, c, p1, p2, v2, v3 uintptr
	var r, t, x int32
	_, _, _, _, _, _, _, _, _, _, _ = a, b, bufend, c, p1, p2, r, t, x, v2, v3
	bufend = buf + uintptr((int64(last)-int64(middle))/4)*4 - uintptr(1)*4
	ss_blockswap(tls, buf, middle, int32((int64(last)-int64(middle))/4))
	x = 0
	if *(*int32)(unsafe.Pointer(bufend)) < 0 {
		p1 = PA + uintptr(^*(*int32)(unsafe.Pointer(bufend)))*4
		x = x | int32(1)
	} else {
		p1 = PA + uintptr(*(*int32)(unsafe.Pointer(bufend)))*4
	}
	if *(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)) < 0 {
		p2 = PA + uintptr(^*(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)))*4
		x = x | int32(2)
	} else {
		p2 = PA + uintptr(*(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)))*4
	}
	v2 = last - libc.UintptrFromInt32(1)*4
	a = v2
	t = *(*int32)(unsafe.Pointer(v2))
	b = bufend
	c = middle - libc.UintptrFromInt32(1)*4
	for {
		r = ss_compare(tls, T, p1, p2, depth)
		if 0 < r {
			if x&int32(1) != 0 {
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
					v2 = a
					a -= 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
					v3 = b
					b -= 4
					*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				}
				x = x ^ int32(1)
			}
			v2 = a
			a -= 4
			*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
			if b <= buf {
				*(*int32)(unsafe.Pointer(buf)) = t
				break
			}
			v2 = b
			b -= 4
			*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
			if *(*int32)(unsafe.Pointer(b)) < 0 {
				p1 = PA + uintptr(^*(*int32)(unsafe.Pointer(b)))*4
				x = x | int32(1)
			} else {
				p1 = PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4
			}
		} else {
			if r < 0 {
				if x&int32(2) != 0 {
					for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
						v3 = c
						c -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					x = x ^ int32(2)
				}
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
				v3 = c
				c -= 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if c < first {
					for buf < b {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
						v3 = b
						b -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = t
					break
				}
				if *(*int32)(unsafe.Pointer(c)) < 0 {
					p2 = PA + uintptr(^*(*int32)(unsafe.Pointer(c)))*4
					x = x | int32(2)
				} else {
					p2 = PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4
				}
			} else {
				if x&int32(1) != 0 {
					for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
						v3 = b
						b -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					x = x ^ int32(1)
				}
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = ^*(*int32)(unsafe.Pointer(b))
				if b <= buf {
					*(*int32)(unsafe.Pointer(buf)) = t
					break
				}
				v2 = b
				b -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
				if x&int32(2) != 0 {
					for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
						v3 = c
						c -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					x = x ^ int32(2)
				}
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
				v3 = c
				c -= 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if c < first {
					for buf < b {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
						v3 = b
						b -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = t
					break
				}
				if *(*int32)(unsafe.Pointer(b)) < 0 {
					p1 = PA + uintptr(^*(*int32)(unsafe.Pointer(b)))*4
					x = x | int32(1)
				} else {
					p1 = PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4
				}
				if *(*int32)(unsafe.Pointer(c)) < 0 {
					p2 = PA + uintptr(^*(*int32)(unsafe.Pointer(c)))*4
					x = x | int32(2)
				} else {
					p2 = PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4
				}
			}
		}
		goto _1
	_1:
	}
}

// C documentation
//
//	/* D&C based merge. */
func ss_swapmerge(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, buf uintptr, bufsize int32, depth int32) {
	var check, half, len1, m, next, ssize, v2, v5 int32
	var l, lm, r, rm, v20 uintptr
	var stack [32]struct {
		Fa uintptr
		Fb uintptr
		Fc uintptr
		Fd int32
	}
	var v3, v4 bool
	var v17 int64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = check, half, l, len1, lm, m, next, r, rm, ssize, stack, v17, v2, v20, v3, v4, v5
	check = 0
	ssize = libc.Int32FromInt32(0)
	for {
		if (int64(last)-int64(middle))/4 <= int64(bufsize) {
			if first < middle && middle < last {
				ss_mergebackward(tls, T, PA, first, middle, last, buf, depth)
			}
			if v4 = check&int32(1) != 0; !v4 {
				if v3 = check&int32(2) != 0; v3 {
					if 0 <= *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)) {
						v2 = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					} else {
						v2 = ^*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					}
				}
			}
			if v4 || v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(first)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
			}
			if v3 = check&int32(4) != 0; v3 {
				if 0 <= *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) {
					v2 = *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				} else {
					v2 = ^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				}
			}
			if v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(last)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(last)) = ^*(*int32)(unsafe.Pointer(last))
			}
			if v3 = !!(libc.Int32FromInt32(0) <= ssize); !v3 {
				libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49211))
			}
			_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v2 = ssize
			first = stack[v2].Fa
			middle = stack[ssize].Fb
			last = stack[ssize].Fc
			check = stack[ssize].Fd
			goto _1
		}
		if (int64(middle)-int64(first))/4 <= int64(bufsize) {
			if first < middle {
				ss_mergeforward(tls, T, PA, first, middle, last, buf, depth)
			}
			if v4 = check&int32(1) != 0; !v4 {
				if v3 = check&int32(2) != 0; v3 {
					if 0 <= *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)) {
						v2 = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					} else {
						v2 = ^*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					}
				}
			}
			if v4 || v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(first)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
			}
			if v3 = check&int32(4) != 0; v3 {
				if 0 <= *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) {
					v2 = *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				} else {
					v2 = ^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				}
			}
			if v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(last)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(last)) = ^*(*int32)(unsafe.Pointer(last))
			}
			if v3 = !!(libc.Int32FromInt32(0) <= ssize); !v3 {
				libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49220))
			}
			_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v2 = ssize
			first = stack[v2].Fa
			middle = stack[ssize].Fb
			last = stack[ssize].Fc
			check = stack[ssize].Fd
			goto _1
		}
		m = 0
		if (int64(middle)-int64(first))/4 < (int64(last)-int64(middle))/4 {
			v17 = (int64(middle) - int64(first)) / 4
		} else {
			v17 = (int64(last) - int64(middle)) / 4
		}
		len1 = int32(v17)
		half = len1 >> libc.Int32FromInt32(1)
		for {
			if !(0 < len1) {
				break
			}
			if 0 <= *(*int32)(unsafe.Pointer(middle + uintptr(m)*4 + uintptr(half)*4)) {
				v2 = *(*int32)(unsafe.Pointer(middle + uintptr(m)*4 + uintptr(half)*4))
			} else {
				v2 = ^*(*int32)(unsafe.Pointer(middle + uintptr(m)*4 + uintptr(half)*4))
			}
			if 0 <= *(*int32)(unsafe.Pointer(middle - uintptr(m)*4 - uintptr(half)*4 - libc.UintptrFromInt32(1)*4)) {
				v5 = *(*int32)(unsafe.Pointer(middle - uintptr(m)*4 - uintptr(half)*4 - libc.UintptrFromInt32(1)*4))
			} else {
				v5 = ^*(*int32)(unsafe.Pointer(middle - uintptr(m)*4 - uintptr(half)*4 - libc.UintptrFromInt32(1)*4))
			}
			if ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(v5)*4, depth) < 0 {
				m = m + (half + int32(1))
				half = half - (len1&int32(1) ^ int32(1))
			}
			goto _16
		_16:
			;
			len1 = half
			half = half >> int32(1)
		}
		if 0 < m {
			lm = middle - uintptr(m)*4
			rm = middle + uintptr(m)*4
			ss_blockswap(tls, lm, middle, m)
			v20 = middle
			r = v20
			l = v20
			next = libc.Int32FromInt32(0)
			if rm < last {
				if *(*int32)(unsafe.Pointer(rm)) < 0 {
					*(*int32)(unsafe.Pointer(rm)) = ^*(*int32)(unsafe.Pointer(rm))
					if first < lm {
						for {
							l -= 4
							v20 = l
							if !(*(*int32)(unsafe.Pointer(v20)) < 0) {
								break
							}
							goto _21
						_21:
						}
						next = next | int32(4)
					}
					next = next | int32(1)
				} else {
					if first < lm {
						for {
							if !(*(*int32)(unsafe.Pointer(r)) < 0) {
								break
							}
							goto _23
						_23:
							;
							r += 4
						}
						next = next | int32(2)
					}
				}
			}
			if (int64(l)-int64(first))/4 <= (int64(last)-int64(r))/4 {
				if v3 = !!(ssize < libc.Int32FromInt32(SS_SMERGE_STACKSIZE)); !v3 {
					libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49250))
				}
				_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
				stack[ssize].Fa = r
				stack[ssize].Fb = rm
				stack[ssize].Fc = last
				v2 = ssize
				ssize = ssize + 1
				stack[v2].Fd = next&libc.Int32FromInt32(3) | check&libc.Int32FromInt32(4)
				middle = lm
				last = l
				check = check&libc.Int32FromInt32(3) | next&libc.Int32FromInt32(4)
			} else {
				if next&int32(2) != 0 && r == middle {
					next = next ^ int32(6)
				}
				if v3 = !!(ssize < libc.Int32FromInt32(SS_SMERGE_STACKSIZE)); !v3 {
					libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49254))
				}
				_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
				stack[ssize].Fa = first
				stack[ssize].Fb = lm
				stack[ssize].Fc = l
				v2 = ssize
				ssize = ssize + 1
				stack[v2].Fd = check&libc.Int32FromInt32(3) | next&libc.Int32FromInt32(4)
				first = r
				middle = rm
				check = next&libc.Int32FromInt32(3) | check&libc.Int32FromInt32(4)
			}
		} else {
			if 0 <= *(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)) {
				v2 = *(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4))
			} else {
				v2 = ^*(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4))
			}
			if ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(middle)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(middle)) = ^*(*int32)(unsafe.Pointer(middle))
			}
			if v4 = check&int32(1) != 0; !v4 {
				if v3 = check&int32(2) != 0; v3 {
					if 0 <= *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)) {
						v2 = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					} else {
						v2 = ^*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					}
				}
			}
			if v4 || v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(first)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
			}
			if v3 = check&int32(4) != 0; v3 {
				if 0 <= *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) {
					v2 = *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				} else {
					v2 = ^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				}
			}
			if v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(last)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(last)) = ^*(*int32)(unsafe.Pointer(last))
			}
			if v3 = !!(libc.Int32FromInt32(0) <= ssize); !v3 {
				libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49262))
			}
			_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v2 = ssize
			first = stack[v2].Fa
			middle = stack[ssize].Fb
			last = stack[ssize].Fc
			check = stack[ssize].Fd
		}
		goto _1
	_1:
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Substring sort */
func sssort(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, last uintptr, buf uintptr, bufsize int32, depth int32, n int32, lastsuffix int32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var a, b, curbuf, middle, v3 uintptr
	var curbufsize, i, j, k, limit, v1 int32
	var v2 bool
	var _ /* PAi at bp+0 */ [2]int32
	_, _, _, _, _, _, _, _, _, _, _, _ = a, b, curbuf, curbufsize, i, j, k, limit, middle, v1, v2, v3
	if lastsuffix != 0 {
		first += 4
	}
	if v2 = bufsize < int32(SS_BLOCKSIZE) && int64(bufsize) < (int64(last)-int64(first))/4; v2 {
		v1 = ss_isqrt(tls, int32((int64(last)-int64(first))/4))
		limit = v1
	}
	if v2 && bufsize < v1 {
		if int32(SS_BLOCKSIZE) < limit {
			limit = int32(SS_BLOCKSIZE)
		}
		v3 = last - uintptr(limit)*4
		middle = v3
		buf = v3
		bufsize = limit
	} else {
		middle = last
		limit = libc.Int32FromInt32(0)
	}
	a = first
	i = libc.Int32FromInt32(0)
	for {
		if !(int64(libc.Int32FromInt32(SS_BLOCKSIZE)) < (int64(middle)-int64(a))/4) {
			break
		}
		ss_mintrosort(tls, T, PA, a, a+uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4, depth)
		curbufsize = int32((int64(last) - int64(a+uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4)) / 4)
		curbuf = a + uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4
		if curbufsize <= bufsize {
			curbufsize = bufsize
			curbuf = buf
		}
		b = a
		k = int32(SS_BLOCKSIZE)
		j = i
		for {
			if !(j&int32(1) != 0) {
				break
			}
			ss_swapmerge(tls, T, PA, b-uintptr(k)*4, b, b+uintptr(k)*4, curbuf, curbufsize, depth)
			goto _5
		_5:
			;
			b = b - uintptr(k)*4
			k = k << int32(1)
			j = j >> int32(1)
		}
		goto _4
	_4:
		;
		a = a + uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4
		i = i + 1
	}
	ss_mintrosort(tls, T, PA, a, middle, depth)
	k = int32(SS_BLOCKSIZE)
	for {
		if !(i != 0) {
			break
		}
		if i&int32(1) != 0 {
			ss_swapmerge(tls, T, PA, a-uintptr(k)*4, a, middle, buf, bufsize, depth)
			a = a - uintptr(k)*4
		}
		goto _6
	_6:
		;
		k = k << int32(1)
		i = i >> int32(1)
	}
	if limit != 0 {
		ss_mintrosort(tls, T, PA, middle, last, depth)
		ss_inplacemerge(tls, T, PA, first, middle, last, depth)
	}
	if lastsuffix != 0 {
		(*(*[2]int32)(unsafe.Pointer(bp)))[0] = *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)))*4))
		(*(*[2]int32)(unsafe.Pointer(bp)))[int32(1)] = n - libc.Int32FromInt32(2)
		a = first
		i = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
		for {
			if !(a < last && (*(*int32)(unsafe.Pointer(a)) < 0 || 0 < ss_compare(tls, T, bp, PA+uintptr(*(*int32)(unsafe.Pointer(a)))*4, depth))) {
				break
			}
			*(*int32)(unsafe.Pointer(a - libc.UintptrFromInt32(1)*4)) = *(*int32)(unsafe.Pointer(a))
			goto _7
		_7:
			;
			a += 4
		}
		*(*int32)(unsafe.Pointer(a - libc.UintptrFromInt32(1)*4)) = i
	}
}

/*---------------------------------------------------------------------------*/

func tr_ilg(tls *libc.TLS, n int32) (r int32) {
	var v1, v2, v3 int32
	_, _, _ = v1, v2, v3
	if uint32(n)&uint32(0xffff0000) != 0 {
		if uint32(n)&uint32(0xff000000) != 0 {
			v2 = int32(24) + lg_table[n>>int32(24)&int32(0xff)]
		} else {
			v2 = int32(16) + lg_table[n>>int32(16)&int32(0xff)]
		}
		v1 = v2
	} else {
		if n&int32(0x0000ff00) != 0 {
			v3 = int32(8) + lg_table[n>>int32(8)&int32(0xff)]
		} else {
			v3 = 0 + lg_table[n>>0&int32(0xff)]
		}
		v1 = v3
	}
	return v1
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Simple insertionsort for small size groups. */
func tr_insertionsort(tls *libc.TLS, ISAd uintptr, first uintptr, last uintptr) {
	var a, b, v4 uintptr
	var r, t, v3 int32
	_, _, _, _, _, _ = a, b, r, t, v3, v4
	a = first + uintptr(1)*4
	for {
		if !(a < last) {
			break
		}
		t = *(*int32)(unsafe.Pointer(a))
		b = a - libc.UintptrFromInt32(1)*4
		for {
			v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(t)*4)) - *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
			r = v3
			if !(0 > v3) {
				break
			}
			for {
				*(*int32)(unsafe.Pointer(b + libc.UintptrFromInt32(1)*4)) = *(*int32)(unsafe.Pointer(b))
				goto _5
			_5:
				;
				b -= 4
				v4 = b
				if !(first <= v4 && *(*int32)(unsafe.Pointer(b)) < 0) {
					break
				}
			}
			if b < first {
				break
			}
			goto _2
		_2:
		}
		if r == 0 {
			*(*int32)(unsafe.Pointer(b)) = ^*(*int32)(unsafe.Pointer(b))
		}
		*(*int32)(unsafe.Pointer(b + libc.UintptrFromInt32(1)*4)) = t
		goto _1
	_1:
		;
		a += 4
	}
}

/*---------------------------------------------------------------------------*/

func tr_fixdown(tls *libc.TLS, ISAd uintptr, SA uintptr, i int32, size int32) {
	var c, d, e, j, k, v, v2, v3, v4 int32
	_, _, _, _, _, _, _, _, _ = c, d, e, j, k, v, v2, v3, v4
	v = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
	c = *(*int32)(unsafe.Pointer(ISAd + uintptr(v)*4))
	for {
		v2 = libc.Int32FromInt32(2)*i + libc.Int32FromInt32(1)
		j = v2
		if !(v2 < size) {
			break
		}
		v4 = j
		j = j + 1
		v3 = v4
		k = v3
		d = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(v3)*4)))*4))
		v2 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(j)*4)))*4))
		e = v2
		if d < v2 {
			k = j
			d = e
		}
		if d <= c {
			break
		}
		goto _1
	_1:
		;
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(k)*4))
		i = k
	}
	*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = v
}

// C documentation
//
//	/* Simple top-down heapsort. */
func tr_heapsort(tls *libc.TLS, ISAd uintptr, SA uintptr, size int32) {
	var i, m, t int32
	_, _, _ = i, m, t
	m = size
	if size%int32(2) == 0 {
		m = m - 1
		if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)))*4)) < *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)))*4)) {
			t = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)) = t
		}
	}
	i = m/int32(2) - int32(1)
	for {
		if !(0 <= i) {
			break
		}
		tr_fixdown(tls, ISAd, SA, i, m)
		goto _1
	_1:
		;
		i = i - 1
	}
	if size%int32(2) == 0 {
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
		*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = t
		tr_fixdown(tls, ISAd, SA, 0, m)
	}
	i = m - int32(1)
	for {
		if !(0 < i) {
			break
		}
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
		tr_fixdown(tls, ISAd, SA, 0, i)
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = t
		goto _2
	_2:
		;
		i = i - 1
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Returns the median of three elements. */
func tr_median3(tls *libc.TLS, ISAd uintptr, v1 uintptr, v2 uintptr, v3 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) {
		t = v1
		v1 = v2
		v2 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
		if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
			return v1
		} else {
			return v3
		}
	}
	return v2
}

// C documentation
//
//	/* Returns the median of five elements. */
func tr_median5(tls *libc.TLS, ISAd uintptr, v1 uintptr, v2 uintptr, v3 uintptr, v4 uintptr, v5 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
		t = v2
		v2 = v3
		v3 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v5)))*4)) {
		t = v4
		v4 = v5
		v5 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) {
		t = v2
		v2 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
		t = v1
		v1 = v3
		v3 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) {
		t = v1
		v1 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) {
		return v4
	}
	return v3
}

// C documentation
//
//	/* Returns the pivot element. */
func tr_pivot(tls *libc.TLS, ISAd uintptr, first uintptr, last uintptr) (r uintptr) {
	var middle uintptr
	var t int32
	_, _ = middle, t
	t = int32((int64(last) - int64(first)) / 4)
	middle = first + uintptr(t/int32(2))*4
	if t <= int32(512) {
		if t <= int32(32) {
			return tr_median3(tls, ISAd, first, middle, last-uintptr(1)*4)
		} else {
			t = t >> int32(2)
			return tr_median5(tls, ISAd, first, first+uintptr(t)*4, middle, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
		}
	}
	t = t >> int32(3)
	first = tr_median3(tls, ISAd, first, first+uintptr(t)*4, first+uintptr(t<<libc.Int32FromInt32(1))*4)
	middle = tr_median3(tls, ISAd, middle-uintptr(t)*4, middle, middle+uintptr(t)*4)
	last = tr_median3(tls, ISAd, last-uintptr(1)*4-uintptr(t<<libc.Int32FromInt32(1))*4, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
	return tr_median3(tls, ISAd, first, middle, last)
}

/*---------------------------------------------------------------------------*/

type trbudget_t = struct {
	Fchance int32
	Fremain int32
	Fincval int32
	Fcount  int32
}

/*---------------------------------------------------------------------------*/

type _trbudget_t = trbudget_t

func trbudget_init(tls *libc.TLS, budget uintptr, chance int32, incval int32) {
	var v1 int32
	_ = v1
	(*trbudget_t)(unsafe.Pointer(budget)).Fchance = chance
	v1 = incval
	(*trbudget_t)(unsafe.Pointer(budget)).Fincval = v1
	(*trbudget_t)(unsafe.Pointer(budget)).Fremain = v1
}

func trbudget_check(tls *libc.TLS, budget uintptr, size int32) (r int32) {
	if size <= (*trbudget_t)(unsafe.Pointer(budget)).Fremain {
		*(*int32)(unsafe.Pointer(budget + 4)) -= size
		return int32(1)
	}
	if (*trbudget_t)(unsafe.Pointer(budget)).Fchance == 0 {
		*(*int32)(unsafe.Pointer(budget + 12)) += size
		return 0
	}
	*(*int32)(unsafe.Pointer(budget + 4)) += (*trbudget_t)(unsafe.Pointer(budget)).Fincval - size
	*(*int32)(unsafe.Pointer(budget)) -= int32(1)
	return int32(1)
}

/*---------------------------------------------------------------------------*/

func tr_partition(tls *libc.TLS, ISAd uintptr, first uintptr, middle uintptr, last uintptr, pa uintptr, pb uintptr, v int32) {
	var a, b, c, d, e, f, v2, v5 uintptr
	var s, t, x, v3, v8 int32
	var v4 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = a, b, c, d, e, f, s, t, x, v2, v3, v4, v5, v8
	x = 0
	b = middle - uintptr(1)*4
	for {
		b += 4
		v2 = b
		if v4 = v2 < last; v4 {
			v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
			x = v3
		}
		if !(v4 && v3 == v) {
			break
		}
		goto _1
	_1:
	}
	v2 = b
	a = v2
	if v2 < last && x < v {
		for {
			b += 4
			v5 = b
			if v4 = v5 < last; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
				x = v3
			}
			if !(v4 && v3 <= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(b))
				*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
				*(*int32)(unsafe.Pointer(a)) = t
				a += 4
			}
			goto _6
		_6:
		}
	}
	c = last
	for {
		c -= 4
		v2 = c
		if v4 = b < v2; v4 {
			v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(c)))*4))
			x = v3
		}
		if !(v4 && v3 == v) {
			break
		}
		goto _10
	_10:
	}
	v2 = c
	d = v2
	if b < v2 && x > v {
		for {
			c -= 4
			v5 = c
			if v4 = b < v5; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(c)))*4))
				x = v3
			}
			if !(v4 && v3 >= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(c))
				*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
				*(*int32)(unsafe.Pointer(d)) = t
				d -= 4
			}
			goto _15
		_15:
		}
	}
	for {
		if !(b < c) {
			break
		}
		t = *(*int32)(unsafe.Pointer(b))
		*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(c))
		*(*int32)(unsafe.Pointer(c)) = t
		for {
			b += 4
			v2 = b
			if v4 = v2 < c; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
				x = v3
			}
			if !(v4 && v3 <= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(b))
				*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
				*(*int32)(unsafe.Pointer(a)) = t
				a += 4
			}
			goto _20
		_20:
		}
		for {
			c -= 4
			v2 = c
			if v4 = b < v2; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(c)))*4))
				x = v3
			}
			if !(v4 && v3 >= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(c))
				*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
				*(*int32)(unsafe.Pointer(d)) = t
				d -= 4
			}
			goto _24
		_24:
		}
		goto _19
	_19:
	}
	if a <= d {
		c = b - uintptr(1)*4
		v3 = int32((int64(a) - int64(first)) / 4)
		s = v3
		v8 = int32((int64(b) - int64(a)) / 4)
		t = v8
		if v3 > v8 {
			s = t
		}
		e = first
		f = b - uintptr(s)*4
		for {
			if !(0 < s) {
				break
			}
			t = *(*int32)(unsafe.Pointer(e))
			*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
			*(*int32)(unsafe.Pointer(f)) = t
			goto _30
		_30:
			;
			s = s - 1
			e += 4
			f += 4
		}
		v3 = int32((int64(d) - int64(c)) / 4)
		s = v3
		v8 = int32((int64(last)-int64(d))/4 - libc.Int64FromInt32(1))
		t = v8
		if v3 > v8 {
			s = t
		}
		e = b
		f = last - uintptr(s)*4
		for {
			if !(0 < s) {
				break
			}
			t = *(*int32)(unsafe.Pointer(e))
			*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
			*(*int32)(unsafe.Pointer(f)) = t
			goto _33
		_33:
			;
			s = s - 1
			e += 4
			f += 4
		}
		first = first + uintptr((int64(b)-int64(a))/4)*4
		last = last - uintptr((int64(d)-int64(c))/4)*4
	}
	*(*uintptr)(unsafe.Pointer(pa)) = first
	*(*uintptr)(unsafe.Pointer(pb)) = last
}

func tr_copy(tls *libc.TLS, ISA uintptr, SA uintptr, first uintptr, a uintptr, b uintptr, last uintptr, depth int32) {
	var c, d, e, v3 uintptr
	var s, v, v2 int32
	_, _, _, _, _, _, _ = c, d, e, s, v, v2, v3
	v = int32((int64(b)-int64(SA))/4 - int64(1))
	c = first
	d = a - libc.UintptrFromInt32(1)*4
	for {
		if !(c <= d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d += 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = int32((int64(d) - int64(SA)) / 4)
		}
		goto _1
	_1:
		;
		c += 4
	}
	c = last - uintptr(1)*4
	e = d + uintptr(1)*4
	d = b
	for {
		if !(e < d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d -= 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = int32((int64(d) - int64(SA)) / 4)
		}
		goto _4
	_4:
		;
		c -= 4
	}
}

func tr_partialcopy(tls *libc.TLS, ISA uintptr, SA uintptr, first uintptr, a uintptr, b uintptr, last uintptr, depth int32) {
	var c, d, e, v3 uintptr
	var lastrank, newrank, rank, s, v, v2 int32
	_, _, _, _, _, _, _, _, _, _ = c, d, e, lastrank, newrank, rank, s, v, v2, v3
	newrank = -int32(1)
	v = int32((int64(b)-int64(SA))/4 - int64(1))
	lastrank = -int32(1)
	c = first
	d = a - libc.UintptrFromInt32(1)*4
	for {
		if !(c <= d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d += 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			rank = *(*int32)(unsafe.Pointer(ISA + uintptr(s+depth)*4))
			if lastrank != rank {
				lastrank = rank
				newrank = int32((int64(d) - int64(SA)) / 4)
			}
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = newrank
		}
		goto _1
	_1:
		;
		c += 4
	}
	lastrank = -int32(1)
	e = d
	for {
		if !(first <= e) {
			break
		}
		rank = *(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(e)))*4))
		if lastrank != rank {
			lastrank = rank
			newrank = int32((int64(e) - int64(SA)) / 4)
		}
		if newrank != rank {
			*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(e)))*4)) = newrank
		}
		goto _4
	_4:
		;
		e -= 4
	}
	lastrank = -int32(1)
	c = last - uintptr(1)*4
	e = d + uintptr(1)*4
	d = b
	for {
		if !(e < d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d -= 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			rank = *(*int32)(unsafe.Pointer(ISA + uintptr(s+depth)*4))
			if lastrank != rank {
				lastrank = rank
				newrank = int32((int64(d) - int64(SA)) / 4)
			}
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = newrank
		}
		goto _5
	_5:
		;
		c -= 4
	}
}

func tr_introsort(tls *libc.TLS, ISA uintptr, ISAd uintptr, SA uintptr, first uintptr, last uintptr, budget uintptr) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var c, v19 uintptr
	var incr, limit, next, ssize, t, trlink, v, x, v5 int32
	var stack [64]struct {
		Fa uintptr
		Fb uintptr
		Fc uintptr
		Fd int32
		Fe int32
	}
	var v4 bool
	var _ /* a at bp+0 */ uintptr
	var _ /* b at bp+8 */ uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _ = c, incr, limit, next, ssize, stack, t, trlink, v, x, v19, v4, v5
	x = 0
	incr = int32((int64(ISAd) - int64(ISA)) / 4)
	trlink = -int32(1)
	ssize = 0
	limit = tr_ilg(tls, int32((int64(last)-int64(first))/4))
	for {
		if limit < 0 {
			if limit == -int32(1) {
				/* tandem repeat partition */
				tr_partition(tls, ISAd-uintptr(incr)*4, first, first, last, bp, bp+8, int32((int64(last)-int64(SA))/4-int64(1)))
				/* update ranks */
				if *(*uintptr)(unsafe.Pointer(bp)) < last {
					c = first
					v = int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(SA))/4 - libc.Int64FromInt32(1))
					for {
						if !(c < *(*uintptr)(unsafe.Pointer(bp))) {
							break
						}
						*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
						goto _2
					_2:
						;
						c += 4
					}
				}
				if *(*uintptr)(unsafe.Pointer(bp + 8)) < last {
					c = *(*uintptr)(unsafe.Pointer(bp))
					v = int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(SA))/4 - libc.Int64FromInt32(1))
					for {
						if !(c < *(*uintptr)(unsafe.Pointer(bp + 8))) {
							break
						}
						*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
						goto _3
					_3:
						;
						c += 4
					}
				}
				/* push */
				if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
					if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49644))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = libc.UintptrFromInt32(0)
					stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
					stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
					stack[ssize].Fd = 0
					v5 = ssize
					ssize = ssize + 1
					stack[v5].Fe = libc.Int32FromInt32(0)
					if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
						libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49645))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = ISAd - uintptr(incr)*4
					stack[ssize].Fb = first
					stack[ssize].Fc = last
					stack[ssize].Fd = -int32(2)
					v5 = ssize
					ssize = ssize + 1
					stack[v5].Fe = trlink
					trlink = ssize - int32(2)
				}
				if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
					if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
						if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49650))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
						stack[ssize].Fc = last
						stack[ssize].Fd = tr_ilg(tls, int32((int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4))
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						last = *(*uintptr)(unsafe.Pointer(bp))
						limit = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4))
					} else {
						if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
							first = *(*uintptr)(unsafe.Pointer(bp + 8))
							limit = tr_ilg(tls, int32((int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4))
						} else {
							if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
								libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49655))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				} else {
					if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
						if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49659))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = first
						stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
						stack[ssize].Fd = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4))
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						first = *(*uintptr)(unsafe.Pointer(bp + 8))
						limit = tr_ilg(tls, int32((int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4))
					} else {
						if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
							last = *(*uintptr)(unsafe.Pointer(bp))
							limit = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4))
						} else {
							if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
								libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49664))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				}
			} else {
				if limit == -int32(2) {
					/* tandem repeat copy */
					ssize = ssize - 1
					v5 = ssize
					*(*uintptr)(unsafe.Pointer(bp)) = stack[v5].Fb
					/* tandem repeat copy */
					*(*uintptr)(unsafe.Pointer(bp + 8)) = stack[ssize].Fc
					if stack[ssize].Fd == 0 {
						tr_copy(tls, ISA, SA, first, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), last, int32((int64(ISAd)-int64(ISA))/4))
					} else {
						if 0 <= trlink {
							stack[trlink].Fd = -int32(1)
						}
						tr_partialcopy(tls, ISA, SA, first, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), last, int32((int64(ISAd)-int64(ISA))/4))
					}
					if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
						libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49676))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if ssize == 0 {
						return
					}
					ssize = ssize - 1
					v5 = ssize
					ISAd = stack[v5].Fa
					first = stack[ssize].Fb
					last = stack[ssize].Fc
					limit = stack[ssize].Fd
					trlink = stack[ssize].Fe
				} else {
					/* sorted partition */
					if 0 <= *(*int32)(unsafe.Pointer(first)) {
						*(*uintptr)(unsafe.Pointer(bp)) = first
						for {
							*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) = int32((int64(*(*uintptr)(unsafe.Pointer(bp))) - int64(SA)) / 4)
							goto _20
						_20:
							;
							*(*uintptr)(unsafe.Pointer(bp)) += 4
							v19 = *(*uintptr)(unsafe.Pointer(bp))
							if !(v19 < last && 0 <= *(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp))))) {
								break
							}
						}
						first = *(*uintptr)(unsafe.Pointer(bp))
					}
					if first < last {
						*(*uintptr)(unsafe.Pointer(bp)) = first
						for {
							*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))) = ^*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp))))
							goto _22
						_22:
							;
							*(*uintptr)(unsafe.Pointer(bp)) += 4
							v19 = *(*uintptr)(unsafe.Pointer(bp))
							if !(*(*int32)(unsafe.Pointer(v19)) < 0) {
								break
							}
						}
						if *(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) != *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) {
							v5 = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4+int64(1)))
						} else {
							v5 = -int32(1)
						}
						next = v5
						*(*uintptr)(unsafe.Pointer(bp)) += 4
						v19 = *(*uintptr)(unsafe.Pointer(bp))
						if v19 < last {
							*(*uintptr)(unsafe.Pointer(bp + 8)) = first
							v = int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(SA))/4 - libc.Int64FromInt32(1))
							for {
								if !(*(*uintptr)(unsafe.Pointer(bp + 8)) < *(*uintptr)(unsafe.Pointer(bp))) {
									break
								}
								*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8)))))*4)) = v
								goto _25
							_25:
								;
								*(*uintptr)(unsafe.Pointer(bp + 8)) += 4
							}
						}
						/* push */
						if trbudget_check(tls, budget, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4)) != 0 {
							if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49692))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = last
								stack[ssize].Fd = -int32(3)
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								ISAd = ISAd + uintptr(incr)*4
								last = *(*uintptr)(unsafe.Pointer(bp))
								limit = next
							} else {
								if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
									if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
										libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49696))
									}
									_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
									stack[ssize].Fa = ISAd + uintptr(incr)*4
									stack[ssize].Fb = first
									stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
									stack[ssize].Fd = next
									v5 = ssize
									ssize = ssize + 1
									stack[v5].Fe = trlink
									first = *(*uintptr)(unsafe.Pointer(bp))
									limit = -libc.Int32FromInt32(3)
								} else {
									ISAd = ISAd + uintptr(incr)*4
									last = *(*uintptr)(unsafe.Pointer(bp))
									limit = next
								}
							}
						} else {
							if 0 <= trlink {
								stack[trlink].Fd = -int32(1)
							}
							if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
								first = *(*uintptr)(unsafe.Pointer(bp))
								limit = -libc.Int32FromInt32(3)
							} else {
								if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
									libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49707))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								if ssize == 0 {
									return
								}
								ssize = ssize - 1
								v5 = ssize
								ISAd = stack[v5].Fa
								first = stack[ssize].Fb
								last = stack[ssize].Fc
								limit = stack[ssize].Fd
								trlink = stack[ssize].Fe
							}
						}
					} else {
						if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
							libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49711))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						if ssize == 0 {
							return
						}
						ssize = ssize - 1
						v5 = ssize
						ISAd = stack[v5].Fa
						first = stack[ssize].Fb
						last = stack[ssize].Fc
						limit = stack[ssize].Fd
						trlink = stack[ssize].Fe
					}
				}
			}
			goto _1
		}
		if (int64(last)-int64(first))/4 <= int64(libc.Int32FromInt32(TR_INSERTIONSORT_THRESHOLD)) {
			tr_insertionsort(tls, ISAd, first, last)
			limit = -int32(3)
			goto _1
		}
		v5 = limit
		limit = limit - 1
		if v5 == 0 {
			tr_heapsort(tls, ISAd, first, int32((int64(last)-int64(first))/4))
			*(*uintptr)(unsafe.Pointer(bp)) = last - uintptr(1)*4
			for {
				if !(first < *(*uintptr)(unsafe.Pointer(bp))) {
					break
				}
				x = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4))
				*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp)) - libc.UintptrFromInt32(1)*4
				for {
					if !(first <= *(*uintptr)(unsafe.Pointer(bp + 8)) && *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8)))))*4)) == x) {
						break
					}
					*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8)))) = ^*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8))))
					goto _36
				_36:
					;
					*(*uintptr)(unsafe.Pointer(bp + 8)) -= 4
				}
				goto _35
			_35:
				;
				*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp + 8))
			}
			limit = -int32(3)
			goto _1
		}
		/* choose pivot */
		*(*uintptr)(unsafe.Pointer(bp)) = tr_pivot(tls, ISAd, first, last)
		t = *(*int32)(unsafe.Pointer(first))
		*(*int32)(unsafe.Pointer(first)) = *(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp))))
		*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))) = t
		v = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(first)))*4))
		/* partition */
		tr_partition(tls, ISAd, first, first+uintptr(1)*4, last, bp, bp+8, v)
		if (int64(last)-int64(first))/4 != (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
			if *(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) != v {
				v5 = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4))
			} else {
				v5 = -int32(1)
			}
			next = v5
			/* update ranks */
			c = first
			v = int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(SA))/4 - libc.Int64FromInt32(1))
			for {
				if !(c < *(*uintptr)(unsafe.Pointer(bp))) {
					break
				}
				*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
				goto _38
			_38:
				;
				c += 4
			}
			if *(*uintptr)(unsafe.Pointer(bp + 8)) < last {
				c = *(*uintptr)(unsafe.Pointer(bp))
				v = int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(SA))/4 - libc.Int64FromInt32(1))
				for {
					if !(c < *(*uintptr)(unsafe.Pointer(bp + 8))) {
						break
					}
					*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
					goto _39
				_39:
					;
					c += 4
				}
			}
			/* push */
			if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 && trbudget_check(tls, budget, int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4)) != 0 {
				if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
					if (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
						if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49751))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd + uintptr(incr)*4
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fd = next
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49752))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fc = last
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							last = *(*uintptr)(unsafe.Pointer(bp))
						} else {
							if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49755))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								first = *(*uintptr)(unsafe.Pointer(bp + 8))
							} else {
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						}
					} else {
						if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
							if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49762))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fc = last
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49763))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								last = *(*uintptr)(unsafe.Pointer(bp))
							} else {
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49766))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fc = last
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						} else {
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49770))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fc = last
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49771))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = first
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							ISAd = ISAd + uintptr(incr)*4
							first = *(*uintptr)(unsafe.Pointer(bp))
							last = *(*uintptr)(unsafe.Pointer(bp + 8))
							limit = next
						}
					}
				} else {
					if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
						if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49777))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd + uintptr(incr)*4
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fd = next
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49778))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = first
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							first = *(*uintptr)(unsafe.Pointer(bp + 8))
						} else {
							if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49781))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								last = *(*uintptr)(unsafe.Pointer(bp))
							} else {
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						}
					} else {
						if (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
							if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49788))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = first
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49789))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								first = *(*uintptr)(unsafe.Pointer(bp + 8))
							} else {
								if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
									libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49792))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = first
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						} else {
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49796))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = first
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
								libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49797))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fc = last
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							ISAd = ISAd + uintptr(incr)*4
							first = *(*uintptr)(unsafe.Pointer(bp))
							last = *(*uintptr)(unsafe.Pointer(bp + 8))
							limit = next
						}
					}
				}
			} else {
				if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 && 0 <= trlink {
					stack[trlink].Fd = -int32(1)
				}
				if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
					if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
						if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49805))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
						stack[ssize].Fc = last
						stack[ssize].Fd = limit
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						last = *(*uintptr)(unsafe.Pointer(bp))
					} else {
						if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
							first = *(*uintptr)(unsafe.Pointer(bp + 8))
						} else {
							if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
								libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49810))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				} else {
					if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
						if v4 = !!(ssize < libc.Int32FromInt32(TR_STACKSIZE)); !v4 {
							libc.X_assert(tls, __ccgo_ts+9661, __ccgo_ts+9627, uint32(49814))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = first
						stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
						stack[ssize].Fd = limit
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						first = *(*uintptr)(unsafe.Pointer(bp + 8))
					} else {
						if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
							last = *(*uintptr)(unsafe.Pointer(bp))
						} else {
							if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
								libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49819))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				}
			}
		} else {
			if trbudget_check(tls, budget, int32((int64(last)-int64(first))/4)) != 0 {
				limit = tr_ilg(tls, int32((int64(last)-int64(first))/4))
				ISAd = ISAd + uintptr(incr)*4
			} else {
				if 0 <= trlink {
					stack[trlink].Fd = -int32(1)
				}
				if v4 = !!(libc.Int32FromInt32(0) <= ssize); !v4 {
					libc.X_assert(tls, __ccgo_ts+9616, __ccgo_ts+9627, uint32(49828))
				}
				_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
				if ssize == 0 {
					return
				}
				ssize = ssize - 1
				v5 = ssize
				ISAd = stack[v5].Fa
				first = stack[ssize].Fb
				last = stack[ssize].Fc
				limit = stack[ssize].Fd
				trlink = stack[ssize].Fe
			}
		}
		goto _1
	_1:
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Tandem repeat sort */
func trsort(tls *libc.TLS, ISA uintptr, SA uintptr, n int32, depth int32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var ISAd, first, last uintptr
	var skip, t, unsorted, v2 int32
	var _ /* budget at bp+0 */ trbudget_t
	_, _, _, _, _, _, _ = ISAd, first, last, skip, t, unsorted, v2
	trbudget_init(tls, bp, tr_ilg(tls, n)*int32(2)/int32(3), n)
	/*  trbudget_init(&budget, tr_ilg(n) * 3 / 4, n); */
	ISAd = ISA + uintptr(depth)*4
	for {
		if !(-n < *(*int32)(unsafe.Pointer(SA))) {
			break
		}
		first = SA
		skip = 0
		unsorted = 0
		for cond := true; cond; cond = first < SA+uintptr(n)*4 {
			v2 = *(*int32)(unsafe.Pointer(first))
			t = v2
			if v2 < 0 {
				first = first - uintptr(t)*4
				skip = skip + t
			} else {
				if skip != 0 {
					*(*int32)(unsafe.Pointer(first + uintptr(skip)*4)) = skip
					skip = 0
				}
				last = SA + uintptr(*(*int32)(unsafe.Pointer(ISA + uintptr(t)*4)))*4 + uintptr(1)*4
				if int64(1) < (int64(last)-int64(first))/4 {
					(*(*trbudget_t)(unsafe.Pointer(bp))).Fcount = 0
					tr_introsort(tls, ISA, ISAd, SA, first, last, bp)
					if (*(*trbudget_t)(unsafe.Pointer(bp))).Fcount != 0 {
						unsorted = unsorted + (*(*trbudget_t)(unsafe.Pointer(bp))).Fcount
					} else {
						skip = int32((int64(first) - int64(last)) / 4)
					}
				} else {
					if (int64(last)-int64(first))/4 == int64(1) {
						skip = -int32(1)
					}
				}
				first = last
			}
		}
		if skip != 0 {
			*(*int32)(unsafe.Pointer(first + uintptr(skip)*4)) = skip
		}
		if unsorted == 0 {
			break
		}
		goto _1
	_1:
		;
		ISAd = ISAd + uintptr((int64(ISAd)-int64(ISA))/4)*4
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Sorts suffixes of type B*. */
func sort_typeBstar(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, openMP int32) (r int32) {
	var ISAb, PAb, buf, v17 uintptr
	var bufsize, c0, c1, i, j, k, m, t, v4, v5, v8 int32
	var v6 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = ISAb, PAb, buf, bufsize, c0, c1, i, j, k, m, t, v17, v4, v5, v6, v8
	_ = openMP
	/* Initialize bucket arrays. */
	i = 0
	for {
		if !(i < int32(ALPHABET_SIZE)) {
			break
		}
		*(*int32)(unsafe.Pointer(bucket_A + uintptr(i)*4)) = 0
		goto _1
	_1:
		;
		i = i + 1
	}
	i = 0
	for {
		if !(i < libc.Int32FromInt32(ALPHABET_SIZE)*libc.Int32FromInt32(ALPHABET_SIZE)) {
			break
		}
		*(*int32)(unsafe.Pointer(bucket_B + uintptr(i)*4)) = 0
		goto _2
	_2:
		;
		i = i + 1
	}
	/* Count the number of occurrences of the first one or two characters of each
	   type A, B and B* suffix. Moreover, store the beginning position of all
	   type B* suffixes into the array SA. */
	i = n - int32(1)
	m = n
	c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	for {
		if !(0 <= i) {
			break
		}
		/* type A suffix. */
		for {
			v8 = c0
			c1 = v8
			*(*int32)(unsafe.Pointer(bucket_A + uintptr(v8)*4)) = *(*int32)(unsafe.Pointer(bucket_A + uintptr(v8)*4)) + 1
			goto _7
		_7:
			;
			i = i - 1
			v4 = i
			if v6 = 0 <= v4; v6 {
				v5 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
				c0 = v5
			}
			if !(v6 && v5 >= c1) {
				break
			}
		}
		if 0 <= i {
			/* type B* suffix. */
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4)) = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4)) + 1
			m = m - 1
			v4 = m
			*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) = i
			/* type B suffix. */
			i = i - 1
			c1 = c0
			for {
				if v6 = 0 <= i; v6 {
					v4 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
					c0 = v4
				}
				if !(v6 && v4 <= c1) {
					break
				}
				*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4)) = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4)) + 1
				goto _10
			_10:
				;
				i = i - 1
				c1 = c0
			}
		}
		goto _3
	_3:
	}
	m = n - m
	/*
	   note:
	     A type B* suffix is lexicographically smaller than a type B suffix that
	     begins with the same first two characters.
	*/
	/* Calculate the index of start/end point of each bucket. */
	c0 = 0
	i = 0
	j = libc.Int32FromInt32(0)
	for {
		if !(c0 < int32(ALPHABET_SIZE)) {
			break
		}
		t = i + *(*int32)(unsafe.Pointer(bucket_A + uintptr(c0)*4))
		*(*int32)(unsafe.Pointer(bucket_A + uintptr(c0)*4)) = i + j /* start point */
		i = t + *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c0)*4))
		c1 = c0 + int32(1)
		for {
			if !(c1 < int32(ALPHABET_SIZE)) {
				break
			}
			j = j + *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4))
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4)) = j /* end point */
			i = i + *(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4))
			goto _14
		_14:
			;
			c1 = c1 + 1
		}
		goto _13
	_13:
		;
		c0 = c0 + 1
	}
	if 0 < m {
		/* Sort the type B* suffixes by their first two characters. */
		PAb = SA + uintptr(n)*4 - uintptr(m)*4
		ISAb = SA + uintptr(m)*4
		i = m - int32(2)
		for {
			if !(0 <= i) {
				break
			}
			t = *(*int32)(unsafe.Pointer(PAb + uintptr(i)*4))
			c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(t))))
			c1 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(t+int32(1)))))
			v17 = bucket_B + uintptr(c0<<int32(8)|c1)*4
			*(*int32)(unsafe.Pointer(v17)) = *(*int32)(unsafe.Pointer(v17)) - 1
			v4 = *(*int32)(unsafe.Pointer(v17))
			*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) = i
			goto _15
		_15:
			;
			i = i - 1
		}
		t = *(*int32)(unsafe.Pointer(PAb + uintptr(m-int32(1))*4))
		c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(t))))
		c1 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(t+int32(1)))))
		v17 = bucket_B + uintptr(c0<<int32(8)|c1)*4
		*(*int32)(unsafe.Pointer(v17)) = *(*int32)(unsafe.Pointer(v17)) - 1
		v4 = *(*int32)(unsafe.Pointer(v17))
		*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) = m - int32(1)
		/* Sort the type B* substrings using sssort. */
		buf = SA + uintptr(m)*4
		/* Sort the type B* substrings using sssort. */
		bufsize = n - libc.Int32FromInt32(2)*m
		c0 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		j = m
		for {
			if !(0 < j) {
				break
			}
			c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(1)
			for {
				if !(c0 < c1) {
					break
				}
				i = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4))
				if int32(1) < j-i {
					sssort(tls, T, PAb, SA+uintptr(i)*4, SA+uintptr(j)*4, buf, bufsize, int32(2), n, libc.BoolInt32(*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) == m-int32(1)))
				}
				goto _21
			_21:
				;
				j = i
				c1 = c1 - 1
			}
			goto _20
		_20:
			;
			c0 = c0 - 1
		}
		/* Compute ranks of type B* substrings. */
		i = m - int32(1)
		for {
			if !(0 <= i) {
				break
			}
			if 0 <= *(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) {
				j = i
				for {
					*(*int32)(unsafe.Pointer(ISAb + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)))*4)) = i
					goto _24
				_24:
					;
					i = i - 1
					v4 = i
					if !(0 <= v4 && 0 <= *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))) {
						break
					}
				}
				*(*int32)(unsafe.Pointer(SA + uintptr(i+int32(1))*4)) = i - j
				if i <= 0 {
					break
				}
			}
			j = i
			for {
				v5 = ^*(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
				*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = v5
				*(*int32)(unsafe.Pointer(ISAb + uintptr(v5)*4)) = j
				goto _26
			_26:
				;
				i = i - 1
				v4 = i
				if !(*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) < 0) {
					break
				}
			}
			*(*int32)(unsafe.Pointer(ISAb + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)))*4)) = j
			goto _22
		_22:
			;
			i = i - 1
		}
		/* Construct the inverse suffix array of type B* suffixes using trsort. */
		trsort(tls, ISAb, SA, m, int32(1))
		/* Set the sorted order of type B* suffixes. */
		i = n - int32(1)
		j = m
		c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
		for {
			if !(0 <= i) {
				break
			}
			i = i - 1
			c1 = c0
			for {
				if v6 = 0 <= i; v6 {
					v4 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
					c0 = v4
				}
				if !(v6 && v4 >= c1) {
					break
				}
				goto _29
			_29:
				;
				i = i - 1
				c1 = c0
			}
			if 0 <= i {
				t = i
				i = i - 1
				c1 = c0
				for {
					if v6 = 0 <= i; v6 {
						v4 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
						c0 = v4
					}
					if !(v6 && v4 <= c1) {
						break
					}
					goto _32
				_32:
					;
					i = i - 1
					c1 = c0
				}
				j = j - 1
				v4 = j
				if t == 0 || int32(1) < t-i {
					v5 = t
				} else {
					v5 = ^t
				}
				*(*int32)(unsafe.Pointer(SA + uintptr(*(*int32)(unsafe.Pointer(ISAb + uintptr(v4)*4)))*4)) = v5
			}
			goto _28
		_28:
		}
		/* Calculate the index of start/end point of each bucket. */
		*(*int32)(unsafe.Pointer(bucket_B + uintptr((libc.Int32FromInt32(ALPHABET_SIZE)-libc.Int32FromInt32(1))<<libc.Int32FromInt32(8)|(libc.Int32FromInt32(ALPHABET_SIZE)-libc.Int32FromInt32(1)))*4)) = n /* end point */
		c0 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		k = m - libc.Int32FromInt32(1)
		for {
			if !(0 <= c0) {
				break
			}
			i = *(*int32)(unsafe.Pointer(bucket_A + uintptr(c0+libc.Int32FromInt32(1))*4)) - int32(1)
			c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(1)
			for {
				if !(c0 < c1) {
					break
				}
				t = i - *(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4))
				*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4)) = i /* end point */
				/* Move all type B* suffixes to the correct position. */
				i = t
				j = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4))
				for {
					if !(j <= k) {
						break
					}
					*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(k)*4))
					goto _39
				_39:
					;
					i = i - 1
					k = k - 1
				}
				goto _38
			_38:
				;
				c1 = c1 - 1
			}
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|(c0+int32(1)))*4)) = i - *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c0)*4)) + int32(1) /* start point */
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c0)*4)) = i                                                                                          /* end point */
			goto _37
		_37:
			;
			c0 = c0 - 1
		}
	}
	return m
}

// C documentation
//
//	/* Constructs the suffix array by using the sorted order of type B* suffixes. */
func construct_SA(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, m int32) {
	var c0, c1, c2, s, v3 int32
	var i, j, k, v11 uintptr
	var v4 bool
	_, _, _, _, _, _, _, _, _, _ = c0, c1, c2, i, j, k, s, v11, v3, v4
	if 0 < m {
		/* Construct the sorted order of type B suffixes by using
		   the sorted order of type B* suffixes. */
		c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		for {
			if !(0 <= c1) {
				break
			}
			/* Scan the suffix array from right to left. */
			i = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|(c1+int32(1)))*4)))*4
			j = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(c1+libc.Int32FromInt32(1))*4)))*4 - uintptr(1)*4
			k = libc.UintptrFromInt32(0)
			c2 = -libc.Int32FromInt32(1)
			for {
				if !(i <= j) {
					break
				}
				v3 = *(*int32)(unsafe.Pointer(j))
				s = v3
				if 0 < v3 {
					if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1); !v4 {
						libc.X_assert(tls, __ccgo_ts+9680, __ccgo_ts+9627, uint32(50070))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(s+int32(1) < n && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) <= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s+int32(1)))))); !v4 {
						libc.X_assert(tls, __ccgo_ts+9691, __ccgo_ts+9627, uint32(50071))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) <= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s))))); !v4 {
						libc.X_assert(tls, __ccgo_ts+9727, __ccgo_ts+9627, uint32(50072))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					*(*int32)(unsafe.Pointer(j)) = ^s
					s = s - 1
					v3 = s
					c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
					if 0 < s && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) > c0 {
						s = ^s
					}
					if c0 != c2 {
						if 0 <= c2 {
							*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c2)*4)) = int32((int64(k) - int64(SA)) / 4)
						}
						v3 = c0
						c2 = v3
						k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|v3)*4)))*4
					}
					if v4 = !!(k < j); !v4 {
						libc.X_assert(tls, __ccgo_ts+9744, __ccgo_ts+9627, uint32(50080))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(k != libc.UintptrFromInt32(0)); !v4 {
						libc.X_assert(tls, __ccgo_ts+9750, __ccgo_ts+9627, uint32(50080))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					v11 = k
					k -= 4
					*(*int32)(unsafe.Pointer(v11)) = s
				} else {
					if v4 = !!(s == 0 && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1 || s < 0); !v4 {
						libc.X_assert(tls, __ccgo_ts+9760, __ccgo_ts+9627, uint32(50083))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					*(*int32)(unsafe.Pointer(j)) = ^s
				}
				goto _2
			_2:
				;
				j -= 4
			}
			goto _1
		_1:
			;
			c1 = c1 - 1
		}
	}
	/* Construct the suffix array by using
	   the sorted order of type B suffixes. */
	v3 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	c2 = v3
	k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
	v11 = k
	k += 4
	if int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2))))) < c2 {
		v3 = ^(n - libc.Int32FromInt32(1))
	} else {
		v3 = n - int32(1)
	}
	*(*int32)(unsafe.Pointer(v11)) = v3
	/* Scan the suffix array from left to right. */
	i = SA
	j = SA + uintptr(n)*4
	for {
		if !(i < j) {
			break
		}
		v3 = *(*int32)(unsafe.Pointer(i))
		s = v3
		if 0 < v3 {
			if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) >= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s))))); !v4 {
				libc.X_assert(tls, __ccgo_ts+9798, __ccgo_ts+9627, uint32(50097))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			s = s - 1
			v3 = s
			c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
			if s == 0 || int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) < c0 {
				s = ^s
			}
			if c0 != c2 {
				*(*int32)(unsafe.Pointer(bucket_A + uintptr(c2)*4)) = int32((int64(k) - int64(SA)) / 4)
				v3 = c0
				c2 = v3
				k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
			}
			if v4 = !!(i < k); !v4 {
				libc.X_assert(tls, __ccgo_ts+9815, __ccgo_ts+9627, uint32(50104))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			v11 = k
			k += 4
			*(*int32)(unsafe.Pointer(v11)) = s
		} else {
			if v4 = !!(s < libc.Int32FromInt32(0)); !v4 {
				libc.X_assert(tls, __ccgo_ts+9821, __ccgo_ts+9627, uint32(50107))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			*(*int32)(unsafe.Pointer(i)) = ^s
		}
		goto _16
	_16:
		;
		i += 4
	}
}

// C documentation
//
//	/* Constructs the burrows-wheeler transformed string directly
//	   by using the sorted order of type B* suffixes. */
func construct_BWT(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, m int32) (r int32) {
	var c0, c1, c2, s, v3 int32
	var i, j, k, orig, v11 uintptr
	var v4 bool
	_, _, _, _, _, _, _, _, _, _, _ = c0, c1, c2, i, j, k, orig, s, v11, v3, v4
	if 0 < m {
		/* Construct the sorted order of type B suffixes by using
		   the sorted order of type B* suffixes. */
		c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		for {
			if !(0 <= c1) {
				break
			}
			/* Scan the suffix array from right to left. */
			i = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|(c1+int32(1)))*4)))*4
			j = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(c1+libc.Int32FromInt32(1))*4)))*4 - uintptr(1)*4
			k = libc.UintptrFromInt32(0)
			c2 = -libc.Int32FromInt32(1)
			for {
				if !(i <= j) {
					break
				}
				v3 = *(*int32)(unsafe.Pointer(j))
				s = v3
				if 0 < v3 {
					if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1); !v4 {
						libc.X_assert(tls, __ccgo_ts+9680, __ccgo_ts+9627, uint32(50134))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(s+int32(1) < n && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) <= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s+int32(1)))))); !v4 {
						libc.X_assert(tls, __ccgo_ts+9691, __ccgo_ts+9627, uint32(50135))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) <= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s))))); !v4 {
						libc.X_assert(tls, __ccgo_ts+9727, __ccgo_ts+9627, uint32(50136))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					s = s - 1
					v3 = s
					c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
					*(*int32)(unsafe.Pointer(j)) = ^c0
					if 0 < s && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) > c0 {
						s = ^s
					}
					if c0 != c2 {
						if 0 <= c2 {
							*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c2)*4)) = int32((int64(k) - int64(SA)) / 4)
						}
						v3 = c0
						c2 = v3
						k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|v3)*4)))*4
					}
					if v4 = !!(k < j); !v4 {
						libc.X_assert(tls, __ccgo_ts+9744, __ccgo_ts+9627, uint32(50144))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(k != libc.UintptrFromInt32(0)); !v4 {
						libc.X_assert(tls, __ccgo_ts+9750, __ccgo_ts+9627, uint32(50144))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					v11 = k
					k -= 4
					*(*int32)(unsafe.Pointer(v11)) = s
				} else {
					if s != 0 {
						*(*int32)(unsafe.Pointer(j)) = ^s
					} else {
						if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1); !v4 {
							libc.X_assert(tls, __ccgo_ts+9680, __ccgo_ts+9627, uint32(50150))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					}
				}
				goto _2
			_2:
				;
				j -= 4
			}
			goto _1
		_1:
			;
			c1 = c1 - 1
		}
	}
	/* Construct the BWTed string by using
	   the sorted order of type B suffixes. */
	v3 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	c2 = v3
	k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
	v11 = k
	k += 4
	if int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2))))) < c2 {
		v3 = ^int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2)))))
	} else {
		v3 = n - int32(1)
	}
	*(*int32)(unsafe.Pointer(v11)) = v3
	/* Scan the suffix array from left to right. */
	i = SA
	j = SA + uintptr(n)*4
	orig = SA
	for {
		if !(i < j) {
			break
		}
		v3 = *(*int32)(unsafe.Pointer(i))
		s = v3
		if 0 < v3 {
			if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) >= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s))))); !v4 {
				libc.X_assert(tls, __ccgo_ts+9798, __ccgo_ts+9627, uint32(50164))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			s = s - 1
			v3 = s
			c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
			*(*int32)(unsafe.Pointer(i)) = c0
			if 0 < s && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) < c0 {
				s = ^int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1)))))
			}
			if c0 != c2 {
				*(*int32)(unsafe.Pointer(bucket_A + uintptr(c2)*4)) = int32((int64(k) - int64(SA)) / 4)
				v3 = c0
				c2 = v3
				k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
			}
			if v4 = !!(i < k); !v4 {
				libc.X_assert(tls, __ccgo_ts+9815, __ccgo_ts+9627, uint32(50172))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			v11 = k
			k += 4
			*(*int32)(unsafe.Pointer(v11)) = s
		} else {
			if s != 0 {
				*(*int32)(unsafe.Pointer(i)) = ^s
			} else {
				orig = i
			}
		}
		goto _16
	_16:
		;
		i += 4
	}
	return int32((int64(orig) - int64(SA)) / 4)
}

// C documentation
//
//	/* Constructs the burrows-wheeler transformed string directly
//	   by using the sorted order of type B* suffixes. */
func construct_BWT_indexes(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, m int32, num_indexes uintptr, indexes uintptr) (r int32) {
	var c0, c1, c2, mod, s, v3 int32
	var i, j, k, orig, v11 uintptr
	var v4 bool
	_, _, _, _, _, _, _, _, _, _, _, _ = c0, c1, c2, i, j, k, mod, orig, s, v11, v3, v4
	mod = n / int32(8)
	mod = mod | mod>>int32(1)
	mod = mod | mod>>int32(2)
	mod = mod | mod>>int32(4)
	mod = mod | mod>>int32(8)
	mod = mod | mod>>int32(16)
	mod = mod >> int32(1)
	*(*uint8)(unsafe.Pointer(num_indexes)) = uint8((n - libc.Int32FromInt32(1)) / (mod + libc.Int32FromInt32(1)))
	if 0 < m {
		/* Construct the sorted order of type B suffixes by using
		   the sorted order of type B* suffixes. */
		c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		for {
			if !(0 <= c1) {
				break
			}
			/* Scan the suffix array from right to left. */
			i = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|(c1+int32(1)))*4)))*4
			j = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(c1+libc.Int32FromInt32(1))*4)))*4 - uintptr(1)*4
			k = libc.UintptrFromInt32(0)
			c2 = -libc.Int32FromInt32(1)
			for {
				if !(i <= j) {
					break
				}
				v3 = *(*int32)(unsafe.Pointer(j))
				s = v3
				if 0 < v3 {
					if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1); !v4 {
						libc.X_assert(tls, __ccgo_ts+9680, __ccgo_ts+9627, uint32(50215))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(s+int32(1) < n && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) <= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s+int32(1)))))); !v4 {
						libc.X_assert(tls, __ccgo_ts+9691, __ccgo_ts+9627, uint32(50216))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) <= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s))))); !v4 {
						libc.X_assert(tls, __ccgo_ts+9727, __ccgo_ts+9627, uint32(50217))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if s&mod == 0 {
						*(*int32)(unsafe.Pointer(indexes + uintptr(s/(mod+int32(1))-int32(1))*4)) = int32((int64(j) - int64(SA)) / 4)
					}
					s = s - 1
					v3 = s
					c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
					*(*int32)(unsafe.Pointer(j)) = ^c0
					if 0 < s && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) > c0 {
						s = ^s
					}
					if c0 != c2 {
						if 0 <= c2 {
							*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c2)*4)) = int32((int64(k) - int64(SA)) / 4)
						}
						v3 = c0
						c2 = v3
						k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|v3)*4)))*4
					}
					if v4 = !!(k < j); !v4 {
						libc.X_assert(tls, __ccgo_ts+9744, __ccgo_ts+9627, uint32(50228))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = !!(k != libc.UintptrFromInt32(0)); !v4 {
						libc.X_assert(tls, __ccgo_ts+9750, __ccgo_ts+9627, uint32(50228))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					v11 = k
					k -= 4
					*(*int32)(unsafe.Pointer(v11)) = s
				} else {
					if s != 0 {
						*(*int32)(unsafe.Pointer(j)) = ^s
					} else {
						if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1); !v4 {
							libc.X_assert(tls, __ccgo_ts+9680, __ccgo_ts+9627, uint32(50234))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					}
				}
				goto _2
			_2:
				;
				j -= 4
			}
			goto _1
		_1:
			;
			c1 = c1 - 1
		}
	}
	/* Construct the BWTed string by using
	   the sorted order of type B suffixes. */
	v3 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	c2 = v3
	k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
	if int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2))))) < c2 {
		if (n-int32(1))&mod == 0 {
			*(*int32)(unsafe.Pointer(indexes + uintptr((n-int32(1))/(mod+int32(1))-int32(1))*4)) = int32((int64(k) - int64(SA)) / 4)
		}
		v11 = k
		k += 4
		*(*int32)(unsafe.Pointer(v11)) = ^int32(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2)))))
	} else {
		v11 = k
		k += 4
		*(*int32)(unsafe.Pointer(v11)) = n - int32(1)
	}
	/* Scan the suffix array from left to right. */
	i = SA
	j = SA + uintptr(n)*4
	orig = SA
	for {
		if !(i < j) {
			break
		}
		v3 = *(*int32)(unsafe.Pointer(i))
		s = v3
		if 0 < v3 {
			if v4 = !!(int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) >= int32(*(*uint8)(unsafe.Pointer(T + uintptr(s))))); !v4 {
				libc.X_assert(tls, __ccgo_ts+9798, __ccgo_ts+9627, uint32(50255))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if s&mod == 0 {
				*(*int32)(unsafe.Pointer(indexes + uintptr(s/(mod+int32(1))-int32(1))*4)) = int32((int64(i) - int64(SA)) / 4)
			}
			s = s - 1
			v3 = s
			c0 = int32(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
			*(*int32)(unsafe.Pointer(i)) = c0
			if c0 != c2 {
				*(*int32)(unsafe.Pointer(bucket_A + uintptr(c2)*4)) = int32((int64(k) - int64(SA)) / 4)
				v3 = c0
				c2 = v3
				k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
			}
			if v4 = !!(i < k); !v4 {
				libc.X_assert(tls, __ccgo_ts+9815, __ccgo_ts+9627, uint32(50265))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if 0 < s && int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) < c0 {
				if s&mod == 0 {
					*(*int32)(unsafe.Pointer(indexes + uintptr(s/(mod+int32(1))-int32(1))*4)) = int32((int64(k) - int64(SA)) / 4)
				}
				v11 = k
				k += 4
				*(*int32)(unsafe.Pointer(v11)) = ^int32(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1)))))
			} else {
				v11 = k
				k += 4
				*(*int32)(unsafe.Pointer(v11)) = s
			}
		} else {
			if s != 0 {
				*(*int32)(unsafe.Pointer(i)) = ^s
			} else {
				orig = i
			}
		}
		goto _16
	_16:
		;
		i += 4
	}
	return int32((int64(orig) - int64(SA)) / 4)
}

/*---------------------------------------------------------------------------*/

/*- Function -*/

func divsufsort(tls *libc.TLS, T uintptr, SA uintptr, n int32, openMP int32) (r int32) {
	var bucket_A, bucket_B uintptr
	var err, m int32
	_, _, _, _ = bucket_A, bucket_B, err, m
	err = 0
	/* Check arguments. */
	if T == libc.UintptrFromInt32(0) || SA == libc.UintptrFromInt32(0) || n < 0 {
		return -int32(1)
	} else {
		if n == 0 {
			return 0
		} else {
			if n == int32(1) {
				*(*int32)(unsafe.Pointer(SA)) = 0
				return 0
			} else {
				if n == int32(2) {
					m = libc.BoolInt32(int32(*(*uint8)(unsafe.Pointer(T))) < int32(*(*uint8)(unsafe.Pointer(T + 1))))
					*(*int32)(unsafe.Pointer(SA + uintptr(m^int32(1))*4)) = 0
					*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = libc.Int32FromInt32(1)
					return 0
				}
			}
		}
	}
	bucket_A = libc.Xmalloc(tls, uint64(libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	bucket_B = libc.Xmalloc(tls, uint64(libc.Int32FromInt32(ALPHABET_SIZE)*libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	/* Suffixsort. */
	if bucket_A != libc.UintptrFromInt32(0) && bucket_B != libc.UintptrFromInt32(0) {
		m = sort_typeBstar(tls, T, SA, bucket_A, bucket_B, n, openMP)
		construct_SA(tls, T, SA, bucket_A, bucket_B, n, m)
	} else {
		err = -int32(2)
	}
	libc.Xfree(tls, bucket_B)
	libc.Xfree(tls, bucket_A)
	return err
}

func divbwt(tls *libc.TLS, T uintptr, U uintptr, A uintptr, n int32, num_indexes uintptr, indexes uintptr, openMP int32) (r int32) {
	var B, bucket_A, bucket_B, v1 uintptr
	var i, m, pidx int32
	_, _, _, _, _, _, _ = B, bucket_A, bucket_B, i, m, pidx, v1
	/* Check arguments. */
	if T == libc.UintptrFromInt32(0) || U == libc.UintptrFromInt32(0) || n < 0 {
		return -int32(1)
	} else {
		if n <= int32(1) {
			if n == int32(1) {
				*(*uint8)(unsafe.Pointer(U)) = *(*uint8)(unsafe.Pointer(T))
			}
			return n
		}
	}
	v1 = A
	B = v1
	if v1 == libc.UintptrFromInt32(0) {
		B = libc.Xmalloc(tls, uint64(n+libc.Int32FromInt32(1))*uint64(4))
	}
	bucket_A = libc.Xmalloc(tls, uint64(libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	bucket_B = libc.Xmalloc(tls, uint64(libc.Int32FromInt32(ALPHABET_SIZE)*libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	/* Burrows-Wheeler Transform. */
	if B != libc.UintptrFromInt32(0) && bucket_A != libc.UintptrFromInt32(0) && bucket_B != libc.UintptrFromInt32(0) {
		m = sort_typeBstar(tls, T, B, bucket_A, bucket_B, n, openMP)
		if num_indexes == libc.UintptrFromInt32(0) || indexes == libc.UintptrFromInt32(0) {
			pidx = construct_BWT(tls, T, B, bucket_A, bucket_B, n, m)
		} else {
			pidx = construct_BWT_indexes(tls, T, B, bucket_A, bucket_B, n, m, num_indexes, indexes)
		}
		/* Copy to output string. */
		*(*uint8)(unsafe.Pointer(U)) = *(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1))))
		i = 0
		for {
			if !(i < pidx) {
				break
			}
			*(*uint8)(unsafe.Pointer(U + uintptr(i+int32(1)))) = uint8(*(*int32)(unsafe.Pointer(B + uintptr(i)*4)))
			goto _2
		_2:
			;
			i = i + 1
		}
		i = i + int32(1)
		for {
			if !(i < n) {
				break
			}
			*(*uint8)(unsafe.Pointer(U + uintptr(i))) = uint8(*(*int32)(unsafe.Pointer(B + uintptr(i)*4)))
			goto _3
		_3:
			;
			i = i + 1
		}
		pidx = pidx + int32(1)
	} else {
		pidx = -int32(2)
	}
	libc.Xfree(tls, bucket_B)
	libc.Xfree(tls, bucket_A)
	if A == libc.UintptrFromInt32(0) {
		libc.Xfree(tls, B)
	}
	return pidx
}

/**** ended inlining dictBuilder/divsufsort.c ****/
/**** start inlining dictBuilder/fastcover.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
*  Dependencies
***************************************/
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/pool.h ****/
/**** skipping file: ../common/threading.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../compress/zstd_compress_internal.h ****/
/**** skipping file: ../zdict.h ****/
/**** skipping file: cover.h ****/

/*-*************************************
*  Constants
***************************************/
/**
* There are 32bit indexes used to ref samples, so limit samples size to 4GB
* on 64bit builds.
* For 32bit builds we choose 1 GB.
* Most 32bit platforms have 2GB user-mode addressable space and we allocate a large
* contiguous buffer, so 1GB is already a high limit.
 */

/*-*************************************
*  Console display
***************************************/

// C documentation
//
//	/*-*************************************
//	* Hash Functions
//	***************************************/
//	/**
//	 * Hash the d-byte value pointed to by p and mod 2^f into the frequency vector
//	 */
func FASTCOVER_hashPtrToIndex(tls *libc.TLS, p uintptr, f U32, d uint32) (r size_t) {
	if d == uint32(6) {
		return ZSTD_hash6Ptr(tls, p, f)
	}
	return ZSTD_hash8Ptr(tls, p, f)
}

// C documentation
//
//	/*-*************************************
//	* Acceleration
//	***************************************/
type FASTCOVER_accel_t = struct {
	Ffinalize uint32
	Fskip     uint32
}

var FASTCOVER_defaultAccelParameters = [11]FASTCOVER_accel_t{
	0: {
		Ffinalize: uint32(100),
	},
	1: {
		Ffinalize: uint32(100),
	},
	2: {
		Ffinalize: uint32(50),
		Fskip:     uint32(1),
	},
	3: {
		Ffinalize: uint32(34),
		Fskip:     uint32(2),
	},
	4: {
		Ffinalize: uint32(25),
		Fskip:     uint32(3),
	},
	5: {
		Ffinalize: uint32(20),
		Fskip:     uint32(4),
	},
	6: {
		Ffinalize: uint32(17),
		Fskip:     uint32(5),
	},
	7: {
		Ffinalize: uint32(14),
		Fskip:     uint32(6),
	},
	8: {
		Ffinalize: uint32(13),
		Fskip:     uint32(7),
	},
	9: {
		Ffinalize: uint32(11),
		Fskip:     uint32(8),
	},
	10: {
		Ffinalize: uint32(10),
		Fskip:     uint32(9),
	},
}

// C documentation
//
//	/*-*************************************
//	* Context
//	***************************************/
type FASTCOVER_ctx_t = struct {
	Fsamples        uintptr
	Foffsets        uintptr
	FsamplesSizes   uintptr
	FnbSamples      size_t
	FnbTrainSamples size_t
	FnbTestSamples  size_t
	FnbDmers        size_t
	Ffreqs          uintptr
	Fd              uint32
	Ff              uint32
	FaccelParams    FASTCOVER_accel_t
}

// C documentation
//
//	/*-*************************************
//	*  Helper functions
//	***************************************/
//	/**
//	 * Selects the best segment in an epoch.
//	 * Segments of are scored according to the function:
//	 *
//	 * Let F(d) be the frequency of all dmers with hash value d.
//	 * Let S_i be hash value of the dmer at position i of segment S which has length k.
//	 *
//	 *     Score(S) = F(S_1) + F(S_2) + ... + F(S_{k-d+1})
//	 *
//	 * Once the dmer with hash value d is in the dictionary we set F(d) = 0.
//	 */
func FASTCOVER_selectSegment(tls *libc.TLS, ctx uintptr, freqs uintptr, begin U32, end U32, parameters ZDICT_cover_params_t, segmentFreqs uintptr) (r COVER_segment_t) {
	var activeSegment, bestSegment COVER_segment_t
	var d, dmersInK, f, k, pos U32
	var delIndex, delIndex1, i, idx size_t
	var v1 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _ = activeSegment, bestSegment, d, delIndex, delIndex1, dmersInK, f, i, idx, k, pos, v1
	/* Constants */
	k = parameters.Fk
	d = parameters.Fd
	f = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff
	dmersInK = k - d + uint32(1)
	/* Try each segment (activeSegment) and save the best (bestSegment) */
	bestSegment = COVER_segment_t{}
	/* Reset the activeDmers in the segment */
	/* The activeSegment starts at the beginning of the epoch. */
	activeSegment.Fbegin = begin
	activeSegment.Fend = begin
	activeSegment.Fscore = uint32(0)
	/* Slide the activeSegment through the whole epoch.
	 * Save the best segment in bestSegment.
	 */
	for activeSegment.Fend < end {
		/* Get hash value of current dmer */
		idx = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(activeSegment.Fend), f, d)
		/* Add frequency of this index to score if this is the first occurrence of index in active segment */
		if int32(*(*U16)(unsafe.Pointer(segmentFreqs + uintptr(idx)*2))) == 0 {
			activeSegment.Fscore += *(*U32)(unsafe.Pointer(freqs + uintptr(idx)*4))
		}
		/* Increment end of segment and segmentFreqs*/
		activeSegment.Fend += uint32(1)
		v1 = segmentFreqs + uintptr(idx)*2
		*(*U16)(unsafe.Pointer(v1)) = U16(int32(*(*U16)(unsafe.Pointer(v1))) + libc.Int32FromInt32(1))
		/* If the window is now too large, drop the first position */
		if activeSegment.Fend-activeSegment.Fbegin == dmersInK+uint32(1) {
			/* Get hash value of the dmer to be eliminated from active segment */
			delIndex = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(activeSegment.Fbegin), f, d)
			v1 = segmentFreqs + uintptr(delIndex)*2
			*(*U16)(unsafe.Pointer(v1)) = U16(int32(*(*U16)(unsafe.Pointer(v1))) - libc.Int32FromInt32(1))
			/* Subtract frequency of this index from score if this is the last occurrence of this index in active segment */
			if int32(*(*U16)(unsafe.Pointer(segmentFreqs + uintptr(delIndex)*2))) == 0 {
				activeSegment.Fscore -= *(*U32)(unsafe.Pointer(freqs + uintptr(delIndex)*4))
			}
			/* Increment start of segment */
			activeSegment.Fbegin += uint32(1)
		}
		/* If this segment is the best so far save it */
		if activeSegment.Fscore > bestSegment.Fscore {
			bestSegment = activeSegment
		}
	}
	/* Zero out rest of segmentFreqs array */
	for activeSegment.Fbegin < end {
		delIndex1 = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(activeSegment.Fbegin), f, d)
		v1 = segmentFreqs + uintptr(delIndex1)*2
		*(*U16)(unsafe.Pointer(v1)) = U16(int32(*(*U16)(unsafe.Pointer(v1))) - libc.Int32FromInt32(1))
		activeSegment.Fbegin += uint32(1)
	}
	pos = bestSegment.Fbegin
	for {
		if !(pos != bestSegment.Fend) {
			break
		}
		i = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(pos), f, d)
		*(*U32)(unsafe.Pointer(freqs + uintptr(i)*4)) = uint32(0)
		goto _4
	_4:
		;
		pos = pos + 1
	}
	return bestSegment
}

func FASTCOVER_checkParameters(tls *libc.TLS, parameters ZDICT_cover_params_t, maxDictSize size_t, f uint32, accel uint32) (r int32) {
	/* k, d, and f are required parameters */
	if parameters.Fd == uint32(0) || parameters.Fk == uint32(0) {
		return 0
	}
	/* d has to be 6 or 8 */
	if parameters.Fd != uint32(6) && parameters.Fd != uint32(8) {
		return 0
	}
	/* k <= maxDictSize */
	if uint64(parameters.Fk) > maxDictSize {
		return 0
	}
	/* d <= k */
	if parameters.Fd > parameters.Fk {
		return 0
	}
	/* 0 < f <= FASTCOVER_MAX_F*/
	if f > uint32(FASTCOVER_MAX_F) || f == uint32(0) {
		return 0
	}
	/* 0 < splitPoint <= 1 */
	if parameters.FsplitPoint <= libc.Float64FromInt32(0) || parameters.FsplitPoint > libc.Float64FromInt32(1) {
		return 0
	}
	/* 0 < accel <= 10 */
	if accel > uint32(10) || accel == uint32(0) {
		return 0
	}
	return int32(1)
}

// C documentation
//
//	/**
//	 * Clean up a context initialized with `FASTCOVER_ctx_init()`.
//	 */
func FASTCOVER_ctx_destroy(tls *libc.TLS, ctx uintptr) {
	if !(ctx != 0) {
		return
	}
	libc.Xfree(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.UintptrFromInt32(0)
	libc.Xfree(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.UintptrFromInt32(0)
}

// C documentation
//
//	/**
//	 * Calculate for frequency of hash value of each dmer in ctx->samples
//	 */
func FASTCOVER_computeFrequency(tls *libc.TLS, freqs uintptr, ctx uintptr) {
	var currSampleEnd, dmerIndex, i, start size_t
	var d, f, readLength, skip, v1 uint32
	var v2 bool
	_, _, _, _, _, _, _, _, _, _ = currSampleEnd, d, dmerIndex, f, i, readLength, skip, start, v1, v2
	f = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff
	d = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fd
	skip = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FaccelParams.Fskip
	if d > uint32(libc.Int32FromInt32(8)) {
		v1 = d
	} else {
		v1 = uint32(libc.Int32FromInt32(8))
	}
	readLength = v1
	if v2 = !!((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples >= libc.Uint64FromInt32(5)); !v2 {
		libc.X_assert(tls, __ccgo_ts+9827, __ccgo_ts+9627, uint32(50646))
	}
	_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
	if v2 = !!((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples <= (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples); !v2 {
		libc.X_assert(tls, __ccgo_ts+9852, __ccgo_ts+9627, uint32(50647))
	}
	_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
	i = uint64(0)
	for {
		if !(i < (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples) {
			break
		}
		start = *(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i)*8)) /* start of current dmer */
		currSampleEnd = *(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i+uint64(1))*8))
		for start+uint64(readLength) <= currSampleEnd {
			dmerIndex = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(start), f, d)
			*(*U32)(unsafe.Pointer(freqs + uintptr(dmerIndex)*4)) = *(*U32)(unsafe.Pointer(freqs + uintptr(dmerIndex)*4)) + 1
			start = start + uint64(skip) + uint64(1)
		}
		goto _4
	_4:
		;
		i = i + 1
	}
}

// C documentation
//
//	/**
//	 * Prepare a context for dictionary building.
//	 * The context is only dependent on the parameter `d` and can be used multiple
//	 * times.
//	 * Returns 0 on success or error code on error.
//	 * The context must be destroyed with `FASTCOVER_ctx_destroy()`.
//	 */
func FASTCOVER_ctx_init(tls *libc.TLS, ctx uintptr, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, d uint32, splitPoint float64, f uint32, accelParams FASTCOVER_accel_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var i U32
	var nbTestSamples, nbTrainSamples, v1, v2 uint32
	var samples uintptr
	var testSamplesSize, totalSamplesSize, trainingSamplesSize size_t
	var v3, v4, v5 uint64
	var v7 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _ = i, nbTestSamples, nbTrainSamples, samples, testSamplesSize, totalSamplesSize, trainingSamplesSize, v1, v2, v3, v4, v5, v7
	samples = samplesBuffer
	totalSamplesSize = COVER_sum(tls, samplesSizes, nbSamples)
	if splitPoint < float64(1) {
		v1 = uint32(float64(float64(nbSamples) * splitPoint))
	} else {
		v1 = nbSamples
	}
	/* Split samples into testing and training sets */
	nbTrainSamples = v1
	if splitPoint < float64(1) {
		v2 = nbSamples - nbTrainSamples
	} else {
		v2 = nbSamples
	}
	nbTestSamples = v2
	if splitPoint < float64(1) {
		v3 = COVER_sum(tls, samplesSizes, nbTrainSamples)
	} else {
		v3 = totalSamplesSize
	}
	trainingSamplesSize = v3
	if splitPoint < float64(1) {
		v4 = COVER_sum(tls, samplesSizes+uintptr(nbTrainSamples)*8, nbTestSamples)
	} else {
		v4 = totalSamplesSize
	}
	testSamplesSize = v4
	/* Checks */
	if uint64(d) > libc.Uint64FromInt64(8) {
		v5 = uint64(d)
	} else {
		v5 = libc.Uint64FromInt64(8)
	}
	if totalSamplesSize < v5 || totalSamplesSize >= uint64(uint32(-libc.Int32FromInt32(1))) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8508, libc.VaList(bp+8, uint32(totalSamplesSize>>libc.Int32FromInt32(20)), uint32(-libc.Int32FromInt32(1))>>libc.Int32FromInt32(20)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there are at least 5 training samples */
	if nbTrainSamples < uint32(5) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9890, libc.VaList(bp+8, nbTrainSamples))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there's testing sample */
	if nbTestSamples < uint32(1) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9945, libc.VaList(bp+8, nbTestSamples))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Zero the context */
	libc.Xmemset(tls, ctx, 0, uint64(80))
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8681, libc.VaList(bp+8, nbTrainSamples, uint32(trainingSamplesSize)))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8722, libc.VaList(bp+8, nbTestSamples, uint32(testSamplesSize)))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples = samples
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes = samplesSizes
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples = uint64(nbSamples)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples = uint64(nbTrainSamples)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTestSamples = uint64(nbTestSamples)
	if uint64(d) > libc.Uint64FromInt64(8) {
		v3 = uint64(d)
	} else {
		v3 = libc.Uint64FromInt64(8)
	}
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbDmers = trainingSamplesSize - v3 + uint64(1)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fd = d
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff = f
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FaccelParams = accelParams
	/* The offsets of each file */
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.Xcalloc(tls, uint64(nbSamples+libc.Uint32FromInt32(1)), uint64(8))
	if (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets == libc.UintptrFromInt32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10000, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		FASTCOVER_ctx_destroy(tls, ctx)
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	/* Fill offsets from the samplesSizes */
	*(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)) = uint64(0)
	if v7 = !!(nbSamples >= libc.Uint32FromInt32(5)); !v7 {
		libc.X_assert(tls, __ccgo_ts+10037, __ccgo_ts+9627, uint32(50730))
	}
	_ = v7 || libc.Bool(libc.Int32FromInt32(0) != 0)
	i = uint32(1)
	for {
		if !(i <= nbSamples) {
			break
		}
		*(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i)*8)) = *(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i-uint32(1))*8)) + *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i-uint32(1))*8))
		goto _8
	_8:
		;
		i = i + 1
	}
	/* Initialize frequency array of size 2^f */
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.Xcalloc(tls, libc.Uint64FromInt32(1)<<f, uint64(4))
	if (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs == libc.UintptrFromInt32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10052, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		FASTCOVER_ctx_destroy(tls, ctx)
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+8833, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	FASTCOVER_computeFrequency(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs, ctx)
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Given the prepared context build the dictionary.
//	 */
func FASTCOVER_buildDictionary(tls *libc.TLS, ctx uintptr, freqs uintptr, dictBuffer uintptr, dictBufferCapacity size_t, parameters ZDICT_cover_params_t, segmentFreqs uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var dict uintptr
	var epoch, maxZeroScoreRun, segmentSize, tail, zeroScoreRun, v2 size_t
	var epochBegin, epochEnd U32
	var epochs COVER_epoch_info_t
	var segment COVER_segment_t
	var v3 uint64
	_, _, _, _, _, _, _, _, _, _, _, _ = dict, epoch, epochBegin, epochEnd, epochs, maxZeroScoreRun, segment, segmentSize, tail, zeroScoreRun, v2, v3
	dict = dictBuffer
	tail = dictBufferCapacity
	/* Divide the data into epochs. We will select one segment from each epoch. */
	epochs = COVER_computeEpochs(tls, uint32(dictBufferCapacity), uint32((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbDmers), parameters.Fk, uint32(1))
	maxZeroScoreRun = uint64(10)
	zeroScoreRun = uint64(0)
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9136, libc.VaList(bp+8, epochs.Fnum, epochs.Fsize))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	/* Loop through the epochs until there are no more segments or the dictionary
	 * is full.
	 */
	epoch = uint64(0)
	for {
		if !(tail > uint64(0)) {
			break
		}
		epochBegin = uint32(epoch * uint64(epochs.Fsize))
		epochEnd = epochBegin + epochs.Fsize
		/* Select a segment */
		segment = FASTCOVER_selectSegment(tls, ctx, freqs, epochBegin, epochEnd, parameters, segmentFreqs)
		/* If the segment covers no dmers, then we are out of content.
		 * There may be new content in other epochs, for continue for some time.
		 */
		if segment.Fscore == uint32(0) {
			zeroScoreRun = zeroScoreRun + 1
			v2 = zeroScoreRun
			if v2 >= maxZeroScoreRun {
				break
			}
			goto _1
		}
		zeroScoreRun = uint64(0)
		/* Trim the segment if necessary and if it is too small then we are done */
		if uint64(segment.Fend-segment.Fbegin+parameters.Fd-libc.Uint32FromInt32(1)) < tail {
			v3 = uint64(segment.Fend - segment.Fbegin + parameters.Fd - libc.Uint32FromInt32(1))
		} else {
			v3 = tail
		}
		segmentSize = v3
		if segmentSize < uint64(parameters.Fd) {
			break
		}
		/* We fill the dictionary from the back to allow the best segments to be
		 * referenced with the smallest offsets.
		 */
		tail = tail - segmentSize
		libc.Xmemcpy(tls, dict+uintptr(tail), (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(segment.Fbegin), segmentSize)
		if g_displayLevel >= int32(2) {
			if clock(tls)-g_time > g_refreshRate || g_displayLevel >= int32(4) {
				g_time = clock(tls)
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9180, libc.VaList(bp+8, uint32((dictBufferCapacity-tail)*libc.Uint64FromInt32(100)/dictBufferCapacity)))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
		}
		goto _1
	_1:
		;
		epoch = (epoch + uint64(1)) % uint64(epochs.Fnum)
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9193, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	return tail
}

// C documentation
//
//	/**
//	 * Parameters for FASTCOVER_tryParameters().
//	 */
type FASTCOVER_tryParameters_data_t = struct {
	Fctx                uintptr
	Fbest               uintptr
	FdictBufferCapacity size_t
	Fparameters         ZDICT_cover_params_t
}

// C documentation
//
//	/**
//	 * Parameters for FASTCOVER_tryParameters().
//	 */
type FASTCOVER_tryParameters_data_s = FASTCOVER_tryParameters_data_t

// C documentation
//
//	/**
//	 * Tries a set of parameters and updates the COVER_best_t with the results.
//	 * This function is thread safe if zstd is compiled with multithreaded support.
//	 * It takes its parameters as an *OWNING* opaque pointer to support threading.
//	 */
func FASTCOVER_tryParameters(tls *libc.TLS, opaque uintptr) {
	var ctx, data, dict, freqs, segmentFreqs uintptr
	var dictBufferCapacity, tail, totalCompressedSize size_t
	var nbFinalizeSamples uint32
	var parameters ZDICT_cover_params_t
	var selection COVER_dictSelection_t
	_, _, _, _, _, _, _, _, _, _, _ = ctx, data, dict, dictBufferCapacity, freqs, nbFinalizeSamples, parameters, segmentFreqs, selection, tail, totalCompressedSize
	/* Save parameters as local variables */
	data = opaque
	ctx = (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx
	parameters = (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters
	dictBufferCapacity = (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity
	totalCompressedSize = uint64(-int32(ZSTD_error_GENERIC))
	/* Initialize array to keep track of frequency of dmer within activeSegment */
	segmentFreqs = libc.Xcalloc(tls, libc.Uint64FromInt32(1)<<(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff, uint64(2))
	/* Allocate space for hash table, dict, and freqs */
	dict = libc.Xmalloc(tls, dictBufferCapacity)
	selection = COVER_dictSelectionError(tls, uint64(-int32(ZSTD_error_GENERIC)))
	freqs = libc.Xmalloc(tls, libc.Uint64FromInt32(1)<<(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff*uint64(4))
	if !(segmentFreqs != 0) || !(dict != 0) || !(freqs != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9409, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	/* Copy the frequencies because we need to modify them */
	libc.Xmemcpy(tls, freqs, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs, libc.Uint64FromInt32(1)<<(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff*uint64(4))
	/* Build the dictionary */
	tail = FASTCOVER_buildDictionary(tls, ctx, freqs, dict, dictBufferCapacity, parameters, segmentFreqs)
	nbFinalizeSamples = uint32((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples * uint64((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FaccelParams.Ffinalize) / libc.Uint64FromInt32(100))
	selection = COVER_selectDict(tls, dict+uintptr(tail), dictBufferCapacity, dictBufferCapacity-tail, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes, nbFinalizeSamples, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples, parameters, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets, totalCompressedSize)
	if COVER_dictSelectionIsError(tls, selection) != 0 {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9452, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	goto _cleanup
_cleanup:
	;
	libc.Xfree(tls, dict)
	COVER_best_finish(tls, (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest, parameters, selection)
	libc.Xfree(tls, data)
	libc.Xfree(tls, segmentFreqs)
	COVER_dictSelectionFree(tls, selection)
	libc.Xfree(tls, freqs)
}

func FASTCOVER_convertToCoverParams(tls *libc.TLS, fastCoverParams ZDICT_fastCover_params_t, coverParams uintptr) {
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).Fk = fastCoverParams.Fk
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).Fd = fastCoverParams.Fd
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).Fsteps = fastCoverParams.Fsteps
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FnbThreads = fastCoverParams.FnbThreads
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FsplitPoint = fastCoverParams.FsplitPoint
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FzParams = fastCoverParams.FzParams
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FshrinkDict = fastCoverParams.FshrinkDict
}

func FASTCOVER_convertToFastCoverParams(tls *libc.TLS, coverParams ZDICT_cover_params_t, fastCoverParams uintptr, f uint32, accel uint32) {
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Fk = coverParams.Fk
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Fd = coverParams.Fd
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Fsteps = coverParams.Fsteps
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FnbThreads = coverParams.FnbThreads
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FsplitPoint = coverParams.FsplitPoint
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Ff = f
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Faccel = accel
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FzParams = coverParams.FzParams
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FshrinkDict = coverParams.FshrinkDict
}

func ZDICT_trainFromBuffer_fastCover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters ZDICT_fastCover_params_t) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	var accelParams FASTCOVER_accel_t
	var dict, segmentFreqs uintptr
	var dictionarySize, initVal, tail size_t
	var nbFinalizeSamples, v1 uint32
	var _ /* coverParams at bp+80 */ ZDICT_cover_params_t
	var _ /* ctx at bp+0 */ FASTCOVER_ctx_t
	_, _, _, _, _, _, _, _ = accelParams, dict, dictionarySize, initVal, nbFinalizeSamples, segmentFreqs, tail, v1
	dict = dictBuffer
	/* Initialize global data */
	g_displayLevel = int32(parameters.FzParams.FnotificationLevel)
	/* Assign splitPoint and f if not provided */
	parameters.FsplitPoint = float64(1)
	if parameters.Ff == uint32(0) {
		v1 = uint32(DEFAULT_F)
	} else {
		v1 = parameters.Ff
	}
	parameters.Ff = v1
	if parameters.Faccel == uint32(0) {
		v1 = uint32(DEFAULT_ACCEL)
	} else {
		v1 = parameters.Faccel
	}
	parameters.Faccel = v1
	/* Convert to cover parameter */
	libc.Xmemset(tls, bp+80, 0, uint64(48))
	FASTCOVER_convertToCoverParams(tls, parameters, bp+80)
	/* Checks */
	if !(FASTCOVER_checkParameters(tls, *(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80)), dictBufferCapacity, parameters.Ff, parameters.Faccel) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10089, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10121, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9269, libc.VaList(bp+136, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* Assign corresponding FASTCOVER_accel_t to accelParams*/
	accelParams = FASTCOVER_defaultAccelParameters[parameters.Faccel]
	/* Initialize context */
	initVal = FASTCOVER_ctx_init(tls, bp, samplesBuffer, samplesSizes, nbSamples, (*(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80))).Fd, parameters.FsplitPoint, parameters.Ff, accelParams)
	if ZSTD_isError(tls, initVal) != 0 {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9549, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return initVal
	}
	COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).FnbDmers, g_displayLevel)
	/* Build the dictionary */
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9353, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	/* Initialize array to keep track of frequency of dmer within activeSegment */
	segmentFreqs = libc.Xcalloc(tls, libc.Uint64FromInt32(1)<<parameters.Ff, uint64(2))
	tail = FASTCOVER_buildDictionary(tls, bp, (*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).Ffreqs, dictBuffer, dictBufferCapacity, *(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80)), segmentFreqs)
	nbFinalizeSamples = uint32((*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).FnbTrainSamples * uint64((*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).FaccelParams.Ffinalize) / libc.Uint64FromInt32(100))
	dictionarySize = ZDICT_finalizeDictionary(tls, dict, dictBufferCapacity, dict+uintptr(tail), dictBufferCapacity-tail, samplesBuffer, samplesSizes, nbFinalizeSamples, (*(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80))).FzParams)
	if !(ZSTD_isError(tls, dictionarySize) != 0) {
		if g_displayLevel >= int32(2) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9374, libc.VaList(bp+136, uint32(dictionarySize)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
	}
	FASTCOVER_ctx_destroy(tls, bp)
	libc.Xfree(tls, segmentFreqs)
	return dictionarySize
	return r
}

func ZDICT_optimizeTrainFromBuffer_fastCover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters uintptr) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var accel, d, f, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, shrinkDict, v2, v3, v4, v5, v6, v7, v8, v9 uint32
	var accelParams FASTCOVER_accel_t
	var compressedSize, dictSize, initVal size_t
	var data, pool uintptr
	var displayLevel, warned, v10 int32
	var splitPoint, v1 float64
	var _ /* best at bp+48 */ COVER_best_t
	var _ /* coverParams at bp+0 */ ZDICT_cover_params_t
	var _ /* ctx at bp+176 */ FASTCOVER_ctx_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = accel, accelParams, compressedSize, d, data, dictSize, displayLevel, f, initVal, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, pool, shrinkDict, splitPoint, warned, v1, v10, v2, v3, v4, v5, v6, v7, v8, v9
	/* constants */
	nbThreads = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FnbThreads
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FsplitPoint <= float64(0) {
		v1 = float64(FASTCOVER_DEFAULT_SPLITPOINT)
	} else {
		v1 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FsplitPoint
	}
	splitPoint = v1
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v2 = uint32(6)
	} else {
		v2 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMinD = v2
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v3 = uint32(8)
	} else {
		v3 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMaxD = v3
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v4 = uint32(50)
	} else {
		v4 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMinK = v4
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v5 = uint32(2000)
	} else {
		v5 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMaxK = v5
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fsteps == uint32(0) {
		v6 = uint32(40)
	} else {
		v6 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fsteps
	}
	kSteps = v6
	if (kMaxK-kMinK)/kSteps > uint32(libc.Int32FromInt32(1)) {
		v7 = (kMaxK - kMinK) / kSteps
	} else {
		v7 = uint32(libc.Int32FromInt32(1))
	}
	kStepSize = v7
	kIterations = (uint32(1) + (kMaxD-kMinD)/uint32(2)) * (uint32(1) + (kMaxK-kMinK)/kStepSize)
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Ff == uint32(0) {
		v8 = uint32(DEFAULT_F)
	} else {
		v8 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Ff
	}
	f = v8
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Faccel == uint32(0) {
		v9 = uint32(DEFAULT_ACCEL)
	} else {
		v9 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Faccel
	}
	accel = v9
	shrinkDict = uint32(0)
	/* Local variables */
	displayLevel = int32((*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FzParams.FnotificationLevel)
	iteration = uint32(1)
	pool = libc.UintptrFromInt32(0)
	warned = 0
	/* Checks */
	if splitPoint <= libc.Float64FromInt32(0) || splitPoint > libc.Float64FromInt32(1) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10166, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if accel == uint32(0) || accel > uint32(FASTCOVER_MAX_ACCEL) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10188, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if kMinK < kMaxD || kMaxK < kMinK {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10205, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10121, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9269, libc.VaList(bp+264, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbThreads > uint32(1) {
		pool = POOL_create(tls, uint64(nbThreads), uint64(1))
		if !(pool != 0) {
			return uint64(-int32(ZSTD_error_memory_allocation))
		}
	}
	/* Initialization */
	COVER_best_init(tls, bp+48)
	libc.Xmemset(tls, bp, 0, uint64(48))
	FASTCOVER_convertToCoverParams(tls, *(*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)), bp)
	accelParams = FASTCOVER_defaultAccelParameters[accel]
	/* Turn down global display level to clean up display at level 2 and below */
	if displayLevel == 0 {
		v10 = 0
	} else {
		v10 = displayLevel - int32(1)
	}
	g_displayLevel = v10
	/* Loop through d first because each new value needs a new context */
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9503, libc.VaList(bp+264, kIterations))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	d = kMinD
	for {
		if !(d <= kMaxD) {
			break
		}
		if displayLevel >= int32(3) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9543, libc.VaList(bp+264, d))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		initVal = FASTCOVER_ctx_init(tls, bp+176, samplesBuffer, samplesSizes, nbSamples, d, splitPoint, f, accelParams)
		if ZSTD_isError(tls, initVal) != 0 {
			if displayLevel >= int32(1) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9549, 0)
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			COVER_best_destroy(tls, bp+48)
			POOL_free(tls, pool)
			return initVal
		}
		if !(warned != 0) {
			COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp + 176))).FnbDmers, displayLevel)
			warned = int32(1)
		}
		/* Loop through k reusing the same context */
		k = kMinK
		for {
			if !(k <= kMaxK) {
				break
			}
			/* Prepare the arguments */
			data = libc.Xmalloc(tls, uint64(72))
			if displayLevel >= int32(3) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9579, libc.VaList(bp+264, k))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			if !(data != 0) {
				if displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9585, 0)
					libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
				}
				COVER_best_destroy(tls, bp+48)
				FASTCOVER_ctx_destroy(tls, bp+176)
				POOL_free(tls, pool)
				return uint64(-int32(ZSTD_error_memory_allocation))
			}
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx = bp + 176
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest = bp + 48
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity = dictBufferCapacity
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters = *(*ZDICT_cover_params_t)(unsafe.Pointer(bp))
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fk = k
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fd = d
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FsplitPoint = splitPoint
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fsteps = kSteps
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FshrinkDict = shrinkDict
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FzParams.FnotificationLevel = uint32(g_displayLevel)
			/* Check the parameters */
			if !(FASTCOVER_checkParameters(tls, (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters, dictBufferCapacity, (*FASTCOVER_ctx_t)(unsafe.Pointer((*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx)).Ff, accel) != 0) {
				if g_displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10089, 0)
					libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
				}
				libc.Xfree(tls, data)
				goto _12
			}
			/* Call the function and pass ownership of data to it */
			COVER_best_start(tls, bp+48)
			if pool != 0 {
				POOL_add(tls, pool, __ccgo_fp(FASTCOVER_tryParameters), data)
			} else {
				FASTCOVER_tryParameters(tls, data)
			}
			/* Print status */
			if displayLevel >= int32(2) {
				if clock(tls)-g_time > g_refreshRate || displayLevel >= int32(4) {
					g_time = clock(tls)
					libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9180, libc.VaList(bp+264, iteration*libc.Uint32FromInt32(100)/kIterations))
					libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
				}
			}
			iteration = iteration + 1
			goto _12
		_12:
			;
			k = k + kStepSize
		}
		COVER_best_wait(tls, bp+48)
		FASTCOVER_ctx_destroy(tls, bp+176)
		goto _11
	_11:
		;
		d = d + uint32(2)
	}
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+9193, libc.VaList(bp+264, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	/* Fill the output buffer and parameters with output of the best parameters */
	dictSize = (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).FdictSize
	if ZSTD_isError(tls, (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).FcompressedSize) != 0 {
		compressedSize = (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).FcompressedSize
		COVER_best_destroy(tls, bp+48)
		POOL_free(tls, pool)
		return compressedSize
	}
	FASTCOVER_convertToFastCoverParams(tls, (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).Fparameters, parameters, f, accel)
	libc.Xmemcpy(tls, dictBuffer, (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).Fdict, dictSize)
	COVER_best_destroy(tls, bp+48)
	POOL_free(tls, pool)
	return dictSize
	return r
}

/**** ended inlining dictBuilder/fastcover.c ****/
/**** start inlining dictBuilder/zdict.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-**************************************
*  Tuning parameters
****************************************/

/*-**************************************
*  Compiler Options
****************************************/
/* Unix Large Files support (>4GB) */

/*-*************************************
*  Dependencies
***************************************/
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */
/**
 * This file has no copyright assigned and is placed in the Public Domain.
 * This file is part of the mingw-w64 runtime package.
 * No warranty is given; refer to the file DISCLAIMER.PD within this package.
 */

/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/xxhash.h ****/
/**** skipping file: ../compress/zstd_compress_internal.h ****/
/**** skipping file: ../zdict.h ****/
/**** skipping file: divsufsort.h ****/
/**** skipping file: ../common/bits.h ****/

/*-*************************************
*  Constants
***************************************/

var g_selectivity_default = uint32(9)

/*-*************************************
*  Console display
***************************************/

func ZDICT_clockSpan(tls *libc.TLS, nPrevious clock_t) (r clock_t) {
	return clock(tls) - nPrevious
}

func ZDICT_printHex(tls *libc.TLS, ptr uintptr, length size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var b uintptr
	var c BYTE
	var u size_t
	_, _, _ = b, c, u
	b = ptr
	u = uint64(0)
	for {
		if !(u < length) {
			break
		}
		c = *(*BYTE)(unsafe.Pointer(b + uintptr(u)))
		if int32(c) < int32(32) || int32(c) > int32(126) {
			c = uint8('.')
		} /* non-printable char */
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10218, libc.VaList(bp+8, int32(c)))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		goto _1
	_1:
		;
		u = u + 1
	}
}

// C documentation
//
//	/*-********************************************************
//	*  Helper functions
//	**********************************************************/
func ZDICT_isError(tls *libc.TLS, errorCode size_t) (r uint32) {
	return ERR_isError(tls, errorCode)
}

func ZDICT_getErrorName(tls *libc.TLS, errorCode size_t) (r uintptr) {
	return ERR_getErrorName(tls, errorCode)
}

func ZDICT_getDictID(tls *libc.TLS, dictBuffer uintptr, dictSize size_t) (r uint32) {
	if dictSize < uint64(8) {
		return uint32(0)
	}
	if MEM_readLE32(tls, dictBuffer) != uint32(ZSTD_MAGIC_DICTIONARY) {
		return uint32(0)
	}
	return MEM_readLE32(tls, dictBuffer+uintptr(4))
}

func ZDICT_getDictHeaderSize(tls *libc.TLS, dictBuffer uintptr, dictSize size_t) (r size_t) {
	var bs, wksp uintptr
	var headerSize size_t
	_, _, _ = bs, headerSize, wksp
	if dictSize <= uint64(8) || MEM_readLE32(tls, dictBuffer) != uint32(ZSTD_MAGIC_DICTIONARY) {
		return uint64(-int32(ZSTD_error_dictionary_corrupted))
	}
	bs = libc.Xmalloc(tls, uint64(5632))
	wksp = libc.Xmalloc(tls, uint64(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))
	if !(bs != 0) || !(wksp != 0) {
		headerSize = uint64(-int32(ZSTD_error_memory_allocation))
	} else {
		ZSTD_reset_compressedBlockState(tls, bs)
		headerSize = ZSTD_loadCEntropy(tls, bs, wksp, dictBuffer, dictSize)
	}
	libc.Xfree(tls, bs)
	libc.Xfree(tls, wksp)
	return headerSize
}

// C documentation
//
//	/*-********************************************************
//	*  Dictionary training functions
//	**********************************************************/
//	/*! ZDICT_count() :
//	    Count the nb of common bytes between 2 pointers.
//	    Note : this function presumes end of buffer followed by noisy guard band.
//	*/
func ZDICT_count(tls *libc.TLS, pIn uintptr, pMatch uintptr) (r size_t) {
	var diff size_t
	var pStart uintptr
	_, _ = diff, pStart
	pStart = pIn
	for {
		diff = MEM_readST(tls, pMatch) ^ MEM_readST(tls, pIn)
		if !(diff != 0) {
			pIn = pIn + uintptr(8)
			pMatch = pMatch + uintptr(8)
			goto _1
		}
		pIn = pIn + uintptr(ZSTD_NbCommonBytes(tls, diff))
		return uint64(int64(pIn) - int64(pStart))
		goto _1
	_1:
	}
	return r
}

type dictItem = struct {
	Fpos     U32
	Flength  U32
	Fsavings U32
}

func ZDICT_initDictItem(tls *libc.TLS, d uintptr) {
	(*dictItem)(unsafe.Pointer(d)).Fpos = uint32(1)
	(*dictItem)(unsafe.Pointer(d)).Flength = uint32(0)
	(*dictItem)(unsafe.Pointer(d)).Fsavings = uint32(-libc.Int32FromInt32(1))
}

func ZDICT_analyzePos(tls *libc.TLS, doneMarks uintptr, suffix uintptr, start U32, buffer uintptr, minRatio U32, notificationLevel U32) (r dictItem) {
	bp := tls.Alloc(576)
	defer tls.Free(576)
	var b uintptr
	var c, currentChar BYTE
	var currentCount, currentID, end, id, id1, idx, l, length4, mml, p, pEnd, patternEnd, refinedEnd, refinedStart, selectedCount, selectedID, testedPos, u U32
	var i int32
	var length, length1, length2, length3, maxLength, pos size_t
	var pattern16 U16
	var savings [64]U32
	var _ /* cumulLength at bp+256 */ [64]U32
	var _ /* lengthList at bp+0 */ [64]U32
	var _ /* solution at bp+512 */ dictItem
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = b, c, currentChar, currentCount, currentID, end, i, id, id1, idx, l, length, length1, length2, length3, length4, maxLength, mml, p, pEnd, pattern16, patternEnd, pos, refinedEnd, refinedStart, savings, selectedCount, selectedID, testedPos, u
	*(*[64]U32)(unsafe.Pointer(bp)) = [64]U32{}
	*(*[64]U32)(unsafe.Pointer(bp + 256)) = [64]U32{}
	savings = [64]U32{}
	b = buffer
	maxLength = uint64(LLIMIT)
	pos = uint64(*(*int32)(unsafe.Pointer(suffix + uintptr(start)*4)))
	end = start
	/* init */
	libc.Xmemset(tls, bp+512, 0, uint64(12))
	*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(pos))) = uint8(1)
	/* trivial repetition cases */
	if int32(MEM_read16(tls, b+uintptr(pos)+uintptr(0))) == int32(MEM_read16(tls, b+uintptr(pos)+uintptr(2))) || int32(MEM_read16(tls, b+uintptr(pos)+uintptr(1))) == int32(MEM_read16(tls, b+uintptr(pos)+uintptr(3))) || int32(MEM_read16(tls, b+uintptr(pos)+uintptr(2))) == int32(MEM_read16(tls, b+uintptr(pos)+uintptr(4))) {
		/* skip and mark segment */
		pattern16 = MEM_read16(tls, b+uintptr(pos)+uintptr(4))
		patternEnd = uint32(6)
		for int32(MEM_read16(tls, b+uintptr(pos)+uintptr(patternEnd))) == int32(pattern16) {
			patternEnd = patternEnd + uint32(2)
		}
		if int32(*(*BYTE)(unsafe.Pointer(b + uintptr(pos+uint64(patternEnd))))) == int32(*(*BYTE)(unsafe.Pointer(b + uintptr(pos+uint64(patternEnd)-uint64(1))))) {
			patternEnd = patternEnd + 1
		}
		u = uint32(1)
		for {
			if !(u < patternEnd) {
				break
			}
			*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(pos+uint64(u)))) = uint8(1)
			goto _1
		_1:
			;
			u = u + 1
		}
		return *(*dictItem)(unsafe.Pointer(bp + 512))
	}
	/* look forward */
	for cond := true; cond; cond = length >= uint64(MINMATCHLENGTH) {
		end = end + 1
		length = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(end)*4))))
	}
	/* look backward */
	for cond := true; cond; cond = length1 >= uint64(MINMATCHLENGTH) {
		length1 = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(start)*4 - libc.UintptrFromInt32(1)*4))))
		if length1 >= uint64(MINMATCHLENGTH) {
			start = start - 1
		}
	}
	/* exit if not found a minimum nb of repetitions */
	if end-start < minRatio {
		idx = start
		for {
			if !(idx < end) {
				break
			}
			*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(idx)*4))))) = uint8(1)
			goto _2
		_2:
			;
			idx = idx + 1
		}
		return *(*dictItem)(unsafe.Pointer(bp + 512))
	}
	refinedStart = start
	refinedEnd = end
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10221, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10223, libc.VaList(bp+536, end-start, int32(MINMATCHLENGTH), uint32(pos)))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10221, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	mml = uint32(MINMATCHLENGTH)
	for {
		currentChar = uint8(0)
		currentCount = uint32(0)
		currentID = refinedStart
		selectedCount = uint32(0)
		selectedID = currentID
		id = refinedStart
		for {
			if !(id < refinedEnd) {
				break
			}
			if int32(*(*BYTE)(unsafe.Pointer(b + uintptr(uint32(*(*int32)(unsafe.Pointer(suffix + uintptr(id)*4)))+mml)))) != int32(currentChar) {
				if currentCount > selectedCount {
					selectedCount = currentCount
					selectedID = currentID
				}
				currentID = id
				currentChar = *(*BYTE)(unsafe.Pointer(b + uintptr(uint32(*(*int32)(unsafe.Pointer(suffix + uintptr(id)*4)))+mml)))
				currentCount = uint32(0)
			}
			currentCount = currentCount + 1
			goto _4
		_4:
			;
			id = id + 1
		}
		if currentCount > selectedCount { /* for last */
			selectedCount = currentCount
			selectedID = currentID
		}
		if selectedCount < minRatio {
			break
		}
		refinedStart = selectedID
		refinedEnd = refinedStart + selectedCount
		goto _3
	_3:
		;
		mml = mml + 1
	}
	/* evaluate gain based on new dict */
	start = refinedStart
	pos = uint64(*(*int32)(unsafe.Pointer(suffix + uintptr(refinedStart)*4)))
	end = start
	libc.Xmemset(tls, bp, 0, uint64(256))
	/* look forward */
	for cond := true; cond; cond = length2 >= uint64(MINMATCHLENGTH) {
		end = end + 1
		length2 = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(end)*4))))
		if length2 >= uint64(LLIMIT) {
			length2 = uint64(libc.Int32FromInt32(LLIMIT) - libc.Int32FromInt32(1))
		}
		(*(*[64]U32)(unsafe.Pointer(bp)))[length2] = (*(*[64]U32)(unsafe.Pointer(bp)))[length2] + 1
	}
	/* look backward */
	length3 = uint64(MINMATCHLENGTH)
	for libc.BoolInt32(length3 >= uint64(MINMATCHLENGTH))&libc.BoolInt32(start > uint32(0)) != 0 {
		length3 = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(start-uint32(1))*4))))
		if length3 >= uint64(LLIMIT) {
			length3 = uint64(libc.Int32FromInt32(LLIMIT) - libc.Int32FromInt32(1))
		}
		(*(*[64]U32)(unsafe.Pointer(bp)))[length3] = (*(*[64]U32)(unsafe.Pointer(bp)))[length3] + 1
		if length3 >= uint64(MINMATCHLENGTH) {
			start = start - 1
		}
	}
	/* largest useful length */
	libc.Xmemset(tls, bp+256, 0, uint64(256))
	(*(*[64]U32)(unsafe.Pointer(bp + 256)))[maxLength-uint64(1)] = (*(*[64]U32)(unsafe.Pointer(bp)))[maxLength-uint64(1)]
	i = int32(maxLength - libc.Uint64FromInt32(2))
	for {
		if !(i >= 0) {
			break
		}
		(*(*[64]U32)(unsafe.Pointer(bp + 256)))[i] = (*(*[64]U32)(unsafe.Pointer(bp + 256)))[i+int32(1)] + (*(*[64]U32)(unsafe.Pointer(bp)))[i]
		goto _5
	_5:
		;
		i = i - 1
	}
	i = libc.Int32FromInt32(LLIMIT) - libc.Int32FromInt32(1)
	for {
		if !(i >= int32(MINMATCHLENGTH)) {
			break
		}
		if (*(*[64]U32)(unsafe.Pointer(bp + 256)))[i] >= minRatio {
			break
		}
		goto _6
	_6:
		;
		i = i - 1
	}
	maxLength = uint64(i)
	/* reduce maxLength in case of final into repetitive data */
	l = uint32(maxLength)
	c = *(*BYTE)(unsafe.Pointer(b + uintptr(pos+maxLength-uint64(1))))
	for int32(*(*BYTE)(unsafe.Pointer(b + uintptr(pos+uint64(l)-uint64(2))))) == int32(c) {
		l = l - 1
	}
	maxLength = uint64(l)
	if maxLength < uint64(MINMATCHLENGTH) {
		return *(*dictItem)(unsafe.Pointer(bp + 512))
	} /* skip : no long-enough solution */
	/* calculate savings */
	savings[int32(5)] = uint32(0)
	i = int32(MINMATCHLENGTH)
	for {
		if !(i <= int32(maxLength)) {
			break
		}
		savings[i] = savings[i-int32(1)] + (*(*[64]U32)(unsafe.Pointer(bp)))[i]*uint32(i-libc.Int32FromInt32(3))
		goto _7
	_7:
		;
		i = i + 1
	}
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10270, libc.VaList(bp+536, uint32(pos), uint32(maxLength), savings[maxLength], float64(savings[maxLength])/float64(maxLength)))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	(*(*dictItem)(unsafe.Pointer(bp + 512))).Fpos = uint32(pos)
	(*(*dictItem)(unsafe.Pointer(bp + 512))).Flength = uint32(maxLength)
	(*(*dictItem)(unsafe.Pointer(bp + 512))).Fsavings = savings[maxLength]
	/* mark positions done */
	id1 = start
	for {
		if !(id1 < end) {
			break
		}
		testedPos = uint32(*(*int32)(unsafe.Pointer(suffix + uintptr(id1)*4)))
		if uint64(testedPos) == pos {
			length4 = (*(*dictItem)(unsafe.Pointer(bp + 512))).Flength
		} else {
			length4 = uint32(ZDICT_count(tls, b+uintptr(pos), b+uintptr(testedPos)))
			if length4 > (*(*dictItem)(unsafe.Pointer(bp + 512))).Flength {
				length4 = (*(*dictItem)(unsafe.Pointer(bp + 512))).Flength
			}
		}
		pEnd = testedPos + length4
		p = testedPos
		for {
			if !(p < pEnd) {
				break
			}
			*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(p))) = uint8(1)
			goto _9
		_9:
			;
			p = p + 1
		}
		goto _8
	_8:
		;
		id1 = id1 + 1
	}
	return *(*dictItem)(unsafe.Pointer(bp + 512))
}

func isIncluded(tls *libc.TLS, in uintptr, container uintptr, length size_t) (r int32) {
	var into, ip uintptr
	var u size_t
	_, _, _ = into, ip, u
	ip = in
	into = container
	u = uint64(0)
	for {
		if !(u < length) {
			break
		} /* works because end of buffer is a noisy guard band */
		if int32(*(*int8)(unsafe.Pointer(ip + uintptr(u)))) != int32(*(*int8)(unsafe.Pointer(into + uintptr(u)))) {
			break
		}
		goto _1
	_1:
		;
		u = u + 1
	}
	return libc.BoolInt32(u == length)
}

// C documentation
//
//	/*! ZDICT_tryMerge() :
//	    check if dictItem can be merged, do it if possible
//	    @return : id of destination elt, 0 if not merged
//	*/
func ZDICT_tryMerge(tls *libc.TLS, table uintptr, elt dictItem, eltNbToSkip U32, buffer uintptr) (r U32) {
	var addedLength, eltEnd, tableSize, u U32
	var addedLength1, v3 int32
	var addedLength2 size_t
	var buf uintptr
	var v4 uint32
	_, _, _, _, _, _, _, _, _ = addedLength, addedLength1, addedLength2, buf, eltEnd, tableSize, u, v3, v4
	tableSize = (*dictItem)(unsafe.Pointer(table)).Fpos
	eltEnd = elt.Fpos + elt.Flength
	buf = buffer
	u = uint32(1)
	for {
		if !(u < tableSize) {
			break
		}
		if u == eltNbToSkip {
			goto _1
		}
		if (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos > elt.Fpos && (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos <= eltEnd { /* overlap, existing > new */
			/* append */
			addedLength = (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos - elt.Fpos
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength += addedLength
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos = elt.Fpos
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Fsavings * addedLength / elt.Flength /* rough approx */
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Flength / uint32(8)                  /* rough approx bonus */
			elt = *(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))
			/* sort : improve rank */
			for u > uint32(1) && (*(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))).Fsavings < elt.Fsavings {
				*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))
				u = u - 1
			}
			*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = elt
			return u
		}
		goto _1
	_1:
		;
		u = u + 1
	}
	/* front overlap */
	u = uint32(1)
	for {
		if !(u < tableSize) {
			break
		}
		if u == eltNbToSkip {
			goto _2
		}
		if (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos+(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength >= elt.Fpos && (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos < elt.Fpos { /* overlap, existing < new */
			/* append */
			addedLength1 = int32(eltEnd) - int32((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos+(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength)
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Flength / uint32(8) /* rough approx bonus */
			if addedLength1 > 0 {                                                                     /* otherwise, elt fully included into existing */
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength += uint32(addedLength1)
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Fsavings * uint32(addedLength1) / elt.Flength /* rough approx */
			}
			/* sort : improve rank */
			elt = *(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))
			for u > uint32(1) && (*(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))).Fsavings < elt.Fsavings {
				*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))
				u = u - 1
			}
			*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = elt
			return u
		}
		if MEM_read64(tls, buf+uintptr((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos)) == MEM_read64(tls, buf+uintptr(elt.Fpos)+uintptr(1)) {
			if isIncluded(tls, buf+uintptr((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos), buf+uintptr(elt.Fpos)+uintptr(1), uint64((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength)) != 0 {
				if int32(elt.Flength)-int32((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength) > int32(1) {
					v3 = int32(elt.Flength) - int32((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength)
				} else {
					v3 = int32(1)
				}
				addedLength2 = uint64(v3)
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos = elt.Fpos
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += uint32(uint64(elt.Fsavings) * addedLength2 / uint64(elt.Flength))
				if elt.Flength < (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength+uint32(1) {
					v4 = elt.Flength
				} else {
					v4 = (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength + uint32(1)
				}
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength = v4
				return u
			}
		}
		goto _2
	_2:
		;
		u = u + 1
	}
	return uint32(0)
}

func ZDICT_removeDictItem(tls *libc.TLS, table uintptr, id U32) {
	var max, u U32
	_, _ = max, u
	/* convention : table[0].pos stores nb of elts */
	max = (*(*dictItem)(unsafe.Pointer(table))).Fpos
	if !(id != 0) {
		return
	} /* protection, should never happen */
	u = id
	for {
		if !(u < max-uint32(1)) {
			break
		}
		*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(u+uint32(1))*12))
		goto _1
	_1:
		;
		u = u + 1
	}
	(*dictItem)(unsafe.Pointer(table)).Fpos = (*dictItem)(unsafe.Pointer(table)).Fpos - 1
}

func ZDICT_insertDictItem(tls *libc.TLS, table uintptr, maxSize U32, elt dictItem, buffer uintptr) {
	var current, mergeId, newMerge, nextElt U32
	_, _, _, _ = current, mergeId, newMerge, nextElt
	/* merge if possible */
	mergeId = ZDICT_tryMerge(tls, table, elt, uint32(0), buffer)
	if mergeId != 0 {
		newMerge = uint32(1)
		for newMerge != 0 {
			newMerge = ZDICT_tryMerge(tls, table, *(*dictItem)(unsafe.Pointer(table + uintptr(mergeId)*12)), mergeId, buffer)
			if newMerge != 0 {
				ZDICT_removeDictItem(tls, table, mergeId)
			}
			mergeId = newMerge
		}
		return
	}
	/* insert */
	nextElt = (*dictItem)(unsafe.Pointer(table)).Fpos
	if nextElt >= maxSize {
		nextElt = maxSize - uint32(1)
	}
	current = nextElt - uint32(1)
	for (*(*dictItem)(unsafe.Pointer(table + uintptr(current)*12))).Fsavings < elt.Fsavings {
		*(*dictItem)(unsafe.Pointer(table + uintptr(current+uint32(1))*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(current)*12))
		current = current - 1
	}
	*(*dictItem)(unsafe.Pointer(table + uintptr(current+uint32(1))*12)) = elt
	(*dictItem)(unsafe.Pointer(table)).Fpos = nextElt + uint32(1)
}

func ZDICT_dictSize(tls *libc.TLS, dictList uintptr) (r U32) {
	var dictSize, u U32
	_, _ = dictSize, u
	dictSize = uint32(0)
	u = uint32(1)
	for {
		if !(u < (*(*dictItem)(unsafe.Pointer(dictList))).Fpos) {
			break
		}
		dictSize = dictSize + (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Flength
		goto _1
	_1:
		;
		u = u + 1
	}
	return dictSize
}

func ZDICT_trainBuffer_legacy(tls *libc.TLS, dictList uintptr, dictListSize U32, buffer uintptr, bufferSize size_t, fileSizes uintptr, nbFiles uint32, minRatio uint32, notificationLevel U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cursor U32
	var displayClock, refreshRate clock_t
	var divSuftSortResult int32
	var doneMarks, filePos, reverseSuffix, suffix, suffix0 uintptr
	var pos, result size_t
	var solution dictItem
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = cursor, displayClock, divSuftSortResult, doneMarks, filePos, pos, refreshRate, result, reverseSuffix, solution, suffix, suffix0, v1
	suffix0 = libc.Xmalloc(tls, (bufferSize+uint64(2))*uint64(4))
	suffix = suffix0 + uintptr(1)*4
	reverseSuffix = libc.Xmalloc(tls, bufferSize*uint64(4))
	doneMarks = libc.Xmalloc(tls, (bufferSize+uint64(16))*uint64(1)) /* +16 for overflow security */
	filePos = libc.Xmalloc(tls, uint64(nbFiles)*uint64(4))
	result = uint64(0)
	displayClock = 0
	refreshRate = int32(libc.Int32FromInt32(CLOCKS_PER_SEC) * libc.Int32FromInt32(3) / libc.Int32FromInt32(10))
	/* init */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10341, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	} /* clean display line */
	if !(suffix0 != 0) || !(reverseSuffix != 0) || !(doneMarks != 0) || !(filePos != 0) {
		result = uint64(-int32(ZSTD_error_memory_allocation))
		goto _cleanup
	}
	if minRatio < uint32(MINRATIO) {
		minRatio = uint32(MINRATIO)
	}
	libc.Xmemset(tls, doneMarks, 0, bufferSize+uint64(16))
	/* limit sample set size (divsufsort limitation)*/
	if bufferSize > uint64(libc.Uint32FromUint32(2000)<<libc.Int32FromInt32(20)) {
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10348, libc.VaList(bp+8, libc.Uint32FromUint32(2000)<<libc.Int32FromInt32(20)>>libc.Int32FromInt32(20)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
	}
	for bufferSize > uint64(libc.Uint32FromUint32(2000)<<libc.Int32FromInt32(20)) {
		nbFiles = nbFiles - 1
		v1 = nbFiles
		bufferSize = bufferSize - *(*size_t)(unsafe.Pointer(fileSizes + uintptr(v1)*8))
	}
	/* sort */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10393, libc.VaList(bp+8, nbFiles, uint32(bufferSize>>libc.Int32FromInt32(20))))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	divSuftSortResult = divsufsort(tls, buffer, suffix, int32(bufferSize), 0)
	if divSuftSortResult != 0 {
		result = uint64(-int32(ZSTD_error_GENERIC))
		goto _cleanup
	}
	*(*int32)(unsafe.Pointer(suffix + uintptr(bufferSize)*4)) = int32(bufferSize) /* leads into noise */
	*(*int32)(unsafe.Pointer(suffix0)) = int32(bufferSize)                        /* leads into noise */
	/* build reverse suffix sort */
	pos = uint64(0)
	for {
		if !(pos < bufferSize) {
			break
		}
		*(*U32)(unsafe.Pointer(reverseSuffix + uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(pos)*4)))*4)) = uint32(pos)
		goto _2
	_2:
		;
		pos = pos + 1
	}
	/* note filePos tracks borders between samples.
	   It's not used at this stage, but planned to become useful in a later update */
	*(*U32)(unsafe.Pointer(filePos)) = uint32(0)
	pos = uint64(1)
	for {
		if !(pos < uint64(nbFiles)) {
			break
		}
		*(*U32)(unsafe.Pointer(filePos + uintptr(pos)*4)) = uint32(uint64(*(*U32)(unsafe.Pointer(filePos + uintptr(pos-uint64(1))*4))) + *(*size_t)(unsafe.Pointer(fileSizes + uintptr(pos-uint64(1))*8)))
		goto _3
	_3:
		;
		pos = pos + 1
	}
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10435, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	if notificationLevel >= uint32(3) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10458, libc.VaList(bp+8, minRatio))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	cursor = uint32(0)
	for {
		if !(uint64(cursor) < bufferSize) {
			break
		}
		if *(*BYTE)(unsafe.Pointer(doneMarks + uintptr(cursor))) != 0 {
			cursor = cursor + 1
			goto _4
		}
		solution = ZDICT_analyzePos(tls, doneMarks, suffix, *(*U32)(unsafe.Pointer(reverseSuffix + uintptr(cursor)*4)), buffer, minRatio, notificationLevel)
		if solution.Flength == uint32(0) {
			cursor = cursor + 1
			goto _4
		}
		ZDICT_insertDictItem(tls, dictList, dictListSize, solution, buffer)
		cursor = cursor + solution.Flength
		if notificationLevel >= uint32(2) {
			if ZDICT_clockSpan(tls, displayClock) > refreshRate {
				displayClock = clock(tls)
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10479, libc.VaList(bp+8, float64(float64(cursor)/float64(bufferSize)*float64(100))))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			if notificationLevel >= uint32(4) {
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
		}
		goto _4
	_4:
	}
	goto _cleanup
_cleanup:
	;
	libc.Xfree(tls, suffix0)
	libc.Xfree(tls, reverseSuffix)
	libc.Xfree(tls, doneMarks)
	libc.Xfree(tls, filePos)
	return result
}

func ZDICT_fillNoise(tls *libc.TLS, buffer uintptr, length size_t) {
	var acc, prime1, prime2 uint32
	var p size_t
	_, _, _, _ = acc, p, prime1, prime2
	prime1 = uint32(2654435761)
	prime2 = uint32(2246822519)
	acc = prime1
	p = uint64(0)
	p = uint64(0)
	for {
		if !(p < length) {
			break
		}
		acc = acc * prime2
		*(*uint8)(unsafe.Pointer(buffer + uintptr(p))) = uint8(acc >> libc.Int32FromInt32(21))
		goto _1
	_1:
		;
		p = p + 1
	}
}

type EStats_ress_t = struct {
	Fdict      uintptr
	Fzc        uintptr
	FworkPlace uintptr
}

func ZDICT_countEStats(tls *libc.TLS, esr EStats_ress_t, params uintptr, countLit uintptr, offsetcodeCount uintptr, matchlengthCount uintptr, litlengthCount uintptr, repOffsets uintptr, src uintptr, srcSize size_t, notificationLevel U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var blockSizeMax, cSize, errorCode size_t
	var bytePtr, codePtr, codePtr1, codePtr2, seq, seqStorePtr uintptr
	var nbSeq, offset1, offset2, u, u1, u2 U32
	var v1 int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSizeMax, bytePtr, cSize, codePtr, codePtr1, codePtr2, errorCode, nbSeq, offset1, offset2, seq, seqStorePtr, u, u1, u2, v1
	if libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX) < int32(1)<<(*ZSTD_parameters)(unsafe.Pointer(params)).FcParams.FwindowLog {
		v1 = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
	} else {
		v1 = int32(1) << (*ZSTD_parameters)(unsafe.Pointer(params)).FcParams.FwindowLog
	}
	blockSizeMax = uint64(v1)
	if srcSize > blockSizeMax {
		srcSize = blockSizeMax
	} /* protection vs large samples */
	errorCode = ZSTD_compressBegin_usingCDict_deprecated(tls, esr.Fzc, esr.Fdict)
	if ZSTD_isError(tls, errorCode) != 0 {
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10491, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return
	}
	cSize = ZSTD_compressBlock_deprecated(tls, esr.Fzc, esr.FworkPlace, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)), src, srcSize)
	if ZSTD_isError(tls, cSize) != 0 {
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10540, libc.VaList(bp+8, uint32(srcSize)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		return
	}
	if cSize != 0 { /* if == 0; block is not compressible */
		seqStorePtr = ZSTD_getSeqStore(tls, esr.Fzc)
		/* literals stats */
		bytePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart
		for {
			if !(bytePtr < (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit) {
				break
			}
			*(*uint32)(unsafe.Pointer(countLit + uintptr(*(*BYTE)(unsafe.Pointer(bytePtr)))*4)) = *(*uint32)(unsafe.Pointer(countLit + uintptr(*(*BYTE)(unsafe.Pointer(bytePtr)))*4)) + 1
			goto _2
		_2:
			;
			bytePtr = bytePtr + 1
		}
		/* seqStats */
		nbSeq = uint32((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
		ZSTD_seqToCodes(tls, seqStorePtr)
		codePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
		u = uint32(0)
		for {
			if !(u < nbSeq) {
				break
			}
			*(*uint32)(unsafe.Pointer(offsetcodeCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr + uintptr(u))))*4)) = *(*uint32)(unsafe.Pointer(offsetcodeCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr + uintptr(u))))*4)) + 1
			goto _3
		_3:
			;
			u = u + 1
		}
		codePtr1 = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
		u1 = uint32(0)
		for {
			if !(u1 < nbSeq) {
				break
			}
			*(*uint32)(unsafe.Pointer(matchlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr1 + uintptr(u1))))*4)) = *(*uint32)(unsafe.Pointer(matchlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr1 + uintptr(u1))))*4)) + 1
			goto _4
		_4:
			;
			u1 = u1 + 1
		}
		codePtr2 = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
		u2 = uint32(0)
		for {
			if !(u2 < nbSeq) {
				break
			}
			*(*uint32)(unsafe.Pointer(litlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr2 + uintptr(u2))))*4)) = *(*uint32)(unsafe.Pointer(litlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr2 + uintptr(u2))))*4)) + 1
			goto _5
		_5:
			;
			u2 = u2 + 1
		}
		if nbSeq >= uint32(2) { /* rep offsets */
			seq = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
			offset1 = (*(*SeqDef)(unsafe.Pointer(seq))).FoffBase - uint32(ZSTD_REP_NUM)
			offset2 = (*(*SeqDef)(unsafe.Pointer(seq + 1*8))).FoffBase - uint32(ZSTD_REP_NUM)
			if offset1 >= uint32(MAXREPOFFSET) {
				offset1 = uint32(0)
			}
			if offset2 >= uint32(MAXREPOFFSET) {
				offset2 = uint32(0)
			}
			*(*U32)(unsafe.Pointer(repOffsets + uintptr(offset1)*4)) += uint32(3)
			*(*U32)(unsafe.Pointer(repOffsets + uintptr(offset2)*4)) += uint32(1)
		}
	}
}

func ZDICT_totalSampleSize(tls *libc.TLS, fileSizes uintptr, nbFiles uint32) (r size_t) {
	var total size_t
	var u uint32
	_, _ = total, u
	total = uint64(0)
	u = uint32(0)
	for {
		if !(u < nbFiles) {
			break
		}
		total = total + *(*size_t)(unsafe.Pointer(fileSizes + uintptr(u)*8))
		goto _1
	_1:
		;
		u = u + 1
	}
	return total
}

type offsetCount_t = struct {
	Foffset U32
	Fcount  U32
}

func ZDICT_insertSortCount(tls *libc.TLS, table uintptr, val U32, count U32) {
	var tmp offsetCount_t
	var u U32
	_, _ = tmp, u
	(*(*offsetCount_t)(unsafe.Pointer(table + 3*8))).Foffset = val
	(*(*offsetCount_t)(unsafe.Pointer(table + 3*8))).Fcount = count
	u = uint32(ZSTD_REP_NUM)
	for {
		if !(u > uint32(0)) {
			break
		}
		if (*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u-uint32(1))*8))).Fcount >= (*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u)*8))).Fcount {
			break
		}
		tmp = *(*offsetCount_t)(unsafe.Pointer(table + uintptr(u-uint32(1))*8))
		*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u-uint32(1))*8)) = *(*offsetCount_t)(unsafe.Pointer(table + uintptr(u)*8))
		*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u)*8)) = tmp
		goto _1
	_1:
		;
		u = u - 1
	}
}

// C documentation
//
//	/* ZDICT_flatLit() :
//	 * rewrite `countLit` to contain a mostly flat but still compressible distribution of literals.
//	 * necessary to avoid generating a non-compressible distribution that HUF_writeCTable() cannot encode.
//	 */
func ZDICT_flatLit(tls *libc.TLS, countLit uintptr) {
	var u int32
	_ = u
	u = int32(1)
	for {
		if !(u < int32(256)) {
			break
		}
		*(*uint32)(unsafe.Pointer(countLit + uintptr(u)*4)) = uint32(2)
		goto _1
	_1:
		;
		u = u + 1
	}
	*(*uint32)(unsafe.Pointer(countLit)) = uint32(4)
	*(*uint32)(unsafe.Pointer(countLit + 253*4)) = uint32(1)
	*(*uint32)(unsafe.Pointer(countLit + 254*4)) = uint32(1)
}

func ZDICT_analyzeEntropy(tls *libc.TLS, dstBuffer uintptr, maxDstSize size_t, compressionLevel int32, srcBuffer uintptr, fileSizes uintptr, nbFiles uint32, dictBuffer uintptr, dictBufferSize size_t, notificationLevel uint32) (r size_t) {
	bp := tls.Alloc(12864)
	defer tls.Free(12864)
	var Offlog, huffLog, llLog, mlLog, offcodeMax, offset, total, u, v5, v6 U32
	var averageSampleSize, eSize, errorCode, hhSize, lhSize, maxNbBits, mhSize, ohSize, pos, totalSrcSize size_t
	var dstPtr uintptr
	var esr EStats_ress_t
	var v9 bool
	var _ /* bestRepOffset at bp+7900 */ [4]offsetCount_t
	var _ /* countLit at bp+0 */ [256]uint32
	var _ /* hufTable at bp+1024 */ [257]HUF_CElt
	var _ /* litLengthCount at bp+3588 */ [36]uint32
	var _ /* litLengthNCount at bp+3732 */ [36]int16
	var _ /* matchLengthCount at bp+3268 */ [53]uint32
	var _ /* matchLengthNCount at bp+3480 */ [53]int16
	var _ /* offcodeCount at bp+3080 */ [31]uint32
	var _ /* offcodeNCount at bp+3204 */ [31]int16
	var _ /* params at bp+7932 */ ZSTD_parameters
	var _ /* repOffset at bp+3804 */ [1024]U32
	var _ /* wksp at bp+7972 */ [1216]U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = Offlog, averageSampleSize, dstPtr, eSize, errorCode, esr, hhSize, huffLog, lhSize, llLog, maxNbBits, mhSize, mlLog, offcodeMax, offset, ohSize, pos, total, totalSrcSize, u, v5, v6, v9
	offcodeMax = ZSTD_highbit32(tls, uint32(dictBufferSize+uint64(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	esr = EStats_ress_t{}
	huffLog = uint32(11)
	Offlog = uint32(OffFSELog)
	mlLog = uint32(MLFSELog)
	llLog = uint32(LLFSELog)
	pos = uint64(0)
	eSize = uint64(0)
	totalSrcSize = ZDICT_totalSampleSize(tls, fileSizes, nbFiles)
	averageSampleSize = totalSrcSize / uint64(nbFiles+libc.BoolUint32(!(nbFiles != 0)))
	dstPtr = dstBuffer
	/* init */
	if offcodeMax > uint32(OFFCODE_MAX) {
		eSize = uint64(-int32(ZSTD_error_dictionaryCreation_failed))
		goto _cleanup
	} /* too large dictionary */
	u = uint32(0)
	for {
		if !(u < uint32(256)) {
			break
		}
		(*(*[256]uint32)(unsafe.Pointer(bp)))[u] = uint32(1)
		goto _1
	_1:
		;
		u = u + 1
	} /* any character must be described */
	u = uint32(0)
	for {
		if !(u <= offcodeMax) {
			break
		}
		(*(*[31]uint32)(unsafe.Pointer(bp + 3080)))[u] = uint32(1)
		goto _2
	_2:
		;
		u = u + 1
	}
	u = uint32(0)
	for {
		if !(u <= uint32(MaxML)) {
			break
		}
		(*(*[53]uint32)(unsafe.Pointer(bp + 3268)))[u] = uint32(1)
		goto _3
	_3:
		;
		u = u + 1
	}
	u = uint32(0)
	for {
		if !(u <= uint32(MaxLL)) {
			break
		}
		(*(*[36]uint32)(unsafe.Pointer(bp + 3588)))[u] = uint32(1)
		goto _4
	_4:
		;
		u = u + 1
	}
	libc.Xmemset(tls, bp+3804, 0, uint64(4096))
	v6 = libc.Uint32FromInt32(1)
	(*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[int32(8)] = v6
	v5 = v6
	(*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[int32(4)] = v5
	(*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[int32(1)] = v5
	libc.Xmemset(tls, bp+7900, 0, uint64(32))
	if compressionLevel == 0 {
		compressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	}
	*(*ZSTD_parameters)(unsafe.Pointer(bp + 7932)) = ZSTD_getParams(tls, compressionLevel, averageSampleSize, dictBufferSize)
	esr.Fdict = ZSTD_createCDict_advanced(tls, dictBuffer, dictBufferSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_rawContent), (*(*ZSTD_parameters)(unsafe.Pointer(bp + 7932))).FcParams, ZSTD_defaultCMem)
	esr.Fzc = ZSTD_createCCtx(tls)
	esr.FworkPlace = libc.Xmalloc(tls, uint64(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)))
	if !(esr.Fdict != 0) || !(esr.Fzc != 0) || !(esr.FworkPlace != 0) {
		eSize = uint64(-int32(ZSTD_error_memory_allocation))
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10586, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	/* collect stats on all samples */
	u = uint32(0)
	for {
		if !(u < nbFiles) {
			break
		}
		ZDICT_countEStats(tls, esr, bp+7932, bp, bp+3080, bp+3268, bp+3588, bp+3804, srcBuffer+uintptr(pos), *(*size_t)(unsafe.Pointer(fileSizes + uintptr(u)*8)), notificationLevel)
		pos = pos + *(*size_t)(unsafe.Pointer(fileSizes + uintptr(u)*8))
		goto _7
	_7:
		;
		u = u + 1
	}
	if notificationLevel >= uint32(4) {
		/* writeStats */
		if notificationLevel >= uint32(4) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10606, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		u = uint32(0)
		for {
			if !(u <= offcodeMax) {
				break
			}
			if notificationLevel >= uint32(4) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10634, libc.VaList(bp+12848, u, (*(*[31]uint32)(unsafe.Pointer(bp + 3080)))[u]))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			goto _8
		_8:
			;
			u = u + 1
		}
	}
	/* analyze, build stats, starting with literals */
	maxNbBits = HUF_buildCTable_wksp(tls, bp+1024, bp, uint32(255), huffLog, bp+7972, uint64(4864))
	if ERR_isError(tls, maxNbBits) != 0 {
		eSize = maxNbBits
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10645, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	if maxNbBits == uint64(8) { /* not compressible : will fail on HUF_writeCTable() */
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10670, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		ZDICT_flatLit(tls, bp) /* replace distribution by a fake "mostly flat but still compressible" distribution, that HUF_writeCTable() can encode */
		maxNbBits = HUF_buildCTable_wksp(tls, bp+1024, bp, uint32(255), huffLog, bp+7972, uint64(4864))
		if v9 = !!(maxNbBits == libc.Uint64FromInt32(9)); !v9 {
			libc.X_assert(tls, __ccgo_ts+10770, __ccgo_ts+9627, uint32(51858))
		}
		_ = v9 || libc.Bool(libc.Int32FromInt32(0) != 0)
	}
	huffLog = uint32(maxNbBits)
	/* looking for most common first offsets */
	offset = uint32(1)
	for {
		if !(offset < uint32(MAXREPOFFSET)) {
			break
		}
		ZDICT_insertSortCount(tls, bp+7900, offset, (*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[offset])
		goto _10
	_10:
		;
		offset = offset + 1
	}
	/* note : the result of this phase should be used to better appreciate the impact on statistics */
	total = uint32(0)
	u = uint32(0)
	for {
		if !(u <= offcodeMax) {
			break
		}
		total = total + (*(*[31]uint32)(unsafe.Pointer(bp + 3080)))[u]
		goto _11
	_11:
		;
		u = u + 1
	}
	errorCode = FSE_normalizeCount(tls, bp+3204, Offlog, bp+3080, uint64(total), offcodeMax, uint32(1))
	if ERR_isError(tls, errorCode) != 0 {
		eSize = errorCode
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10783, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	Offlog = uint32(errorCode)
	total = uint32(0)
	u = uint32(0)
	for {
		if !(u <= uint32(MaxML)) {
			break
		}
		total = total + (*(*[53]uint32)(unsafe.Pointer(bp + 3268)))[u]
		goto _12
	_12:
		;
		u = u + 1
	}
	errorCode = FSE_normalizeCount(tls, bp+3480, mlLog, bp+3268, uint64(total), uint32(MaxML), uint32(1))
	if ERR_isError(tls, errorCode) != 0 {
		eSize = errorCode
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10828, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	mlLog = uint32(errorCode)
	total = uint32(0)
	u = uint32(0)
	for {
		if !(u <= uint32(MaxLL)) {
			break
		}
		total = total + (*(*[36]uint32)(unsafe.Pointer(bp + 3588)))[u]
		goto _13
	_13:
		;
		u = u + 1
	}
	errorCode = FSE_normalizeCount(tls, bp+3732, llLog, bp+3588, uint64(total), uint32(MaxLL), uint32(1))
	if ERR_isError(tls, errorCode) != 0 {
		eSize = errorCode
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10877, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	llLog = uint32(errorCode)
	/* write result to buffer */
	hhSize = HUF_writeCTable_wksp(tls, dstPtr, maxDstSize, bp+1024, uint32(255), huffLog, bp+7972, uint64(4864))
	if ERR_isError(tls, hhSize) != 0 {
		eSize = hhSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10924, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(hhSize)
	maxDstSize = maxDstSize - hhSize
	eSize = eSize + hhSize
	ohSize = FSE_writeNCount(tls, dstPtr, maxDstSize, bp+3204, uint32(OFFCODE_MAX), Offlog)
	if ERR_isError(tls, ohSize) != 0 {
		eSize = ohSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10948, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(ohSize)
	maxDstSize = maxDstSize - ohSize
	eSize = eSize + ohSize
	mhSize = FSE_writeNCount(tls, dstPtr, maxDstSize, bp+3480, uint32(MaxML), mlLog)
	if ERR_isError(tls, mhSize) != 0 {
		eSize = mhSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10991, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(mhSize)
	maxDstSize = maxDstSize - mhSize
	eSize = eSize + mhSize
	lhSize = FSE_writeNCount(tls, dstPtr, maxDstSize, bp+3732, uint32(MaxLL), llLog)
	if ERR_isError(tls, lhSize) != 0 {
		eSize = lhSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11038, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(lhSize)
	maxDstSize = maxDstSize - lhSize
	eSize = eSize + lhSize
	if maxDstSize < uint64(12) {
		eSize = uint64(-int32(ZSTD_error_dstSize_tooSmall))
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11083, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		goto _cleanup
	}
	/* at this stage, we don't use the result of "most common first offset",
	 * as the impact of statistics is not properly evaluated */
	MEM_writeLE32(tls, dstPtr+uintptr(0), repStartValue[0])
	MEM_writeLE32(tls, dstPtr+uintptr(4), repStartValue[int32(1)])
	MEM_writeLE32(tls, dstPtr+uintptr(8), repStartValue[int32(2)])
	eSize = eSize + uint64(12)
	goto _cleanup
_cleanup:
	;
	ZSTD_freeCDict(tls, esr.Fdict)
	ZSTD_freeCCtx(tls, esr.Fzc)
	libc.Xfree(tls, esr.FworkPlace)
	return eSize
}

// C documentation
//
//	/**
//	 * @returns the maximum repcode value
//	 */
func ZDICT_maxRep(tls *libc.TLS, reps uintptr) (r1 U32) {
	var maxRep U32
	var r int32
	var v2 uint32
	_, _, _ = maxRep, r, v2
	maxRep = *(*U32)(unsafe.Pointer(reps))
	r = int32(1)
	for {
		if !(r < int32(ZSTD_REP_NUM)) {
			break
		}
		if maxRep > *(*U32)(unsafe.Pointer(reps + uintptr(r)*4)) {
			v2 = maxRep
		} else {
			v2 = *(*U32)(unsafe.Pointer(reps + uintptr(r)*4))
		}
		maxRep = v2
		goto _1
	_1:
		;
		r = r + 1
	}
	return maxRep
}

func ZDICT_finalizeDictionary(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, customDictContent uintptr, dictContentSize size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_params_t) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var compliantID, dictID, notificationLevel U32
	var compressionLevel, v1 int32
	var dictSize, eSize, hSize, minContentSize, paddingSize size_t
	var outDictContent, outDictHeader, outDictPadding uintptr
	var randomID U64
	var v2 uint32
	var v3 bool
	var _ /* header at bp+0 */ [256]BYTE
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = compliantID, compressionLevel, dictID, dictSize, eSize, hSize, minContentSize, notificationLevel, outDictContent, outDictHeader, outDictPadding, paddingSize, randomID, v1, v2, v3
	if params.FcompressionLevel == 0 {
		v1 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v1 = params.FcompressionLevel
	}
	compressionLevel = v1
	notificationLevel = params.FnotificationLevel
	/* The final dictionary content must be at least as large as the largest repcode */
	minContentSize = uint64(ZDICT_maxRep(tls, uintptr(unsafe.Pointer(&repStartValue))))
	/* check conditions */
	if dictBufferCapacity < dictContentSize {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* dictionary header */
	MEM_writeLE32(tls, bp, uint32(ZSTD_MAGIC_DICTIONARY))
	randomID = XXH_INLINE_XXH64(tls, customDictContent, dictContentSize, uint64(0))
	compliantID = uint32(randomID%uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)-libc.Uint32FromInt32(32768)) + uint64(32768))
	if params.FdictID != 0 {
		v2 = params.FdictID
	} else {
		v2 = compliantID
	}
	dictID = v2
	MEM_writeLE32(tls, bp+uintptr(4), dictID)
	hSize = uint64(8)
	/* entropy tables */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10341, libc.VaList(bp+264, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	} /* clean display line */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11122, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	eSize = ZDICT_analyzeEntropy(tls, bp+uintptr(hSize), uint64(HBUFFSIZE)-hSize, compressionLevel, samplesBuffer, samplesSizes, nbSamples, customDictContent, dictContentSize, notificationLevel)
	if ZDICT_isError(tls, eSize) != 0 {
		return eSize
	}
	hSize = hSize + eSize
	/* Shrink the content size if it doesn't fit in the buffer */
	if hSize+dictContentSize > dictBufferCapacity {
		dictContentSize = dictBufferCapacity - hSize
	}
	/* Pad the dictionary content with zeros if it is too small */
	if dictContentSize < minContentSize {
		if hSize+minContentSize > dictBufferCapacity {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+11139, 0)
			}
			return uint64(-int32(ZSTD_error_dstSize_tooSmall))
		}
		paddingSize = minContentSize - dictContentSize
	} else {
		paddingSize = uint64(0)
	}
	dictSize = hSize + paddingSize + dictContentSize
	/* The dictionary consists of the header, optional padding, and the content.
	 * The padding comes before the content because the "best" position in the
	 * dictionary is the last byte.
	 */
	outDictHeader = dictBuffer
	outDictPadding = outDictHeader + uintptr(hSize)
	outDictContent = outDictPadding + uintptr(paddingSize)
	if v3 = !!(dictSize <= dictBufferCapacity); !v3 {
		libc.X_assert(tls, __ccgo_ts+11187, __ccgo_ts+9627, uint32(52046))
	}
	_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
	if v3 = !!(outDictContent+uintptr(dictContentSize) == dictBuffer+uintptr(dictSize)); !v3 {
		libc.X_assert(tls, __ccgo_ts+11218, __ccgo_ts+9627, uint32(52047))
	}
	_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
	/* First copy the customDictContent into its final location.
	 * `customDictContent` and `dictBuffer` may overlap, so we must
	 * do this before any other writes into the output buffer.
	 * Then copy the header & padding into the output buffer.
	 */
	libc.Xmemmove(tls, outDictContent, customDictContent, dictContentSize)
	libc.Xmemcpy(tls, outDictHeader, bp, hSize)
	libc.Xmemset(tls, outDictPadding, 0, paddingSize)
	return dictSize
	return r
}

func ZDICT_addEntropyTablesFromBuffer_advanced(tls *libc.TLS, dictBuffer uintptr, dictContentSize size_t, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_params_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var compliantID, dictID, notificationLevel U32
	var compressionLevel, v1 int32
	var eSize, hSize size_t
	var randomID U64
	var v2 uint32
	var v3 uint64
	_, _, _, _, _, _, _, _, _, _ = compliantID, compressionLevel, dictID, eSize, hSize, notificationLevel, randomID, v1, v2, v3
	if params.FcompressionLevel == 0 {
		v1 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v1 = params.FcompressionLevel
	}
	compressionLevel = v1
	notificationLevel = params.FnotificationLevel
	hSize = uint64(8)
	/* calculate entropy tables */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+10341, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	} /* clean display line */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11122, 0)
		libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
	}
	eSize = ZDICT_analyzeEntropy(tls, dictBuffer+uintptr(hSize), dictBufferCapacity-hSize, compressionLevel, samplesBuffer, samplesSizes, nbSamples, dictBuffer+uintptr(dictBufferCapacity)-uintptr(dictContentSize), dictContentSize, notificationLevel)
	if ZDICT_isError(tls, eSize) != 0 {
		return eSize
	}
	hSize = hSize + eSize
	/* add dictionary header (after entropy tables) */
	MEM_writeLE32(tls, dictBuffer, uint32(ZSTD_MAGIC_DICTIONARY))
	randomID = XXH_INLINE_XXH64(tls, dictBuffer+uintptr(dictBufferCapacity)-uintptr(dictContentSize), dictContentSize, uint64(0))
	compliantID = uint32(randomID%uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)-libc.Uint32FromInt32(32768)) + uint64(32768))
	if params.FdictID != 0 {
		v2 = params.FdictID
	} else {
		v2 = compliantID
	}
	dictID = v2
	MEM_writeLE32(tls, dictBuffer+uintptr(4), dictID)
	if hSize+dictContentSize < dictBufferCapacity {
		libc.Xmemmove(tls, dictBuffer+uintptr(hSize), dictBuffer+uintptr(dictBufferCapacity)-uintptr(dictContentSize), dictContentSize)
	}
	if dictBufferCapacity < hSize+dictContentSize {
		v3 = dictBufferCapacity
	} else {
		v3 = hSize + dictContentSize
	}
	return v3
}

// C documentation
//
//	/*! ZDICT_trainFromBuffer_unsafe_legacy() :
//	*   Warning : `samplesBuffer` must be followed by noisy guard band !!!
//	*   @return : size of dictionary, or an error code which can be tested with ZDICT_isError()
//	*/
func ZDICT_trainFromBuffer_unsafe_legacy(tls *libc.TLS, dictBuffer uintptr, maxDictSize size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_legacy_params_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var currentSize, dictListSize, l, max, n, notificationLevel, printedLength, u1 U32
	var dictContentSize, dictContentSize1, length, minRep, nb, pos, proposedSelectivity, selectivity, u, v1, v2, v3, v4, v5 uint32
	var dictList, ptr uintptr
	var dictSize, samplesBuffSize, targetDictSize size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = currentSize, dictContentSize, dictContentSize1, dictList, dictListSize, dictSize, l, length, max, minRep, n, nb, notificationLevel, pos, printedLength, proposedSelectivity, ptr, samplesBuffSize, selectivity, targetDictSize, u, u1, v1, v2, v3, v4, v5
	if uint32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT)) > nbSamples {
		v2 = uint32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT))
	} else {
		v2 = nbSamples
	}
	if v2 > uint32(maxDictSize/libc.Uint64FromInt32(16)) {
		if uint32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT)) > nbSamples {
			v3 = uint32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT))
		} else {
			v3 = nbSamples
		}
		v1 = v3
	} else {
		v1 = uint32(maxDictSize / libc.Uint64FromInt32(16))
	}
	dictListSize = v1
	dictList = libc.Xmalloc(tls, uint64(dictListSize)*uint64(12))
	if params.FselectivityLevel == uint32(0) {
		v4 = g_selectivity_default
	} else {
		v4 = params.FselectivityLevel
	}
	selectivity = v4
	if selectivity > uint32(30) {
		v5 = uint32(MINRATIO)
	} else {
		v5 = nbSamples >> selectivity
	}
	minRep = v5
	targetDictSize = maxDictSize
	samplesBuffSize = ZDICT_totalSampleSize(tls, samplesSizes, nbSamples)
	dictSize = uint64(0)
	notificationLevel = params.FzParams.FnotificationLevel
	/* checks */
	if !(dictList != 0) {
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	if maxDictSize < uint64(ZDICT_DICTSIZE_MIN) {
		libc.Xfree(tls, dictList)
		return uint64(-int32(ZSTD_error_dstSize_tooSmall))
	} /* requested dictionary size is too small */
	if samplesBuffSize < uint64(libc.Int32FromInt32(ZDICT_CONTENTSIZE_MIN)*libc.Int32FromInt32(MINRATIO)) {
		libc.Xfree(tls, dictList)
		return uint64(-int32(ZSTD_error_dictionaryCreation_failed))
	} /* not enough source to create dictionary */
	/* init */
	ZDICT_initDictItem(tls, dictList)
	/* build dictionary */
	ZDICT_trainBuffer_legacy(tls, dictList, dictListSize, samplesBuffer, samplesBuffSize, samplesSizes, nbSamples, minRep, notificationLevel)
	/* display best matches */
	if params.FzParams.FnotificationLevel >= uint32(3) {
		if uint32(libc.Int32FromInt32(25)) < (*(*dictItem)(unsafe.Pointer(dictList))).Fpos {
			v1 = uint32(libc.Int32FromInt32(25))
		} else {
			v1 = (*(*dictItem)(unsafe.Pointer(dictList))).Fpos
		}
		nb = v1
		dictContentSize = ZDICT_dictSize(tls, dictList)
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11283, libc.VaList(bp+8, (*(*dictItem)(unsafe.Pointer(dictList))).Fpos-uint32(1), dictContentSize))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11323, libc.VaList(bp+8, nb-uint32(1)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		u = uint32(1)
		for {
			if !(u < nb) {
				break
			}
			pos = (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Fpos
			length = (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Flength
			if uint32(libc.Int32FromInt32(40)) < length {
				v1 = uint32(libc.Int32FromInt32(40))
			} else {
				v1 = length
			}
			printedLength = v1
			if uint64(pos) > samplesBuffSize || uint64(pos+length) > samplesBuffSize {
				libc.Xfree(tls, dictList)
				return uint64(-int32(ZSTD_error_GENERIC)) /* should never happen */
			}
			if notificationLevel >= uint32(3) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11347, libc.VaList(bp+8, u, length, pos, (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Fsavings))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			ZDICT_printHex(tls, samplesBuffer+uintptr(pos), uint64(printedLength))
			if notificationLevel >= uint32(3) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11393, 0)
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			goto _7
		_7:
			;
			u = u + 1
		}
	}
	/* create dictionary */
	dictContentSize1 = ZDICT_dictSize(tls, dictList)
	if dictContentSize1 < uint32(ZDICT_CONTENTSIZE_MIN) {
		libc.Xfree(tls, dictList)
		return uint64(-int32(ZSTD_error_dictionaryCreation_failed))
	} /* dictionary content too small */
	if uint64(dictContentSize1) < targetDictSize/uint64(4) {
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11397, libc.VaList(bp+8, dictContentSize1, uint32(maxDictSize)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		if samplesBuffSize < uint64(10)*targetDictSize {
			if notificationLevel >= uint32(2) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11476, libc.VaList(bp+8, uint32(samplesBuffSize>>libc.Int32FromInt32(20))))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
		}
		if minRep > uint32(MINRATIO) {
			if notificationLevel >= uint32(2) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11543, libc.VaList(bp+8, selectivity+uint32(1)))
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
			if notificationLevel >= uint32(2) {
				libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11616, 0)
				libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
			}
		}
	}
	if uint64(dictContentSize1) > targetDictSize*uint64(3) && nbSamples > uint32(libc.Int32FromInt32(2)*libc.Int32FromInt32(MINRATIO)) && selectivity > uint32(1) {
		proposedSelectivity = selectivity - uint32(1)
		for nbSamples>>proposedSelectivity <= uint32(MINRATIO) {
			proposedSelectivity = proposedSelectivity - 1
		}
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11707, libc.VaList(bp+8, dictContentSize1, uint32(maxDictSize)))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11787, libc.VaList(bp+8, proposedSelectivity))
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.X__acrt_iob_func(tls, uint32(2)), __ccgo_ts+11865, 0)
			libc.Xfflush(tls, libc.X__acrt_iob_func(tls, uint32(2)))
		}
	}
	/* limit dictionary size */
	max = (*dictItem)(unsafe.Pointer(dictList)).Fpos /* convention : nb of useful elts within dictList */
	currentSize = uint32(0)
	n = uint32(1)
	for {
		if !(n < max) {
			break
		}
		currentSize = currentSize + (*(*dictItem)(unsafe.Pointer(dictList + uintptr(n)*12))).Flength
		if uint64(currentSize) > targetDictSize {
			currentSize = currentSize - (*(*dictItem)(unsafe.Pointer(dictList + uintptr(n)*12))).Flength
			break
		}
		goto _9
	_9:
		;
		n = n + 1
	}
	(*dictItem)(unsafe.Pointer(dictList)).Fpos = n
	dictContentSize1 = currentSize
	/* build dict content */
	ptr = dictBuffer + uintptr(maxDictSize)
	u1 = uint32(1)
	for {
		if !(u1 < (*dictItem)(unsafe.Pointer(dictList)).Fpos) {
			break
		}
		l = (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u1)*12))).Flength
		ptr = ptr - uintptr(l)
		if ptr < dictBuffer {
			libc.Xfree(tls, dictList)
			return uint64(-int32(ZSTD_error_GENERIC))
		} /* should not happen */
		libc.Xmemcpy(tls, ptr, samplesBuffer+uintptr((*(*dictItem)(unsafe.Pointer(dictList + uintptr(u1)*12))).Fpos), uint64(l))
		goto _10
	_10:
		;
		u1 = u1 + 1
	}
	dictSize = ZDICT_addEntropyTablesFromBuffer_advanced(tls, dictBuffer, uint64(dictContentSize1), maxDictSize, samplesBuffer, samplesSizes, nbSamples, params.FzParams)
	/* clean up */
	libc.Xfree(tls, dictList)
	return dictSize
}

// C documentation
//
//	/* ZDICT_trainFromBuffer_legacy() :
//	 * issue : samplesBuffer need to be followed by a noisy guard band.
//	 * work around : duplicate the buffer, and add the noise */
func ZDICT_trainFromBuffer_legacy(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_legacy_params_t) (r size_t) {
	var newBuff uintptr
	var result, sBuffSize size_t
	_, _, _ = newBuff, result, sBuffSize
	sBuffSize = ZDICT_totalSampleSize(tls, samplesSizes, nbSamples)
	if sBuffSize < uint64(libc.Int32FromInt32(ZDICT_CONTENTSIZE_MIN)*libc.Int32FromInt32(MINRATIO)) {
		return uint64(0)
	} /* not enough content => no dictionary */
	newBuff = libc.Xmalloc(tls, sBuffSize+uint64(NOISELENGTH))
	if !(newBuff != 0) {
		return uint64(-int32(ZSTD_error_memory_allocation))
	}
	libc.Xmemcpy(tls, newBuff, samplesBuffer, sBuffSize)
	ZDICT_fillNoise(tls, newBuff+uintptr(sBuffSize), uint64(NOISELENGTH)) /* guard band, for end of buffer condition */
	result = ZDICT_trainFromBuffer_unsafe_legacy(tls, dictBuffer, dictBufferCapacity, newBuff, samplesSizes, nbSamples, params)
	libc.Xfree(tls, newBuff)
	return result
}

func ZDICT_trainFromBuffer(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32) (r size_t) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var _ /* params at bp+0 */ ZDICT_fastCover_params_t
	libc.Xmemset(tls, bp, 0, uint64(56))
	(*(*ZDICT_fastCover_params_t)(unsafe.Pointer(bp))).Fd = uint32(8)
	(*(*ZDICT_fastCover_params_t)(unsafe.Pointer(bp))).Fsteps = uint32(4)
	/* Use default level since no compression level information is available */
	(*(*ZDICT_fastCover_params_t)(unsafe.Pointer(bp))).FzParams.FcompressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	return ZDICT_optimizeTrainFromBuffer_fastCover(tls, dictBuffer, dictBufferCapacity, samplesBuffer, samplesSizes, nbSamples, bp)
}

func ZDICT_addEntropyTablesFromBuffer(tls *libc.TLS, dictBuffer uintptr, dictContentSize size_t, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* params at bp+0 */ ZDICT_params_t
	libc.Xmemset(tls, bp, 0, uint64(12))
	return ZDICT_addEntropyTablesFromBuffer_advanced(tls, dictBuffer, dictContentSize, dictBufferCapacity, samplesBuffer, samplesSizes, nbSamples, *(*ZDICT_params_t)(unsafe.Pointer(bp)))
}

func __ccgo_fp(f interface{}) uintptr {
	type iface [2]uintptr
	return (*iface)(unsafe.Pointer(&f))[1]
}

/**
 * add here more wrappers as required
 */

/**** ended inlining threading.h ****/

// C documentation
//
//	/* create fake symbol to avoid empty translation unit warning */
var g_ZSTD_threading_useless_symbol int32

/*
 * Provides 64-bit math support.
 * Need:
 * U64 ZSTD_div64(U64 dividend, U32 divisor)
 */

/* Need:
 * assert()
 */

/* Need:
 * ZSTD_DEBUG_PRINT()
 */

/* Only requested when <stdint.h> is known to be present.
 * Need:
 * intptr_t
 */
/**** ended inlining common/zstd_deps.h ****/

/**** start inlining common/debug.c ****/
/* ******************************************************************
 * debug
 * Part of FSE library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * You can contact the author at :
 * - Source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/*
 * This module only hosts one global variable
 * which can be used to dynamically influence the verbosity of traces,
 * such as DEBUGLOG and RAWLOG
 */

/**** start inlining debug.h ****/
/* ******************************************************************
 * debug
 * Part of FSE library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * You can contact the author at :
 * - Source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/*
 * The purpose of this header is to enable debug functions.
 * They regroup assert(), DEBUGLOG() and RAWLOG() for run-time,
 * and DEBUG_STATIC_ASSERT() for compile-time.
 *
 * By default, DEBUGLEVEL==0, which means run-time debug is disabled.
 *
 * Level 1 enables assert() only.
 * Starting level 2, traces can be generated and pushed to stderr.
 * The higher the level, the more verbose the traces.
 *
 * It's possible to dynamically adjust level using variable g_debug_level,
 * which is only declared if DEBUGLEVEL>=2,
 * and is a global variable, not multi-thread protected (use with care)
 */

/* static assert is triggered at compile time, leaving no runtime artefact.
 * static assert only works with compile-time constants.
 * Also, this variant can only be used inside a function. */

/* DEBUGLEVEL is expected to be defined externally,
 * typically through compiler command line.
 * Value must be a number. */

/* recommended values for DEBUGLEVEL :
 * 0 : release mode, no debug, all run-time checks disabled
 * 1 : enables assert() only, no display
 * 2 : reserved, for currently active debug path
 * 3 : events once per object lifetime (CCtx, CDict, etc.)
 * 4 : events once per frame
 * 5 : events once per block
 * 6 : events once per sequence (verbose)
 * 7+: events at every position (*very* verbose)
 *
 * It's generally inconvenient to output traces > 5.
 * In which case, it's possible to selectively trigger high verbosity levels
 * by modifying g_debug_level.
 */

/**** ended inlining debug.h ****/

// C documentation
//
//	/* We only use this when DEBUGLEVEL>=2, but we get -Werror=pedantic errors if a
//	 * translation unit is empty. So remove this from Linux kernel builds, but
//	 * otherwise just leave it in.
//	 */
var g_debuglevel int32

var __ccgo_ts = (*reflect.StringHeader)(unsafe.Pointer(&__ccgo_ts1)).Data

var __ccgo_ts1 = "Unspecified error code\x00No error detected\x00Error (generic)\x00Unknown frame descriptor\x00Version not supported\x00Unsupported frame parameter\x00Frame requires too much memory for decoding\x00Data corruption detected\x00Restored data doesn't match checksum\x00Header of Literals' block doesn't respect format specification\x00Unsupported parameter\x00Unsupported combination of parameters\x00Parameter is out of bound\x00Context should be init first\x00Allocation error : not enough memory\x00workSpace buffer is not large enough\x00Operation not authorized at current processing stage\x00tableLog requires too much memory : unsupported\x00Unsupported max Symbol Value : too large\x00Specified maxSymbolValue is too small\x00This mode cannot generate an uncompressed block\x00pledged buffer stability condition is not respected\x00Dictionary is corrupted\x00Dictionary mismatch\x00Cannot create Dictionary from provided samples\x00Destination buffer is too small\x00Src size is incorrect\x00Operation on NULL destination buffer\x00Operation made no progress over multiple calls, due to output buffer being full\x00Operation made no progress over multiple calls, due to input being empty\x00Frame index is too large\x00An I/O error occurred when reading/seeking\x00Destination buffer is wrong\x00Source buffer is wrong\x00Block-level external sequence producer returned an error code\x00External sequences are not valid\x00\x001.5.7\x00table phase - alignment initial allocation failed!\x00NULL pointer!\x00dst buf too small for uncompressed block\x00 \x00not enough space for compression\x00not enough space\x00FSE_normalizeCount failed\x00FSE_writeNCount failed\x00FSE_buildCTable_wksp failed\x00impossible to reach\x00not enough space remaining\x00ZSTD_encodeSequences failed\x00ZSTD_compressSubBlock_literal failed\x00ZSTD_compressSubBlock_sequences failed\x00ZSTD_compressSubBlock failed\x00ZSTD_noCompressBlock failed\x00not compatible with static CCtx\x00can only set params in cctx init stage\x00MT not compatible with static alloc\x00unknown parameter\x00Param out of bounds\x00The context is in the wrong stage!\x00Can't override parameters with cdict attached (some must be inherited from the cdict).\x00Can't set pledgedSrcSize when not in init stage.\x00ZSTD_createCDict_advanced failed\x00Can't load a dictionary when cctx is not in init stage.\x00static CCtx can't allocate for an internal copy of dictionary\x00allocation failed for dictionary content\x00Can't ref a dict when ctx not in init stage.\x00Can't ref a pool when ctx not in init stage.\x00Can't ref a prefix when ctx not in init stage.\x00Reset parameters is only possible during init stage.\x00Estimate CCtx size is supported for single-threaded compression only.\x00failed a workspace allocation in ZSTD_reset_matchState\x00cctx size estimate failed!\x00static cctx : no resize\x00couldn't allocate prevCBlock\x00couldn't allocate nextCBlock\x00couldn't allocate tmpWorkspace\x00Can't copy a ctx that's not in init stage.\x00ZSTD_compressLiterals failed\x00Can't fit seq hdr in output buf!\x00ZSTD_buildSequencesStatistics failed!\x00ZSTD_entropyCompressSeqStore_internal failed\x00External sequence producer returned error code %lu\x00Got zero sequences from external sequence producer for a non-empty src buffer!\x00nbExternalSeqs == outSeqsCapacity but lastSeq is not a block delimiter!\x00Long-distance matching with external sequence producer enabled is not currently supported.\x00External sequences imply too large a block!\x00Failed to copy external sequences to seqStore!\x00Not enough space to copy sequences\x00targetCBlockSize != 0\x00nbWorkers != 0\x00ZSTD_compress2 failed\x00HIST_count_wksp failed\x00HUF_buildCTable_wksp\x00ZSTD_buildBlockEntropyStats_literals failed\x00ZSTD_buildBlockEntropyStats_sequences failed\x00Block header doesn't fit\x00ZSTD_entropyCompressSeqStore failed!\x00copyBlockSequences failed\x00Nocompress block failed\x00RLE compress block failed\x00Compressing single block from splitBlock_internal() failed!\x00Compressing chunk failed!\x00ZSTD_buildSeqStore failed\x00Uncompressible block\x00Splitting blocks failed!\x00ZSTD_compressSuperBlock failed\x00ZSTD_compressBlock_targetCBlockSize_body failed\x00not enough space to store compressed block\x00ZSTD_compressBlock_targetCBlockSize failed\x00ZSTD_compressBlock_splitBlock failed\x00ZSTD_compressBlock_internal failed\x00dst buf is too small to fit worst-case frame header size.\x00Not enough room for skippable frame\x00Src size too large for skippable frame\x00Skippable frame magic number variant not supported\x00dst buf is too small to write frame trailer empty block.\x00missing init (ZSTD_compressBegin)\x00ZSTD_writeFrameHeader failed\x00ZSTD_compress_frameChunk failed\x00%s\x00error : pledgedSrcSize = %u, while realSrcSize >= %u\x00input is larger than a block\x00ZSTD_loadCEntropy failed\x00ZSTD_compress_insertDictionary failed\x00init missing\x00no room for epilogue\x00no room for checksum\x00ZSTD_compressContinue_internal failed\x00ZSTD_writeEpilogue failed\x00error : pledgedSrcSize = %u, while realSrcSize = %u\x00call ZSTD_initCStream() first!\x00ZSTD_compressEnd failed\x00ZSTD_compressContinue failed\x00ZSTD_c_stableInBuffer enabled but input differs!\x00ZSTD_c_stableOutBuffer enabled but output size differs!\x00External sequence producer isn't supported with nbWorkers >= 1\x00invalid output buffer\x00invalid input buffer\x00invalid endDirective\x00stableInBuffer condition not respected: wrong src pointer\x00stableInBuffer condition not respected: externally modified pos\x00compressStream2 initialization failed\x00invalid buffers\x00ZSTDMT_compressStream_generic failed\x00ZSTD_compressStream2_simpleArgs failed\x00Offset too large!\x00Matchlength too small for the minMatch\x00Sequence validation failed\x00Not enough memory allocated. Try adjusting ZSTD_c_minMatch.\x00Block delimiter not found.\x00Blocksize doesn't agree with block delimiter!\x00delimiter format error : both matchlength and offset must be == 0\x00Reached end of sequences without finding a block delimiter\x00Error while determining block size with explicit delimiters\x00sequences incorrectly define a too large block\x00sequences define a frame longer than source\x00No room for empty frame block header\x00Error while trying to determine block size\x00Bad sequence copy\x00not enough dstCapacity to write a new compressed block\x00Compressing sequences of block failed\x00ZSTD_rleCompressBlock failed\x00CCtx initialization failed\x00Compressing blocks failed!\x00Requires at least 1 end-of-block\x00Error while trying to determine nb of sequences for a block\x00discrepancy: Sequences require more literals than present in buffer\x00Bad sequence conversion\x00ZSTD_compressSequencesAndLiterals cannot generate an uncompressed block\x00literals must be entirely and exactly consumed\x00Sequences must represent a total of exactly srcSize=%zu\x00literals buffer is not large enough: must be at least 8 bytes larger than litSize (risk of read out-of-bound)\x00This mode is only compatible with explicit delimiters\x00This mode is not compatible with Sequence validation\x00this mode is not compatible with frame checksum\x00ZSTD_compressStream2(,,ZSTD_e_end) failed\x00Failed to init fast loop args\x00corruption\x00Failed to init asm args\x00Hash set is full!\x00Expanded hashset allocation failed!\x00not compatible with static DCtx\x00invalid parameter : src==NULL, but srcSize>0\x00first bytes don't correspond to any supported magic number\x00reserved bits, must be zero\x00headerSize too small\x00invalid block type\x00Block decompression failure\x00invalid skippable frame\x00At least one frame successfully completed, but following bytes are garbage: it's more likely to be a srcSize error, specifying more input bytes than size of frame(s). Note: one could be unlucky, it might be a corruption error instead, happening right at the place where we expect zstd magic bytes. But this is _much_ less likely than a srcSize field error.\x00input not entirely consumed\x00not allowed\x00Block Size Exceeds Maximum\x00ZSTD_copyRawBlock failed\x00Decompressed Block Size Exceeds Maximum\x00dict is too small\x00Failed to allocate memory for hash set!\x00Static dctx does not support multiple DDicts!\x00ZSTD_d_stableOutBuffer enabled but output differs!\x00forbidden. in: pos: %u   vs size: %u\x00forbidden. out: pos: %u   vs size: %u\x00First few bytes detected incorrect\x00ZSTD_obm_stable passed but ZSTD_outBuffer is too small\x00should never happen\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need up to 5 for case 3\x00NULL not handled\x00Not enough literals (%zu) for the 4-streams mode (min %u)\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need lhSize = 3\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need lhSize+1 = 3\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need lhSize+1 = 4\x00impossible\x00extraneous data present in the Sequences section\x00ZSTD_buildSeqTable failed\x00last match must fit within dstBuffer\x00try to read beyond literal buffer\x00output should not catch up to and overwrite literal buffer\x00remaining lit must fit within dstBuffer\x00invalid dst\x00Total samples size is too large (%u MB), maximum size is %u MB\n\x00Total number of training samples is %u and is invalid.\x00Total number of testing samples is %u and is invalid.\x00Training on %u samples of total size %u\n\x00Testing on %u samples of total size %u\n\x00Failed to allocate scratch buffers\n\x00Constructing partial suffix array\n\x00Computing frequencies\n\x00WARNING: The maximum dictionary size %u is too large compared to the source size %u! size(source)/size(dictionary) = %f, but it should be >= 10! This may lead to a subpar dictionary! We recommend training on sources at least 10x, and preferably 100x the size of the dictionary! \n\x00Breaking content into %u epochs of size %u\n\x00\r%u%%       \x00\r%79s\r\x00Cover parameters incorrect\n\x00Cover must have at least one input file\n\x00dictBufferCapacity must be at least %u\n\x00Failed to allocate dmer map: out of memory\n\x00Building dictionary\n\x00Constructed dictionary of size %u\n\x00Failed to allocate buffers: out of memory\n\x00Failed to select dictionary\n\x00Incorrect parameters\n\x00Trying %u different sets of parameters\n\x00d=%u\n\x00Failed to initialize context\n\x00k=%u\n\x00Failed to allocate parameters\n\x000 <= ssize\x00/tmp/zstd-build-1536185704/zstd.c\x00ssize < STACK_SIZE\x00T[s] == c1\x00((s + 1) < n) && (T[s] <= T[s + 1])\x00T[s - 1] <= T[s]\x00k < j\x00k != NULL\x00((s == 0) && (T[s] == c1)) || (s < 0)\x00T[s - 1] >= T[s]\x00i < k\x00s < 0\x00ctx->nbTrainSamples >= 5\x00ctx->nbTrainSamples <= ctx->nbSamples\x00Total number of training samples is %u and is invalid\n\x00Total number of testing samples is %u and is invalid.\n\x00Failed to allocate scratch buffers \n\x00nbSamples >= 5\x00Failed to allocate frequency table \n\x00FASTCOVER parameters incorrect\n\x00FASTCOVER must have at least one input file\n\x00Incorrect splitPoint\n\x00Incorrect accel\n\x00Incorrect k\n\x00%c\x00\n\x00found %3u matches of length >= %i at pos %7u  \x00Selected dict at position %u, of length %u : saves %u (ratio: %.2f)  \n\x00\r%70s\r\x00sample set too large : reduced to %u MB ...\n\x00sorting %u files of total size %u MB ...\n\x00finding patterns ... \n\x00minimum ratio : %u \n\x00\r%4.2f %% \r\x00warning : ZSTD_compressBegin_usingCDict failed \n\x00warning : could not compress sample size %u \n\x00Not enough memory \n\x00Offset Code Frequencies : \n\x00%2u :%7u \n\x00 HUF_buildCTable error \n\x00warning : pathological dataset : literals are not compressible : samples are noisy or too regular \n\x00maxNbBits==9\x00FSE_normalizeCount error with offcodeCount \n\x00FSE_normalizeCount error with matchLengthCount \n\x00FSE_normalizeCount error with litLengthCount \n\x00HUF_writeCTable error \n\x00FSE_writeNCount error with offcodeNCount \n\x00FSE_writeNCount error with matchLengthNCount \n\x00FSE_writeNCount error with litlengthNCount \n\x00not enough space to write RepOffsets \n\x00statistics ... \n\x00dictBufferCapacity too small to fit max repcode\x00dictSize <= dictBufferCapacity\x00outDictContent + dictContentSize == (BYTE*)dictBuffer + dictSize\x00\n %u segments found, of total size %u \n\x00list %u best segments \n\x00%3u:%3u bytes at pos %8u, savings %7u bytes |\x00| \n\x00!  warning : selected content significantly smaller than requested (%u < %u) \n\x00!  consider increasing the number of samples (total size : %u MB)\n\x00!  consider increasing selectivity to produce larger dictionary (-s%u) \n\x00!  note : larger dictionaries are not necessarily better, test its efficiency on samples \n\x00!  note : calculated dictionary significantly larger than requested (%u > %u) \n\x00!  consider increasing dictionary size, or produce denser dictionary (-s%u) \n\x00!  always test dictionary efficiency on real samples \n\x00"
