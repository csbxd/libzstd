// Code generated for linux/arm64 by 'ccgo --package-name libzstd -std=c17 /tmp/zstd-build-3522643984/zstd.c -o /home/bxd/GolandProjects/libzstd/zstd_linux_arm64.go', DO NOT EDIT.

//go:build linux && arm64

package libzstd

import (
	"reflect"
	"unsafe"

	"modernc.org/libc"
)

var _ reflect.Type
var _ unsafe.Pointer

const ADVANCED_SEQS = "STORED_SEQS"
const ALPHABET_SIZE = 256
const ARG_MAX = 131072
const BC_BASE_MAX = 99
const BC_DIM_MAX = 2048
const BC_SCALE_MAX = 99
const BC_STRING_MAX = 1000
const BIT0 = 1
const BIT1 = 2
const BIT4 = 16
const BIT5 = 32
const BIT6 = 64
const BIT7 = 128
const BITCOST_ACCURACY = 8
const BLOCKSIZE_MIN = 3500
const BUCKET_A_SIZE = "ALPHABET_SIZE"
const BUFSIZ = 1024
const BYTESCALE = 256
const CACHELINE_SIZE = 64
const CHARCLASS_NAME_MAX = 14
const CHAR_BIT = 8
const CHAR_MAX = 255
const CHAR_MIN = 0
const CLOCKS_PER_SEC = 1000000
const CLOCK_BOOTTIME = 7
const CLOCK_BOOTTIME_ALARM = 9
const CLOCK_MONOTONIC = 1
const CLOCK_MONOTONIC_COARSE = 6
const CLOCK_MONOTONIC_RAW = 4
const CLOCK_PROCESS_CPUTIME_ID = 2
const CLOCK_REALTIME = 0
const CLOCK_REALTIME_ALARM = 8
const CLOCK_REALTIME_COARSE = 5
const CLOCK_SGI_CYCLE = 10
const CLOCK_TAI = 11
const CLOCK_THREAD_CPUTIME_ID = 3
const CLONE_CHILD_CLEARTID = 0x00200000
const CLONE_CHILD_SETTID = 0x01000000
const CLONE_DETACHED = 0x00400000
const CLONE_FILES = 0x00000400
const CLONE_FS = 0x00000200
const CLONE_IO = 0x80000000
const CLONE_NEWCGROUP = 0x02000000
const CLONE_NEWIPC = 0x08000000
const CLONE_NEWNET = 0x40000000
const CLONE_NEWNS = 0x00020000
const CLONE_NEWPID = 0x20000000
const CLONE_NEWTIME = 0x00000080
const CLONE_NEWUSER = 0x10000000
const CLONE_NEWUTS = 0x04000000
const CLONE_PARENT = 0x00008000
const CLONE_PARENT_SETTID = 0x00100000
const CLONE_PIDFD = 0x00001000
const CLONE_PTRACE = 0x00002000
const CLONE_SETTLS = 0x00080000
const CLONE_SIGHAND = 0x00000800
const CLONE_SYSVSEM = 0x00040000
const CLONE_THREAD = 0x00010000
const CLONE_UNTRACED = 0x00800000
const CLONE_VFORK = 0x00004000
const CLONE_VM = 0x00000100
const COLL_WEIGHTS_MAX = 2
const COMPRESS_LITERALS_SIZE_MIN = 63
const COVER_DEFAULT_SPLITPOINT = 1
const CPU_SETSIZE = 1024
const CSIGNAL = 0x000000ff
const DDICT_HASHSET_MAX_LOAD_FACTOR_COUNT_MULT = 4
const DDICT_HASHSET_MAX_LOAD_FACTOR_SIZE_MULT = 3
const DDICT_HASHSET_RESIZE_FACTOR = 2
const DDICT_HASHSET_TABLE_BASE_SIZE = 64
const DEBUGLEVEL = 0
const DEFAULT_ACCEL = 1
const DEFAULT_F = 20
const DELAYTIMER_MAX = 0x7fffffff
const DICTLISTSIZE_DEFAULT = 10000
const DYNAMIC_BMI2 = 1
const DefaultMaxOff = 28
const EXIT_FAILURE = 1
const EXIT_SUCCESS = 0
const EXPR_NEST_MAX = 32
const FASTCOVER_DEFAULT_SPLITPOINT = 0.75
const FASTCOVER_MAX_ACCEL = 10
const FASTCOVER_MAX_F = 31
const FILENAME_MAX = 4096
const FILESIZEBITS = 64
const FOPEN_MAX = 1000
const FSE_DECODE_TYPE = "FSE_decode_t"
const FSE_DEFAULT_MEMORY_USAGE = 13
const FSE_FUNCTION_TYPE = "BYTE"
const FSE_MAX_MEMORY_USAGE = 14
const FSE_MAX_SYMBOL_VALUE = 255
const FSE_MIN_TABLELOG = 5
const FSE_NCOUNTBOUND = 512
const FSE_TABLELOG_ABSOLUTE_MAX = 15
const FSE_VERSION_MAJOR = 0
const FSE_VERSION_MINOR = 9
const FSE_VERSION_RELEASE = 0
const FSE_isError1 = "ERR_isError"
const HASHLENGTH = 2
const HASHLOG_MAX = 10
const HASH_READ_SIZE = 8
const HBUFFSIZE = 256
const HINT_INLINE = "FORCE_INLINE_TEMPLATE"
const HIST_WKSP_SIZE_U32 = 1024
const HOST_NAME_MAX = 255
const HUF_ASM_DECL = "HUF_EXTERN_C"
const HUF_CTABLEBOUND = 129
const HUF_DECODER_FAST_TABLELOG = 11
const HUF_ENABLE_FAST_DECODE = 1
const HUF_FAST_BMI2_ATTRS = "BMI2_TARGET_ATTRIBUTE"
const HUF_NEED_BMI2_FUNCTION = 1
const HUF_OPTIMAL_DEPTH_THRESHOLD = 8
const HUF_SYMBOLVALUE_MAX = 255
const HUF_TABLELOG_ABSOLUTEMAX = 12
const HUF_TABLELOG_DEFAULT = 11
const HUF_TABLELOG_MAX = 12
const HUF_WORKSPACE_MAX_ALIGNMENT = 8
const HUF_isError1 = "ERR_isError"
const INLINE = "__inline"
const INLINE_KEYWORD = "inline"
const INT16_MAX = 0x7fff
const INT32_MAX = 0x7fffffff
const INT64_MAX = 0x7fffffffffffffff
const INT8_MAX = 0x7f
const INTMAX_MAX = "INT64_MAX"
const INTMAX_MIN = "INT64_MIN"
const INTPTR_MAX = "INT64_MAX"
const INTPTR_MIN = "INT64_MIN"
const INT_FAST16_MAX = "INT32_MAX"
const INT_FAST16_MIN = "INT32_MIN"
const INT_FAST32_MAX = "INT32_MAX"
const INT_FAST32_MIN = "INT32_MIN"
const INT_FAST64_MAX = "INT64_MAX"
const INT_FAST64_MIN = "INT64_MIN"
const INT_FAST8_MAX = "INT8_MAX"
const INT_FAST8_MIN = "INT8_MIN"
const INT_LEAST16_MAX = "INT16_MAX"
const INT_LEAST16_MIN = "INT16_MIN"
const INT_LEAST32_MAX = "INT32_MAX"
const INT_LEAST32_MIN = "INT32_MIN"
const INT_LEAST64_MAX = "INT64_MAX"
const INT_LEAST64_MIN = "INT64_MIN"
const INT_LEAST8_MAX = "INT8_MAX"
const INT_LEAST8_MIN = "INT8_MIN"
const INT_MAX = 2147483647
const IOV_MAX = 1024
const KNUTH = 2654435769
const LDM_BATCH_SIZE = 64
const LDM_BUCKET_SIZE_LOG = 4
const LDM_HASH_RLOG = 7
const LDM_MIN_MATCH_LENGTH = 64
const LINE_MAX = 4096
const LLFSELog = 9
const LLIMIT = 64
const LLONG_MAX = 0x7fffffffffffffff
const LL_DEFAULTNORMLOG = 6
const LOGIN_NAME_MAX = 256
const LONGNBSEQ = 32512
const LONG_BIT = 64
const LONG_MAX = "__LONG_MAX"
const L_ctermid = 20
const L_cuserid = 20
const L_tmpnam = 20
const LitHufLog = 11
const Litbits = 8
const MAXREPOFFSET = 1024
const MAX_FSE_TABLELOG_FOR_HUFF_HEADER = 6
const MB_LEN_MAX = 4
const MEM_FORCE_MEMORY_ACCESS = 1
const MINMATCH = 3
const MINMATCHLENGTH = 7
const MINRATIO = 4
const MIN_LITERALS_FOR_4_STREAMS = 6
const MIN_SEQUENCES_BLOCK_SPLITTING = 300
const MIN_SEQUENCES_SIZE = 1
const MLFSELog = 9
const ML_DEFAULTNORMLOG = 6
const MQ_PRIO_MAX = 32768
const MaxLL = 35
const MaxLLBits = 16
const MaxML = 52
const MaxMLBits = 16
const MaxOff = 31
const NAME_MAX = 255
const NGROUPS_MAX = 32
const NL_ARGMAX = 9
const NL_LANGMAX = 32
const NL_MSGMAX = 32767
const NL_NMAX = 16
const NL_SETMAX = 255
const NL_TEXTMAX = 2048
const NOISELENGTH = 32
const NZERO = 20
const OFFCODE_MAX = 30
const OF_DEFAULTNORMLOG = 5
const OffFSELog = 8
const PATH_MAX = 4096
const PIPE_BUF = 4096
const PTHREAD_CANCEL_ASYNCHRONOUS = 1
const PTHREAD_CANCEL_DEFERRED = 0
const PTHREAD_CANCEL_DISABLE = 1
const PTHREAD_CANCEL_ENABLE = 0
const PTHREAD_CANCEL_MASKED = 2
const PTHREAD_CREATE_DETACHED = 1
const PTHREAD_CREATE_JOINABLE = 0
const PTHREAD_DESTRUCTOR_ITERATIONS = 4
const PTHREAD_EXPLICIT_SCHED = 1
const PTHREAD_INHERIT_SCHED = 0
const PTHREAD_KEYS_MAX = 128
const PTHREAD_MUTEX_DEFAULT = 0
const PTHREAD_MUTEX_ERRORCHECK = 2
const PTHREAD_MUTEX_NORMAL = 0
const PTHREAD_MUTEX_RECURSIVE = 1
const PTHREAD_MUTEX_ROBUST = 1
const PTHREAD_MUTEX_STALLED = 0
const PTHREAD_ONCE_INIT = 0
const PTHREAD_PRIO_INHERIT = 1
const PTHREAD_PRIO_NONE = 0
const PTHREAD_PRIO_PROTECT = 2
const PTHREAD_PROCESS_PRIVATE = 0
const PTHREAD_PROCESS_SHARED = 1
const PTHREAD_SCOPE_PROCESS = 1
const PTHREAD_SCOPE_SYSTEM = 0
const PTHREAD_STACK_MIN = 2048
const PTRDIFF_MAX = "INT64_MAX"
const PTRDIFF_MIN = "INT64_MIN"
const P_tmpdir = "/tmp"
const RAND_MAX = 0x7fffffff
const RANK_POSITION_MAX_COUNT_LOG = 32
const RANK_POSITION_TABLE_SIZE = 192
const RE_DUP_MAX = 255
const RSYNC_LENGTH = 32
const RSYNC_MIN_BLOCK_LOG = "ZSTD_BLOCKSIZELOG_MAX"
const SCHAR_MAX = 127
const SCHED_BATCH = 3
const SCHED_DEADLINE = 6
const SCHED_FIFO = 1
const SCHED_IDLE = 5
const SCHED_OTHER = 0
const SCHED_RESET_ON_FORK = 0x40000000
const SCHED_RR = 2
const SEGMENT_SIZE = 512
const SEM_NSEMS_MAX = 256
const SEM_VALUE_MAX = 0x7fffffff
const SHRT_MAX = 0x7fff
const SIG_ATOMIC_MAX = "INT32_MAX"
const SIG_ATOMIC_MIN = "INT32_MIN"
const SIZE_MAX = "UINT64_MAX"
const SSIZE_MAX = "LONG_MAX"
const SS_BLOCKSIZE = 1024
const SS_INSERTIONSORT_THRESHOLD = 8
const SS_MISORT_STACKSIZE = 16
const SS_SMERGE_STACKSIZE = 32
const STATIC_BMI2 = 0
const STORED_SEQS = 8
const STREAM_ACCUMULATOR_MIN_32 = 25
const STREAM_ACCUMULATOR_MIN_64 = 57
const SUSPECT_INCOMPRESSIBLE_SAMPLE_RATIO = 10
const SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE = 4096
const SUSPECT_UNCOMPRESSIBLE_LITERAL_RATIO = 20
const SYMLOOP_MAX = 40
const THRESHOLD_PENALTY = 3
const THRESHOLD_PENALTY_RATE = 16
const TIMER_ABSTIME = 1
const TIME_UTC = 1
const TMP_MAX = 10000
const TR_INSERTIONSORT_THRESHOLD = 8
const TR_STACKSIZE = 64
const TTY_NAME_MAX = 32
const TZNAME_MAX = 6
const UCHAR_MAX = 255
const UINT16_MAX = 0xffff
const UINT32_MAX = "0xffffffffu"
const UINT64_MAX = "0xffffffffffffffffu"
const UINT8_MAX = 0xff
const UINTMAX_MAX = "UINT64_MAX"
const UINTPTR_MAX = "UINT64_MAX"
const UINT_FAST16_MAX = "UINT32_MAX"
const UINT_FAST32_MAX = "UINT32_MAX"
const UINT_FAST64_MAX = "UINT64_MAX"
const UINT_FAST8_MAX = "UINT8_MAX"
const UINT_LEAST16_MAX = "UINT16_MAX"
const UINT_LEAST32_MAX = "UINT32_MAX"
const UINT_LEAST64_MAX = "UINT64_MAX"
const UINT_LEAST8_MAX = "UINT8_MAX"
const UINT_MAX = 4294967295
const USHRT_MAX = 0xffff
const WILDCOPY_OVERLENGTH = 32
const WILDCOPY_VECLEN = 16
const WINT_MAX = "UINT32_MAX"
const WINT_MIN = 0
const WNOHANG = 1
const WORD_BIT = 32
const WUNTRACED = 2
const XXH32_ENDJMP = 0
const XXH3_INLINE_SECRET = 0
const XXH3_WITH_SECRET_INLINE = "XXH_NO_INLINE"
const XXH_C23_VN = 201711
const XXH_CPU_LITTLE_ENDIAN = 1
const XXH_DEBUGLEVEL = "DEBUGLEVEL"
const XXH_FORCE_ALIGN_CHECK = 0
const XXH_FORCE_MEMORY_ACCESS = 1
const XXH_NO_INLINE = "static"
const XXH_NO_INLINE_HINTS = 1
const XXH_PRIME32_1 = 2654435761
const XXH_PRIME32_2 = 2246822519
const XXH_PRIME32_3 = 3266489917
const XXH_PRIME32_4 = 668265263
const XXH_PRIME32_5 = 374761393
const XXH_PRIME64_1 = 11400714785074694791
const XXH_PRIME64_2 = 14029467366897019727
const XXH_PRIME64_3 = 1609587929392839161
const XXH_PRIME64_4 = 9650029242287828579
const XXH_PRIME64_5 = 2870177450012600261
const XXH_SIZE_OPT = 0
const XXH_VERSION_MAJOR = 0
const XXH_VERSION_MINOR = 8
const XXH_VERSION_RELEASE = 2
const XXH_swap32 = "__builtin_bswap32"
const XXH_swap64 = "__builtin_bswap64"
const ZDICTLIB_API = "ZDICTLIB_VISIBLE"
const ZDICTLIB_STATIC_API = "ZDICTLIB_VISIBLE"
const ZDICT_CONTENTSIZE_MIN = 128
const ZDICT_DICTSIZE_MIN = 256
const ZSTDERRORLIB_API = "ZSTDERRORLIB_VISIBLE"
const ZSTDLIB_API = "ZSTDLIB_VISIBLE"
const ZSTDLIB_STATIC_API = "ZSTDLIB_VISIBLE"
const ZSTD_ADDRESS_SANITIZER = 0
const ZSTD_ASM_SUPPORTED = 1
const ZSTD_BLOCKHEADERSIZE = 3
const ZSTD_BLOCKSIZELOG_MAX = 17
const ZSTD_BLOCKSPLITTER_LEVEL_MAX = 6
const ZSTD_CET_ENDBRANCH = "_CET_ENDBR"
const ZSTD_CHAINLOG_MAX_32 = 29
const ZSTD_CHAINLOG_MAX_64 = 30
const ZSTD_CHAINLOG_MIN = "ZSTD_HASHLOG_MIN"
const ZSTD_CLEVEL_DEFAULT = 3
const ZSTD_COMPRESSBLOCK_BTLAZY2 = "ZSTD_compressBlock_btlazy2"
const ZSTD_COMPRESSBLOCK_BTLAZY2_DICTMATCHSTATE = "ZSTD_compressBlock_btlazy2_dictMatchState"
const ZSTD_COMPRESSBLOCK_BTLAZY2_EXTDICT = "ZSTD_compressBlock_btlazy2_extDict"
const ZSTD_COMPRESSBLOCK_BTOPT = "ZSTD_compressBlock_btopt"
const ZSTD_COMPRESSBLOCK_BTOPT_DICTMATCHSTATE = "ZSTD_compressBlock_btopt_dictMatchState"
const ZSTD_COMPRESSBLOCK_BTOPT_EXTDICT = "ZSTD_compressBlock_btopt_extDict"
const ZSTD_COMPRESSBLOCK_BTULTRA = "ZSTD_compressBlock_btultra"
const ZSTD_COMPRESSBLOCK_BTULTRA2 = "ZSTD_compressBlock_btultra2"
const ZSTD_COMPRESSBLOCK_BTULTRA_DICTMATCHSTATE = "ZSTD_compressBlock_btultra_dictMatchState"
const ZSTD_COMPRESSBLOCK_BTULTRA_EXTDICT = "ZSTD_compressBlock_btultra_extDict"
const ZSTD_COMPRESSBLOCK_DOUBLEFAST = "ZSTD_compressBlock_doubleFast"
const ZSTD_COMPRESSBLOCK_DOUBLEFAST_DICTMATCHSTATE = "ZSTD_compressBlock_doubleFast_dictMatchState"
const ZSTD_COMPRESSBLOCK_DOUBLEFAST_EXTDICT = "ZSTD_compressBlock_doubleFast_extDict"
const ZSTD_COMPRESSBLOCK_GREEDY = "ZSTD_compressBlock_greedy"
const ZSTD_COMPRESSBLOCK_GREEDY_DEDICATEDDICTSEARCH = "ZSTD_compressBlock_greedy_dedicatedDictSearch"
const ZSTD_COMPRESSBLOCK_GREEDY_DEDICATEDDICTSEARCH_ROW = "ZSTD_compressBlock_greedy_dedicatedDictSearch_row"
const ZSTD_COMPRESSBLOCK_GREEDY_DICTMATCHSTATE = "ZSTD_compressBlock_greedy_dictMatchState"
const ZSTD_COMPRESSBLOCK_GREEDY_DICTMATCHSTATE_ROW = "ZSTD_compressBlock_greedy_dictMatchState_row"
const ZSTD_COMPRESSBLOCK_GREEDY_EXTDICT = "ZSTD_compressBlock_greedy_extDict"
const ZSTD_COMPRESSBLOCK_GREEDY_EXTDICT_ROW = "ZSTD_compressBlock_greedy_extDict_row"
const ZSTD_COMPRESSBLOCK_GREEDY_ROW = "ZSTD_compressBlock_greedy_row"
const ZSTD_COMPRESSBLOCK_LAZY = "ZSTD_compressBlock_lazy"
const ZSTD_COMPRESSBLOCK_LAZY2 = "ZSTD_compressBlock_lazy2"
const ZSTD_COMPRESSBLOCK_LAZY2_DEDICATEDDICTSEARCH = "ZSTD_compressBlock_lazy2_dedicatedDictSearch"
const ZSTD_COMPRESSBLOCK_LAZY2_DEDICATEDDICTSEARCH_ROW = "ZSTD_compressBlock_lazy2_dedicatedDictSearch_row"
const ZSTD_COMPRESSBLOCK_LAZY2_DICTMATCHSTATE = "ZSTD_compressBlock_lazy2_dictMatchState"
const ZSTD_COMPRESSBLOCK_LAZY2_DICTMATCHSTATE_ROW = "ZSTD_compressBlock_lazy2_dictMatchState_row"
const ZSTD_COMPRESSBLOCK_LAZY2_EXTDICT = "ZSTD_compressBlock_lazy2_extDict"
const ZSTD_COMPRESSBLOCK_LAZY2_EXTDICT_ROW = "ZSTD_compressBlock_lazy2_extDict_row"
const ZSTD_COMPRESSBLOCK_LAZY2_ROW = "ZSTD_compressBlock_lazy2_row"
const ZSTD_COMPRESSBLOCK_LAZY_DEDICATEDDICTSEARCH = "ZSTD_compressBlock_lazy_dedicatedDictSearch"
const ZSTD_COMPRESSBLOCK_LAZY_DEDICATEDDICTSEARCH_ROW = "ZSTD_compressBlock_lazy_dedicatedDictSearch_row"
const ZSTD_COMPRESSBLOCK_LAZY_DICTMATCHSTATE = "ZSTD_compressBlock_lazy_dictMatchState"
const ZSTD_COMPRESSBLOCK_LAZY_DICTMATCHSTATE_ROW = "ZSTD_compressBlock_lazy_dictMatchState_row"
const ZSTD_COMPRESSBLOCK_LAZY_EXTDICT = "ZSTD_compressBlock_lazy_extDict"
const ZSTD_COMPRESSBLOCK_LAZY_EXTDICT_ROW = "ZSTD_compressBlock_lazy_extDict_row"
const ZSTD_COMPRESSBLOCK_LAZY_ROW = "ZSTD_compressBlock_lazy_row"
const ZSTD_COMPRESS_HEAPMODE = 0
const ZSTD_CWKSP_ALIGNMENT_BYTES = 64
const ZSTD_CWKSP_ASAN_REDZONE_SIZE = 128
const ZSTD_DATAFLOW_SANITIZER = 0
const ZSTD_DISABLE_ASM = 1
const ZSTD_DUBT_UNSORTED_MARK = 1
const ZSTD_ENABLE_ASM_X86_64_BMI2 = 0
const ZSTD_FRAMECHECKSUMSIZE = 4
const ZSTD_FRAMEHEADERSIZE_MAX = 18
const ZSTD_FRAMEIDSIZE = 4
const ZSTD_HASHLOG3_MAX = 17
const ZSTD_HASHLOG_MIN = 6
const ZSTD_HAVE_WEAK_SYMBOLS = 1
const ZSTD_HEAPMODE = 1
const ZSTD_HUFFDTABLE_CAPACITY_LOG = 12
const ZSTD_LAZY_DDSS_BUCKET_LOG = 2
const ZSTD_LBMIN = 64
const ZSTD_LDM_BUCKETSIZELOG_MAX = 8
const ZSTD_LDM_BUCKETSIZELOG_MIN = 1
const ZSTD_LDM_DEFAULT_WINDOW_LOG = "ZSTD_WINDOWLOG_LIMIT_DEFAULT"
const ZSTD_LDM_HASHLOG_MAX = "ZSTD_HASHLOG_MAX"
const ZSTD_LDM_HASHLOG_MIN = "ZSTD_HASHLOG_MIN"
const ZSTD_LDM_HASHRATELOG_MIN = 0
const ZSTD_LDM_MINMATCH_MAX = 4096
const ZSTD_LDM_MINMATCH_MIN = 4
const ZSTD_LEGACY_SUPPORT = 0
const ZSTD_LITFREQ_ADD = 2
const ZSTD_MAGICNUMBER = 4247762216
const ZSTD_MAGIC_DICTIONARY = 3962610743
const ZSTD_MAGIC_SKIPPABLE_MASK = 4294967280
const ZSTD_MAGIC_SKIPPABLE_START = 407710288
const ZSTD_MAX_CLEVEL = 22
const ZSTD_MAX_HUF_HEADER_SIZE = 128
const ZSTD_MAX_NB_BLOCK_SPLITS = 196
const ZSTD_MEMORY_SANITIZER = 0
const ZSTD_MINMATCH_MAX = 7
const ZSTD_MINMATCH_MIN = 3
const ZSTD_NO_CLEVEL = 0
const ZSTD_NO_FORWARD_PROGRESS_MAX = 16
const ZSTD_NO_INTRINSICS = 1
const ZSTD_OVERLAPLOG_MAX = 9
const ZSTD_OVERLAPLOG_MIN = 0
const ZSTD_PREDEF_THRESHOLD = 8
const ZSTD_REP_NUM = 3
const ZSTD_RESIZE_SEQPOOL = 0
const ZSTD_ROLL_HASH_CHAR_OFFSET = 10
const ZSTD_ROWSIZE = 16
const ZSTD_ROW_HASH_CACHE_SIZE = 8
const ZSTD_ROW_HASH_MAX_ENTRIES = 64
const ZSTD_ROW_HASH_TAG_BITS = 8
const ZSTD_SEARCHLOG_MIN = 1
const ZSTD_SEARCH_FN_ATTRS = "FORCE_NOINLINE"
const ZSTD_SHORT_CACHE_TAG_BITS = 8
const ZSTD_SKIPPABLEHEADERSIZE = 8
const ZSTD_SLIPBLOCK_WORKSPACESIZE = 8208
const ZSTD_SRCSIZEHINT_MAX = "INT_MAX"
const ZSTD_SRCSIZEHINT_MIN = 0
const ZSTD_STRATEGY_MAX = 9
const ZSTD_STRATEGY_MIN = 1
const ZSTD_TARGETCBLOCKSIZE_MAX = "ZSTD_BLOCKSIZE_MAX"
const ZSTD_TARGETCBLOCKSIZE_MIN = 1340
const ZSTD_TARGETLENGTH_MAX = "ZSTD_BLOCKSIZE_MAX"
const ZSTD_TARGETLENGTH_MIN = 0
const ZSTD_TRACE = 0
const ZSTD_USE_CDICT_PARAMS_DICTSIZE_MULTIPLIER = 6
const ZSTD_VERSION_MAJOR = 1
const ZSTD_VERSION_MINOR = 5
const ZSTD_VERSION_RELEASE = 7
const ZSTD_WINDOWLOG_ABSOLUTEMIN = 10
const ZSTD_WINDOWLOG_LIMIT_DEFAULT = 27
const ZSTD_WINDOWLOG_MAX_32 = 30
const ZSTD_WINDOWLOG_MAX_64 = 31
const ZSTD_WINDOWLOG_MIN = 10
const ZSTD_WINDOW_OVERFLOW_CORRECT_FREQUENTLY = 0
const ZSTD_WINDOW_START_INDEX = 2
const ZSTD_WORKSPACETOOLARGE_FACTOR = 3
const ZSTD_WORKSPACETOOLARGE_MAXDURATION = 128
const ZSTD_c_blockDelimiters = 1008
const ZSTD_c_blockSplitterLevel = 1017
const ZSTD_c_deterministicRefPrefix = 1012
const ZSTD_c_enableDedicatedDictSearch = 1005
const ZSTD_c_enableSeqProducerFallback = 1014
const ZSTD_c_forceAttachDict = 1001
const ZSTD_c_forceMaxWindow = 1000
const ZSTD_c_format = 10
const ZSTD_c_literalCompressionMode = 1002
const ZSTD_c_maxBlockSize = 1015
const ZSTD_c_prefetchCDictTables = 1013
const ZSTD_c_repcodeResolution = 1016
const ZSTD_c_rsyncable = 500
const ZSTD_c_searchForExternalRepcodes = "ZSTD_c_experimentalParam19"
const ZSTD_c_splitAfterSequences = 1010
const ZSTD_c_srcSizeHint = 1004
const ZSTD_c_stableInBuffer = 1006
const ZSTD_c_stableOutBuffer = 1007
const ZSTD_c_useRowMatchFinder = 1011
const ZSTD_c_validateSequences = 1009
const ZSTD_d_disableHuffmanAssembly = 1004
const ZSTD_d_forceIgnoreChecksum = 1002
const ZSTD_d_format = 1000
const ZSTD_d_maxBlockSize = 1005
const ZSTD_d_refMultipleDDicts = 1003
const ZSTD_d_stableOutBuffer = 1001
const ZSTD_frameHeader = "ZSTD_FrameHeader"
const ZSTD_frameType_e = "ZSTD_FrameType_e"
const ZSTD_paramSwitch_e = "ZSTD_ParamSwitch_e"
const ZSTD_pthread_cond_t = "pthread_cond_t"
const ZSTD_pthread_mutex_t = "pthread_mutex_t"
const ZSTD_pthread_t = "pthread_t"
const ZSTD_sequenceFormat_e = "ZSTD_SequenceFormat_e"
const _DIVSUFSORT_H = 1
const _FILE_OFFSET_BITS = 64
const _GNU_SOURCE = 1
const _IOFBF = 0
const _IOLBF = 1
const _IONBF = 2
const _LP64 = 1
const _POSIX2_BC_BASE_MAX = 99
const _POSIX2_BC_DIM_MAX = 2048
const _POSIX2_BC_SCALE_MAX = 99
const _POSIX2_BC_STRING_MAX = 1000
const _POSIX2_CHARCLASS_NAME_MAX = 14
const _POSIX2_COLL_WEIGHTS_MAX = 2
const _POSIX2_EXPR_NEST_MAX = 32
const _POSIX2_LINE_MAX = 2048
const _POSIX2_RE_DUP_MAX = 255
const _POSIX_AIO_LISTIO_MAX = 2
const _POSIX_AIO_MAX = 1
const _POSIX_ARG_MAX = 4096
const _POSIX_CHILD_MAX = 25
const _POSIX_CLOCKRES_MIN = 20000000
const _POSIX_DELAYTIMER_MAX = 32
const _POSIX_HOST_NAME_MAX = 255
const _POSIX_LINK_MAX = 8
const _POSIX_LOGIN_NAME_MAX = 9
const _POSIX_MAX_CANON = 255
const _POSIX_MAX_INPUT = 255
const _POSIX_MQ_OPEN_MAX = 8
const _POSIX_MQ_PRIO_MAX = 32
const _POSIX_NAME_MAX = 14
const _POSIX_NGROUPS_MAX = 8
const _POSIX_OPEN_MAX = 20
const _POSIX_PATH_MAX = 256
const _POSIX_PIPE_BUF = 512
const _POSIX_RE_DUP_MAX = 255
const _POSIX_RTSIG_MAX = 8
const _POSIX_SEM_NSEMS_MAX = 256
const _POSIX_SEM_VALUE_MAX = 32767
const _POSIX_SIGQUEUE_MAX = 32
const _POSIX_SSIZE_MAX = 32767
const _POSIX_SS_REPL_MAX = 4
const _POSIX_STREAM_MAX = 8
const _POSIX_SYMLINK_MAX = 255
const _POSIX_SYMLOOP_MAX = 8
const _POSIX_THREAD_DESTRUCTOR_ITERATIONS = 4
const _POSIX_THREAD_KEYS_MAX = 128
const _POSIX_THREAD_THREADS_MAX = 64
const _POSIX_TIMER_MAX = 32
const _POSIX_TRACE_EVENT_NAME_MAX = 30
const _POSIX_TRACE_NAME_MAX = 8
const _POSIX_TRACE_SYS_MAX = 8
const _POSIX_TRACE_USER_EVENT_MAX = 32
const _POSIX_TTY_NAME_MAX = 9
const _POSIX_TZNAME_MAX = 6
const _STDC_PREDEF_H = 1
const _XOPEN_IOV_MAX = 16
const _XOPEN_NAME_MAX = 255
const _XOPEN_PATH_MAX = 1024
const __ATOMIC_ACQUIRE = 2
const __ATOMIC_ACQ_REL = 4
const __ATOMIC_CONSUME = 1
const __ATOMIC_HLE_ACQUIRE = 65536
const __ATOMIC_HLE_RELEASE = 131072
const __ATOMIC_RELAXED = 0
const __ATOMIC_RELEASE = 3
const __ATOMIC_SEQ_CST = 5
const __BFLT16_DECIMAL_DIG__ = 4
const __BFLT16_DENORM_MIN__ = "9.18354961579912115600575419704879436e-41B"
const __BFLT16_DIG__ = 2
const __BFLT16_EPSILON__ = "7.81250000000000000000000000000000000e-3B"
const __BFLT16_HAS_DENORM__ = 1
const __BFLT16_HAS_INFINITY__ = 1
const __BFLT16_HAS_QUIET_NAN__ = 1
const __BFLT16_IS_IEC_60559__ = 0
const __BFLT16_MANT_DIG__ = 8
const __BFLT16_MAX_10_EXP__ = 38
const __BFLT16_MAX_EXP__ = 128
const __BFLT16_MAX__ = "3.38953138925153547590470800371487867e+38B"
const __BFLT16_MIN__ = "1.17549435082228750796873653722224568e-38B"
const __BFLT16_NORM_MAX__ = "3.38953138925153547590470800371487867e+38B"
const __BIGGEST_ALIGNMENT__ = 16
const __BIG_ENDIAN = 4321
const __BITINT_MAXWIDTH__ = 65535
const __BYTE_ORDER = 1234
const __BYTE_ORDER__ = "__ORDER_LITTLE_ENDIAN__"
const __CCGO__ = 1
const __CET__ = 3
const __CHAR_BIT__ = 8
const __DBL_DECIMAL_DIG__ = 17
const __DBL_DIG__ = 15
const __DBL_HAS_DENORM__ = 1
const __DBL_HAS_INFINITY__ = 1
const __DBL_HAS_QUIET_NAN__ = 1
const __DBL_IS_IEC_60559__ = 1
const __DBL_MANT_DIG__ = 53
const __DBL_MAX_10_EXP__ = 308
const __DBL_MAX_EXP__ = 1024
const __DEC128_EPSILON__ = 1e-33
const __DEC128_MANT_DIG__ = 34
const __DEC128_MAX_EXP__ = 6145
const __DEC128_MAX__ = "9.999999999999999999999999999999999E6144"
const __DEC128_MIN__ = 1e-6143
const __DEC128_SUBNORMAL_MIN__ = 0.000000000000000000000000000000001e-6143
const __DEC32_EPSILON__ = 1e-6
const __DEC32_MANT_DIG__ = 7
const __DEC32_MAX_EXP__ = 97
const __DEC32_MAX__ = 9.999999e96
const __DEC32_MIN__ = 1e-95
const __DEC32_SUBNORMAL_MIN__ = 0.000001e-95
const __DEC64X_EPSILON__ = "1E-33D64x"
const __DEC64X_MANT_DIG__ = 34
const __DEC64X_MAX_EXP__ = 6145
const __DEC64X_MAX__ = "9.999999999999999999999999999999999E6144D64x"
const __DEC64X_MIN__ = "1E-6143D64x"
const __DEC64X_SUBNORMAL_MIN__ = "0.000000000000000000000000000000001E-6143D64x"
const __DEC64_EPSILON__ = 1e-15
const __DEC64_MANT_DIG__ = 16
const __DEC64_MAX_EXP__ = 385
const __DEC64_MAX__ = "9.999999999999999E384"
const __DEC64_MIN__ = 1e-383
const __DEC64_SUBNORMAL_MIN__ = 0.000000000000001e-383
const __DECIMAL_BID_FORMAT__ = 1
const __DECIMAL_DIG__ = 21
const __DEC_EVAL_METHOD__ = 2
const __ELF__ = 1
const __FINITE_MATH_ONLY__ = 0
const __FLOAT_WORD_ORDER__ = "__ORDER_LITTLE_ENDIAN__"
const __FLT128_DECIMAL_DIG__ = 36
const __FLT128_DENORM_MIN__ = 6.47517511943802511092443895822764655e-4966
const __FLT128_DIG__ = 33
const __FLT128_EPSILON__ = 1.92592994438723585305597794258492732e-34
const __FLT128_HAS_DENORM__ = 1
const __FLT128_HAS_INFINITY__ = 1
const __FLT128_HAS_QUIET_NAN__ = 1
const __FLT128_IS_IEC_60559__ = 1
const __FLT128_MANT_DIG__ = 113
const __FLT128_MAX_10_EXP__ = 4932
const __FLT128_MAX_EXP__ = 16384
const __FLT128_MAX__ = "1.18973149535723176508575932662800702e+4932"
const __FLT128_MIN__ = 3.36210314311209350626267781732175260e-4932
const __FLT128_NORM_MAX__ = "1.18973149535723176508575932662800702e+4932"
const __FLT16_DECIMAL_DIG__ = 5
const __FLT16_DENORM_MIN__ = 5.96046447753906250000000000000000000e-8
const __FLT16_DIG__ = 3
const __FLT16_EPSILON__ = 9.76562500000000000000000000000000000e-4
const __FLT16_HAS_DENORM__ = 1
const __FLT16_HAS_INFINITY__ = 1
const __FLT16_HAS_QUIET_NAN__ = 1
const __FLT16_IS_IEC_60559__ = 1
const __FLT16_MANT_DIG__ = 11
const __FLT16_MAX_10_EXP__ = 4
const __FLT16_MAX_EXP__ = 16
const __FLT16_MAX__ = 6.55040000000000000000000000000000000e+4
const __FLT16_MIN__ = 6.10351562500000000000000000000000000e-5
const __FLT16_NORM_MAX__ = 6.55040000000000000000000000000000000e+4
const __FLT32X_DECIMAL_DIG__ = 17
const __FLT32X_DENORM_MIN__ = 4.94065645841246544176568792868221372e-324
const __FLT32X_DIG__ = 15
const __FLT32X_EPSILON__ = 2.22044604925031308084726333618164062e-16
const __FLT32X_HAS_DENORM__ = 1
const __FLT32X_HAS_INFINITY__ = 1
const __FLT32X_HAS_QUIET_NAN__ = 1
const __FLT32X_IS_IEC_60559__ = 1
const __FLT32X_MANT_DIG__ = 53
const __FLT32X_MAX_10_EXP__ = 308
const __FLT32X_MAX_EXP__ = 1024
const __FLT32X_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT32X_MIN__ = 2.22507385850720138309023271733240406e-308
const __FLT32X_NORM_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT32_DECIMAL_DIG__ = 9
const __FLT32_DENORM_MIN__ = 1.40129846432481707092372958328991613e-45
const __FLT32_DIG__ = 6
const __FLT32_EPSILON__ = 1.19209289550781250000000000000000000e-7
const __FLT32_HAS_DENORM__ = 1
const __FLT32_HAS_INFINITY__ = 1
const __FLT32_HAS_QUIET_NAN__ = 1
const __FLT32_IS_IEC_60559__ = 1
const __FLT32_MANT_DIG__ = 24
const __FLT32_MAX_10_EXP__ = 38
const __FLT32_MAX_EXP__ = 128
const __FLT32_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT32_MIN__ = 1.17549435082228750796873653722224568e-38
const __FLT32_NORM_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT64X_DECIMAL_DIG__ = 21
const __FLT64X_DENORM_MIN__ = 3.64519953188247460252840593361941982e-4951
const __FLT64X_DIG__ = 18
const __FLT64X_EPSILON__ = 1.08420217248550443400745280086994171e-19
const __FLT64X_HAS_DENORM__ = 1
const __FLT64X_HAS_INFINITY__ = 1
const __FLT64X_HAS_QUIET_NAN__ = 1
const __FLT64X_IS_IEC_60559__ = 1
const __FLT64X_MANT_DIG__ = 64
const __FLT64X_MAX_10_EXP__ = 4932
const __FLT64X_MAX_EXP__ = 16384
const __FLT64X_MAX__ = "1.18973149535723176502126385303097021e+4932"
const __FLT64X_MIN__ = 3.36210314311209350626267781732175260e-4932
const __FLT64X_NORM_MAX__ = "1.18973149535723176502126385303097021e+4932"
const __FLT64_DECIMAL_DIG__ = 17
const __FLT64_DENORM_MIN__ = 4.94065645841246544176568792868221372e-324
const __FLT64_DIG__ = 15
const __FLT64_EPSILON__ = 2.22044604925031308084726333618164062e-16
const __FLT64_HAS_DENORM__ = 1
const __FLT64_HAS_INFINITY__ = 1
const __FLT64_HAS_QUIET_NAN__ = 1
const __FLT64_IS_IEC_60559__ = 1
const __FLT64_MANT_DIG__ = 53
const __FLT64_MAX_10_EXP__ = 308
const __FLT64_MAX_EXP__ = 1024
const __FLT64_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT64_MIN__ = 2.22507385850720138309023271733240406e-308
const __FLT64_NORM_MAX__ = 1.79769313486231570814527423731704357e+308
const __FLT_DECIMAL_DIG__ = 9
const __FLT_DENORM_MIN__ = 1.40129846432481707092372958328991613e-45
const __FLT_DIG__ = 6
const __FLT_EPSILON__ = 1.19209289550781250000000000000000000e-7
const __FLT_EVAL_METHOD_TS_18661_3__ = 0
const __FLT_EVAL_METHOD__ = 0
const __FLT_HAS_DENORM__ = 1
const __FLT_HAS_INFINITY__ = 1
const __FLT_HAS_QUIET_NAN__ = 1
const __FLT_IS_IEC_60559__ = 1
const __FLT_MANT_DIG__ = 24
const __FLT_MAX_10_EXP__ = 38
const __FLT_MAX_EXP__ = 128
const __FLT_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT_MIN__ = 1.17549435082228750796873653722224568e-38
const __FLT_NORM_MAX__ = 3.40282346638528859811704183484516925e+38
const __FLT_RADIX__ = 2
const __FUNCTION__ = "__func__"
const __FXSR__ = 1
const __GCC_ASM_FLAG_OUTPUTS__ = 1
const __GCC_ATOMIC_BOOL_LOCK_FREE = 2
const __GCC_ATOMIC_CHAR16_T_LOCK_FREE = 2
const __GCC_ATOMIC_CHAR32_T_LOCK_FREE = 2
const __GCC_ATOMIC_CHAR_LOCK_FREE = 2
const __GCC_ATOMIC_INT_LOCK_FREE = 2
const __GCC_ATOMIC_LLONG_LOCK_FREE = 2
const __GCC_ATOMIC_LONG_LOCK_FREE = 2
const __GCC_ATOMIC_POINTER_LOCK_FREE = 2
const __GCC_ATOMIC_SHORT_LOCK_FREE = 2
const __GCC_ATOMIC_TEST_AND_SET_TRUEVAL = 1
const __GCC_ATOMIC_WCHAR_T_LOCK_FREE = 2
const __GCC_CONSTRUCTIVE_SIZE = 64
const __GCC_DESTRUCTIVE_SIZE = 64
const __GCC_HAVE_DWARF2_CFI_ASM = 1
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 = 1
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 = 1
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_4 = 1
const __GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 = 1
const __GCC_IEC_559 = 2
const __GCC_IEC_559_COMPLEX = 2
const __GNUC_EXECUTION_CHARSET_NAME = "UTF-8"
const __GNUC_MINOR__ = 2
const __GNUC_PATCHLEVEL__ = 1
const __GNUC_STDC_INLINE__ = 1
const __GNUC_WIDE_EXECUTION_CHARSET_NAME = "UTF-32LE"
const __GNUC__ = 15
const __GXX_ABI_VERSION = 1020
const __HAVE_SPECULATION_SAFE_VALUE = 1
const __INT16_MAX__ = 0x7fff
const __INT32_MAX__ = 0x7fffffff
const __INT32_TYPE__ = "int"
const __INT64_MAX__ = 0x7fffffffffffffff
const __INT8_MAX__ = 0x7f
const __INTMAX_MAX__ = 0x7fffffffffffffff
const __INTMAX_WIDTH__ = 64
const __INTPTR_MAX__ = 0x7fffffffffffffff
const __INTPTR_WIDTH__ = 64
const __INT_FAST16_MAX__ = 0x7fffffffffffffff
const __INT_FAST16_WIDTH__ = 64
const __INT_FAST32_MAX__ = 0x7fffffffffffffff
const __INT_FAST32_WIDTH__ = 64
const __INT_FAST64_MAX__ = 0x7fffffffffffffff
const __INT_FAST64_WIDTH__ = 64
const __INT_FAST8_MAX__ = 0x7f
const __INT_FAST8_WIDTH__ = 8
const __INT_LEAST16_MAX__ = 0x7fff
const __INT_LEAST16_WIDTH__ = 16
const __INT_LEAST32_MAX__ = 0x7fffffff
const __INT_LEAST32_TYPE__ = "int"
const __INT_LEAST32_WIDTH__ = 32
const __INT_LEAST64_MAX__ = 0x7fffffffffffffff
const __INT_LEAST64_WIDTH__ = 64
const __INT_LEAST8_MAX__ = 0x7f
const __INT_LEAST8_WIDTH__ = 8
const __INT_MAX__ = 0x7fffffff
const __INT_WIDTH__ = 32
const __LDBL_DECIMAL_DIG__ = 21
const __LDBL_DENORM_MIN__ = 3.64519953188247460252840593361941982e-4951
const __LDBL_DIG__ = 18
const __LDBL_EPSILON__ = 1.08420217248550443400745280086994171e-19
const __LDBL_HAS_DENORM__ = 1
const __LDBL_HAS_INFINITY__ = 1
const __LDBL_HAS_QUIET_NAN__ = 1
const __LDBL_IS_IEC_60559__ = 1
const __LDBL_MANT_DIG__ = 64
const __LDBL_MAX_10_EXP__ = 4932
const __LDBL_MAX_EXP__ = 16384
const __LDBL_MAX__ = "1.18973149535723176502126385303097021e+4932"
const __LDBL_MIN__ = 3.36210314311209350626267781732175260e-4932
const __LDBL_NORM_MAX__ = "1.18973149535723176502126385303097021e+4932"
const __LITTLE_ENDIAN = 1234
const __LONG_LONG_MAX__ = 0x7fffffffffffffff
const __LONG_LONG_WIDTH__ = 64
const __LONG_MAX = 0x7fffffffffffffff
const __LONG_MAX__ = 0x7fffffffffffffff
const __LONG_WIDTH__ = 64
const __LP64__ = 1
const __MMX_WITH_SSE__ = 1
const __MMX__ = 1
const __NO_INLINE__ = 1
const __ORDER_BIG_ENDIAN__ = 4321
const __ORDER_LITTLE_ENDIAN__ = 1234
const __ORDER_PDP_ENDIAN__ = 3412
const __PIC__ = 2
const __PIE__ = 2
const __PRAGMA_REDEFINE_EXTNAME = 1
const __PRETTY_FUNCTION__ = "__func__"
const __PTRDIFF_MAX__ = 0x7fffffffffffffff
const __PTRDIFF_WIDTH__ = 64
const __SCHAR_MAX__ = 0x7f
const __SCHAR_WIDTH__ = 8
const __SEG_FS = 1
const __SEG_GS = 1
const __SHRT_MAX__ = 0x7fff
const __SHRT_WIDTH__ = 16
const __SIG_ATOMIC_MAX__ = 0x7fffffff
const __SIG_ATOMIC_TYPE__ = "int"
const __SIG_ATOMIC_WIDTH__ = 32
const __SIZEOF_DOUBLE__ = 8
const __SIZEOF_FLOAT128__ = 16
const __SIZEOF_FLOAT80__ = 16
const __SIZEOF_FLOAT__ = 4
const __SIZEOF_INT128__ = 16
const __SIZEOF_INT__ = 4
const __SIZEOF_LONG_DOUBLE__ = 8
const __SIZEOF_LONG_LONG__ = 8
const __SIZEOF_LONG__ = 8
const __SIZEOF_POINTER__ = 8
const __SIZEOF_PTRDIFF_T__ = 8
const __SIZEOF_SHORT__ = 2
const __SIZEOF_SIZE_T__ = 8
const __SIZEOF_WCHAR_T__ = 4
const __SIZEOF_WINT_T__ = 4
const __SIZE_MAX__ = 0xffffffffffffffff
const __SIZE_WIDTH__ = 64
const __SSE2_MATH__ = 1
const __SSE2__ = 1
const __SSE_MATH__ = 1
const __SSE__ = 1
const __SSP_STRONG__ = 3
const __STDC_EMBED_EMPTY__ = 2
const __STDC_EMBED_FOUND__ = 1
const __STDC_EMBED_NOT_FOUND__ = 0
const __STDC_HOSTED__ = 1
const __STDC_IEC_559_COMPLEX__ = 1
const __STDC_IEC_559__ = 1
const __STDC_IEC_60559_BFP__ = 201404
const __STDC_IEC_60559_COMPLEX__ = 201404
const __STDC_ISO_10646__ = 201706
const __STDC_UTF_16__ = 1
const __STDC_UTF_32__ = 1
const __STDC_VERSION__ = 201710
const __STDC__ = 1
const __STRICT_ANSI__ = 1
const __UINT16_MAX__ = 0xffff
const __UINT32_MAX__ = 0xffffffff
const __UINT64_MAX__ = 0xffffffffffffffff
const __UINT8_MAX__ = 0xff
const __UINTMAX_MAX__ = 0xffffffffffffffff
const __UINTPTR_MAX__ = 0xffffffffffffffff
const __UINT_FAST16_MAX__ = 0xffffffffffffffff
const __UINT_FAST32_MAX__ = 0xffffffffffffffff
const __UINT_FAST64_MAX__ = 0xffffffffffffffff
const __UINT_FAST8_MAX__ = 0xff
const __UINT_LEAST16_MAX__ = 0xffff
const __UINT_LEAST32_MAX__ = 0xffffffff
const __UINT_LEAST64_MAX__ = 0xffffffffffffffff
const __UINT_LEAST8_MAX__ = 0xff
const __USE_TIME_BITS64 = 1
const __VERSION__ = "15.2.1 20251018"
const __WCHAR_MAX__ = 0x7fffffff
const __WCHAR_TYPE__ = "int"
const __WCHAR_WIDTH__ = 32
const __WINT_MAX__ = 0xffffffff
const __WINT_MIN__ = 0
const __WINT_WIDTH__ = 32
const __amd64 = 1
const __amd64__ = 1
const __code_model_small__ = 1
const __gnu_linux__ = 1
const __inline = "inline"
const __k8 = 1
const __k8__ = 1
const __linux = 1
const __linux__ = 1
const __pic__ = 2
const __pie__ = 2
const __restrict = "restrict"
const __restrict_arr = "restrict"
const __tm_gmtoff = "tm_gmtoff"
const __tm_zone = "tm_zone"
const __unix = 1
const __unix__ = 1
const __x86_64 = 1
const __x86_64__ = 1
const alloca1 = "__builtin_alloca"
const kLazySkippingStep = 8
const kSearchStrength = 8
const static_assert = "_Static_assert"

type __builtin_va_list = uintptr

type __predefined_size_t = uint64

type __predefined_wchar_t = int32

type __predefined_ptrdiff_t = int64

type wchar_t = uint32

type max_align_t = struct {
	F__ll int64
	F__ld float64
}

type size_t = uint64

type ptrdiff_t = int64

type locale_t = uintptr

type div_t = struct {
	Fquot int32
	Frem  int32
}

type ldiv_t = struct {
	Fquot int64
	Frem  int64
}

type lldiv_t = struct {
	Fquot int64
	Frem  int64
}

/**** ended inlining common/debug.c ****/
/**** start inlining common/entropy_common.c ****/
/* ******************************************************************
 * Common functions of New Generation Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* *************************************
*  Dependencies
***************************************/
/**** start inlining mem.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-****************************************
*  Dependencies
******************************************/
/**** start inlining compiler.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** start inlining portability_macros.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**
 * This header file contains macro definitions to support portability.
 * This header is shared between C and ASM code, so it MUST only
 * contain macro definitions. It MUST not contain any C code.
 *
 * This header ONLY defines macros to detect platforms/feature support.
 *
 */

/* compat. with non-clang compilers */

/* compat. with non-clang compilers */

/* compat. with non-clang compilers */

/* detects whether we are being compiled under msan */

/* detects whether we are being compiled under asan */

/* detects whether we are being compiled under dfsan */

/* Mark the internal assembly functions as hidden  */

/* Compile time determination of BMI2 support */

/* Enable runtime BMI2 dispatch based on the CPU.
 * Enabled for clang & gcc >=4.8 on x86 when BMI2 isn't enabled by default.
 */

/**
 * Only enable assembly for GNU C compatible compilers,
 * because other platforms may not support GAS assembly syntax.
 *
 * Only enable assembly for Linux / MacOS / Win32, other platforms may
 * work, but they haven't been tested. This could likely be
 * extended to BSD systems.
 *
 * Disable assembly when MSAN is enabled, because MSAN requires
 * 100% of code to be instrumented to work.
 */

/**
 * Determines whether we should enable assembly for x86-64
 * with BMI2.
 *
 * Enable if all of the following conditions hold:
 * - ASM hasn't been explicitly disabled by defining ZSTD_DISABLE_ASM
 * - Assembly is supported
 * - We are compiling for x86-64 and either:
 *   - DYNAMIC_BMI2 is enabled
 *   - BMI2 is supported at compile time
 */

/*
 * For x86 ELF targets, add .note.gnu.property section for Intel CET in
 * assembly sources when CET is enabled.
 *
 * Additionally, any function that may be called indirectly must begin
 * with ZSTD_CET_ENDBRANCH.
 */
/* ELF program property for Intel CET.
   Copyright (C) 2017-2025 Free Software Foundation, Inc.

   This file is free software; you can redistribute it and/or modify it
   under the terms of the GNU General Public License as published by the
   Free Software Foundation; either version 3, or (at your option) any
   later version.

   This file is distributed in the hope that it will be useful, but
   WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   Under Section 7 of GPL version 3, you are granted additional
   permissions described in the GCC Runtime Library Exception, version
   3.1, as published by the Free Software Foundation.

   You should have received a copy of the GNU General Public License and
   a copy of the GCC Runtime Library Exception along with this program;
   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
   <http://www.gnu.org/licenses/>.
*/

/* Add x86 feature with IBT and/or SHSTK bits to ELF program property
   if they are enabled.  Otherwise, contents in this header file are
   unused.  Define _CET_ENDBR for assembly codes.  _CET_ENDBR should be
   placed unconditionally at the entrance of a function whose address
   may be taken.  */

/**** ended inlining portability_macros.h ****/

/*-*******************************************************
*  Compiler specifics
*********************************************************/
/* force inlining */

/**
  On MSVC qsort requires that functions passed into it use the __cdecl calling conversion(CC).
  This explicitly marks such functions as __cdecl so that the code will still compile
  if a CC other than __cdecl has been made the default.
*/

/* UNUSED_ATTR tells the compiler it is okay if the function is unused. */

/**
 * FORCE_INLINE_TEMPLATE is used to define C "templates", which take constant
 * parameters. They must be inlined for the compiler to eliminate the constant
 * branches.
 */
/**
 * HINT_INLINE is used to help the compiler generate better code. It is *not*
 * used for "templates", so it can be tweaked based on the compilers
 * performance.
 *
 * gcc-4.8 and gcc-4.9 have been shown to benefit from leaving off the
 * always_inline attribute.
 *
 * clang up to 5.0.0 (trunk) benefit tremendously from the always_inline
 * attribute.
 */

/* "soft" inline :
 * The compiler is free to select if it's a good idea to inline or not.
 * The main objective is to silence compiler warnings
 * when a defined function in included but not used.
 *
 * Note : this macro is prefixed `MEM_` because it used to be provided by `mem.h` unit.
 * Updating the prefix is probably preferable, but requires a fairly large codemod,
 * since this name is used everywhere.
 */

/* force no inlining */

/* target attribute */

/* Target attribute for BMI2 dynamic dispatch.
 * Enable lzcnt, bmi, and bmi2.
 * We test for bmi1 & bmi2. lzcnt is included in bmi1.
 */

/* prefetch
 * can be disabled, by declaring NO_PREFETCH build macro */

/* vectorization
 * older GCC (pre gcc-4.3 picked as the cutoff) uses a different syntax,
 * and some compilers, like Intel ICC and MCST LCC, do not support it at all. */

/* Tell the compiler that a branch is likely or unlikely.
 * Only use these macros if it causes the compiler to generate better code.
 * If you can remove a LIKELY/UNLIKELY annotation without speed changes in gcc
 * and clang, please do.
 */

/* disable warnings */

/* compile time determination of SIMD support */

/* C-language Attributes are added in C23. */

/* Only use C++ attributes in C++. Some compilers report support for C++
 * attributes when compiling with C.
 */

/* Define ZSTD_FALLTHROUGH macro for annotating switch case with the 'fallthrough' attribute.
 * - C23: https://en.cppreference.com/w/c/language/attributes/fallthrough
 * - CPP17: https://en.cppreference.com/w/cpp/language/attributes/fallthrough
 * - Else: __attribute__((__fallthrough__))
 */

/*-**************************************************************
*  Alignment
*****************************************************************/

// C documentation
//
//	/* @return 1 if @u is a 2^n value, 0 otherwise
//	 * useful to check a value is valid for alignment restrictions */
func ZSTD_isPower2(tls *libc.TLS, u size_t) (r int32) {
	return libc.BoolInt32(u&(u-uint64(1)) == uint64(0))
}

/* this test was initially positioned in mem.h,
 * but this file is removed (or replaced) for linux kernel
 * so it's now hosted in compiler.h,
 * which remains valid for both user & kernel spaces.
 */

/* covers gcc, clang & MSVC */
/* note : this section must come first, before C11,
 * due to a limitation in the kernel source generator */

/* C90-compatible alignment macro (GCC/Clang). Adjust for other compilers if needed. */

/*-**************************************************************
*  Sanitizer
*****************************************************************/

/**
 * Zstd relies on pointer overflow in its decompressor.
 * We add this attribute to functions that rely on pointer overflow.
 */

// C documentation
//
//	/**
//	 * Helper function to perform a wrapped pointer difference without triggering
//	 * UBSAN.
//	 *
//	 * @returns lhs - rhs with wrapping
//	 */
func ZSTD_wrappedPtrDiff(tls *libc.TLS, lhs uintptr, rhs uintptr) (r ptrdiff_t) {
	return int64(lhs) - int64(rhs)
}

// C documentation
//
//	/**
//	 * Helper function to perform a wrapped pointer add without triggering UBSAN.
//	 *
//	 * @return ptr + add with wrapping
//	 */
func ZSTD_wrappedPtrAdd(tls *libc.TLS, ptr uintptr, add ptrdiff_t) (r uintptr) {
	return ptr + uintptr(add)
}

// C documentation
//
//	/**
//	 * Helper function to perform a wrapped pointer subtraction without triggering
//	 * UBSAN.
//	 *
//	 * @return ptr - sub with wrapping
//	 */
func ZSTD_wrappedPtrSub(tls *libc.TLS, ptr uintptr, sub ptrdiff_t) (r uintptr) {
	return ptr - uintptr(sub)
}

// C documentation
//
//	/**
//	 * Helper function to add to a pointer that works around C's undefined behavior
//	 * of adding 0 to NULL.
//	 *
//	 * @returns `ptr + add` except it defines `NULL + 0 == NULL`.
//	 */
func ZSTD_maybeNullPtrAdd(tls *libc.TLS, ptr uintptr, add ptrdiff_t) (r uintptr) {
	var v1 uintptr
	_ = v1
	if add > 0 {
		v1 = ptr + uintptr(add)
	} else {
		v1 = ptr
	}
	return v1
}

type uintptr_t = uint64

type intptr_t = int64

type int8_t = int8

type int16_t = int16

type int32_t = int32

type int64_t = int64

type intmax_t = int64

type uint8_t = uint8

type uint16_t = uint16

type uint32_t = uint32

type uint64_t = uint64

type uintmax_t = uint64

type int_fast8_t = int8

type int_fast64_t = int64

type int_least8_t = int8

type int_least16_t = int16

type int_least32_t = int32

type int_least64_t = int64

type uint_fast8_t = uint8

type uint_fast64_t = uint64

type uint_least8_t = uint8

type uint_least16_t = uint16

type uint_least32_t = uint32

type uint_least64_t = uint64

type int_fast16_t = int32

type int_fast32_t = int32

type uint_fast16_t = uint32

type uint_fast32_t = uint32

type BYTE = uint8

type U8 = uint8

type S8 = int8

type U16 = uint16

type S16 = int16

type U32 = uint32

type S32 = int32

type U64 = uint64

type S64 = int64

/*-**************************************************************
*  Memory I/O Implementation
*****************************************************************/
/* MEM_FORCE_MEMORY_ACCESS : For accessing unaligned memory:
 * Method 0 : always use `memcpy()`. Safe and portable.
 * Method 1 : Use compiler extension to set unaligned access.
 * Method 2 : direct access. This method is portable but violate C standard.
 *            It can generate buggy code on targets depending on alignment.
 * Default  : method 1 if supported, else method 0
 */
func MEM_32bits(tls *libc.TLS) (r uint32) {
	return libc.BoolUint32(uint64(8) == uint64(4))
}

func MEM_64bits(tls *libc.TLS) (r uint32) {
	return libc.BoolUint32(uint64(8) == uint64(8))
}

func MEM_isLittleEndian(tls *libc.TLS) (r uint32) {
	return uint32(1)
}

type unalign16 = uint16

type unalign32 = uint32

type unalign64 = uint64

type unalignArch = uint64

func MEM_read16(tls *libc.TLS, ptr uintptr) (r U16) {
	return *(*unalign16)(unsafe.Pointer(ptr))
}

func MEM_read32(tls *libc.TLS, ptr uintptr) (r U32) {
	return *(*unalign32)(unsafe.Pointer(ptr))
}

func MEM_read64(tls *libc.TLS, ptr uintptr) (r U64) {
	return *(*unalign64)(unsafe.Pointer(ptr))
}

func MEM_readST(tls *libc.TLS, ptr uintptr) (r size_t) {
	return *(*unalignArch)(unsafe.Pointer(ptr))
}

func MEM_write16(tls *libc.TLS, memPtr uintptr, value U16) {
	*(*unalign16)(unsafe.Pointer(memPtr)) = value
}

func MEM_write32(tls *libc.TLS, memPtr uintptr, value U32) {
	*(*unalign32)(unsafe.Pointer(memPtr)) = value
}

func MEM_write64(tls *libc.TLS, memPtr uintptr, value U64) {
	*(*unalign64)(unsafe.Pointer(memPtr)) = value
}

func MEM_swap32_fallback(tls *libc.TLS, in U32) (r U32) {
	return in<<libc.Int32FromInt32(24)&uint32(0xff000000) | in<<libc.Int32FromInt32(8)&uint32(0x00ff0000) | in>>libc.Int32FromInt32(8)&uint32(0x0000ff00) | in>>libc.Int32FromInt32(24)&uint32(0x000000ff)
}

func MEM_swap32(tls *libc.TLS, in U32) (r U32) {
	return libc.X__builtin_bswap32(tls, in)
}

func MEM_swap64_fallback(tls *libc.TLS, in U64) (r U64) {
	return uint64(in<<libc.Int32FromInt32(56))&uint64(0xff00000000000000) | uint64(in<<libc.Int32FromInt32(40))&uint64(0x00ff000000000000) | uint64(in<<libc.Int32FromInt32(24))&uint64(0x0000ff0000000000) | uint64(in<<libc.Int32FromInt32(8))&uint64(0x000000ff00000000) | uint64(in>>libc.Int32FromInt32(8))&uint64(0x00000000ff000000) | uint64(in>>libc.Int32FromInt32(24))&uint64(0x0000000000ff0000) | uint64(in>>libc.Int32FromInt32(40))&uint64(0x000000000000ff00) | uint64(in>>libc.Int32FromInt32(56))&uint64(0x00000000000000ff)
}

func MEM_swap64(tls *libc.TLS, in U64) (r U64) {
	return libc.X__builtin_bswap64(tls, in)
}

func MEM_swapST(tls *libc.TLS, in size_t) (r size_t) {
	if MEM_32bits(tls) != 0 {
		return uint64(MEM_swap32(tls, uint32(in)))
	} else {
		return MEM_swap64(tls, in)
	}
	return r
}

/*=== Little endian r/w ===*/
func MEM_readLE16(tls *libc.TLS, memPtr uintptr) (r U16) {
	var p uintptr
	_ = p
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_read16(tls, memPtr)
	} else {
		p = memPtr
		return libc.Uint16FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(p))) + libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(p + 1)))<<libc.Int32FromInt32(8))
	}
	return r
}

func MEM_writeLE16(tls *libc.TLS, memPtr uintptr, val U16) {
	var p uintptr
	_ = p
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write16(tls, memPtr, val)
	} else {
		p = memPtr
		*(*BYTE)(unsafe.Pointer(p)) = uint8(val)
		*(*BYTE)(unsafe.Pointer(p + 1)) = libc.Uint8FromInt32(libc.Int32FromUint16(val) >> libc.Int32FromInt32(8))
	}
}

func MEM_readLE24(tls *libc.TLS, memPtr uintptr) (r U32) {
	return uint32(MEM_readLE16(tls, memPtr)) + uint32(*(*BYTE)(unsafe.Pointer(memPtr + 2)))<<libc.Int32FromInt32(16)
}

func MEM_writeLE24(tls *libc.TLS, memPtr uintptr, val U32) {
	MEM_writeLE16(tls, memPtr, uint16(val))
	*(*BYTE)(unsafe.Pointer(memPtr + 2)) = uint8(val >> libc.Int32FromInt32(16))
}

func MEM_readLE32(tls *libc.TLS, memPtr uintptr) (r U32) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_read32(tls, memPtr)
	} else {
		return MEM_swap32(tls, MEM_read32(tls, memPtr))
	}
	return r
}

func MEM_writeLE32(tls *libc.TLS, memPtr uintptr, val32 U32) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write32(tls, memPtr, val32)
	} else {
		MEM_write32(tls, memPtr, MEM_swap32(tls, val32))
	}
}

func MEM_readLE64(tls *libc.TLS, memPtr uintptr) (r U64) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_read64(tls, memPtr)
	} else {
		return MEM_swap64(tls, MEM_read64(tls, memPtr))
	}
	return r
}

func MEM_writeLE64(tls *libc.TLS, memPtr uintptr, val64 U64) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write64(tls, memPtr, val64)
	} else {
		MEM_write64(tls, memPtr, MEM_swap64(tls, val64))
	}
}

func MEM_readLEST(tls *libc.TLS, memPtr uintptr) (r size_t) {
	if MEM_32bits(tls) != 0 {
		return uint64(MEM_readLE32(tls, memPtr))
	} else {
		return MEM_readLE64(tls, memPtr)
	}
	return r
}

func MEM_writeLEST(tls *libc.TLS, memPtr uintptr, val size_t) {
	if MEM_32bits(tls) != 0 {
		MEM_writeLE32(tls, memPtr, uint32(val))
	} else {
		MEM_writeLE64(tls, memPtr, val)
	}
}

/*=== Big endian r/w ===*/
func MEM_readBE32(tls *libc.TLS, memPtr uintptr) (r U32) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_swap32(tls, MEM_read32(tls, memPtr))
	} else {
		return MEM_read32(tls, memPtr)
	}
	return r
}

func MEM_writeBE32(tls *libc.TLS, memPtr uintptr, val32 U32) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write32(tls, memPtr, MEM_swap32(tls, val32))
	} else {
		MEM_write32(tls, memPtr, val32)
	}
}

func MEM_readBE64(tls *libc.TLS, memPtr uintptr) (r U64) {
	if MEM_isLittleEndian(tls) != 0 {
		return MEM_swap64(tls, MEM_read64(tls, memPtr))
	} else {
		return MEM_read64(tls, memPtr)
	}
	return r
}

func MEM_writeBE64(tls *libc.TLS, memPtr uintptr, val64 U64) {
	if MEM_isLittleEndian(tls) != 0 {
		MEM_write64(tls, memPtr, MEM_swap64(tls, val64))
	} else {
		MEM_write64(tls, memPtr, val64)
	}
}

func MEM_readBEST(tls *libc.TLS, memPtr uintptr) (r size_t) {
	if MEM_32bits(tls) != 0 {
		return uint64(MEM_readBE32(tls, memPtr))
	} else {
		return MEM_readBE64(tls, memPtr)
	}
	return r
}

func MEM_writeBEST(tls *libc.TLS, memPtr uintptr, val size_t) {
	if MEM_32bits(tls) != 0 {
		MEM_writeBE32(tls, memPtr, uint32(val))
	} else {
		MEM_writeBE64(tls, memPtr, val)
	}
}

// C documentation
//
//	/* code only tested on 32 and 64 bits systems */
func MEM_check(tls *libc.TLS) {
	_ = libc.Uint64FromInt64(1)
}

/**** ended inlining mem.h ****/
/**** start inlining error_private.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* Note : this module is expected to remain private, do not expose it */

/* ****************************************
*  Dependencies
******************************************/
/**** start inlining ../zstd_errors.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* =====   ZSTDERRORLIB_API : control library symbols visibility   ===== */
/* Backwards compatibility with old macro name */

// C documentation
//
//	/*-*********************************************
//	 *  Error codes list
//	 *-*********************************************
//	 *  Error codes _values_ are pinned down since v1.3.1 only.
//	 *  Therefore, don't rely on values if you may link to any version < v1.3.1.
//	 *
//	 *  Only values < 100 are considered stable.
//	 *
//	 *  note 1 : this API shall be used with static linking only.
//	 *           dynamic linking is not yet officially supported.
//	 *  note 2 : Prefer relying on the enum than on its value whenever possible
//	 *           This is the only supported way to use the error list < v1.3.1
//	 *  note 3 : ZSTD_isError() is always correct, whatever the library version.
//	 **********************************************/
type ZSTD_ErrorCode = int32

const ZSTD_error_no_error = 0
const ZSTD_error_GENERIC = 1
const ZSTD_error_prefix_unknown = 10
const ZSTD_error_version_unsupported = 12
const ZSTD_error_frameParameter_unsupported = 14
const ZSTD_error_frameParameter_windowTooLarge = 16
const ZSTD_error_corruption_detected = 20
const ZSTD_error_checksum_wrong = 22
const ZSTD_error_literals_headerWrong = 24
const ZSTD_error_dictionary_corrupted = 30
const ZSTD_error_dictionary_wrong = 32
const ZSTD_error_dictionaryCreation_failed = 34
const ZSTD_error_parameter_unsupported = 40
const ZSTD_error_parameter_combination_unsupported = 41
const ZSTD_error_parameter_outOfBound = 42
const ZSTD_error_tableLog_tooLarge = 44
const ZSTD_error_maxSymbolValue_tooLarge = 46
const ZSTD_error_maxSymbolValue_tooSmall = 48
const ZSTD_error_cannotProduce_uncompressedBlock = 49
const ZSTD_error_stabilityCondition_notRespected = 50
const ZSTD_error_stage_wrong = 60
const ZSTD_error_init_missing = 62
const ZSTD_error_memory_allocation = 64
const ZSTD_error_workSpace_tooSmall = 66
const ZSTD_error_dstSize_tooSmall = 70
const ZSTD_error_srcSize_wrong = 72
const ZSTD_error_dstBuffer_null = 74
const ZSTD_error_noForwardProgress_destFull = 80
const ZSTD_error_noForwardProgress_inputEmpty = 82
const
/* following error codes are __NOT STABLE__, they can be removed or changed in future versions */
ZSTD_error_frameIndex_tooLarge = 100
const ZSTD_error_seekableIO = 102
const ZSTD_error_dstBuffer_wrong = 104
const ZSTD_error_srcBuffer_wrong = 105
const ZSTD_error_sequenceProducer_failed = 106
const ZSTD_error_externalSequences_invalid = 107
const ZSTD_error_maxCode = 120

/**< Same as ZSTD_getErrorName, but using a `ZSTD_ErrorCode` enum argument */

/**** ended inlining ../zstd_errors.h ****/
/**** skipping file: compiler.h ****/
/**** skipping file: debug.h ****/
/**** skipping file: zstd_deps.h ****/

/* ****************************************
*  Compiler-specific
******************************************/

// C documentation
//
//	/*-****************************************
//	*  Customization (error_public.h)
//	******************************************/
type ERR_enum = int32

/*-****************************************
*  Error codes handling
******************************************/
func ERR_isError(tls *libc.TLS, code size_t) (r uint32) {
	return libc.BoolUint32(code > libc.Uint64FromInt32(-int32(ZSTD_error_maxCode)))
}

func ERR_getErrorCode(tls *libc.TLS, code size_t) (r ERR_enum) {
	if !(ERR_isError(tls, code) != 0) {
		return libc.Int32FromInt32(0)
	}
	return libc.Int32FromUint64(libc.Uint64FromInt32(0) - code)
}

/* error_private.c */
func ERR_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorString(tls, ERR_getErrorCode(tls, code))
}

// C documentation
//
//	/**
//	 * Ignore: this is an internal helper.
//	 *
//	 * This is a helper function to help force C99-correctness during compilation.
//	 * Under strict compilation modes, variadic macro arguments can't be empty.
//	 * However, variadic function arguments can be. Using a function therefore lets
//	 * us statically check that at least one (string) argument was passed,
//	 * independent of the compilation flags.
//	 */
func _force_has_format_string(tls *libc.TLS, format uintptr, va uintptr) {
	_ = format
}

// C documentation
//
//	/*! Constructor and Destructor of FSE_CTable.
//	    Note that FSE_CTable size depends on 'tableLog' and 'maxSymbolValue' */
type FSE_CTable = uint32

type FSE_DTable = uint32 /* don't allocate that. It's just a way to be more restrictive than void* */

/*!
Tutorial :
----------
(Note : these functions only decompress FSE-compressed blocks.
 If block is uncompressed, use memcpy() instead
 If block is a single repeated byte, use memset() instead )

The first step is to obtain the normalized frequencies of symbols.
This can be performed by FSE_readNCount() if it was saved using FSE_writeNCount().
'normalizedCounter' must be already allocated, and have at least 'maxSymbolValuePtr[0]+1' cells of signed short.
In practice, that means it's necessary to know 'maxSymbolValue' beforehand,
or size the table to handle worst case situations (typically 256).
FSE_readNCount() will provide 'tableLog' and 'maxSymbolValue'.
The result of FSE_readNCount() is the number of bytes read from 'rBuffer'.
Note that 'rBufferSize' must be at least 4 bytes, even if useful information is less than that.
If there is an error, the function will return an error code, which can be tested using FSE_isError().

The next step is to build the decompression tables 'FSE_DTable' from 'normalizedCounter'.
This is performed by the function FSE_buildDTable().
The space required by 'FSE_DTable' must be already allocated using FSE_createDTable().
If there is an error, the function will return an error code, which can be tested using FSE_isError().

`FSE_DTable` can then be used to decompress `cSrc`, with FSE_decompress_usingDTable().
`cSrcSize` must be strictly correct, otherwise decompression will fail.
FSE_decompress_usingDTable() result will tell how many bytes were regenerated (<=`dstCapacity`).
If there is an error, the function will return an error code, which can be tested using FSE_isError(). (ex: dst buffer too small)
*/

/**** start inlining bitstream.h ****/
/* ******************************************************************
 * bitstream
 * Part of FSE library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * You can contact the author at :
 * - Source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/*
*  This API consists of small unitary functions, which must be inlined for best performance.
*  Since link-time-optimization is not available for all compilers,
*  these functions are defined into a .h to be included.
 */

/*-****************************************
*  Dependencies
******************************************/
/**** skipping file: mem.h ****/
/**** skipping file: compiler.h ****/
/**** skipping file: debug.h ****/
/**** skipping file: error_private.h ****/
/**** start inlining bits.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: mem.h ****/
func ZSTD_countTrailingZeros32_fallback(tls *libc.TLS, val U32) (r uint32) {
	return DeBruijnBytePos[val&libc.Uint32FromInt32(-libc.Int32FromUint32(val))*libc.Uint32FromUint32(0x077CB531)>>int32(27)]
	return r
}

var DeBruijnBytePos = [32]U32{
	1:  uint32(1),
	2:  uint32(28),
	3:  uint32(2),
	4:  uint32(29),
	5:  uint32(14),
	6:  uint32(24),
	7:  uint32(3),
	8:  uint32(30),
	9:  uint32(22),
	10: uint32(20),
	11: uint32(15),
	12: uint32(25),
	13: uint32(17),
	14: uint32(4),
	15: uint32(8),
	16: uint32(31),
	17: uint32(27),
	18: uint32(13),
	19: uint32(23),
	20: uint32(21),
	21: uint32(19),
	22: uint32(16),
	23: uint32(7),
	24: uint32(26),
	25: uint32(12),
	26: uint32(18),
	27: uint32(6),
	28: uint32(11),
	29: uint32(5),
	30: uint32(10),
	31: uint32(9),
}

func ZSTD_countTrailingZeros32(tls *libc.TLS, val U32) (r uint32) {
	return libc.Uint32FromInt32(libc.X__builtin_ctz(tls, val))
}

func ZSTD_countLeadingZeros32_fallback(tls *libc.TLS, val U32) (r uint32) {
	val = val | val>>int32(1)
	val = val | val>>int32(2)
	val = val | val>>int32(4)
	val = val | val>>int32(8)
	val = val | val>>int32(16)
	return uint32(31) - DeBruijnClz[val*uint32(0x07C4ACDD)>>int32(27)]
	return r
}

var DeBruijnClz = [32]U32{
	1:  uint32(9),
	2:  uint32(1),
	3:  uint32(10),
	4:  uint32(13),
	5:  uint32(21),
	6:  uint32(2),
	7:  uint32(29),
	8:  uint32(11),
	9:  uint32(14),
	10: uint32(16),
	11: uint32(18),
	12: uint32(22),
	13: uint32(25),
	14: uint32(3),
	15: uint32(30),
	16: uint32(8),
	17: uint32(12),
	18: uint32(20),
	19: uint32(28),
	20: uint32(15),
	21: uint32(17),
	22: uint32(24),
	23: uint32(7),
	24: uint32(19),
	25: uint32(27),
	26: uint32(23),
	27: uint32(6),
	28: uint32(26),
	29: uint32(5),
	30: uint32(4),
	31: uint32(31),
}

func ZSTD_countLeadingZeros32(tls *libc.TLS, val U32) (r uint32) {
	return libc.Uint32FromInt32(libc.X__builtin_clz(tls, val))
}

func ZSTD_countTrailingZeros64(tls *libc.TLS, val U64) (r uint32) {
	var leastSignificantWord, mostSignificantWord U32
	_, _ = leastSignificantWord, mostSignificantWord
	mostSignificantWord = uint32(val >> libc.Int32FromInt32(32))
	leastSignificantWord = uint32(val)
	if leastSignificantWord == uint32(0) {
		return uint32(32) + ZSTD_countTrailingZeros32(tls, mostSignificantWord)
	} else {
		return ZSTD_countTrailingZeros32(tls, leastSignificantWord)
	}
	return r
}

func ZSTD_countLeadingZeros64(tls *libc.TLS, val U64) (r uint32) {
	return libc.Uint32FromInt32(libc.X__builtin_clzll(tls, val))
}

func ZSTD_NbCommonBytes(tls *libc.TLS, val size_t) (r uint32) {
	if MEM_isLittleEndian(tls) != 0 {
		if MEM_64bits(tls) != 0 {
			return ZSTD_countTrailingZeros64(tls, val) >> int32(3)
		} else {
			return ZSTD_countTrailingZeros32(tls, uint32(val)) >> int32(3)
		}
	} else { /* Big Endian CPU */
		if MEM_64bits(tls) != 0 {
			return ZSTD_countLeadingZeros64(tls, val) >> int32(3)
		} else {
			return ZSTD_countLeadingZeros32(tls, uint32(val)) >> int32(3)
		}
	}
	return r
}

func ZSTD_highbit32(tls *libc.TLS, val U32) (r uint32) {
	/* compress, dictBuilder, decodeCorpus */
	return uint32(31) - ZSTD_countLeadingZeros32(tls, val)
}

// C documentation
//
//	/* ZSTD_rotateRight_*():
//	 * Rotates a bitfield to the right by "count" bits.
//	 * https://en.wikipedia.org/w/index.php?title=Circular_shift&oldid=991635599#Implementing_circular_shifts
//	 */
func ZSTD_rotateRight_U64(tls *libc.TLS, value U64, count U32) (r U64) {
	count = count & uint32(0x3F) /* for fickle pattern recognition */
	return value>>count | value<<((libc.Uint32FromUint32(0)-count)&libc.Uint32FromInt32(0x3F))
}

func ZSTD_rotateRight_U32(tls *libc.TLS, value U32, count U32) (r U32) {
	count = count & uint32(0x1F) /* for fickle pattern recognition */
	return value>>count | value<<((libc.Uint32FromUint32(0)-count)&libc.Uint32FromInt32(0x1F))
}

func ZSTD_rotateRight_U16(tls *libc.TLS, value U16, count U32) (r U16) {
	count = count & uint32(0x0F) /* for fickle pattern recognition */
	return libc.Uint16FromInt32(libc.Int32FromUint16(value)>>count | libc.Int32FromUint16(libc.Uint16FromInt32(libc.Int32FromUint16(value)<<((libc.Uint32FromUint32(0)-count)&libc.Uint32FromInt32(0x0F)))))
}

/**** ended inlining bits.h ****/

/*=========================================
*  Target specific
=========================================*/

// C documentation
//
//	/*-******************************************
//	*  bitStream encoding API (write forward)
//	********************************************/
type BitContainerType = uint64

// C documentation
//
//	/* bitStream can mix input from multiple sources.
//	 * A critical property of these streams is that they encode and decode in **reverse** direction.
//	 * So the first bit sequence you add will be the last to be read, like a LIFO stack.
//	 */
type BIT_CStream_t = struct {
	FbitContainer BitContainerType
	FbitPos       uint32
	FstartPtr     uintptr
	Fptr          uintptr
	FendPtr       uintptr
}

/* Start with initCStream, providing the size of buffer to write into.
*  bitStream will never write outside of this buffer.
*  `dstCapacity` must be >= sizeof(bitD->bitContainer), otherwise @return will be an error code.
*
*  bits are first added to a local register.
*  Local register is BitContainerType, 64-bits on 64-bits systems, or 32-bits on 32-bits systems.
*  Writing data into memory is an explicit operation, performed by the flushBits function.
*  Hence keep track how many bits are potentially stored into local register to avoid register overflow.
*  After a flushBits, a maximum of 7 bits might still be stored into local register.
*
*  Avoid storing elements of more than 24 bits if you want compatibility with 32-bits bitstream readers.
*
*  Last operation is to close the bitStream.
*  The function returns the final size of CStream in bytes.
*  If data couldn't fit into `dstBuffer`, it will return a 0 ( == not storable)
 */

// C documentation
//
//	/*-********************************************
//	*  bitStream decoding API (read backward)
//	**********************************************/
type BIT_DStream_t = struct {
	FbitContainer BitContainerType
	FbitsConsumed uint32
	Fptr          uintptr
	Fstart        uintptr
	FlimitPtr     uintptr
}

type BIT_DStream_status = int32

const BIT_DStream_unfinished = 0
const /* fully refilled */
BIT_DStream_endOfBuffer = 1
const /* still some bits left in bitstream */
BIT_DStream_completed = 2
const /* bitstream entirely consumed, bit-exact */
BIT_DStream_overflow = 3

/* faster, but works only if nbBits >= 1 */

// C documentation
//
//	/*=====    Local Constants   =====*/
var BIT_mask = [32]uint32{
	1:  uint32(1),
	2:  uint32(3),
	3:  uint32(7),
	4:  uint32(0xF),
	5:  uint32(0x1F),
	6:  uint32(0x3F),
	7:  uint32(0x7F),
	8:  uint32(0xFF),
	9:  uint32(0x1FF),
	10: uint32(0x3FF),
	11: uint32(0x7FF),
	12: uint32(0xFFF),
	13: uint32(0x1FFF),
	14: uint32(0x3FFF),
	15: uint32(0x7FFF),
	16: uint32(0xFFFF),
	17: uint32(0x1FFFF),
	18: uint32(0x3FFFF),
	19: uint32(0x7FFFF),
	20: uint32(0xFFFFF),
	21: uint32(0x1FFFFF),
	22: uint32(0x3FFFFF),
	23: uint32(0x7FFFFF),
	24: uint32(0xFFFFFF),
	25: uint32(0x1FFFFFF),
	26: uint32(0x3FFFFFF),
	27: uint32(0x7FFFFFF),
	28: uint32(0xFFFFFFF),
	29: uint32(0x1FFFFFFF),
	30: uint32(0x3FFFFFFF),
	31: uint32(0x7FFFFFFF),
} /* up to 31 bits */

// C documentation
//
//	/*-**************************************************************
//	*  bitStream encoding
//	****************************************************************/
//	/*! BIT_initCStream() :
//	 *  `dstCapacity` must be > sizeof(size_t)
//	 *  @return : 0 if success,
//	 *            otherwise an error code (can be tested using ERR_isError()) */
func BIT_initCStream(tls *libc.TLS, bitC uintptr, startPtr uintptr, dstCapacity size_t) (r size_t) {
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitContainer = uint64(0)
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos = uint32(0)
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr = startPtr
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr
	(*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr = (*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr + uintptr(dstCapacity) - uintptr(8)
	if dstCapacity <= uint64(8) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	return uint64(0)
}

func BIT_getLowerBits(tls *libc.TLS, bitContainer BitContainerType, nbBits U32) (r BitContainerType) {
	return bitContainer & uint64(BIT_mask[nbBits])
}

// C documentation
//
//	/*! BIT_addBits() :
//	 *  can add up to 31 bits into `bitC`.
//	 *  Note : does not check for register overflow ! */
func BIT_addBits(tls *libc.TLS, bitC uintptr, value BitContainerType, nbBits uint32) {
	_ = libc.Uint64FromInt64(1)
	*(*BitContainerType)(unsafe.Pointer(bitC)) |= BIT_getLowerBits(tls, value, nbBits) << (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos
	*(*uint32)(unsafe.Pointer(bitC + 8)) += nbBits
}

// C documentation
//
//	/*! BIT_addBitsFast() :
//	 *  works only if `value` is _clean_,
//	 *  meaning all high bits above nbBits are 0 */
func BIT_addBitsFast(tls *libc.TLS, bitC uintptr, value BitContainerType, nbBits uint32) {
	*(*BitContainerType)(unsafe.Pointer(bitC)) |= value << (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos
	*(*uint32)(unsafe.Pointer(bitC + 8)) += nbBits
}

// C documentation
//
//	/*! BIT_flushBitsFast() :
//	 *  assumption : bitContainer has not overflowed
//	 *  unsafe version; does not check buffer overflow */
func BIT_flushBitsFast(tls *libc.TLS, bitC uintptr) {
	var nbBytes size_t
	_ = nbBytes
	nbBytes = uint64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos >> int32(3))
	MEM_writeLEST(tls, (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr, (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitContainer)
	*(*uintptr)(unsafe.Pointer(bitC + 24)) += uintptr(nbBytes)
	*(*uint32)(unsafe.Pointer(bitC + 8)) &= uint32(7)
	*(*BitContainerType)(unsafe.Pointer(bitC)) >>= nbBytes * uint64(8)
}

// C documentation
//
//	/*! BIT_flushBits() :
//	 *  assumption : bitContainer has not overflowed
//	 *  safe version; check for buffer overflow, and prevents it.
//	 *  note : does not signal buffer overflow.
//	 *  overflow will be revealed later on using BIT_closeCStream() */
func BIT_flushBits(tls *libc.TLS, bitC uintptr) {
	var nbBytes size_t
	_ = nbBytes
	nbBytes = uint64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos >> int32(3))
	MEM_writeLEST(tls, (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr, (*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitContainer)
	*(*uintptr)(unsafe.Pointer(bitC + 24)) += uintptr(nbBytes)
	if (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr > (*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		(*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr
	}
	*(*uint32)(unsafe.Pointer(bitC + 8)) &= uint32(7)
	*(*BitContainerType)(unsafe.Pointer(bitC)) >>= nbBytes * uint64(8)
}

// C documentation
//
//	/*! BIT_closeCStream() :
//	 *  @return : size of CStream, in bytes,
//	 *            or 0 if it could not fit into dstBuffer */
func BIT_closeCStream(tls *libc.TLS, bitC uintptr) (r size_t) {
	BIT_addBitsFast(tls, bitC, uint64(1), uint32(1)) /* endMark */
	BIT_flushBits(tls, bitC)
	if (*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr >= (*BIT_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		return uint64(0)
	} /* overflow detected */
	return libc.Uint64FromInt64(int64((*BIT_CStream_t)(unsafe.Pointer(bitC)).Fptr)-int64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FstartPtr)) + libc.BoolUint64((*BIT_CStream_t)(unsafe.Pointer(bitC)).FbitPos > libc.Uint32FromInt32(0))
}

// C documentation
//
//	/*-********************************************************
//	*  bitStream decoding
//	**********************************************************/
//	/*! BIT_initDStream() :
//	 *  Initialize a BIT_DStream_t.
//	 * `bitD` : a pointer to an already allocated BIT_DStream_t structure.
//	 * `srcSize` must be the *exact* size of the bitStream, in bytes.
//	 * @return : size of stream (== srcSize), or an errorCode if a problem is detected
//	 */
func BIT_initDStream(tls *libc.TLS, bitD uintptr, srcBuffer uintptr, srcSize size_t) (r size_t) {
	var lastByte, lastByte1 BYTE
	var v1 uint32
	_, _, _ = lastByte, lastByte1, v1
	if srcSize < uint64(1) {
		libc.Xmemset(tls, bitD, 0, libc.Uint64FromInt64(40))
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart = srcBuffer
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).FlimitPtr = (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart + uintptr(8)
	if srcSize >= uint64(8) { /* normal case */
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr = srcBuffer + uintptr(srcSize) - uintptr(8)
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = MEM_readLEST(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr)
		lastByte = *(*BYTE)(unsafe.Pointer(srcBuffer + uintptr(srcSize-uint64(1))))
		if lastByte != 0 {
			v1 = uint32(8) - ZSTD_highbit32(tls, uint32(lastByte))
		} else {
			v1 = uint32(0)
		}
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed = v1 /* ensures bitsConsumed is always set */
		if libc.Int32FromUint8(lastByte) == 0 {
			return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		} /* endMark not present */
	} else {
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr = (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = uint64(*(*BYTE)(unsafe.Pointer((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart)))
		switch srcSize {
		case uint64(7):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 6))) << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(16))
			fallthrough
		case uint64(6):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 5))) << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(24))
			fallthrough
		case uint64(5):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 4))) << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(32))
			fallthrough
		case uint64(4):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 3))) << int32(24)
			fallthrough
		case uint64(3):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 2))) << int32(16)
			fallthrough
		case uint64(2):
			*(*BitContainerType)(unsafe.Pointer(bitD)) += uint64(*(*BYTE)(unsafe.Pointer(srcBuffer + 1))) << int32(8)
			fallthrough
		default:
			break
		}
		lastByte1 = *(*BYTE)(unsafe.Pointer(srcBuffer + uintptr(srcSize-uint64(1))))
		if lastByte1 != 0 {
			v1 = uint32(8) - ZSTD_highbit32(tls, uint32(lastByte1))
		} else {
			v1 = uint32(0)
		}
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed = v1
		if libc.Int32FromUint8(lastByte1) == 0 {
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		} /* endMark not present */
		*(*uint32)(unsafe.Pointer(bitD + 8)) += uint32(libc.Uint64FromInt64(8)-srcSize) * uint32(8)
	}
	return srcSize
}

func BIT_getUpperBits(tls *libc.TLS, bitContainer BitContainerType, start U32) (r BitContainerType) {
	return bitContainer >> start
}

func BIT_getMiddleBits(tls *libc.TLS, bitContainer BitContainerType, start U32, nbBits U32) (r BitContainerType) {
	var regMask U32
	_ = regMask
	regMask = uint32(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(1))
	/* if start > regMask, bitstream is corrupted, and result is undefined */
	/* x86 transform & ((1 << nbBits) - 1) to bzhi instruction, it is better
	 * than accessing memory. When bmi2 instruction is not present, we consider
	 * such cpus old (pre-Haswell, 2013) and their performance is not of that
	 * importance.
	 */
	return bitContainer >> (start & regMask) & (libc.Uint64FromInt32(1)<<nbBits - uint64(1))
}

// C documentation
//
//	/*! BIT_lookBits() :
//	 *  Provides next n bits from local register.
//	 *  local register is not modified.
//	 *  On 32-bits, maxNbBits==24.
//	 *  On 64-bits, maxNbBits==56.
//	 * @return : value extracted */
func BIT_lookBits(tls *libc.TLS, bitD uintptr, nbBits U32) (r BitContainerType) {
	/* arbitrate between double-shift and shift+mask */
	/* if bitD->bitsConsumed + nbBits > sizeof(bitD->bitContainer)*8,
	 * bitstream is likely corrupted, and result is undefined */
	return BIT_getMiddleBits(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer, uint32(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8)-uint64((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed)-uint64(nbBits)), nbBits)
}

// C documentation
//
//	/*! BIT_lookBitsFast() :
//	 *  unsafe version; only works if nbBits >= 1 */
func BIT_lookBitsFast(tls *libc.TLS, bitD uintptr, nbBits U32) (r BitContainerType) {
	var regMask U32
	_ = regMask
	regMask = uint32(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - libc.Uint64FromInt32(1))
	return (*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer << ((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed & regMask) >> ((regMask + uint32(1) - nbBits) & regMask)
}

func BIT_skipBits(tls *libc.TLS, bitD uintptr, nbBits U32) {
	*(*uint32)(unsafe.Pointer(bitD + 8)) += nbBits
}

// C documentation
//
//	/*! BIT_readBits() :
//	 *  Read (consume) next n bits from local register and update.
//	 *  Pay attention to not read more than nbBits contained into local register.
//	 * @return : extracted value. */
func BIT_readBits(tls *libc.TLS, bitD uintptr, nbBits uint32) (r BitContainerType) {
	var value BitContainerType
	_ = value
	value = BIT_lookBits(tls, bitD, nbBits)
	BIT_skipBits(tls, bitD, nbBits)
	return value
}

// C documentation
//
//	/*! BIT_readBitsFast() :
//	 *  unsafe version; only works if nbBits >= 1 */
func BIT_readBitsFast(tls *libc.TLS, bitD uintptr, nbBits uint32) (r BitContainerType) {
	var value BitContainerType
	_ = value
	value = BIT_lookBitsFast(tls, bitD, nbBits)
	BIT_skipBits(tls, bitD, nbBits)
	return value
}

// C documentation
//
//	/*! BIT_reloadDStream_internal() :
//	 *  Simple variant of BIT_reloadDStream(), with two conditions:
//	 *  1. bitstream is valid : bitsConsumed <= sizeof(bitD->bitContainer)*8
//	 *  2. look window is valid after shifted down : bitD->ptr >= bitD->start
//	 */
func BIT_reloadDStream_internal(tls *libc.TLS, bitD uintptr) (r BIT_DStream_status) {
	*(*uintptr)(unsafe.Pointer(bitD + 16)) -= uintptr((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed >> int32(3))
	*(*uint32)(unsafe.Pointer(bitD + 8)) &= uint32(7)
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = MEM_readLEST(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr)
	return int32(BIT_DStream_unfinished)
}

// C documentation
//
//	/*! BIT_reloadDStreamFast() :
//	 *  Similar to BIT_reloadDStream(), but with two differences:
//	 *  1. bitsConsumed <= sizeof(bitD->bitContainer)*8 must hold!
//	 *  2. Returns BIT_DStream_overflow when bitD->ptr < bitD->limitPtr, at this
//	 *     point you must use BIT_reloadDStream() to reload.
//	 */
func BIT_reloadDStreamFast(tls *libc.TLS, bitD uintptr) (r BIT_DStream_status) {
	if libc.BoolInt64((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr < (*BIT_DStream_t)(unsafe.Pointer(bitD)).FlimitPtr) != 0 {
		return int32(BIT_DStream_overflow)
	}
	return BIT_reloadDStream_internal(tls, bitD)
}

// C documentation
//
//	/*! BIT_reloadDStream() :
//	 *  Refill `bitD` from buffer previously set in BIT_initDStream() .
//	 *  This function is safe, it guarantees it will not never beyond src buffer.
//	 * @return : status of `BIT_DStream_t` internal register.
//	 *           when status == BIT_DStream_unfinished, internal register is filled with at least 25 or 57 bits */
func BIT_reloadDStream(tls *libc.TLS, bitD uintptr) (r BIT_DStream_status) {
	var nbBytes U32
	var result BIT_DStream_status
	_, _ = nbBytes, result
	/* note : once in overflow mode, a bitstream remains in this mode until it's reset */
	if libc.BoolInt64(uint64((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8)) != 0 {
		(*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr = uintptr(unsafe.Pointer(&zeroFilled)) /* aliasing is allowed for char */
		/* overflow detected, erroneous scenario or end of stream: no update */
		return int32(BIT_DStream_overflow)
	}
	if (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr >= (*BIT_DStream_t)(unsafe.Pointer(bitD)).FlimitPtr {
		return BIT_reloadDStream_internal(tls, bitD)
	}
	if (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr == (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart {
		/* reached end of bitStream => no update */
		if uint64((*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed) < libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) {
			return int32(BIT_DStream_endOfBuffer)
		}
		return int32(BIT_DStream_completed)
	}
	/* start < ptr < limitPtr => cautious update */
	nbBytes = (*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitsConsumed >> int32(3)
	result = int32(BIT_DStream_unfinished)
	if (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr-uintptr(nbBytes) < (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart {
		nbBytes = libc.Uint32FromInt64(int64((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr) - int64((*BIT_DStream_t)(unsafe.Pointer(bitD)).Fstart)) /* ptr > start */
		result = int32(BIT_DStream_endOfBuffer)
	}
	*(*uintptr)(unsafe.Pointer(bitD + 16)) -= uintptr(nbBytes)
	*(*uint32)(unsafe.Pointer(bitD + 8)) -= nbBytes * uint32(8)
	(*BIT_DStream_t)(unsafe.Pointer(bitD)).FbitContainer = MEM_readLEST(tls, (*BIT_DStream_t)(unsafe.Pointer(bitD)).Fptr) /* reminder : srcSize > sizeof(bitD->bitContainer), otherwise bitD->ptr == bitD->start */
	return result
	return r
}

var zeroFilled BitContainerType

// C documentation
//
//	/*! BIT_endOfDStream() :
//	 * @return : 1 if DStream has _exactly_ reached its end (all bits consumed).
//	 */
func BIT_endOfDStream(tls *libc.TLS, DStream uintptr) (r uint32) {
	return libc.BoolUint32((*BIT_DStream_t)(unsafe.Pointer(DStream)).Fptr == (*BIT_DStream_t)(unsafe.Pointer(DStream)).Fstart && uint64((*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed) == libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8))
}

/**< same as FSE_decompress(), using an externally allocated `workSpace` produced with `FSE_DECOMPRESS_WKSP_SIZE_U32(maxLog, maxSymbolValue)`.
 * Set bmi2 to 1 if your CPU supports BMI2 or 0 if it doesn't */

type FSE_repeat = int32

const FSE_repeat_none = 0
const /**< Cannot use the previous table */
FSE_repeat_check = 1
const /**< Can use the previous table but it must be checked */
FSE_repeat_valid = 2

// C documentation
//
//	/* *****************************************
//	*  FSE symbol compression API
//	*******************************************/
//	/*!
//	   This API consists of small unitary functions, which highly benefit from being inlined.
//	   Hence their body are included in next section.
//	*/
type FSE_CState_t = struct {
	Fvalue      ptrdiff_t
	FstateTable uintptr
	FsymbolTT   uintptr
	FstateLog   uint32
}

/**<
These functions are inner components of FSE_compress_usingCTable().
They allow the creation of custom streams, mixing multiple tables and bit sources.

A key property to keep in mind is that encoding and decoding are done **in reverse direction**.
So the first symbol you will encode is the last you will decode, like a LIFO stack.

You will need a few variables to track your CStream. They are :

FSE_CTable    ct;         // Provided by FSE_buildCTable()
BIT_CStream_t bitStream;  // bitStream tracking structure
FSE_CState_t  state;      // State tracking structure (can have several)


The first thing to do is to init bitStream and state.
    size_t errorCode = BIT_initCStream(&bitStream, dstBuffer, maxDstSize);
    FSE_initCState(&state, ct);

Note that BIT_initCStream() can produce an error code, so its result should be tested, using FSE_isError();
You can then encode your input data, byte after byte.
FSE_encodeSymbol() outputs a maximum of 'tableLog' bits at a time.
Remember decoding will be done in reverse direction.
    FSE_encodeByte(&bitStream, &state, symbol);

At any time, you can also add any bit sequence.
Note : maximum allowed nbBits is 25, for compatibility with 32-bits decoders
    BIT_addBits(&bitStream, bitField, nbBits);

The above methods don't commit data to memory, they just store it into local register, for speed.
Local register size is 64-bits on 64-bits systems, 32-bits on 32-bits systems (size_t).
Writing data to memory is a manual operation, performed by the flushBits function.
    BIT_flushBits(&bitStream);

Your last FSE encoding operation shall be to flush your last state value(s).
    FSE_flushState(&bitStream, &state);

Finally, you must close the bitStream.
The function returns the size of CStream in bytes.
If data couldn't fit into dstBuffer, it will return a 0 ( == not compressible)
If there is an error, it returns an errorCode (which can be tested using FSE_isError()).
    size_t size = BIT_closeCStream(&bitStream);
*/

// C documentation
//
//	/* *****************************************
//	*  FSE symbol decompression API
//	*******************************************/
type FSE_DState_t = struct {
	Fstate size_t
	Ftable uintptr
}

/* faster, but works only if nbBits is always >= 1 (otherwise, result will be corrupted) */

// C documentation
//
//	/* *****************************************
//	*  Implementation of inlined functions
//	*******************************************/
type FSE_symbolCompressionTransform = struct {
	FdeltaFindState int32
	FdeltaNbBits    U32
} /* total 8 bytes */
func FSE_initCState(tls *libc.TLS, statePtr uintptr, ct uintptr) {
	var ptr, u16ptr uintptr
	var tableLog U32
	var v1 int32
	_, _, _, _ = ptr, tableLog, u16ptr, v1
	ptr = ct
	u16ptr = ptr
	tableLog = uint32(MEM_read16(tls, ptr))
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = libc.Int64FromInt32(1) << tableLog
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateTable = u16ptr + uintptr(2)*2
	if tableLog != 0 {
		v1 = int32(1) << (tableLog - uint32(1))
	} else {
		v1 = int32(1)
	}
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).FsymbolTT = ct + uintptr(1)*4 + uintptr(v1)*4
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateLog = tableLog
}

// C documentation
//
//	/*! FSE_initCState2() :
//	*   Same as FSE_initCState(), but the first symbol to include (which will be the last to be read)
//	*   uses the smallest state value possible, saving the cost of this symbol */
func FSE_initCState2(tls *libc.TLS, statePtr uintptr, ct uintptr, symbol U32) {
	var nbBitsOut U32
	var stateTable uintptr
	var symbolTT FSE_symbolCompressionTransform
	_, _, _ = nbBitsOut, stateTable, symbolTT
	FSE_initCState(tls, statePtr, ct)
	symbolTT = *(*FSE_symbolCompressionTransform)(unsafe.Pointer((*FSE_CState_t)(unsafe.Pointer(statePtr)).FsymbolTT + uintptr(symbol)*8))
	stateTable = (*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateTable
	nbBitsOut = (symbolTT.FdeltaNbBits + libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(15))) >> libc.Int32FromInt32(16)
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = libc.Int64FromUint32(nbBitsOut<<libc.Int32FromInt32(16) - symbolTT.FdeltaNbBits)
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = libc.Int64FromUint16(*(*U16)(unsafe.Pointer(stateTable + uintptr((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue>>nbBitsOut+int64(symbolTT.FdeltaFindState))*2)))
}

func FSE_encodeSymbol(tls *libc.TLS, bitC uintptr, statePtr uintptr, symbol uint32) {
	var nbBitsOut U32
	var stateTable uintptr
	var symbolTT FSE_symbolCompressionTransform
	_, _, _ = nbBitsOut, stateTable, symbolTT
	symbolTT = *(*FSE_symbolCompressionTransform)(unsafe.Pointer((*FSE_CState_t)(unsafe.Pointer(statePtr)).FsymbolTT + uintptr(symbol)*8))
	stateTable = (*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateTable
	nbBitsOut = libc.Uint32FromInt64(((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue + libc.Int64FromUint32(symbolTT.FdeltaNbBits)) >> libc.Int32FromInt32(16))
	BIT_addBits(tls, bitC, libc.Uint64FromInt64((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue), nbBitsOut)
	(*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue = libc.Int64FromUint16(*(*U16)(unsafe.Pointer(stateTable + uintptr((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue>>nbBitsOut+int64(symbolTT.FdeltaFindState))*2)))
}

func FSE_flushCState(tls *libc.TLS, bitC uintptr, statePtr uintptr) {
	BIT_addBits(tls, bitC, libc.Uint64FromInt64((*FSE_CState_t)(unsafe.Pointer(statePtr)).Fvalue), (*FSE_CState_t)(unsafe.Pointer(statePtr)).FstateLog)
	BIT_flushBits(tls, bitC)
}

// C documentation
//
//	/* FSE_getMaxNbBits() :
//	 * Approximate maximum cost of a symbol, in bits.
//	 * Fractional get rounded up (i.e. a symbol with a normalized frequency of 3 gives the same result as a frequency of 2)
//	 * note 1 : assume symbolValue is valid (<= maxSymbolValue)
//	 * note 2 : if freq[symbolValue]==0, @return a fake cost of tableLog+1 bits */
func FSE_getMaxNbBits(tls *libc.TLS, symbolTTPtr uintptr, symbolValue U32) (r U32) {
	var symbolTT uintptr
	_ = symbolTT
	symbolTT = symbolTTPtr
	return ((*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits + libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)-libc.Int32FromInt32(1))) >> int32(16)
}

// C documentation
//
//	/* FSE_bitCost() :
//	 * Approximate symbol cost, as fractional value, using fixed-point format (accuracyLog fractional bits)
//	 * note 1 : assume symbolValue is valid (<= maxSymbolValue)
//	 * note 2 : if freq[symbolValue]==0, @return a fake cost of tableLog+1 bits */
func FSE_bitCost(tls *libc.TLS, symbolTTPtr uintptr, tableLog U32, symbolValue U32, accuracyLog U32) (r U32) {
	var bitMultiplier, deltaFromThreshold, minNbBits, normalizedDeltaFromThreshold, tableSize, threshold U32
	var symbolTT uintptr
	_, _, _, _, _, _, _ = bitMultiplier, deltaFromThreshold, minNbBits, normalizedDeltaFromThreshold, symbolTT, tableSize, threshold
	symbolTT = symbolTTPtr
	minNbBits = (*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits >> int32(16)
	threshold = (minNbBits + uint32(1)) << int32(16)
	/* ensure enough room for renormalization double shift */
	tableSize = libc.Uint32FromInt32(int32(1) << tableLog)
	deltaFromThreshold = threshold - ((*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits + tableSize)
	normalizedDeltaFromThreshold = deltaFromThreshold << accuracyLog >> tableLog /* linear interpolation (very approximate) */
	bitMultiplier = libc.Uint32FromInt32(int32(1) << accuracyLog)
	return (minNbBits+uint32(1))*bitMultiplier - normalizedDeltaFromThreshold
	return r
}

/* ======    Decompression    ====== */

type FSE_DTableHeader = struct {
	FtableLog U16
	FfastMode U16
}

/* sizeof U32 */

type FSE_decode_t = struct {
	FnewState uint16
	Fsymbol   uint8
	FnbBits   uint8
} /* size == U32 */
func FSE_initDState(tls *libc.TLS, DStatePtr uintptr, bitD uintptr, dt uintptr) {
	var DTableH, ptr uintptr
	_, _ = DTableH, ptr
	ptr = dt
	DTableH = ptr
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = BIT_readBits(tls, bitD, uint32((*FSE_DTableHeader)(unsafe.Pointer(DTableH)).FtableLog))
	BIT_reloadDStream(tls, bitD)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable = dt + uintptr(1)*4
}

func FSE_peekSymbol(tls *libc.TLS, DStatePtr uintptr) (r BYTE) {
	var DInfo FSE_decode_t
	_ = DInfo
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	return DInfo.Fsymbol
}

func FSE_updateState(tls *libc.TLS, DStatePtr uintptr, bitD uintptr) {
	var DInfo FSE_decode_t
	var lowBits size_t
	var nbBits U32
	_, _, _ = DInfo, lowBits, nbBits
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	nbBits = uint32(DInfo.FnbBits)
	lowBits = BIT_readBits(tls, bitD, nbBits)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = uint64(DInfo.FnewState) + lowBits
}

func FSE_decodeSymbol(tls *libc.TLS, DStatePtr uintptr, bitD uintptr) (r BYTE) {
	var DInfo FSE_decode_t
	var lowBits size_t
	var nbBits U32
	var symbol BYTE
	_, _, _, _ = DInfo, lowBits, nbBits, symbol
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	nbBits = uint32(DInfo.FnbBits)
	symbol = DInfo.Fsymbol
	lowBits = BIT_readBits(tls, bitD, nbBits)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = uint64(DInfo.FnewState) + lowBits
	return symbol
}

// C documentation
//
//	/*! FSE_decodeSymbolFast() :
//	    unsafe, only works if no symbol has a probability > 50% */
func FSE_decodeSymbolFast(tls *libc.TLS, DStatePtr uintptr, bitD uintptr) (r BYTE) {
	var DInfo FSE_decode_t
	var lowBits size_t
	var nbBits U32
	var symbol BYTE
	_, _, _, _ = DInfo, lowBits, nbBits, symbol
	DInfo = *(*FSE_decode_t)(unsafe.Pointer((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Ftable + uintptr((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate)*4))
	nbBits = uint32(DInfo.FnbBits)
	symbol = DInfo.Fsymbol
	lowBits = BIT_readBitsFast(tls, bitD, nbBits)
	(*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate = uint64(DInfo.FnewState) + lowBits
	return symbol
}

func FSE_endOfDState(tls *libc.TLS, DStatePtr uintptr) (r uint32) {
	return libc.BoolUint32((*FSE_DState_t)(unsafe.Pointer(DStatePtr)).Fstate == uint64(0))
}

/**< provides error code string (useful for debugging) */

/* *** Constants *** */

/* ****************************************
*  Static allocation
******************************************/
/* HUF buffer bounds */

// C documentation
//
//	/* static allocation of HUF's Compression Table */
//	/* this is a private definition, just exposed for allocation and strict aliasing purpose. never EVER access its members directly */
type HUF_CElt = uint64

/* consider it an incomplete type */

// C documentation
//
//	/* static allocation of HUF's DTable */
type HUF_DTable = uint32

/* ****************************************
*  Advanced decompression functions
******************************************/

// C documentation
//
//	/**
//	 * Huffman flags bitset.
//	 * For all flags, 0 is the default value.
//	 */
type HUF_flags_e = int32

const
/**
 * If compiled with DYNAMIC_BMI2: Set flag only if the CPU supports BMI2 at runtime.
 * Otherwise: Ignored.
 */
HUF_flags_bmi2 = 1
const
/**
 * If set: Test possible table depths to find the one that produces the smallest header + encoded size.
 * If unset: Use heuristic to find the table depth.
 */
HUF_flags_optimalDepth = 2
const
/**
 * If set: If the previous table can encode the input, always reuse the previous table.
 * If unset: If the previous table can encode the input, reuse the previous table if it results in a smaller output.
 */
HUF_flags_preferRepeat = 4
const
/**
 * If set: Sample the input and check if the sample is uncompressible, if it is then don't attempt to compress.
 * If unset: Always histogram the entire input.
 */
HUF_flags_suspectUncompressible = 8
const
/**
 * If set: Don't use assembly implementations
 * If unset: Allow using assembly implementations
 */
HUF_flags_disableAsm = 16
const
/**
 * If set: Don't use the fast decoding loop, always use the fallback decoding loop.
 * If unset: Use the fast decoding loop when possible.
 */
HUF_flags_disableFast = 32

type HUF_repeat = int32

const HUF_repeat_none = 0
const /**< Cannot use the previous table */
HUF_repeat_check = 1
const /**< Can use the previous table but it must be checked. Note : The previous table must have been constructed by HUF_compress{1, 4}X_repeat */
HUF_repeat_valid = 2

type HUF_CTableHeader = struct {
	FtableLog       BYTE
	FmaxSymbolValue BYTE
	Funused         [6]BYTE
}

/**** ended inlining huf.h ****/
/**** skipping file: bits.h ****/

// C documentation
//
//	/*===   Version   ===*/
func FSE_versionNumber(tls *libc.TLS) (r uint32) {
	return libc.Uint32FromInt32(libc.Int32FromInt32(FSE_VERSION_MAJOR)*libc.Int32FromInt32(100)*libc.Int32FromInt32(100) + libc.Int32FromInt32(FSE_VERSION_MINOR)*libc.Int32FromInt32(100) + libc.Int32FromInt32(FSE_VERSION_RELEASE))
}

// C documentation
//
//	/*===   Error Management   ===*/
func FSE_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

func FSE_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorName(tls, code)
}

func HUF_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

func HUF_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorName(tls, code)
}

// C documentation
//
//	/*-**************************************************************
//	*  FSE NCount encoding-decoding
//	****************************************************************/
func FSE_readNCount_body(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var bitCount, count, max, nbBits, previous0, remaining, repeats, threshold int32
	var bitStream U32
	var charnum, maxSV1, v2 uint32
	var countSize size_t
	var iend, ip, istart uintptr
	var _ /* buffer at bp+0 */ [8]uint8
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bitCount, bitStream, charnum, count, countSize, iend, ip, istart, max, maxSV1, nbBits, previous0, remaining, repeats, threshold, v2
	istart = headerBuffer
	iend = istart + uintptr(hbSize)
	ip = istart
	charnum = uint32(0)
	maxSV1 = *(*uint32)(unsafe.Pointer(maxSVPtr)) + uint32(1)
	previous0 = 0
	if hbSize < uint64(8) {
		/* This function only works when hbSize >= 8 */
		*(*[8]uint8)(unsafe.Pointer(bp)) = [8]uint8{}
		libc.Xmemcpy(tls, bp, headerBuffer, hbSize)
		countSize = FSE_readNCount(tls, normalizedCounter, maxSVPtr, tableLogPtr, bp, uint64(8))
		if FSE_isError(tls, countSize) != 0 {
			return countSize
		}
		if countSize > hbSize {
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		return countSize
	}
	/* init */
	libc.Xmemset(tls, normalizedCounter, 0, uint64(*(*uint32)(unsafe.Pointer(maxSVPtr))+libc.Uint32FromInt32(1))*libc.Uint64FromInt64(2)) /* all symbols not present in NCount have a frequency of 0 */
	bitStream = MEM_readLE32(tls, ip)
	nbBits = libc.Int32FromUint32(bitStream&uint32(0xF) + uint32(FSE_MIN_TABLELOG)) /* extract tableLog */
	if nbBits > int32(FSE_TABLELOG_ABSOLUTE_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	bitStream = bitStream >> uint32(4)
	bitCount = int32(4)
	*(*uint32)(unsafe.Pointer(tableLogPtr)) = libc.Uint32FromInt32(nbBits)
	remaining = int32(1)<<nbBits + int32(1)
	threshold = int32(1) << nbBits
	nbBits = nbBits + 1
	for {
		if previous0 != 0 {
			/* Count the number of repeats. Each time the
			 * 2-bit repeat code is 0b11 there is another
			 * repeat.
			 * Avoid UB by setting the high bit to 1.
			 */
			repeats = libc.Int32FromUint32(ZSTD_countTrailingZeros32(tls, ^bitStream|uint32(0x80000000)) >> int32(1))
			for repeats >= int32(12) {
				charnum = charnum + libc.Uint32FromInt32(libc.Int32FromInt32(3)*libc.Int32FromInt32(12))
				if libc.BoolInt64(ip <= iend-libc.UintptrFromInt32(7)) != 0 {
					ip = ip + uintptr(3)
				} else {
					bitCount = bitCount - int32(libc.Int64FromInt32(8)*(int64(iend-libc.UintptrFromInt32(7))-int64(ip)))
					bitCount = bitCount & int32(31)
					ip = iend - uintptr(4)
				}
				bitStream = MEM_readLE32(tls, ip) >> bitCount
				repeats = libc.Int32FromUint32(ZSTD_countTrailingZeros32(tls, ^bitStream|uint32(0x80000000)) >> int32(1))
			}
			charnum = charnum + libc.Uint32FromInt32(int32(3)*repeats)
			bitStream = bitStream >> libc.Uint32FromInt32(int32(2)*repeats)
			bitCount = bitCount + int32(2)*repeats
			/* Add the final repeat which isn't 0b11. */
			charnum = charnum + bitStream&uint32(3)
			bitCount = bitCount + int32(2)
			/* This is an error, but break and return an error
			 * at the end, because returning out of a loop makes
			 * it harder for the compiler to optimize.
			 */
			if charnum >= maxSV1 {
				break
			}
			/* We don't need to set the normalized count to 0
			 * because we already memset the whole buffer to 0.
			 */
			if libc.BoolInt64(ip <= iend-libc.UintptrFromInt32(7)) != 0 || ip+uintptr(bitCount>>libc.Int32FromInt32(3)) <= iend-uintptr(4) {
				/* For first condition to work */
				ip = ip + uintptr(bitCount>>int32(3))
				bitCount = bitCount & int32(7)
			} else {
				bitCount = bitCount - int32(libc.Int64FromInt32(8)*(int64(iend-libc.UintptrFromInt32(4))-int64(ip)))
				bitCount = bitCount & int32(31)
				ip = iend - uintptr(4)
			}
			bitStream = MEM_readLE32(tls, ip) >> bitCount
		}
		max = int32(2)*threshold - int32(1) - remaining
		if bitStream&libc.Uint32FromInt32(threshold-libc.Int32FromInt32(1)) < libc.Uint32FromInt32(max) {
			count = libc.Int32FromUint32(bitStream & libc.Uint32FromInt32(threshold-libc.Int32FromInt32(1)))
			bitCount = bitCount + (nbBits - int32(1))
		} else {
			count = libc.Int32FromUint32(bitStream & libc.Uint32FromInt32(libc.Int32FromInt32(2)*threshold-libc.Int32FromInt32(1)))
			if count >= threshold {
				count = count - max
			}
			bitCount = bitCount + nbBits
		}
		count = count - 1 /* extra accuracy */
		/* When it matters (small blocks), this is a
		 * predictable branch, because we don't use -1.
		 */
		if count >= 0 {
			remaining = remaining - count
		} else {
			remaining = remaining + count
		}
		v2 = charnum
		charnum = charnum + 1
		*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(v2)*2)) = int16(count)
		previous0 = libc.BoolInt32(!(count != 0))
		if remaining < threshold {
			/* This branch can be folded into the
			 * threshold update condition because we
			 * know that threshold > 1.
			 */
			if remaining <= int32(1) {
				break
			}
			nbBits = libc.Int32FromUint32(ZSTD_highbit32(tls, libc.Uint32FromInt32(remaining)) + uint32(1))
			threshold = int32(1) << (nbBits - int32(1))
		}
		if charnum >= maxSV1 {
			break
		}
		if libc.BoolInt64(ip <= iend-libc.UintptrFromInt32(7)) != 0 || ip+uintptr(bitCount>>libc.Int32FromInt32(3)) <= iend-uintptr(4) {
			ip = ip + uintptr(bitCount>>int32(3))
			bitCount = bitCount & int32(7)
		} else {
			bitCount = bitCount - int32(libc.Int64FromInt32(8)*(int64(iend-libc.UintptrFromInt32(4))-int64(ip)))
			bitCount = bitCount & int32(31)
			ip = iend - uintptr(4)
		}
		bitStream = MEM_readLE32(tls, ip) >> bitCount
		goto _1
	_1:
	}
	if remaining != int32(1) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* Only possible when there are too many zeros. */
	if charnum > maxSV1 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooSmall))
	}
	if bitCount > int32(32) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	*(*uint32)(unsafe.Pointer(maxSVPtr)) = charnum - uint32(1)
	ip = ip + uintptr((bitCount+int32(7))>>int32(3))
	return libc.Uint64FromInt64(int64(ip) - int64(istart))
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func FSE_readNCount_body_default(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	return FSE_readNCount_body(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
}

func FSE_readNCount_body_bmi2(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	return FSE_readNCount_body(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
}

func FSE_readNCount_bmi2(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t, bmi2 int32) (r size_t) {
	if bmi2 != 0 {
		return FSE_readNCount_body_bmi2(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
	}
	_ = bmi2
	return FSE_readNCount_body_default(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize)
}

func FSE_readNCount(tls *libc.TLS, normalizedCounter uintptr, maxSVPtr uintptr, tableLogPtr uintptr, headerBuffer uintptr, hbSize size_t) (r size_t) {
	return FSE_readNCount_bmi2(tls, normalizedCounter, maxSVPtr, tableLogPtr, headerBuffer, hbSize, 0)
}

// C documentation
//
//	/*! HUF_readStats() :
//	    Read compact Huffman tree, saved by HUF_writeCTable().
//	    `huffWeight` is destination buffer.
//	    `rankStats` is assumed to be a table of at least HUF_TABLELOG_MAX U32.
//	    @return : size read from `src` , or an error Code .
//	    Note : Needed by HUF_readCTable() and HUF_readDTableX?() .
//	*/
func HUF_readStats(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(880)
	defer tls.Free(880)
	var _ /* wksp at bp+0 */ [219]U32
	return HUF_readStats_wksp(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, bp, uint64(876), 0)
}

func HUF_readStats_body(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	var iSize, oSize size_t
	var ip uintptr
	var lastWeight, n, n1, rest, tableLog, total, verif, weightTotal U32
	_, _, _, _, _, _, _, _, _, _, _ = iSize, ip, lastWeight, n, n1, oSize, rest, tableLog, total, verif, weightTotal
	ip = src
	if !(srcSize != 0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	iSize = uint64(*(*BYTE)(unsafe.Pointer(ip)))
	/* ZSTD_memset(huffWeight, 0, hwSize);   */ /* is not necessary, even though some analyzer complain ... */
	if iSize >= uint64(128) {                   /* special header */
		oSize = iSize - uint64(127)
		iSize = (oSize + libc.Uint64FromInt32(1)) / libc.Uint64FromInt32(2)
		if iSize+uint64(1) > srcSize {
			return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
		}
		if oSize >= hwSize {
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		ip = ip + uintptr(1)
		n = uint32(0)
		for {
			if !(uint64(n) < oSize) {
				break
			}
			*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n))) = libc.Uint8FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(n/uint32(2))))) >> int32(4))
			*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n+uint32(1)))) = libc.Uint8FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(n/uint32(2))))) & int32(15))
			goto _1
		_1:
			;
			n = n + uint32(2)
		}
	} else { /* header compressed with FSE (normal case) */
		if iSize+uint64(1) > srcSize {
			return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
		}
		/* max (hwSize-1) values decoded, as last one is implied */
		oSize = FSE_decompress_wksp_bmi2(tls, huffWeight, hwSize-uint64(1), ip+uintptr(1), iSize, uint32(6), workSpace, wkspSize, bmi2)
		if FSE_isError(tls, oSize) != 0 {
			return oSize
		}
	}
	/* collect weight stats */
	libc.Xmemset(tls, rankStats, 0, libc.Uint64FromInt32(libc.Int32FromInt32(HUF_TABLELOG_MAX)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
	weightTotal = uint32(0)
	n1 = uint32(0)
	for {
		if !(uint64(n1) < oSize) {
			break
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1)))) > int32(HUF_TABLELOG_MAX) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		*(*U32)(unsafe.Pointer(rankStats + uintptr(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1))))*4)) = *(*U32)(unsafe.Pointer(rankStats + uintptr(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1))))*4)) + 1
		weightTotal = weightTotal + libc.Uint32FromInt32(int32(1)<<*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(n1)))>>int32(1))
		goto _2
	_2:
		;
		n1 = n1 + 1
	}
	if weightTotal == uint32(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* get last non-null symbol weight (implied, total must be 2^n) */
	tableLog = ZSTD_highbit32(tls, weightTotal) + uint32(1)
	if tableLog > uint32(HUF_TABLELOG_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	*(*U32)(unsafe.Pointer(tableLogPtr)) = tableLog
	/* determine last weight */
	total = libc.Uint32FromInt32(int32(1) << tableLog)
	rest = total - weightTotal
	verif = libc.Uint32FromInt32(int32(1) << ZSTD_highbit32(tls, rest))
	lastWeight = ZSTD_highbit32(tls, rest) + uint32(1)
	if verif != rest {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* last value must be a clean power of 2 */
	*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(oSize))) = uint8(lastWeight)
	*(*U32)(unsafe.Pointer(rankStats + uintptr(lastWeight)*4)) = *(*U32)(unsafe.Pointer(rankStats + uintptr(lastWeight)*4)) + 1
	/* check tree construction validity */
	if *(*U32)(unsafe.Pointer(rankStats + 1*4)) < uint32(2) || *(*U32)(unsafe.Pointer(rankStats + 1*4))&uint32(1) != 0 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* by construction : at least 2 elts of rank 1, must be even */
	/* results */
	*(*U32)(unsafe.Pointer(nbSymbolsPtr)) = uint32(oSize + libc.Uint64FromInt32(1))
	return iSize + uint64(1)
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func HUF_readStats_body_default(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t) (r size_t) {
	return HUF_readStats_body(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize, 0)
}

func HUF_readStats_body_bmi2(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t) (r size_t) {
	return HUF_readStats_body(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize, int32(1))
}

func HUF_readStats_wksp(tls *libc.TLS, huffWeight uintptr, hwSize size_t, rankStats uintptr, nbSymbolsPtr uintptr, tableLogPtr uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_readStats_body_bmi2(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize)
	}
	_ = flags
	return HUF_readStats_body_default(tls, huffWeight, hwSize, rankStats, nbSymbolsPtr, tableLogPtr, src, srcSize, workSpace, wkspSize)
}

/**** ended inlining common/entropy_common.c ****/
/**** start inlining common/error_private.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* The purpose of this file is to have a single list of error strings embedded in binary */

/**** skipping file: error_private.h ****/

func ERR_getErrorString(tls *libc.TLS, code ERR_enum) (r uintptr) {
	switch code {
	case int32(ZSTD_error_no_error):
		return __ccgo_ts + 23
	case int32(ZSTD_error_GENERIC):
		return __ccgo_ts + 41
	case int32(ZSTD_error_prefix_unknown):
		return __ccgo_ts + 57
	case int32(ZSTD_error_version_unsupported):
		return __ccgo_ts + 82
	case int32(ZSTD_error_frameParameter_unsupported):
		return __ccgo_ts + 104
	case int32(ZSTD_error_frameParameter_windowTooLarge):
		return __ccgo_ts + 132
	case int32(ZSTD_error_corruption_detected):
		return __ccgo_ts + 176
	case int32(ZSTD_error_checksum_wrong):
		return __ccgo_ts + 201
	case int32(ZSTD_error_literals_headerWrong):
		return __ccgo_ts + 238
	case int32(ZSTD_error_parameter_unsupported):
		return __ccgo_ts + 301
	case int32(ZSTD_error_parameter_combination_unsupported):
		return __ccgo_ts + 323
	case int32(ZSTD_error_parameter_outOfBound):
		return __ccgo_ts + 361
	case int32(ZSTD_error_init_missing):
		return __ccgo_ts + 387
	case int32(ZSTD_error_memory_allocation):
		return __ccgo_ts + 416
	case int32(ZSTD_error_workSpace_tooSmall):
		return __ccgo_ts + 453
	case int32(ZSTD_error_stage_wrong):
		return __ccgo_ts + 490
	case int32(ZSTD_error_tableLog_tooLarge):
		return __ccgo_ts + 543
	case int32(ZSTD_error_maxSymbolValue_tooLarge):
		return __ccgo_ts + 591
	case int32(ZSTD_error_maxSymbolValue_tooSmall):
		return __ccgo_ts + 632
	case int32(ZSTD_error_cannotProduce_uncompressedBlock):
		return __ccgo_ts + 670
	case int32(ZSTD_error_stabilityCondition_notRespected):
		return __ccgo_ts + 718
	case int32(ZSTD_error_dictionary_corrupted):
		return __ccgo_ts + 770
	case int32(ZSTD_error_dictionary_wrong):
		return __ccgo_ts + 794
	case int32(ZSTD_error_dictionaryCreation_failed):
		return __ccgo_ts + 814
	case int32(ZSTD_error_dstSize_tooSmall):
		return __ccgo_ts + 861
	case int32(ZSTD_error_srcSize_wrong):
		return __ccgo_ts + 893
	case int32(ZSTD_error_dstBuffer_null):
		return __ccgo_ts + 915
	case int32(ZSTD_error_noForwardProgress_destFull):
		return __ccgo_ts + 952
	case int32(ZSTD_error_noForwardProgress_inputEmpty):
		return __ccgo_ts + 1032
		/* following error codes are not stable and may be removed or changed in a future version */
		fallthrough
	case int32(ZSTD_error_frameIndex_tooLarge):
		return __ccgo_ts + 1105
	case int32(ZSTD_error_seekableIO):
		return __ccgo_ts + 1130
	case int32(ZSTD_error_dstBuffer_wrong):
		return __ccgo_ts + 1173
	case int32(ZSTD_error_srcBuffer_wrong):
		return __ccgo_ts + 1201
	case int32(ZSTD_error_sequenceProducer_failed):
		return __ccgo_ts + 1224
	case int32(ZSTD_error_externalSequences_invalid):
		return __ccgo_ts + 1286
	case int32(ZSTD_error_maxCode):
		fallthrough
	default:
		return notErrorCode
	}
	return r
}

var notErrorCode = __ccgo_ts

/**** ended inlining common/error_private.c ****/
/**** start inlining common/fse_decompress.c ****/
/* ******************************************************************
 * FSE : Finite State Entropy decoder
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Includes
****************************************************************/
/**** skipping file: debug.h ****/
/**** skipping file: bitstream.h ****/
/**** skipping file: compiler.h ****/
/**** skipping file: fse.h ****/
/**** skipping file: error_private.h ****/
/**** skipping file: zstd_deps.h ****/
/**** skipping file: bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

/* **************************************************************
*  Templates
****************************************************************/
/*
  designed to be included
  for type-specific functions (template emulation in C)
  Objective is to write these functions only once, for improved maintenance
*/

/* safety checks */

/* Function names */

func FSE_buildDTable_internal(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var add, sv U64
	var highThreshold, maxSV1, nextState, position1, s, s1, s3, step1, tableMask1, tableSize, u1, v2 U32
	var i, i1, n int32
	var largeLimit S16
	var pos, position, s2, step, tableMask, u, uPosition, unroll size_t
	var spread, symbolNext, tableDecode, tdPtr, v11 uintptr
	var symbol BYTE
	var v10 U16
	var _ /* DTableH at bp+0 */ FSE_DTableHeader
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = add, highThreshold, i, i1, largeLimit, maxSV1, n, nextState, pos, position, position1, s, s1, s2, s3, spread, step, step1, sv, symbol, symbolNext, tableDecode, tableMask, tableMask1, tableSize, tdPtr, u, u1, uPosition, unroll, v10, v11, v2
	tdPtr = dt + uintptr(1)*4 /* because *dt is unsigned, 32-bits aligned on 32-bits */
	tableDecode = tdPtr
	symbolNext = workSpace
	spread = symbolNext + uintptr(maxSymbolValue)*2 + libc.UintptrFromInt32(1)*2
	maxSV1 = maxSymbolValue + uint32(1)
	tableSize = libc.Uint32FromInt32(int32(1) << tableLog)
	highThreshold = tableSize - uint32(1)
	/* Sanity Checks */
	if uint64(2)*uint64(maxSymbolValue+libc.Uint32FromInt32(1))+uint64(1)<<tableLog+uint64(8) > wkspSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	if maxSymbolValue > uint32(FSE_MAX_SYMBOL_VALUE) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	if tableLog > libc.Uint32FromInt32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	/* Init, lay down lowprob symbols */
	(*(*FSE_DTableHeader)(unsafe.Pointer(bp))).FtableLog = uint16(tableLog)
	(*(*FSE_DTableHeader)(unsafe.Pointer(bp))).FfastMode = uint16(1)
	largeLimit = int16(libc.Int32FromInt32(1) << (tableLog - libc.Uint32FromInt32(1)))
	s = uint32(0)
	for {
		if !(s < maxSV1) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) == -int32(1) {
			v2 = highThreshold
			highThreshold = highThreshold - 1
			(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(v2)*4))).Fsymbol = uint8(s)
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = uint16(1)
		} else {
			if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) >= int32(largeLimit) {
				(*(*FSE_DTableHeader)(unsafe.Pointer(bp))).FfastMode = uint16(0)
			}
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = libc.Uint16FromInt16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)))
		}
		goto _1
	_1:
		;
		s = s + 1
	}
	libc.Xmemcpy(tls, dt, bp, libc.Uint64FromInt64(4))
	/* Spread symbols */
	if highThreshold == tableSize-uint32(1) {
		tableMask = uint64(tableSize - uint32(1))
		step = uint64(tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3))
		/* First lay down the symbols in order.
		 * We use a uint64_t to lay down 8 bytes at a time. This reduces branch
		 * misses since small blocks generally have small table logs, so nearly
		 * all symbols have counts <= 8. We ensure we have 8 bytes at the end of
		 * our buffer to handle the over-write.
		 */
		add = uint64(0x0101010101010101)
		pos = uint64(0)
		sv = uint64(0)
		s1 = uint32(0)
		for {
			if !(s1 < maxSV1) {
				break
			}
			n = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s1)*2)))
			MEM_write64(tls, spread+uintptr(pos), sv)
			i = int32(8)
			for {
				if !(i < n) {
					break
				}
				MEM_write64(tls, spread+uintptr(pos)+uintptr(i), sv)
				goto _4
			_4:
				;
				i = i + int32(8)
			}
			pos = pos + libc.Uint64FromInt32(n)
			goto _3
		_3:
			;
			s1 = s1 + 1
			sv = sv + add
		}
		/* Now we spread those positions across the table.
		 * The benefit of doing it in two stages is that we avoid the
		 * variable size inner loop, which caused lots of branch misses.
		 * Now we can run through all the positions without any branch misses.
		 * We unroll the loop twice, since that is what empirically worked best.
		 */
		position = uint64(0)
		unroll = uint64(2)
		/* FSE_MIN_TABLELOG is 5 */
		s2 = uint64(0)
		for {
			if !(s2 < uint64(tableSize)) {
				break
			}
			u = uint64(0)
			for {
				if !(u < unroll) {
					break
				}
				uPosition = (position + u*step) & tableMask
				(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(uPosition)*4))).Fsymbol = *(*BYTE)(unsafe.Pointer(spread + uintptr(s2+u)))
				goto _6
			_6:
				;
				u = u + 1
			}
			position = (position + unroll*step) & tableMask
			goto _5
		_5:
			;
			s2 = s2 + unroll
		}
	} else {
		tableMask1 = tableSize - uint32(1)
		step1 = tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3)
		position1 = uint32(0)
		s3 = uint32(0)
		for {
			if !(s3 < maxSV1) {
				break
			}
			i1 = 0
			for {
				if !(i1 < int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))) {
					break
				}
				(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(position1)*4))).Fsymbol = uint8(s3)
				position1 = (position1 + step1) & tableMask1
				for position1 > highThreshold {
					position1 = (position1 + step1) & tableMask1
				} /* lowprob area */
				goto _8
			_8:
				;
				i1 = i1 + 1
			}
			goto _7
		_7:
			;
			s3 = s3 + 1
		}
		if position1 != uint32(0) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		} /* position must reach all cells once, otherwise normalizedCounter is incorrect */
	}
	/* Build Decoding table */
	u1 = uint32(0)
	for {
		if !(u1 < tableSize) {
			break
		}
		symbol = (*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).Fsymbol
		v11 = symbolNext + uintptr(symbol)*2
		v10 = *(*U16)(unsafe.Pointer(v11))
		*(*U16)(unsafe.Pointer(v11)) = *(*U16)(unsafe.Pointer(v11)) + 1
		nextState = uint32(v10)
		(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).FnbBits = uint8(tableLog - ZSTD_highbit32(tls, nextState))
		(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).FnewState = uint16(nextState<<(*(*FSE_decode_t)(unsafe.Pointer(tableDecode + uintptr(u1)*4))).FnbBits - tableSize)
		goto _9
	_9:
		;
		u1 = u1 + 1
	}
	return uint64(0)
}

func FSE_buildDTable_wksp(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	return FSE_buildDTable_internal(tls, dt, normalizedCounter, maxSymbolValue, tableLog, workSpace, wkspSize)
}

/*-*******************************************************
*  Decompression (Byte symbols)
*********************************************************/
func FSE_decompress_usingDTable_generic(tls *libc.TLS, dst uintptr, maxDstSize size_t, cSrc uintptr, cSrcSize size_t, dt uintptr, fast uint32) (r size_t) {
	bp := tls.Alloc(80)
	defer tls.Free(80)
	var _var_err__ size_t
	var olimit, omax, op, ostart, v6 uintptr
	var v2 int32
	var _ /* bitD at bp+0 */ BIT_DStream_t
	var _ /* state1 at bp+40 */ FSE_DState_t
	var _ /* state2 at bp+56 */ FSE_DState_t
	_, _, _, _, _, _, _ = _var_err__, olimit, omax, op, ostart, v2, v6
	ostart = dst
	op = ostart
	omax = op + uintptr(maxDstSize)
	olimit = omax - uintptr(3)
	/* Init */
	_var_err__ = BIT_initDStream(tls, bp, cSrc, cSrcSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	FSE_initDState(tls, bp+40, bp, dt)
	FSE_initDState(tls, bp+56, bp, dt)
	if BIT_reloadDStream(tls, bp) == int32(BIT_DStream_overflow) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* 4 symbols per loop */
	for {
		if !(libc.BoolInt32(BIT_reloadDStream(tls, bp) == int32(BIT_DStream_unfinished))&libc.BoolInt32(op < olimit) != 0) {
			break
		}
		if fast != 0 {
			v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+40, bp))
		} else {
			v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+40, bp))
		}
		*(*BYTE)(unsafe.Pointer(op)) = libc.Uint8FromInt32(v2)
		if libc.Uint64FromInt32((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(2)+libc.Int32FromInt32(7)) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) { /* This test must be static */
			BIT_reloadDStream(tls, bp)
		}
		if fast != 0 {
			v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+56, bp))
		} else {
			v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+56, bp))
		}
		*(*BYTE)(unsafe.Pointer(op + 1)) = libc.Uint8FromInt32(v2)
		if libc.Uint64FromInt32((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(4)+libc.Int32FromInt32(7)) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) { /* This test must be static */
			if BIT_reloadDStream(tls, bp) > int32(BIT_DStream_unfinished) {
				op = op + uintptr(2)
				break
			}
		}
		if fast != 0 {
			v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+40, bp))
		} else {
			v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+40, bp))
		}
		*(*BYTE)(unsafe.Pointer(op + 2)) = libc.Uint8FromInt32(v2)
		if libc.Uint64FromInt32((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(2)+libc.Int32FromInt32(7)) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) { /* This test must be static */
			BIT_reloadDStream(tls, bp)
		}
		if fast != 0 {
			v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+56, bp))
		} else {
			v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+56, bp))
		}
		*(*BYTE)(unsafe.Pointer(op + 3)) = libc.Uint8FromInt32(v2)
		goto _1
	_1:
		;
		op = op + uintptr(4)
	}
	/* tail */
	/* note : BIT_reloadDStream(&bitD) >= FSE_DStream_partiallyFilled; Ends at exactly BIT_DStream_completed */
	for int32(1) != 0 {
		if op > omax-libc.UintptrFromInt32(2) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		v6 = op
		op = op + 1
		if fast != 0 {
			v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+40, bp))
		} else {
			v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+40, bp))
		}
		*(*BYTE)(unsafe.Pointer(v6)) = libc.Uint8FromInt32(v2)
		if BIT_reloadDStream(tls, bp) == int32(BIT_DStream_overflow) {
			v6 = op
			op = op + 1
			if fast != 0 {
				v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+56, bp))
			} else {
				v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+56, bp))
			}
			*(*BYTE)(unsafe.Pointer(v6)) = libc.Uint8FromInt32(v2)
			break
		}
		if op > omax-libc.UintptrFromInt32(2) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		v6 = op
		op = op + 1
		if fast != 0 {
			v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+56, bp))
		} else {
			v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+56, bp))
		}
		*(*BYTE)(unsafe.Pointer(v6)) = libc.Uint8FromInt32(v2)
		if BIT_reloadDStream(tls, bp) == int32(BIT_DStream_overflow) {
			v6 = op
			op = op + 1
			if fast != 0 {
				v2 = libc.Int32FromUint8(FSE_decodeSymbolFast(tls, bp+40, bp))
			} else {
				v2 = libc.Int32FromUint8(FSE_decodeSymbol(tls, bp+40, bp))
			}
			*(*BYTE)(unsafe.Pointer(v6)) = libc.Uint8FromInt32(v2)
			break
		}
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

type FSE_DecompressWksp = struct {
	Fncount [256]int16
}

func FSE_decompress_wksp_body(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var DTableH, dtable, ip, istart, ptr, wksp uintptr
	var NCountLength, _var_err__, dtablePos size_t
	var fastMode U32
	var _ /* maxSymbolValue at bp+4 */ uint32
	var _ /* tableLog at bp+0 */ uint32
	_, _, _, _, _, _, _, _, _, _ = DTableH, NCountLength, _var_err__, dtable, dtablePos, fastMode, ip, istart, ptr, wksp
	istart = cSrc
	ip = istart
	*(*uint32)(unsafe.Pointer(bp + 4)) = uint32(FSE_MAX_SYMBOL_VALUE)
	wksp = workSpace
	dtablePos = libc.Uint64FromInt64(512) / libc.Uint64FromInt64(4)
	dtable = workSpace + uintptr(dtablePos)*4
	_ = libc.Uint64FromInt64(1)
	if wkspSize < uint64(512) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	/* correct offset to dtable depends on this property */
	_ = libc.Uint64FromInt64(1)
	/* normal FSE decoding mode */
	NCountLength = FSE_readNCount_bmi2(tls, wksp, bp+4, bp, istart, cSrcSize, bmi2)
	if ERR_isError(tls, NCountLength) != 0 {
		return NCountLength
	}
	if *(*uint32)(unsafe.Pointer(bp)) > maxLog {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	ip = ip + uintptr(NCountLength)
	cSrcSize = cSrcSize - NCountLength
	if (libc.Uint64FromInt32(int32(1)+int32(1)<<*(*uint32)(unsafe.Pointer(bp))+int32(1))+(uint64(2)*uint64(*(*uint32)(unsafe.Pointer(bp + 4))+libc.Uint32FromInt32(1))+uint64(1)<<*(*uint32)(unsafe.Pointer(bp))+uint64(8)+uint64(4)-uint64(1))/uint64(4)+libc.Uint64FromInt32((libc.Int32FromInt32(FSE_MAX_SYMBOL_VALUE)+libc.Int32FromInt32(1))/libc.Int32FromInt32(2))+uint64(1))*uint64(4) > wkspSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	workSpace = workSpace + uintptr(512) + uintptr(libc.Uint64FromInt32(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)<<*(*uint32)(unsafe.Pointer(bp)))*libc.Uint64FromInt64(4))
	wkspSize = wkspSize - (uint64(512) + libc.Uint64FromInt32(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)<<*(*uint32)(unsafe.Pointer(bp)))*uint64(4))
	_var_err__ = FSE_buildDTable_internal(tls, dtable, wksp, *(*uint32)(unsafe.Pointer(bp + 4)), *(*uint32)(unsafe.Pointer(bp)), workSpace, wkspSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	ptr = dtable
	DTableH = ptr
	fastMode = uint32((*FSE_DTableHeader)(unsafe.Pointer(DTableH)).FfastMode)
	/* select fast mode (static) */
	if fastMode != 0 {
		return FSE_decompress_usingDTable_generic(tls, dst, dstCapacity, ip, cSrcSize, dtable, uint32(1))
	}
	return FSE_decompress_usingDTable_generic(tls, dst, dstCapacity, ip, cSrcSize, dtable, uint32(0))
	return r
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func FSE_decompress_wksp_body_default(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	return FSE_decompress_wksp_body(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize, 0)
}

func FSE_decompress_wksp_body_bmi2(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	return FSE_decompress_wksp_body(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize, int32(1))
}

func FSE_decompress_wksp_bmi2(tls *libc.TLS, dst uintptr, dstCapacity size_t, cSrc uintptr, cSrcSize size_t, maxLog uint32, workSpace uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	if bmi2 != 0 {
		return FSE_decompress_wksp_body_bmi2(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize)
	}
	_ = bmi2
	return FSE_decompress_wksp_body_default(tls, dst, dstCapacity, cSrc, cSrcSize, maxLog, workSpace, wkspSize)
}

type time_t = int64

type clockid_t = int32

type timespec = struct {
	Ftv_sec  time_t
	Ftv_nsec int64
}

type pthread_t = uintptr

type pthread_once_t = int32

type pthread_key_t = uint32

type pthread_spinlock_t = int32

type pthread_mutexattr_t = struct {
	F__attr uint32
}

type pthread_condattr_t = struct {
	F__attr uint32
}

type pthread_barrierattr_t = struct {
	F__attr uint32
}

type pthread_rwlockattr_t = struct {
	F__attr [2]uint32
}

type sigset_t = struct {
	F__bits [16]uint64
}

type __sigset_t = sigset_t

type pthread_attr_t = struct {
	F__u struct {
		F__vi [0][14]int32
		F__s  [0][7]uint64
		F__i  [14]int32
	}
}

type pthread_mutex_t = struct {
	F__u struct {
		F__vi [0][10]int32
		F__p  [0][5]uintptr
		F__i  [10]int32
	}
}

type pthread_cond_t = struct {
	F__u struct {
		F__vi [0][12]int32
		F__p  [0][6]uintptr
		F__i  [12]int32
	}
}

type pthread_rwlock_t = struct {
	F__u struct {
		F__vi [0][14]int32
		F__p  [0][7]uintptr
		F__i  [14]int32
	}
}

type pthread_barrier_t = struct {
	F__u struct {
		F__vi [0][8]int32
		F__p  [0][4]uintptr
		F__i  [8]int32
	}
}

type pid_t = int32

type sched_param = struct {
	Fsched_priority int32
	F__reserved1    int32
	F__reserved2    [2]struct {
		F__reserved1 time_t
		F__reserved2 int64
	}
	F__reserved3 int32
}

type cpu_set_t = struct {
	F__bits [16]uint64
}

type timer_t = uintptr

type clock_t = int64

type tm = struct {
	Ftm_sec    int32
	Ftm_min    int32
	Ftm_hour   int32
	Ftm_mday   int32
	Ftm_mon    int32
	Ftm_year   int32
	Ftm_wday   int32
	Ftm_yday   int32
	Ftm_isdst  int32
	Ftm_gmtoff int64
	Ftm_zone   uintptr
}

type itimerspec = struct {
	Fit_interval timespec
	Fit_value    timespec
}

type __ptcb = struct {
	F__f    uintptr
	F__x    uintptr
	F__next uintptr
}

/*!< default compression level, specified by ZSTD_CLEVEL_DEFAULT, requires v1.5.0+ */

// C documentation
//
//	/***************************************
//	*  Explicit context
//	***************************************/
//	/*= Compression context
//	 *  When compressing many times,
//	 *  it is recommended to allocate a compression context just once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make the workload easier for system's memory.
//	 *  Note : re-using context is just a speed / resource optimization.
//	 *         It doesn't change the compression ratio, which remains identical.
//	 *  Note 2: For parallel execution in multi-threaded environments,
//	 *         use one different context per thread .
//	 */
type ZSTD_CCtx = struct {
	Fstage                 ZSTD_compressionStage_e
	FcParamsChanged        int32
	Fbmi2                  int32
	FrequestedParams       ZSTD_CCtx_params
	FappliedParams         ZSTD_CCtx_params
	FsimpleApiParams       ZSTD_CCtx_params
	FdictID                U32
	FdictContentSize       size_t
	Fworkspace             ZSTD_cwksp
	FblockSizeMax          size_t
	FpledgedSrcSizePlusOne uint64
	FconsumedSrcSize       uint64
	FproducedCSize         uint64
	FxxhState              XXH_NAMESPACEXXH64_state_t
	FcustomMem             ZSTD_customMem
	Fpool                  uintptr
	FstaticSize            size_t
	FseqCollector          SeqCollector
	FisFirstBlock          int32
	Finitialized           int32
	FseqStore              SeqStore_t
	FldmState              ldmState_t
	FldmSequences          uintptr
	FmaxNbLdmSequences     size_t
	FexternSeqStore        RawSeqStore_t
	FblockState            ZSTD_blockState_t
	FtmpWorkspace          uintptr
	FtmpWkspSize           size_t
	FbufferedPolicy        ZSTD_buffered_policy_e
	FinBuff                uintptr
	FinBuffSize            size_t
	FinToCompress          size_t
	FinBuffPos             size_t
	FinBuffTarget          size_t
	FoutBuff               uintptr
	FoutBuffSize           size_t
	FoutBuffContentSize    size_t
	FoutBuffFlushedSize    size_t
	FstreamStage           ZSTD_cStreamStage
	FframeEnded            U32
	FexpectedInBuffer      ZSTD_inBuffer
	FstableIn_notConsumed  size_t
	FexpectedOutBufferSize size_t
	FlocalDict             ZSTD_localDict
	Fcdict                 uintptr
	FprefixDict            ZSTD_prefixDict
	Fmtctx                 uintptr
	FblockSplitCtx         ZSTD_blockSplitCtx
	FextSeqBuf             uintptr
	FextSeqBufCapacity     size_t
}

/*!< default compression level, specified by ZSTD_CLEVEL_DEFAULT, requires v1.5.0+ */

// C documentation
//
//	/***************************************
//	*  Explicit context
//	***************************************/
//	/*= Compression context
//	 *  When compressing many times,
//	 *  it is recommended to allocate a compression context just once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make the workload easier for system's memory.
//	 *  Note : re-using context is just a speed / resource optimization.
//	 *         It doesn't change the compression ratio, which remains identical.
//	 *  Note 2: For parallel execution in multi-threaded environments,
//	 *         use one different context per thread .
//	 */
type ZSTD_CCtx_s = ZSTD_CCtx

const ZSTDcs_created = 0
const ZSTDcs_init = 1
const ZSTDcs_ongoing = 2
const ZSTDcs_ending = 3
const ZSTDb_not_buffered = 0
const ZSTDb_buffered = 1
const zcss_init = 0
const zcss_load = 1
const zcss_flush = 2

// C documentation
//
//	/*= Decompression context
//	 *  When decompressing many times,
//	 *  it is recommended to allocate a context only once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make workload friendlier for system's memory.
//	 *  Use one context per thread for parallel execution. */
type ZSTD_DCtx = struct {
	FLLTptr               uintptr
	FMLTptr               uintptr
	FOFTptr               uintptr
	FHUFptr               uintptr
	Fentropy              ZSTD_entropyDTables_t
	Fworkspace            [640]U32
	FpreviousDstEnd       uintptr
	FprefixStart          uintptr
	FvirtualStart         uintptr
	FdictEnd              uintptr
	Fexpected             size_t
	FfParams              ZSTD_FrameHeader
	FprocessedCSize       U64
	FdecodedSize          U64
	FbType                blockType_e
	Fstage                ZSTD_dStage
	FlitEntropy           U32
	FfseEntropy           U32
	FxxhState             XXH_NAMESPACEXXH64_state_t
	FheaderSize           size_t
	Fformat               ZSTD_format_e
	FforceIgnoreChecksum  ZSTD_forceIgnoreChecksum_e
	FvalidateChecksum     U32
	FlitPtr               uintptr
	FcustomMem            ZSTD_customMem
	FlitSize              size_t
	FrleSize              size_t
	FstaticSize           size_t
	FisFrameDecompression int32
	Fbmi2                 int32
	FddictLocal           uintptr
	Fddict                uintptr
	FdictID               U32
	FddictIsCold          int32
	FdictUses             ZSTD_dictUses_e
	FddictSet             uintptr
	FrefMultipleDDicts    ZSTD_refMultipleDDicts_e
	FdisableHufAsm        int32
	FmaxBlockSizeParam    int32
	FstreamStage          ZSTD_dStreamStage
	FinBuff               uintptr
	FinBuffSize           size_t
	FinPos                size_t
	FmaxWindowSize        size_t
	FoutBuff              uintptr
	FoutBuffSize          size_t
	FoutStart             size_t
	FoutEnd               size_t
	FlhSize               size_t
	FhostageByte          U32
	FnoForwardProgress    int32
	FoutBufferMode        ZSTD_bufferMode_e
	FexpectedOutBuffer    ZSTD_outBuffer
	FlitBuffer            uintptr
	FlitBufferEnd         uintptr
	FlitBufferLocation    ZSTD_litLocation_e
	FlitExtraBuffer       [65568]BYTE
	FheaderBuffer         [18]BYTE
	FoversizedDuration    size_t
}

// C documentation
//
//	/*= Decompression context
//	 *  When decompressing many times,
//	 *  it is recommended to allocate a context only once,
//	 *  and reuse it for each successive compression operation.
//	 *  This will make workload friendlier for system's memory.
//	 *  Use one context per thread for parallel execution. */
type ZSTD_DCtx_s = ZSTD_DCtx

const bt_raw = 0
const bt_rle = 1
const bt_compressed = 2
const bt_reserved = 3
const ZSTDds_getFrameHeaderSize = 0
const ZSTDds_decodeFrameHeader = 1
const ZSTDds_decodeBlockHeader = 2
const ZSTDds_decompressBlock = 3
const ZSTDds_decompressLastBlock = 4
const ZSTDds_checkChecksum = 5
const ZSTDds_decodeSkippableHeader = 6
const ZSTDds_skipFrame = 7
const ZSTD_f_zstd1 = 0
const /* zstd frame format, specified in zstd_compression_format.md (default) */
ZSTD_f_zstd1_magicless = 1
const
/* Note: this enum controls ZSTD_d_forceIgnoreChecksum */
ZSTD_d_validateChecksum = 0
const ZSTD_d_ignoreChecksum = 1
const ZSTD_use_indefinitely = -1
const /* Use the dictionary indefinitely */
ZSTD_dont_use = 0
const /* Do not use the dictionary (if one exists free it) */
ZSTD_use_once = 1
const
/* Note: this enum controls ZSTD_d_refMultipleDDicts */
ZSTD_rmd_refSingleDDict = 0
const ZSTD_rmd_refMultipleDDicts = 1
const zdss_init = 0
const zdss_loadHeader = 1
const zdss_read = 2
const zdss_load = 3
const zdss_flush = 4
const ZSTD_bm_buffered = 0
const /* Buffer the input/output */
ZSTD_bm_stable = 1
const ZSTD_not_in_dst = 0
const /* Stored entirely within litExtraBuffer */
ZSTD_in_dst = 1
const /* Stored entirely within dst (in memory after current output write) */
ZSTD_split = 2

/*********************************************
*  Advanced compression API (Requires v1.4.0+)
**********************************************/

/* API design :
 *   Parameters are pushed one by one into an existing context,
 *   using ZSTD_CCtx_set*() functions.
 *   Pushed parameters are sticky : they are valid for next compressed frame, and any subsequent frame.
 *   "sticky" parameters are applicable to `ZSTD_compress2()` and `ZSTD_compressStream*()` !
 *   __They do not apply to one-shot variants such as ZSTD_compressCCtx()__ .
 *
 *   It's possible to reset all parameters to "default" using ZSTD_CCtx_reset().
 *
 *   This API supersedes all other "advanced" API entry points in the experimental section.
 *   In the future, we expect to remove API entry points from experimental which are redundant with this API.
 */

// C documentation
//
//	/* Compression strategies, listed from fastest to strongest */
type ZSTD_strategy = int32

const ZSTD_fast = 1
const ZSTD_dfast = 2
const ZSTD_greedy = 3
const ZSTD_lazy = 4
const ZSTD_lazy2 = 5
const ZSTD_btlazy2 = 6
const ZSTD_btopt = 7
const ZSTD_btultra = 8
const ZSTD_btultra2 = 9

type ZSTD_cParameter = int32

const

/* compression parameters
 * Note: When compressing with a ZSTD_CDict these parameters are superseded
 * by the parameters used to construct the ZSTD_CDict.
 * See ZSTD_CCtx_refCDict() for more info (superseded-by-cdict). */
ZSTD_c_compressionLevel = 100
const /* Set compression parameters according to pre-defined cLevel table.
 * Note that exact compression parameters are dynamically determined,
 * depending on both compression level and srcSize (when known).
 * Default level is ZSTD_CLEVEL_DEFAULT==3.
 * Special: value 0 means default, which is controlled by ZSTD_CLEVEL_DEFAULT.
 * Note 1 : it's possible to pass a negative compression level.
 * Note 2 : setting a level does not automatically set all other compression parameters
 *   to default. Setting this will however eventually dynamically impact the compression
 *   parameters which have not been manually set. The manually set
 *   ones will 'stick'. */
/* Advanced compression parameters :
 * It's possible to pin down compression parameters to some specific values.
 * In which case, these values are no longer dynamically selected by the compressor */
ZSTD_c_windowLog = 101
const /* Maximum allowed back-reference distance, expressed as power of 2.
 * This will set a memory budget for streaming decompression,
 * with larger values requiring more memory
 * and typically compressing more.
 * Must be clamped between ZSTD_WINDOWLOG_MIN and ZSTD_WINDOWLOG_MAX.
 * Special: value 0 means "use default windowLog".
 * Note: Using a windowLog greater than ZSTD_WINDOWLOG_LIMIT_DEFAULT
 *       requires explicitly allowing such size at streaming decompression stage. */
ZSTD_c_hashLog = 102
const /* Size of the initial probe table, as a power of 2.
 * Resulting memory usage is (1 << (hashLog+2)).
 * Must be clamped between ZSTD_HASHLOG_MIN and ZSTD_HASHLOG_MAX.
 * Larger tables improve compression ratio of strategies <= dFast,
 * and improve speed of strategies > dFast.
 * Special: value 0 means "use default hashLog". */
ZSTD_c_chainLog = 103
const /* Size of the multi-probe search table, as a power of 2.
 * Resulting memory usage is (1 << (chainLog+2)).
 * Must be clamped between ZSTD_CHAINLOG_MIN and ZSTD_CHAINLOG_MAX.
 * Larger tables result in better and slower compression.
 * This parameter is useless for "fast" strategy.
 * It's still useful when using "dfast" strategy,
 * in which case it defines a secondary probe table.
 * Special: value 0 means "use default chainLog". */
ZSTD_c_searchLog = 104
const /* Number of search attempts, as a power of 2.
 * More attempts result in better and slower compression.
 * This parameter is useless for "fast" and "dFast" strategies.
 * Special: value 0 means "use default searchLog". */
ZSTD_c_minMatch = 105
const /* Minimum size of searched matches.
 * Note that Zstandard can still find matches of smaller size,
 * it just tweaks its search algorithm to look for this size and larger.
 * Larger values increase compression and decompression speed, but decrease ratio.
 * Must be clamped between ZSTD_MINMATCH_MIN and ZSTD_MINMATCH_MAX.
 * Note that currently, for all strategies < btopt, effective minimum is 4.
 *                    , for all strategies > fast, effective maximum is 6.
 * Special: value 0 means "use default minMatchLength". */
ZSTD_c_targetLength = 106
const /* Impact of this field depends on strategy.
 * For strategies btopt, btultra & btultra2:
 *     Length of Match considered "good enough" to stop search.
 *     Larger values make compression stronger, and slower.
 * For strategy fast:
 *     Distance between match sampling.
 *     Larger values make compression faster, and weaker.
 * Special: value 0 means "use default targetLength". */
ZSTD_c_strategy = 107
const /* See ZSTD_strategy enum definition.
 * The higher the value of selected strategy, the more complex it is,
 * resulting in stronger and slower compression.
 * Special: value 0 means "use default strategy". */

ZSTD_c_targetCBlockSize = 130
const /* v1.5.6+
 * Attempts to fit compressed block size into approximately targetCBlockSize.
 * Bound by ZSTD_TARGETCBLOCKSIZE_MIN and ZSTD_TARGETCBLOCKSIZE_MAX.
 * Note that it's not a guarantee, just a convergence target (default:0).
 * No target when targetCBlockSize == 0.
 * This is helpful in low bandwidth streaming environments to improve end-to-end latency,
 * when a client can make use of partial documents (a prominent example being Chrome).
 * Note: this parameter is stable since v1.5.6.
 * It was present as an experimental parameter in earlier versions,
 * but it's not recommended using it with earlier library versions
 * due to massive performance regressions.
 */
/* LDM mode parameters */
ZSTD_c_enableLongDistanceMatching = 160
const /* Enable long distance matching.
 * This parameter is designed to improve compression ratio
 * for large inputs, by finding large matches at long distance.
 * It increases memory usage and window size.
 * Note: enabling this parameter increases default ZSTD_c_windowLog to 128 MB
 * except when expressly set to a different value.
 * Note: will be enabled by default if ZSTD_c_windowLog >= 128 MB and
 * compression strategy >= ZSTD_btopt (== compression level 16+) */
ZSTD_c_ldmHashLog = 161
const /* Size of the table for long distance matching, as a power of 2.
 * Larger values increase memory usage and compression ratio,
 * but decrease compression speed.
 * Must be clamped between ZSTD_HASHLOG_MIN and ZSTD_HASHLOG_MAX
 * default: windowlog - 7.
 * Special: value 0 means "automatically determine hashlog". */
ZSTD_c_ldmMinMatch = 162
const /* Minimum match size for long distance matcher.
 * Larger/too small values usually decrease compression ratio.
 * Must be clamped between ZSTD_LDM_MINMATCH_MIN and ZSTD_LDM_MINMATCH_MAX.
 * Special: value 0 means "use default value" (default: 64). */
ZSTD_c_ldmBucketSizeLog = 163
const /* Log size of each bucket in the LDM hash table for collision resolution.
 * Larger values improve collision resolution but decrease compression speed.
 * The maximum value is ZSTD_LDM_BUCKETSIZELOG_MAX.
 * Special: value 0 means "use default value" (default: 3). */
ZSTD_c_ldmHashRateLog = 164
const /* Frequency of inserting/looking up entries into the LDM hash table.
 * Must be clamped between 0 and (ZSTD_WINDOWLOG_MAX - ZSTD_HASHLOG_MIN).
 * Default is MAX(0, (windowLog - ldmHashLog)), optimizing hash table usage.
 * Larger values improve compression speed.
 * Deviating far from default value will likely result in a compression ratio decrease.
 * Special: value 0 means "automatically determine hashRateLog". */

/* frame parameters */
ZSTD_c_contentSizeFlag = 200
const /* Content size will be written into frame header _whenever known_ (default:1)
 * Content size must be known at the beginning of compression.
 * This is automatically the case when using ZSTD_compress2(),
 * For streaming scenarios, content size must be provided with ZSTD_CCtx_setPledgedSrcSize() */
ZSTD_c_checksumFlag = 201
const /* A 32-bits checksum of content is written at end of frame (default:0) */
ZSTD_c_dictIDFlag = 202
const /* When applicable, dictionary's ID is written into frame header (default:1) */

/* multi-threading parameters */
/* These parameters are only active if multi-threading is enabled (compiled with build macro ZSTD_MULTITHREAD).
 * Otherwise, trying to set any other value than default (0) will be a no-op and return an error.
 * In a situation where it's unknown if the linked library supports multi-threading or not,
 * setting ZSTD_c_nbWorkers to any value >= 1 and consulting the return value provides a quick way to check this property.
 */
ZSTD_c_nbWorkers = 400
const /* Select how many threads will be spawned to compress in parallel.
 * When nbWorkers >= 1, triggers asynchronous mode when invoking ZSTD_compressStream*() :
 * ZSTD_compressStream*() consumes input and flush output if possible, but immediately gives back control to caller,
 * while compression is performed in parallel, within worker thread(s).
 * (note : a strong exception to this rule is when first invocation of ZSTD_compressStream2() sets ZSTD_e_end :
 *  in which case, ZSTD_compressStream2() delegates to ZSTD_compress2(), which is always a blocking call).
 * More workers improve speed, but also increase memory usage.
 * Default value is `0`, aka "single-threaded mode" : no worker is spawned,
 * compression is performed inside Caller's thread, and all invocations are blocking */
ZSTD_c_jobSize = 401
const /* Size of a compression job. This value is enforced only when nbWorkers >= 1.
 * Each compression job is completed in parallel, so this value can indirectly impact the nb of active threads.
 * 0 means default, which is dynamically determined based on compression parameters.
 * Job size must be a minimum of overlap size, or ZSTDMT_JOBSIZE_MIN (= 512 KB), whichever is largest.
 * The minimum size is automatically and transparently enforced. */
ZSTD_c_overlapLog = 402
const /* Control the overlap size, as a fraction of window size.
 * The overlap size is an amount of data reloaded from previous job at the beginning of a new job.
 * It helps preserve compression ratio, while each job is compressed in parallel.
 * This value is enforced only when nbWorkers >= 1.
 * Larger values increase compression ratio, but decrease speed.
 * Possible values range from 0 to 9 :
 * - 0 means "default" : value will be determined by the library, depending on strategy
 * - 1 means "no overlap"
 * - 9 means "full overlap", using a full window size.
 * Each intermediate rank increases/decreases load size by a factor 2 :
 * 9: full window;  8: w/2;  7: w/4;  6: w/8;  5:w/16;  4: w/32;  3:w/64;  2:w/128;  1:no overlap;  0:default
 * default value varies between 6 and 9, depending on strategy */

/* note : additional experimental parameters are also available
 * within the experimental section of the API.
 * At the time of this writing, they include :
 * ZSTD_c_rsyncable
 * ZSTD_c_format
 * ZSTD_c_forceMaxWindow
 * ZSTD_c_forceAttachDict
 * ZSTD_c_literalCompressionMode
 * ZSTD_c_srcSizeHint
 * ZSTD_c_enableDedicatedDictSearch
 * ZSTD_c_stableInBuffer
 * ZSTD_c_stableOutBuffer
 * ZSTD_c_blockDelimiters
 * ZSTD_c_validateSequences
 * ZSTD_c_blockSplitterLevel
 * ZSTD_c_splitAfterSequences
 * ZSTD_c_useRowMatchFinder
 * ZSTD_c_prefetchCDictTables
 * ZSTD_c_enableSeqProducerFallback
 * ZSTD_c_maxBlockSize
 * Because they are not stable, it's necessary to define ZSTD_STATIC_LINKING_ONLY to access them.
 * note : never ever use experimentalParam? names directly;
 *        also, the enums values themselves are unstable and can still change.
 */
ZSTD_c_experimentalParam1 = 500
const ZSTD_c_experimentalParam2 = 10
const ZSTD_c_experimentalParam3 = 1000
const ZSTD_c_experimentalParam4 = 1001
const ZSTD_c_experimentalParam5 = 1002
const
/* was ZSTD_c_experimentalParam6=1003; is now ZSTD_c_targetCBlockSize */
ZSTD_c_experimentalParam7 = 1004
const ZSTD_c_experimentalParam8 = 1005
const ZSTD_c_experimentalParam9 = 1006
const ZSTD_c_experimentalParam10 = 1007
const ZSTD_c_experimentalParam11 = 1008
const ZSTD_c_experimentalParam12 = 1009
const ZSTD_c_experimentalParam13 = 1010
const ZSTD_c_experimentalParam14 = 1011
const ZSTD_c_experimentalParam15 = 1012
const ZSTD_c_experimentalParam16 = 1013
const ZSTD_c_experimentalParam17 = 1014
const ZSTD_c_experimentalParam18 = 1015
const ZSTD_c_experimentalParam19 = 1016
const ZSTD_c_experimentalParam20 = 1017

type ZSTD_bounds = struct {
	Ferror1     size_t
	FlowerBound int32
	FupperBound int32
}

type ZSTD_ResetDirective = int32

const ZSTD_reset_session_only = 1
const ZSTD_reset_parameters = 2
const ZSTD_reset_session_and_parameters = 3

/***********************************************
*  Advanced decompression API (Requires v1.4.0+)
************************************************/

/* The advanced API pushes parameters one by one into an existing DCtx context.
 * Parameters are sticky, and remain valid for all following frames
 * using the same DCtx context.
 * It's possible to reset parameters to default values using ZSTD_DCtx_reset().
 * Note : This API is compatible with existing ZSTD_decompressDCtx() and ZSTD_decompressStream().
 *        Therefore, no new decompression function is necessary.
 */

type ZSTD_dParameter = int32

const ZSTD_d_windowLogMax = 100
const /* Select a size limit (in power of 2) beyond which
 * the streaming API will refuse to allocate memory buffer
 * in order to protect the host from unreasonable memory requirements.
 * This parameter is only useful in streaming mode, since no internal buffer is allocated in single-pass mode.
 * By default, a decompression context accepts window sizes <= (1 << ZSTD_WINDOWLOG_LIMIT_DEFAULT).
 * Special: value 0 means "use default maximum windowLog". */

/* note : additional experimental parameters are also available
 * within the experimental section of the API.
 * At the time of this writing, they include :
 * ZSTD_d_format
 * ZSTD_d_stableOutBuffer
 * ZSTD_d_forceIgnoreChecksum
 * ZSTD_d_refMultipleDDicts
 * ZSTD_d_disableHuffmanAssembly
 * ZSTD_d_maxBlockSize
 * Because they are not stable, it's necessary to define ZSTD_STATIC_LINKING_ONLY to access them.
 * note : never ever use experimentalParam? names directly
 */
ZSTD_d_experimentalParam1 = 1000
const ZSTD_d_experimentalParam2 = 1001
const ZSTD_d_experimentalParam3 = 1002
const ZSTD_d_experimentalParam4 = 1003
const ZSTD_d_experimentalParam5 = 1004
const ZSTD_d_experimentalParam6 = 1005

/****************************
*  Streaming
****************************/

type ZSTD_inBuffer = struct {
	Fsrc  uintptr
	Fsize size_t
	Fpos  size_t
}

/****************************
*  Streaming
****************************/

type ZSTD_inBuffer_s = ZSTD_inBuffer

type ZSTD_outBuffer = struct {
	Fdst  uintptr
	Fsize size_t
	Fpos  size_t
}

type ZSTD_outBuffer_s = ZSTD_outBuffer

/*-***********************************************************************
*  Streaming compression - HowTo
*
*  A ZSTD_CStream object is required to track streaming operation.
*  Use ZSTD_createCStream() and ZSTD_freeCStream() to create/release resources.
*  ZSTD_CStream objects can be reused multiple times on consecutive compression operations.
*  It is recommended to reuse ZSTD_CStream since it will play nicer with system's memory, by re-using already allocated memory.
*
*  For parallel execution, use one separate ZSTD_CStream per thread.
*
*  note : since v1.3.0, ZSTD_CStream and ZSTD_CCtx are the same thing.
*
*  Parameters are sticky : when starting a new compression on the same context,
*  it will reuse the same sticky parameters as previous compression session.
*  When in doubt, it's recommended to fully initialize the context before usage.
*  Use ZSTD_CCtx_reset() to reset the context and ZSTD_CCtx_setParameter(),
*  ZSTD_CCtx_setPledgedSrcSize(), or ZSTD_CCtx_loadDictionary() and friends to
*  set more specific parameters, the pledged source size, or load a dictionary.
*
*  Use ZSTD_compressStream2() with ZSTD_e_continue as many times as necessary to
*  consume input stream. The function will automatically update both `pos`
*  fields within `input` and `output`.
*  Note that the function may not consume the entire input, for example, because
*  the output buffer is already full, in which case `input.pos < input.size`.
*  The caller must check if input has been entirely consumed.
*  If not, the caller must make some room to receive more compressed data,
*  and then present again remaining input data.
*  note: ZSTD_e_continue is guaranteed to make some forward progress when called,
*        but doesn't guarantee maximal forward progress. This is especially relevant
*        when compressing with multiple threads. The call won't block if it can
*        consume some input, but if it can't it will wait for some, but not all,
*        output to be flushed.
* @return : provides a minimum amount of data remaining to be flushed from internal buffers
*           or an error code, which can be tested using ZSTD_isError().
*
*  At any moment, it's possible to flush whatever data might remain stuck within internal buffer,
*  using ZSTD_compressStream2() with ZSTD_e_flush. `output->pos` will be updated.
*  Note that, if `output->size` is too small, a single invocation with ZSTD_e_flush might not be enough (return code > 0).
*  In which case, make some room to receive more compressed data, and call again ZSTD_compressStream2() with ZSTD_e_flush.
*  You must continue calling ZSTD_compressStream2() with ZSTD_e_flush until it returns 0, at which point you can change the
*  operation.
*  note: ZSTD_e_flush will flush as much output as possible, meaning when compressing with multiple threads, it will
*        block until the flush is complete or the output buffer is full.
*  @return : 0 if internal buffers are entirely flushed,
*            >0 if some data still present within internal buffer (the value is minimal estimation of remaining size),
*            or an error code, which can be tested using ZSTD_isError().
*
*  Calling ZSTD_compressStream2() with ZSTD_e_end instructs to finish a frame.
*  It will perform a flush and write frame epilogue.
*  The epilogue is required for decoders to consider a frame completed.
*  flush operation is the same, and follows same rules as calling ZSTD_compressStream2() with ZSTD_e_flush.
*  You must continue calling ZSTD_compressStream2() with ZSTD_e_end until it returns 0, at which point you are free to
*  start a new frame.
*  note: ZSTD_e_end will flush as much output as possible, meaning when compressing with multiple threads, it will
*        block until the flush is complete or the output buffer is full.
*  @return : 0 if frame fully completed and fully flushed,
*            >0 if some data still present within internal buffer (the value is minimal estimation of remaining size),
*            or an error code, which can be tested using ZSTD_isError().
*
* *******************************************************************/

type ZSTD_CStream = struct {
	Fstage                 ZSTD_compressionStage_e
	FcParamsChanged        int32
	Fbmi2                  int32
	FrequestedParams       ZSTD_CCtx_params
	FappliedParams         ZSTD_CCtx_params
	FsimpleApiParams       ZSTD_CCtx_params
	FdictID                U32
	FdictContentSize       size_t
	Fworkspace             ZSTD_cwksp
	FblockSizeMax          size_t
	FpledgedSrcSizePlusOne uint64
	FconsumedSrcSize       uint64
	FproducedCSize         uint64
	FxxhState              XXH_NAMESPACEXXH64_state_t
	FcustomMem             ZSTD_customMem
	Fpool                  uintptr
	FstaticSize            size_t
	FseqCollector          SeqCollector
	FisFirstBlock          int32
	Finitialized           int32
	FseqStore              SeqStore_t
	FldmState              ldmState_t
	FldmSequences          uintptr
	FmaxNbLdmSequences     size_t
	FexternSeqStore        RawSeqStore_t
	FblockState            ZSTD_blockState_t
	FtmpWorkspace          uintptr
	FtmpWkspSize           size_t
	FbufferedPolicy        ZSTD_buffered_policy_e
	FinBuff                uintptr
	FinBuffSize            size_t
	FinToCompress          size_t
	FinBuffPos             size_t
	FinBuffTarget          size_t
	FoutBuff               uintptr
	FoutBuffSize           size_t
	FoutBuffContentSize    size_t
	FoutBuffFlushedSize    size_t
	FstreamStage           ZSTD_cStreamStage
	FframeEnded            U32
	FexpectedInBuffer      ZSTD_inBuffer
	FstableIn_notConsumed  size_t
	FexpectedOutBufferSize size_t
	FlocalDict             ZSTD_localDict
	Fcdict                 uintptr
	FprefixDict            ZSTD_prefixDict
	Fmtctx                 uintptr
	FblockSplitCtx         ZSTD_blockSplitCtx
	FextSeqBuf             uintptr
	FextSeqBufCapacity     size_t
}

/* accept NULL pointer */

// C documentation
//
//	/*===== Streaming compression functions =====*/
type ZSTD_EndDirective = int32

const ZSTD_e_continue = 0
const /* collect more data, encoder decides when to output compressed result, for optimal compression ratio */
ZSTD_e_flush = 1
const /* flush any data provided so far,
 * it creates (at least) one new block, that can be decoded immediately on reception;
 * frame will continue: any future data can still reference previously compressed data, improving compression.
 * note : multithreaded compression will block to flush as much output as possible. */
ZSTD_e_end = 2

/*-***************************************************************************
*  Streaming decompression - HowTo
*
*  A ZSTD_DStream object is required to track streaming operations.
*  Use ZSTD_createDStream() and ZSTD_freeDStream() to create/release resources.
*  ZSTD_DStream objects can be re-employed multiple times.
*
*  Use ZSTD_initDStream() to start a new decompression operation.
* @return : recommended first input size
*  Alternatively, use advanced API to set specific properties.
*
*  Use ZSTD_decompressStream() repetitively to consume your input.
*  The function will update both `pos` fields.
*  If `input.pos < input.size`, some input has not been consumed.
*  It's up to the caller to present again remaining data.
*
*  The function tries to flush all data decoded immediately, respecting output buffer size.
*  If `output.pos < output.size`, decoder has flushed everything it could.
*
*  However, when `output.pos == output.size`, it's more difficult to know.
*  If @return > 0, the frame is not complete, meaning
*  either there is still some data left to flush within internal buffers,
*  or there is more input to read to complete the frame (or both).
*  In which case, call ZSTD_decompressStream() again to flush whatever remains in the buffer.
*  Note : with no additional input provided, amount of data flushed is necessarily <= ZSTD_BLOCKSIZE_MAX.
* @return : 0 when a frame is completely decoded and fully flushed,
*        or an error code, which can be tested using ZSTD_isError(),
*        or any other value > 0, which means there is still some decoding or flushing to do to complete current frame :
*                                the return value is a suggested next input size (just a hint for better latency)
*                                that will never request more than the remaining content of the compressed frame.
* *******************************************************************************/

type ZSTD_DStream = struct {
	FLLTptr               uintptr
	FMLTptr               uintptr
	FOFTptr               uintptr
	FHUFptr               uintptr
	Fentropy              ZSTD_entropyDTables_t
	Fworkspace            [640]U32
	FpreviousDstEnd       uintptr
	FprefixStart          uintptr
	FvirtualStart         uintptr
	FdictEnd              uintptr
	Fexpected             size_t
	FfParams              ZSTD_FrameHeader
	FprocessedCSize       U64
	FdecodedSize          U64
	FbType                blockType_e
	Fstage                ZSTD_dStage
	FlitEntropy           U32
	FfseEntropy           U32
	FxxhState             XXH_NAMESPACEXXH64_state_t
	FheaderSize           size_t
	Fformat               ZSTD_format_e
	FforceIgnoreChecksum  ZSTD_forceIgnoreChecksum_e
	FvalidateChecksum     U32
	FlitPtr               uintptr
	FcustomMem            ZSTD_customMem
	FlitSize              size_t
	FrleSize              size_t
	FstaticSize           size_t
	FisFrameDecompression int32
	Fbmi2                 int32
	FddictLocal           uintptr
	Fddict                uintptr
	FdictID               U32
	FddictIsCold          int32
	FdictUses             ZSTD_dictUses_e
	FddictSet             uintptr
	FrefMultipleDDicts    ZSTD_refMultipleDDicts_e
	FdisableHufAsm        int32
	FmaxBlockSizeParam    int32
	FstreamStage          ZSTD_dStreamStage
	FinBuff               uintptr
	FinBuffSize           size_t
	FinPos                size_t
	FmaxWindowSize        size_t
	FoutBuff              uintptr
	FoutBuffSize          size_t
	FoutStart             size_t
	FoutEnd               size_t
	FlhSize               size_t
	FhostageByte          U32
	FnoForwardProgress    int32
	FoutBufferMode        ZSTD_bufferMode_e
	FexpectedOutBuffer    ZSTD_outBuffer
	FlitBuffer            uintptr
	FlitBufferEnd         uintptr
	FlitBufferLocation    ZSTD_litLocation_e
	FlitExtraBuffer       [65568]BYTE
	FheaderBuffer         [18]BYTE
	FoversizedDuration    size_t
}

// C documentation
//
//	/***********************************
//	 *  Bulk processing dictionary API
//	 **********************************/
type ZSTD_CDict = struct {
	FdictContent       uintptr
	FdictContentSize   size_t
	FdictContentType   ZSTD_dictContentType_e
	FentropyWorkspace  uintptr
	Fworkspace         ZSTD_cwksp
	FmatchState        ZSTD_MatchState_t
	FcBlockState       ZSTD_compressedBlockState_t
	FcustomMem         ZSTD_customMem
	FdictID            U32
	FcompressionLevel  int32
	FuseRowMatchFinder ZSTD_ParamSwitch_e
}

// C documentation
//
//	/***********************************
//	 *  Bulk processing dictionary API
//	 **********************************/
type ZSTD_CDict_s = ZSTD_CDict

const ZSTD_dct_auto = 0
const /* dictionary is "full" when starting with ZSTD_MAGIC_DICTIONARY, otherwise it is "rawContent" */
ZSTD_dct_rawContent = 1
const /* ensures dictionary is always loaded as rawContent, even if it starts with ZSTD_MAGIC_DICTIONARY */
ZSTD_dct_fullDict = 2
const
/* Note: This enum controls features which are conditionally beneficial.
 * Zstd can take a decision on whether or not to enable the feature (ZSTD_ps_auto),
 * but setting the switch to ZSTD_ps_enable or ZSTD_ps_disable force enable/disable the feature.
 */
ZSTD_ps_auto = 0
const /* Let the library automatically determine whether the feature shall be enabled */
ZSTD_ps_enable = 1
const /* Force-enable the feature */
ZSTD_ps_disable = 2

type ZSTD_DDict = struct {
	FdictBuffer     uintptr
	FdictContent    uintptr
	FdictSize       size_t
	Fentropy        ZSTD_entropyDTables_t
	FdictID         U32
	FentropyPresent U32
	FcMem           ZSTD_customMem
}

type ZSTD_DDict_s = ZSTD_DDict

/* **************************************************************************************
 *   ADVANCED AND EXPERIMENTAL FUNCTIONS
 ****************************************************************************************
 * The definitions in the following section are considered experimental.
 * They are provided for advanced scenarios.
 * They should never be used with a dynamic library, as prototypes may change in the future.
 * Use them only in association with static linking.
 * ***************************************************************************************/

/* This can be overridden externally to hide static symbols. */

/****************************************************************************************
 *   experimental API (static linking only)
 ****************************************************************************************
 * The following symbols and constants
 * are not planned to join "stable API" status in the near future.
 * They can still change in future versions.
 * Some of them are planned to remain in the static_only section indefinitely.
 * Some of them might be removed in the future (especially when redundant with existing stable functions)
 * ***************************************************************************************/

/* compression parameter bounds */

/* LDM parameter bounds */

/* Advanced parameter bounds */

/* ---  Advanced types  --- */

type ZSTD_CCtx_params = struct {
	Fformat                    ZSTD_format_e
	FcParams                   ZSTD_compressionParameters
	FfParams                   ZSTD_frameParameters
	FcompressionLevel          int32
	FforceWindow               int32
	FtargetCBlockSize          size_t
	FsrcSizeHint               int32
	FattachDictPref            ZSTD_dictAttachPref_e
	FliteralCompressionMode    ZSTD_ParamSwitch_e
	FnbWorkers                 int32
	FjobSize                   size_t
	FoverlapLog                int32
	Frsyncable                 int32
	FldmParams                 ldmParams_t
	FenableDedicatedDictSearch int32
	FinBufferMode              ZSTD_bufferMode_e
	FoutBufferMode             ZSTD_bufferMode_e
	FblockDelimiters           ZSTD_SequenceFormat_e
	FvalidateSequences         int32
	FpostBlockSplitter         ZSTD_ParamSwitch_e
	FpreBlockSplitter_level    int32
	FmaxBlockSize              size_t
	FuseRowMatchFinder         ZSTD_ParamSwitch_e
	FdeterministicRefPrefix    int32
	FcustomMem                 ZSTD_customMem
	FprefetchCDictTables       ZSTD_ParamSwitch_e
	FenableMatchFinderFallback int32
	FextSeqProdState           uintptr
	FextSeqProdFunc            ZSTD_sequenceProducer_F
	FsearchForExternalRepcodes ZSTD_ParamSwitch_e
}

/* **************************************************************************************
 *   ADVANCED AND EXPERIMENTAL FUNCTIONS
 ****************************************************************************************
 * The definitions in the following section are considered experimental.
 * They are provided for advanced scenarios.
 * They should never be used with a dynamic library, as prototypes may change in the future.
 * Use them only in association with static linking.
 * ***************************************************************************************/

/* This can be overridden externally to hide static symbols. */

/****************************************************************************************
 *   experimental API (static linking only)
 ****************************************************************************************
 * The following symbols and constants
 * are not planned to join "stable API" status in the near future.
 * They can still change in future versions.
 * Some of them are planned to remain in the static_only section indefinitely.
 * Some of them might be removed in the future (especially when redundant with existing stable functions)
 * ***************************************************************************************/

/* compression parameter bounds */

/* LDM parameter bounds */

/* Advanced parameter bounds */

/* ---  Advanced types  --- */

type ZSTD_CCtx_params_s = ZSTD_CCtx_params

const
/* Note: this enum and the behavior it controls are effectively internal
 * implementation details of the compressor. They are expected to continue
 * to evolve and should be considered only in the context of extremely
 * advanced performance tuning.
 *
 * Zstd currently supports the use of a CDict in three ways:
 *
 * - The contents of the CDict can be copied into the working context. This
 *   means that the compression can search both the dictionary and input
 *   while operating on a single set of internal tables. This makes
 *   the compression faster per-byte of input. However, the initial copy of
 *   the CDict's tables incurs a fixed cost at the beginning of the
 *   compression. For small compressions (< 8 KB), that copy can dominate
 *   the cost of the compression.
 *
 * - The CDict's tables can be used in-place. In this model, compression is
 *   slower per input byte, because the compressor has to search two sets of
 *   tables. However, this model incurs no start-up cost (as long as the
 *   working context's tables can be reused). For small inputs, this can be
 *   faster than copying the CDict's tables.
 *
 * - The CDict's tables are not used at all, and instead we use the working
 *   context alone to reload the dictionary and use params based on the source
 *   size. See ZSTD_compress_insertDictionary() and ZSTD_compress_usingDict().
 *   This method is effective when the dictionary sizes are very small relative
 *   to the input size, and the input size is fairly large to begin with.
 *
 * Zstd has a simple internal heuristic that selects which strategy to use
 * at the beginning of a compression. However, if experimentation shows that
 * Zstd is making poor choices, it is possible to override that choice with
 * this enum.
 */
ZSTD_dictDefaultAttach = 0
const /* Use the default heuristic. */
ZSTD_dictForceAttach = 1
const /* Never copy the dictionary. */
ZSTD_dictForceCopy = 2
const /* Always copy the dictionary. */
ZSTD_dictForceLoad = 3
const ZSTD_sf_noBlockDelimiters = 0
const /* ZSTD_Sequence[] has no block delimiters, just sequences */
ZSTD_sf_explicitBlockDelimiters = 1

type ZSTD_Sequence = struct {
	Foffset      uint32
	FlitLength   uint32
	FmatchLength uint32
	Frep         uint32
}

type ZSTD_compressionParameters = struct {
	FwindowLog    uint32
	FchainLog     uint32
	FhashLog      uint32
	FsearchLog    uint32
	FminMatch     uint32
	FtargetLength uint32
	Fstrategy     ZSTD_strategy
}

type ZSTD_frameParameters = struct {
	FcontentSizeFlag int32
	FchecksumFlag    int32
	FnoDictIDFlag    int32
}

type ZSTD_parameters = struct {
	FcParams ZSTD_compressionParameters
	FfParams ZSTD_frameParameters
}

type ZSTD_dictContentType_e = int32

type ZSTD_dictLoadMethod_e = int32

const ZSTD_dlm_byCopy = 0
const /**< Copy dictionary content internally */
ZSTD_dlm_byRef = 1

type ZSTD_format_e = int32

type ZSTD_forceIgnoreChecksum_e = int32

type ZSTD_refMultipleDDicts_e = int32

type ZSTD_dictAttachPref_e = int32

type ZSTD_literalCompressionMode_e = int32

const ZSTD_lcm_auto = 0
const /**< Automatically determine the compression mode based on the compression level.
 *   Negative compression levels will be uncompressed, and positive compression
 *   levels will be compressed. */
ZSTD_lcm_huffman = 1
const /**< Always attempt Huffman compression. Uncompressed literals will still be
 *   emitted if Huffman compression is not profitable. */
ZSTD_lcm_uncompressed = 2

type ZSTD_ParamSwitch_e = int32

type ZSTD_FrameType_e = int32

const ZSTD_frame = 0
const ZSTD_skippableFrame = 1

type ZSTD_FrameHeader = struct {
	FframeContentSize uint64
	FwindowSize       uint64
	FblockSizeMax     uint32
	FframeType        ZSTD_FrameType_e
	FheaderSize       uint32
	FdictID           uint32
	FchecksumFlag     uint32
	F_reserved1       uint32
	F_reserved2       uint32
}

/*! ZSTD_DECOMPRESS_MARGIN() :
 * Similar to ZSTD_decompressionMargin(), but instead of computing the margin from
 * the compressed frame, compute it from the original size and the blockSizeLog.
 * See ZSTD_decompressionMargin() for details.
 *
 * WARNING: This macro does not support multi-frame input, the input must be a single
 * zstd frame. If you need that support use the function, or implement it yourself.
 *
 * @param originalSize The original uncompressed size of the data.
 * @param blockSize    The block size == MIN(windowSize, ZSTD_BLOCKSIZE_MAX).
 *                     Unless you explicitly set the windowLog smaller than
 *                     ZSTD_BLOCKSIZELOG_MAX you can just use ZSTD_BLOCKSIZE_MAX.
 */

type ZSTD_SequenceFormat_e = int32

// C documentation
//
//	/*! Custom memory allocation :
//	 *  These prototypes make it possible to pass your own allocation/free functions.
//	 *  ZSTD_customMem is provided at creation time, using ZSTD_create*_advanced() variants listed below.
//	 *  All allocation/free operations will be completed using these custom variants instead of regular <stdlib.h> ones.
//	 */
type ZSTD_allocFunction = uintptr

type ZSTD_freeFunction = uintptr

type ZSTD_customMem = struct {
	FcustomAlloc ZSTD_allocFunction
	FcustomFree  ZSTD_freeFunction
	Fopaque      uintptr
}

var ZSTD_defaultCMem = ZSTD_customMem{}

// C documentation
//
//	/*! Thread pool :
//	 *  These prototypes make it possible to share a thread pool among multiple compression contexts.
//	 *  This can limit resources for applications with multiple threads where each one uses
//	 *  a threaded compression mode (via ZSTD_c_nbWorkers parameter).
//	 *  ZSTD_createThreadPool creates a new thread pool with a given number of threads.
//	 *  Note that the lifetime of such pool must exist while being used.
//	 *  ZSTD_CCtx_refThreadPool assigns a thread pool to a context (use NULL argument value
//	 *  to use an internal thread pool).
//	 *  ZSTD_freeThreadPool frees a thread pool, accepts NULL pointer.
//	 */
type ZSTD_threadPool = struct {
	FcustomMem      ZSTD_customMem
	Fthreads        uintptr
	FthreadCapacity size_t
	FthreadLimit    size_t
	Fqueue          uintptr
	FqueueHead      size_t
	FqueueTail      size_t
	FqueueSize      size_t
	FnumThreadsBusy size_t
	FqueueEmpty     int32
	FqueueMutex     pthread_mutex_t
	FqueuePushCond  pthread_cond_t
	FqueuePopCond   pthread_cond_t
	Fshutdown       int32
}

// C documentation
//
//	/*! Thread pool :
//	 *  These prototypes make it possible to share a thread pool among multiple compression contexts.
//	 *  This can limit resources for applications with multiple threads where each one uses
//	 *  a threaded compression mode (via ZSTD_c_nbWorkers parameter).
//	 *  ZSTD_createThreadPool creates a new thread pool with a given number of threads.
//	 *  Note that the lifetime of such pool must exist while being used.
//	 *  ZSTD_CCtx_refThreadPool assigns a thread pool to a context (use NULL argument value
//	 *  to use an internal thread pool).
//	 *  ZSTD_freeThreadPool frees a thread pool, accepts NULL pointer.
//	 */
type POOL_ctx_s = ZSTD_threadPool

type ZSTD_frameProgression = struct {
	Fingested        uint64
	Fconsumed        uint64
	Fproduced        uint64
	Fflushed         uint64
	FcurrentJobID    uint32
	FnbActiveWorkers uint32
}

/* ********************* BLOCK-LEVEL SEQUENCE PRODUCER API *********************
 *
 * *** OVERVIEW ***
 * The Block-Level Sequence Producer API allows users to provide their own custom
 * sequence producer which libzstd invokes to process each block. The produced list
 * of sequences (literals and matches) is then post-processed by libzstd to produce
 * valid compressed blocks.
 *
 * This block-level offload API is a more granular complement of the existing
 * frame-level offload API compressSequences() (introduced in v1.5.1). It offers
 * an easier migration story for applications already integrated with libzstd: the
 * user application continues to invoke the same compression functions
 * ZSTD_compress2() or ZSTD_compressStream2() as usual, and transparently benefits
 * from the specific advantages of the external sequence producer. For example,
 * the sequence producer could be tuned to take advantage of known characteristics
 * of the input, to offer better speed / ratio, or could leverage hardware
 * acceleration not available within libzstd itself.
 *
 * See contrib/externalSequenceProducer for an example program employing the
 * Block-Level Sequence Producer API.
 *
 * *** USAGE ***
 * The user is responsible for implementing a function of type
 * ZSTD_sequenceProducer_F. For each block, zstd will pass the following
 * arguments to the user-provided function:
 *
 *   - sequenceProducerState: a pointer to a user-managed state for the sequence
 *     producer.
 *
 *   - outSeqs, outSeqsCapacity: an output buffer for the sequence producer.
 *     outSeqsCapacity is guaranteed >= ZSTD_sequenceBound(srcSize). The memory
 *     backing outSeqs is managed by the CCtx.
 *
 *   - src, srcSize: an input buffer for the sequence producer to parse.
 *     srcSize is guaranteed to be <= ZSTD_BLOCKSIZE_MAX.
 *
 *   - dict, dictSize: a history buffer, which may be empty, which the sequence
 *     producer may reference as it parses the src buffer. Currently, zstd will
 *     always pass dictSize == 0 into external sequence producers, but this will
 *     change in the future.
 *
 *   - compressionLevel: a signed integer representing the zstd compression level
 *     set by the user for the current operation. The sequence producer may choose
 *     to use this information to change its compression strategy and speed/ratio
 *     tradeoff. Note: the compression level does not reflect zstd parameters set
 *     through the advanced API.
 *
 *   - windowSize: a size_t representing the maximum allowed offset for external
 *     sequences. Note that sequence offsets are sometimes allowed to exceed the
 *     windowSize if a dictionary is present, see doc/zstd_compression_format.md
 *     for details.
 *
 * The user-provided function shall return a size_t representing the number of
 * sequences written to outSeqs. This return value will be treated as an error
 * code if it is greater than outSeqsCapacity. The return value must be non-zero
 * if srcSize is non-zero. The ZSTD_SEQUENCE_PRODUCER_ERROR macro is provided
 * for convenience, but any value greater than outSeqsCapacity will be treated as
 * an error code.
 *
 * If the user-provided function does not return an error code, the sequences
 * written to outSeqs must be a valid parse of the src buffer. Data corruption may
 * occur if the parse is not valid. A parse is defined to be valid if the
 * following conditions hold:
 *   - The sum of matchLengths and literalLengths must equal srcSize.
 *   - All sequences in the parse, except for the final sequence, must have
 *     matchLength >= ZSTD_MINMATCH_MIN. The final sequence must have
 *     matchLength >= ZSTD_MINMATCH_MIN or matchLength == 0.
 *   - All offsets must respect the windowSize parameter as specified in
 *     doc/zstd_compression_format.md.
 *   - If the final sequence has matchLength == 0, it must also have offset == 0.
 *
 * zstd will only validate these conditions (and fail compression if they do not
 * hold) if the ZSTD_c_validateSequences cParam is enabled. Note that sequence
 * validation has a performance cost.
 *
 * If the user-provided function returns an error, zstd will either fall back
 * to an internal sequence producer or fail the compression operation. The user can
 * choose between the two behaviors by setting the ZSTD_c_enableSeqProducerFallback
 * cParam. Fallback compression will follow any other cParam settings, such as
 * compression level, the same as in a normal compression operation.
 *
 * The user shall instruct zstd to use a particular ZSTD_sequenceProducer_F
 * function by calling
 *         ZSTD_registerSequenceProducer(cctx,
 *                                       sequenceProducerState,
 *                                       sequenceProducer)
 * This setting will persist until the next parameter reset of the CCtx.
 *
 * The sequenceProducerState must be initialized by the user before calling
 * ZSTD_registerSequenceProducer(). The user is responsible for destroying the
 * sequenceProducerState.
 *
 * *** LIMITATIONS ***
 * This API is compatible with all zstd compression APIs which respect advanced parameters.
 * However, there are three limitations:
 *
 * First, the ZSTD_c_enableLongDistanceMatching cParam is not currently supported.
 * COMPRESSION WILL FAIL if it is enabled and the user tries to compress with a block-level
 * external sequence producer.
 *   - Note that ZSTD_c_enableLongDistanceMatching is auto-enabled by default in some
 *     cases (see its documentation for details). Users must explicitly set
 *     ZSTD_c_enableLongDistanceMatching to ZSTD_ps_disable in such cases if an external
 *     sequence producer is registered.
 *   - As of this writing, ZSTD_c_enableLongDistanceMatching is disabled by default
 *     whenever ZSTD_c_windowLog < 128MB, but that's subject to change. Users should
 *     check the docs on ZSTD_c_enableLongDistanceMatching whenever the Block-Level Sequence
 *     Producer API is used in conjunction with advanced settings (like ZSTD_c_windowLog).
 *
 * Second, history buffers are not currently supported. Concretely, zstd will always pass
 * dictSize == 0 to the external sequence producer (for now). This has two implications:
 *   - Dictionaries are not currently supported. Compression will *not* fail if the user
 *     references a dictionary, but the dictionary won't have any effect.
 *   - Stream history is not currently supported. All advanced compression APIs, including
 *     streaming APIs, work with external sequence producers, but each block is treated as
 *     an independent chunk without history from previous blocks.
 *
 * Third, multi-threading within a single compression is not currently supported. In other words,
 * COMPRESSION WILL FAIL if ZSTD_c_nbWorkers > 0 and an external sequence producer is registered.
 * Multi-threading across compressions is fine: simply create one CCtx per thread.
 *
 * Long-term, we plan to overcome all three limitations. There is no technical blocker to
 * overcoming them. It is purely a question of engineering effort.
 */

type ZSTD_sequenceProducer_F = uintptr

type ZSTD_nextInputType_e = int32

const ZSTDnit_frameHeader = 0
const ZSTDnit_blockHeader = 1
const ZSTDnit_block = 2
const ZSTDnit_lastBlock = 3
const ZSTDnit_checksum = 4
const ZSTDnit_skippableFrame = 5 /**< insert uncompressed block into `dctx` history. Useful for multi-blocks decompression. */

/**** ended inlining ../zstd.h ****/

/* custom memory allocation functions */
func ZSTD_customMalloc(tls *libc.TLS, size size_t, customMem ZSTD_customMem) (r uintptr) {
	if customMem.FcustomAlloc != 0 {
		return (*(*func(*libc.TLS, uintptr, size_t) uintptr)(unsafe.Pointer(&struct{ uintptr }{customMem.FcustomAlloc})))(tls, customMem.Fopaque, size)
	}
	return libc.Xmalloc(tls, size)
}

func ZSTD_customCalloc(tls *libc.TLS, size size_t, customMem ZSTD_customMem) (r uintptr) {
	var ptr uintptr
	_ = ptr
	if customMem.FcustomAlloc != 0 {
		/* calloc implemented as malloc+memset;
		 * not as efficient as calloc, but next best guess for custom malloc */
		ptr = (*(*func(*libc.TLS, uintptr, size_t) uintptr)(unsafe.Pointer(&struct{ uintptr }{customMem.FcustomAlloc})))(tls, customMem.Fopaque, size)
		libc.Xmemset(tls, ptr, 0, size)
		return ptr
	}
	return libc.Xcalloc(tls, libc.Uint64FromInt32(libc.Int32FromInt32(1)), size)
}

func ZSTD_customFree(tls *libc.TLS, ptr uintptr, customMem ZSTD_customMem) {
	if ptr != libc.UintptrFromInt32(0) {
		if customMem.FcustomFree != 0 {
			(*(*func(*libc.TLS, uintptr, uintptr))(unsafe.Pointer(&struct{ uintptr }{customMem.FcustomFree})))(tls, customMem.Fopaque, ptr)
		} else {
			libc.Xfree(tls, ptr)
		}
	}
}

/**** ended inlining ../common/allocations.h ****/
/**** skipping file: zstd_deps.h ****/
/**** skipping file: debug.h ****/
/**** start inlining pool.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/

type POOL_ctx = struct {
	FcustomMem      ZSTD_customMem
	Fthreads        uintptr
	FthreadCapacity size_t
	FthreadLimit    size_t
	Fqueue          uintptr
	FqueueHead      size_t
	FqueueTail      size_t
	FqueueSize      size_t
	FnumThreadsBusy size_t
	FqueueEmpty     int32
	FqueueMutex     pthread_mutex_t
	FqueuePushCond  pthread_cond_t
	FqueuePopCond   pthread_cond_t
	Fshutdown       int32
}

// C documentation
//
//	/*! POOL_function :
//	 *  The function type that can be added to a thread pool.
//	 */
type POOL_function = uintptr

/**** ended inlining pool.h ****/

/* ======   Compiler specifics   ====== */

/**** skipping file: threading.h ****/

// C documentation
//
//	/* A job is a function and an opaque argument */
type POOL_job = struct {
	Ffunction POOL_function
	Fopaque   uintptr
}

/**** ended inlining pool.h ****/

/* ======   Compiler specifics   ====== */

/**** skipping file: threading.h ****/

// C documentation
//
//	/* A job is a function and an opaque argument */
type POOL_job_s = POOL_job

// C documentation
//
//	/* POOL_thread() :
//	 * Work thread for the thread pool.
//	 * Waits for jobs and executes them.
//	 * @returns : NULL on failure else non-null.
//	 */
func POOL_thread(tls *libc.TLS, opaque uintptr) (r uintptr) {
	var ctx uintptr
	var job POOL_job
	_, _ = ctx, job
	ctx = opaque
	if !(ctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	for {
		/* Lock the mutex and wait for a non-empty queue or until shutdown */
		libc.Xpthread_mutex_lock(tls, ctx+96)
		for (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty != 0 || (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy >= (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit {
			if (*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown != 0 {
				/* even if !queueEmpty, (possible if numThreadsBusy >= threadLimit),
				 * a few threads will be shutdown while !queueEmpty,
				 * but enough threads will remain active to finish the queue */
				libc.Xpthread_mutex_unlock(tls, ctx+96)
				return opaque
			}
			libc.Xpthread_cond_wait(tls, ctx+184, ctx+96)
		}
		/* Pop a job off the queue */
		job = *(*POOL_job)(unsafe.Pointer((*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue + uintptr((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead)*16))
		(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead = ((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead + uint64(1)) % (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize
		(*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy = (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy + 1
		(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty = libc.BoolInt32((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead == (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail)
		/* Unlock the mutex, signal a pusher, and run the job */
		libc.Xpthread_cond_signal(tls, ctx+136)
		libc.Xpthread_mutex_unlock(tls, ctx+96)
		(*(*func(*libc.TLS, uintptr))(unsafe.Pointer(&struct{ uintptr }{job.Ffunction})))(tls, job.Fopaque)
		/* If the intended queue size was 0, signal after finishing job */
		libc.Xpthread_mutex_lock(tls, ctx+96)
		(*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy = (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy - 1
		libc.Xpthread_cond_signal(tls, ctx+136)
		libc.Xpthread_mutex_unlock(tls, ctx+96)
		goto _1
	_1:
	} /* for (;;) */
	/* Unreachable */
	return r
}

// C documentation
//
//	/* ZSTD_createThreadPool() : public access point */
func ZSTD_createThreadPool(tls *libc.TLS, numThreads size_t) (r uintptr) {
	return POOL_create(tls, numThreads, uint64(0))
}

func POOL_create(tls *libc.TLS, numThreads size_t, queueSize size_t) (r uintptr) {
	return POOL_create_advanced(tls, numThreads, queueSize, ZSTD_defaultCMem)
}

func POOL_create_advanced(tls *libc.TLS, numThreads size_t, queueSize size_t, customMem ZSTD_customMem) (r uintptr) {
	var ctx uintptr
	var error1 int32
	var i size_t
	_, _, _ = ctx, error1, i
	/* Check parameters */
	if !(numThreads != 0) {
		return libc.UintptrFromInt32(0)
	}
	/* Allocate the context and zero initialize */
	ctx = ZSTD_customCalloc(tls, uint64(240), customMem)
	if !(ctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	/* Initialize the job queue.
	 * It needs one extra space since one space is wasted to differentiate
	 * empty and full queues.
	 */
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize = queueSize + uint64(1)
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue = ZSTD_customCalloc(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize*uint64(16), customMem)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty = int32(1)
	error1 = 0
	error1 = error1 | libc.Xpthread_mutex_init(tls, ctx+96, libc.UintptrFromInt32(0))
	error1 = error1 | libc.Xpthread_cond_init(tls, ctx+136, libc.UintptrFromInt32(0))
	error1 = error1 | libc.Xpthread_cond_init(tls, ctx+184, libc.UintptrFromInt32(0))
	if error1 != 0 {
		POOL_free(tls, ctx)
		return libc.UintptrFromInt32(0)
	}
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown = 0
	/* Allocate space for the thread handles */
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads = ZSTD_customCalloc(tls, numThreads*uint64(8), customMem)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = uint64(0)
	(*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem = customMem
	/* Check for errors */
	if !((*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads != 0) || !((*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue != 0) {
		POOL_free(tls, ctx)
		return libc.UintptrFromInt32(0)
	}
	/* Initialize the threads */
	i = uint64(0)
	for {
		if !(i < numThreads) {
			break
		}
		if libc.Xpthread_create(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads+uintptr(i)*8, libc.UintptrFromInt32(0), __ccgo_fp(POOL_thread), ctx) != 0 {
			(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = i
			POOL_free(tls, ctx)
			return libc.UintptrFromInt32(0)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = numThreads
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit = numThreads
	return ctx
}

// C documentation
//
//	/*! POOL_join() :
//	    Shutdown the queue, wake any sleeping threads, and join all of the threads.
//	*/
func POOL_join(tls *libc.TLS, ctx uintptr) {
	var i size_t
	_ = i
	/* Shut down the queue */
	libc.Xpthread_mutex_lock(tls, ctx+96)
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown = int32(1)
	libc.Xpthread_mutex_unlock(tls, ctx+96)
	/* Wake up sleeping threads */
	libc.Xpthread_cond_broadcast(tls, ctx+136)
	libc.Xpthread_cond_broadcast(tls, ctx+184)
	/* Join all of the threads */
	i = uint64(0)
	for {
		if !(i < (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity) {
			break
		}
		libc.Xpthread_join(tls, *(*pthread_t)(unsafe.Pointer((*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads + uintptr(i)*8)), libc.UintptrFromInt32(0)) /* note : could fail */
		goto _1
	_1:
		;
		i = i + 1
	}
}

func POOL_free(tls *libc.TLS, ctx uintptr) {
	if !(ctx != 0) {
		return
	}
	POOL_join(tls, ctx)
	libc.Xpthread_mutex_destroy(tls, ctx+96)
	libc.Xpthread_cond_destroy(tls, ctx+136)
	libc.Xpthread_cond_destroy(tls, ctx+184)
	ZSTD_customFree(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	ZSTD_customFree(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	ZSTD_customFree(tls, ctx, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
}

// C documentation
//
//	/*! POOL_joinJobs() :
//	 *  Waits for all queued jobs to finish executing.
//	 */
func POOL_joinJobs(tls *libc.TLS, ctx uintptr) {
	libc.Xpthread_mutex_lock(tls, ctx+96)
	for !((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty != 0) || (*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy > uint64(0) {
		libc.Xpthread_cond_wait(tls, ctx+136, ctx+96)
	}
	libc.Xpthread_mutex_unlock(tls, ctx+96)
}

func ZSTD_freeThreadPool(tls *libc.TLS, pool uintptr) {
	POOL_free(tls, pool)
}

func POOL_sizeof(tls *libc.TLS, ctx uintptr) (r size_t) {
	if ctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* supports sizeof NULL */
	return uint64(240) + (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize*uint64(16) + (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity*uint64(8)
}

// C documentation
//
//	/* @return : 0 on success, 1 on error */
func POOL_resize_internal(tls *libc.TLS, ctx uintptr, numThreads size_t) (r int32) {
	var threadId size_t
	var threadPool uintptr
	_, _ = threadId, threadPool
	if numThreads <= (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity {
		if !(numThreads != 0) {
			return int32(1)
		}
		(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit = numThreads
		return 0
	}
	/* numThreads > threadCapacity */
	threadPool = ZSTD_customCalloc(tls, numThreads*uint64(8), (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	if !(threadPool != 0) {
		return int32(1)
	}
	/* replace existing thread pool */
	libc.Xmemcpy(tls, threadPool, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads, (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity*libc.Uint64FromInt64(8))
	ZSTD_customFree(tls, (*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads, (*POOL_ctx)(unsafe.Pointer(ctx)).FcustomMem)
	(*POOL_ctx)(unsafe.Pointer(ctx)).Fthreads = threadPool
	/* Initialize additional threads */
	threadId = (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity
	for {
		if !(threadId < numThreads) {
			break
		}
		if libc.Xpthread_create(tls, threadPool+uintptr(threadId)*8, libc.UintptrFromInt32(0), __ccgo_fp(POOL_thread), ctx) != 0 {
			(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = threadId
			return int32(1)
		}
		goto _1
	_1:
		;
		threadId = threadId + 1
	}
	/* successfully expanded */
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadCapacity = numThreads
	(*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit = numThreads
	return 0
}

// C documentation
//
//	/* @return : 0 on success, 1 on error */
func POOL_resize(tls *libc.TLS, ctx uintptr, numThreads size_t) (r int32) {
	var result int32
	_ = result
	if ctx == libc.UintptrFromInt32(0) {
		return int32(1)
	}
	libc.Xpthread_mutex_lock(tls, ctx+96)
	result = POOL_resize_internal(tls, ctx, numThreads)
	libc.Xpthread_cond_broadcast(tls, ctx+184)
	libc.Xpthread_mutex_unlock(tls, ctx+96)
	return result
}

// C documentation
//
//	/**
//	 * Returns 1 if the queue is full and 0 otherwise.
//	 *
//	 * When queueSize is 1 (pool was created with an intended queueSize of 0),
//	 * then a queue is empty if there is a thread free _and_ no job is waiting.
//	 */
func isQueueFull(tls *libc.TLS, ctx uintptr) (r int32) {
	if (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize > uint64(1) {
		return libc.BoolInt32((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueHead == ((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail+uint64(1))%(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize)
	} else {
		return libc.BoolInt32((*POOL_ctx)(unsafe.Pointer(ctx)).FnumThreadsBusy == (*POOL_ctx)(unsafe.Pointer(ctx)).FthreadLimit || !((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty != 0))
	}
	return r
}

func POOL_add_internal(tls *libc.TLS, ctx uintptr, __ccgo_fp_function POOL_function, opaque uintptr) {
	var job POOL_job
	_ = job
	job.Ffunction = __ccgo_fp_function
	job.Fopaque = opaque
	if (*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown != 0 {
		return
	}
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueEmpty = 0
	*(*POOL_job)(unsafe.Pointer((*POOL_ctx)(unsafe.Pointer(ctx)).Fqueue + uintptr((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail)*16)) = job
	(*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail = ((*POOL_ctx)(unsafe.Pointer(ctx)).FqueueTail + uint64(1)) % (*POOL_ctx)(unsafe.Pointer(ctx)).FqueueSize
	libc.Xpthread_cond_signal(tls, ctx+184)
}

type __ccgo_fp__XPOOL_add_1 = func(*libc.TLS, uintptr)

func POOL_add(tls *libc.TLS, ctx uintptr, __ccgo_fp_function POOL_function, opaque uintptr) {
	libc.Xpthread_mutex_lock(tls, ctx+96)
	/* Wait until there is space in the queue for the new job */
	for isQueueFull(tls, ctx) != 0 && !((*POOL_ctx)(unsafe.Pointer(ctx)).Fshutdown != 0) {
		libc.Xpthread_cond_wait(tls, ctx+136, ctx+96)
	}
	POOL_add_internal(tls, ctx, __ccgo_fp_function, opaque)
	libc.Xpthread_mutex_unlock(tls, ctx+96)
}

type __ccgo_fp__XPOOL_tryAdd_1 = func(*libc.TLS, uintptr)

func POOL_tryAdd(tls *libc.TLS, ctx uintptr, __ccgo_fp_function POOL_function, opaque uintptr) (r int32) {
	libc.Xpthread_mutex_lock(tls, ctx+96)
	if isQueueFull(tls, ctx) != 0 {
		libc.Xpthread_mutex_unlock(tls, ctx+96)
		return 0
	}
	POOL_add_internal(tls, ctx, __ccgo_fp_function, opaque)
	libc.Xpthread_mutex_unlock(tls, ctx+96)
	return int32(1)
}

/**** ended inlining common/pool.c ****/
/**** start inlining common/zstd_common.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: error_private.h ****/
/**** start inlining zstd_internal.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* this module contains definitions which must be identical
 * across compression, decompression and dictBuilder.
 * It also contains a few functions useful to at least 2 of them
 * and which benefit from being inlined */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: compiler.h ****/
/**** start inlining cpu.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**
 * Implementation taken from folly/CpuId.h
 * https://github.com/facebook/folly/blob/master/folly/CpuId.h
 */

/**** skipping file: mem.h ****/

type ZSTD_cpuid_t = struct {
	Ff1c U32
	Ff1d U32
	Ff7b U32
	Ff7c U32
}

func ZSTD_cpuid(tls *libc.TLS) (r ZSTD_cpuid_t) {
	var cpuid ZSTD_cpuid_t
	var f1c, f1d, f7b, f7c U32
	_, _, _, _, _ = cpuid, f1c, f1d, f7b, f7c
	f1c = uint32(0)
	f1d = uint32(0)
	f7b = uint32(0)
	f7c = uint32(0)
	cpuid.Ff1c = f1c
	cpuid.Ff1d = f1d
	cpuid.Ff7b = f7b
	cpuid.Ff7c = f7c
	return cpuid
	return r
}

// C documentation
//
//	/* cpuid(1): Processor Info and Feature Bits. */
func ZSTD_cpuid_sse3(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(0)) != uint32(0))
}

func ZSTD_cpuid_pclmuldq(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(1)) != uint32(0))
}

func ZSTD_cpuid_dtes64(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(2)) != uint32(0))
}

func ZSTD_cpuid_monitor(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(3)) != uint32(0))
}

func ZSTD_cpuid_dscpl(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(4)) != uint32(0))
}

func ZSTD_cpuid_vmx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(5)) != uint32(0))
}

func ZSTD_cpuid_smx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(6)) != uint32(0))
}

func ZSTD_cpuid_eist(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(7)) != uint32(0))
}

func ZSTD_cpuid_tm2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(8)) != uint32(0))
}

func ZSTD_cpuid_ssse3(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(9)) != uint32(0))
}

func ZSTD_cpuid_cnxtid(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(10)) != uint32(0))
}

func ZSTD_cpuid_fma(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(12)) != uint32(0))
}

func ZSTD_cpuid_cx16(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(13)) != uint32(0))
}

func ZSTD_cpuid_xtpr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(14)) != uint32(0))
}

func ZSTD_cpuid_pdcm(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(15)) != uint32(0))
}

func ZSTD_cpuid_pcid(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(17)) != uint32(0))
}

func ZSTD_cpuid_dca(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(18)) != uint32(0))
}

func ZSTD_cpuid_sse41(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) != uint32(0))
}

func ZSTD_cpuid_sse42(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(20)) != uint32(0))
}

func ZSTD_cpuid_x2apic(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(21)) != uint32(0))
}

func ZSTD_cpuid_movbe(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(22)) != uint32(0))
}

func ZSTD_cpuid_popcnt(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(23)) != uint32(0))
}

func ZSTD_cpuid_tscdeadline(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) != uint32(0))
}

func ZSTD_cpuid_aes(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(25)) != uint32(0))
}

func ZSTD_cpuid_xsave(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(26)) != uint32(0))
}

func ZSTD_cpuid_osxsave(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(27)) != uint32(0))
}

func ZSTD_cpuid_avx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(28)) != uint32(0))
}

func ZSTD_cpuid_f16c(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(29)) != uint32(0))
}

func ZSTD_cpuid_rdrand(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(30)) != uint32(0))
}

func ZSTD_cpuid_fpu(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(0)) != uint32(0))
}

func ZSTD_cpuid_vme(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(1)) != uint32(0))
}

func ZSTD_cpuid_de(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(2)) != uint32(0))
}

func ZSTD_cpuid_pse(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(3)) != uint32(0))
}

func ZSTD_cpuid_tsc(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(4)) != uint32(0))
}

func ZSTD_cpuid_msr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(5)) != uint32(0))
}

func ZSTD_cpuid_pae(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(6)) != uint32(0))
}

func ZSTD_cpuid_mce(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(7)) != uint32(0))
}

func ZSTD_cpuid_cx8(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(8)) != uint32(0))
}

func ZSTD_cpuid_apic(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(9)) != uint32(0))
}

func ZSTD_cpuid_sep(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(11)) != uint32(0))
}

func ZSTD_cpuid_mtrr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(12)) != uint32(0))
}

func ZSTD_cpuid_pge(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(13)) != uint32(0))
}

func ZSTD_cpuid_mca(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(14)) != uint32(0))
}

func ZSTD_cpuid_cmov(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(15)) != uint32(0))
}

func ZSTD_cpuid_pat(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(16)) != uint32(0))
}

func ZSTD_cpuid_pse36(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(17)) != uint32(0))
}

func ZSTD_cpuid_psn(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(18)) != uint32(0))
}

func ZSTD_cpuid_clfsh(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) != uint32(0))
}

func ZSTD_cpuid_ds(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(21)) != uint32(0))
}

func ZSTD_cpuid_acpi(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(22)) != uint32(0))
}

func ZSTD_cpuid_mmx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(23)) != uint32(0))
}

func ZSTD_cpuid_fxsr(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) != uint32(0))
}

func ZSTD_cpuid_sse(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(25)) != uint32(0))
}

func ZSTD_cpuid_sse2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(26)) != uint32(0))
}

func ZSTD_cpuid_ss(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(27)) != uint32(0))
}

func ZSTD_cpuid_htt(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(28)) != uint32(0))
}

func ZSTD_cpuid_tm(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(29)) != uint32(0))
}

func ZSTD_cpuid_pbe(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff1d&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)) != uint32(0))
}

// C documentation
//
//	/* cpuid(7): Extended Features. */
func ZSTD_cpuid_bmi1(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(3)) != uint32(0))
}

func ZSTD_cpuid_hle(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(4)) != uint32(0))
}

func ZSTD_cpuid_avx2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(5)) != uint32(0))
}

func ZSTD_cpuid_smep(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(7)) != uint32(0))
}

func ZSTD_cpuid_bmi2(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(8)) != uint32(0))
}

func ZSTD_cpuid_erms(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(9)) != uint32(0))
}

func ZSTD_cpuid_invpcid(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(10)) != uint32(0))
}

func ZSTD_cpuid_rtm(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(11)) != uint32(0))
}

func ZSTD_cpuid_mpx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(14)) != uint32(0))
}

func ZSTD_cpuid_avx512f(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(16)) != uint32(0))
}

func ZSTD_cpuid_avx512dq(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(17)) != uint32(0))
}

func ZSTD_cpuid_rdseed(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(18)) != uint32(0))
}

func ZSTD_cpuid_adx(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) != uint32(0))
}

func ZSTD_cpuid_smap(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(20)) != uint32(0))
}

func ZSTD_cpuid_avx512ifma(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(21)) != uint32(0))
}

func ZSTD_cpuid_pcommit(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(22)) != uint32(0))
}

func ZSTD_cpuid_clflushopt(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(23)) != uint32(0))
}

func ZSTD_cpuid_clwb(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) != uint32(0))
}

func ZSTD_cpuid_avx512pf(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(26)) != uint32(0))
}

func ZSTD_cpuid_avx512er(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(27)) != uint32(0))
}

func ZSTD_cpuid_avx512cd(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(28)) != uint32(0))
}

func ZSTD_cpuid_sha(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(29)) != uint32(0))
}

func ZSTD_cpuid_avx512bw(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(30)) != uint32(0))
}

func ZSTD_cpuid_avx512vl(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7b&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)) != uint32(0))
}

func ZSTD_cpuid_prefetchwt1(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(0)) != uint32(0))
}

func ZSTD_cpuid_avx512vbmi(tls *libc.TLS, cpuid ZSTD_cpuid_t) (r int32) {
	return libc.BoolInt32(cpuid.Ff7c&(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(1)) != uint32(0))
}

// C documentation
//
//	/* ****************************
//	*  Common basic types
//	******************************/
//	/*!
//	 * @brief Exit code for the streaming API.
//	 */
type XXH_NAMESPACEXXH_errorcode = int32

const XXH_NAMESPACEXXH_OK = 0
const /*!< OK */
XXH_NAMESPACEXXH_ERROR = 1

// C documentation
//
//	/*-**********************************************************************
//	*  32-bit hash
//	************************************************************************/
type XXH32_hash_t = uint32

// C documentation
//
//	/*!
//	 * @typedef struct XXH32_state_s XXH32_state_t
//	 * @brief The opaque state struct for the XXH32 streaming API.
//	 *
//	 * @see XXH32_state_s for details.
//	 */
type XXH_NAMESPACEXXH32_state_t = struct {
	Ftotal_len_32 XXH32_hash_t
	Flarge_len    XXH32_hash_t
	Fv            [4]XXH32_hash_t
	Fmem32        [4]XXH32_hash_t
	Fmemsize      XXH32_hash_t
	Freserved     XXH32_hash_t
}

// C documentation
//
//	/*!
//	 * @typedef struct XXH32_state_s XXH32_state_t
//	 * @brief The opaque state struct for the XXH32 streaming API.
//	 *
//	 * @see XXH32_state_s for details.
//	 */
type XXH_NAMESPACEXXH32_state_s = XXH_NAMESPACEXXH32_state_t

/*******   Canonical representation   *******/

// C documentation
//
//	/*!
//	 * @brief Canonical (big endian) representation of @ref XXH32_hash_t.
//	 */
type XXH_NAMESPACEXXH32_canonical_t = struct {
	Fdigest [4]uint8
}

/*! @cond Doxygen ignores this part */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*
 * C23 __STDC_VERSION__ number hasn't been specified yet. For now
 * leave as `201711L` (C17 + 1).
 * TODO: Update to correct value when its been specified.
 */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/* C-language Attributes are added in C23. */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*
 * Define XXH_FALLTHROUGH macro for annotating switch case with the 'fallthrough' attribute
 * introduced in CPP17 and C23.
 * CPP17 : https://en.cppreference.com/w/cpp/language/attributes/fallthrough
 * C23   : https://en.cppreference.com/w/c/language/attributes/fallthrough
 */
/*! @endcond */

/*! @cond Doxygen ignores this part */
/*
 * Define XXH_NOESCAPE for annotated pointers in public API.
 * https://clang.llvm.org/docs/AttributeReference.html#noescape
 * As of writing this, only supported by clang.
 */
/*! @endcond */

/*!
 * @}
 * @ingroup public
 * @{
 */

// C documentation
//
//	/*-**********************************************************************
//	*  64-bit hash
//	************************************************************************/
type XXH64_hash_t = uint64

// C documentation
//
//	/*******   Streaming   *******/
//	/*!
//	 * @brief The opaque state struct for the XXH64 streaming API.
//	 *
//	 * @see XXH64_state_s for details.
//	 */
type XXH_NAMESPACEXXH64_state_t = struct {
	Ftotal_len  XXH64_hash_t
	Fv          [4]XXH64_hash_t
	Fmem64      [4]XXH64_hash_t
	Fmemsize    XXH32_hash_t
	Freserved32 XXH32_hash_t
	Freserved64 XXH64_hash_t
}

// C documentation
//
//	/*******   Streaming   *******/
//	/*!
//	 * @brief The opaque state struct for the XXH64 streaming API.
//	 *
//	 * @see XXH64_state_s for details.
//	 */
type XXH_NAMESPACEXXH64_state_s = XXH_NAMESPACEXXH64_state_t

/*******   Canonical representation   *******/

// C documentation
//
//	/*!
//	 * @brief Canonical (big endian) representation of @ref XXH64_hash_t.
//	 */
type XXH_NAMESPACEXXH64_canonical_t = struct {
	Fdigest [8]uint8
} /* typedef'd to XXH64_state_t */

/* ======================================================================== */
/* ======================================================================== */
/* ======================================================================== */

/*-**********************************************************************
 * xxHash implementation
 *-**********************************************************************
 * xxHash's implementation used to be hosted inside xxhash.c.
 *
 * However, inlining requires implementation to be visible to the compiler,
 * hence be included alongside the header.
 * Previously, implementation was hosted inside xxhash.c,
 * which was then #included when inlining was activated.
 * This construction created issues with a few build and install systems,
 * as it required xxhash.c to be stored in /include directory.
 *
 * xxHash implementation is now directly integrated within xxhash.h.
 * As a consequence, xxhash.c is no longer needed in /include.
 *
 * xxhash.c is still available and is still useful.
 * In a "normal" setup, when xxhash is not inlined,
 * xxhash.h only exposes the prototypes and public symbols,
 * while xxhash.c can be built into an object file xxhash.o
 * which can then be linked into the final binary.
 ************************************************************************/

/* *************************************
*  Tuning parameters
***************************************/

/*!
 * @defgroup tuning Tuning parameters
 * @{
 *
 * Various macros to control xxHash's behavior.
 */
/*!
 * @}
 */

/* prefer __packed__ structures (method 1) for GCC
 * < ARMv7 with unaligned access (e.g. Raspbian armhf) still uses byte shifting, so we use memcpy
 * which for some reason does unaligned loads. */

/* default to 1 for -Os or -Oz */

/* don't check on sizeopt, x86, aarch64, or arm when unaligned access is available */

/* generally preferable for performance */

/*!
 * @defgroup impl Implementation
 * @{
 */

/* *************************************
*  Includes & Memory related functions
***************************************/

/*
 * Modify the local functions below should you wish to use
 * different memory routines for malloc() and free()
 */

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Modify this function to use a different routine than malloc().
//	 */
func XXH_malloc(tls *libc.TLS, s size_t) (r uintptr) {
	return libc.Xmalloc(tls, s)
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Modify this function to use a different routine than free().
//	 */
func XXH_free(tls *libc.TLS, p uintptr) {
	libc.Xfree(tls, p)
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Modify this function to use a different routine than memcpy().
//	 */
func XXH_memcpy(tls *libc.TLS, dest uintptr, src uintptr, size size_t) (r uintptr) {
	return libc.Xmemcpy(tls, dest, src, size)
}

/* *************************************
*  Compiler Specific Options
***************************************/

/* enable inlining hints */

/* *************************************
*  Debug
***************************************/
/*!
 * @ingroup tuning
 * @def XXH_DEBUGLEVEL
 * @brief Sets the debugging level.
 *
 * XXH_DEBUGLEVEL is expected to be defined externally, typically via the
 * compiler's command line options. The value must be a number.
 */

/* note: use after variable declarations */

/*!
 * @internal
 * @def XXH_COMPILER_GUARD(var)
 * @brief Used to prevent unwanted optimizations for @p var.
 *
 * It uses an empty GCC inline assembly statement with a register constraint
 * which forces @p var into a general purpose register (eg eax, ebx, ecx
 * on x86) and marks it as modified.
 *
 * This is used in a few places to avoid unwanted autovectorization (e.g.
 * XXH32_round()). All vectorization we want is explicit via intrinsics,
 * and _usually_ isn't wanted elsewhere.
 *
 * We also use it to prevent unwanted constant folding for AArch64 in
 * XXH3_initCustomSecret_scalar().
 */

/* Specifically for NEON vectors which use the "w" constraint, on
 * Clang. */

// C documentation
//
//	/* *************************************
//	*  Basic Types
//	***************************************/
type xxh_u8 = uint8

type xxh_u32 = uint32

/* ***   Memory access   *** */

/*!
 * @internal
 * @fn xxh_u32 XXH_read32(const void* ptr)
 * @brief Reads an unaligned 32-bit integer from @p ptr in native endianness.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 *
 * @param ptr The pointer to read from.
 * @return The 32-bit native endian integer from the bytes at @p ptr.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_readLE32(const void* ptr)
 * @brief Reads an unaligned 32-bit little endian integer from @p ptr.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 *
 * @param ptr The pointer to read from.
 * @return The 32-bit little endian integer from the bytes at @p ptr.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_readBE32(const void* ptr)
 * @brief Reads an unaligned 32-bit big endian integer from @p ptr.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 *
 * @param ptr The pointer to read from.
 * @return The 32-bit big endian integer from the bytes at @p ptr.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_readLE32_align(const void* ptr, XXH_alignment align)
 * @brief Like @ref XXH_readLE32(), but has an option for aligned reads.
 *
 * Affected by @ref XXH_FORCE_MEMORY_ACCESS.
 * Note that when @ref XXH_FORCE_ALIGN_CHECK == 0, the @p align parameter is
 * always @ref XXH_alignment::XXH_unaligned.
 *
 * @param ptr The pointer to read from.
 * @param align Whether @p ptr is aligned.
 * @pre
 *   If @p align == @ref XXH_alignment::XXH_aligned, @p ptr must be 4 byte
 *   aligned.
 * @return The 32-bit little endian integer from the bytes at @p ptr.
 */

// C documentation
//
//	/*
//	 * __attribute__((aligned(1))) is supported by gcc and clang. Originally the
//	 * documentation claimed that it only increased the alignment, but actually it
//	 * can decrease it on gcc, clang, and icc:
//	 * https://gcc.gnu.org/bugzilla/show_bug.cgi?id=69502,
//	 * https://gcc.godbolt.org/z/xYez1j67Y.
//	 */
func XXH_read32(tls *libc.TLS, ptr uintptr) (r xxh_u32) {
	return *(*uint32)(unsafe.Pointer(ptr))
}

/* ***   Endianness   *** */

/*!
 * @ingroup tuning
 * @def XXH_CPU_LITTLE_ENDIAN
 * @brief Whether the target is little endian.
 *
 * Defined to 1 if the target is little endian, or 0 if it is big endian.
 * It can be defined externally, for example on the compiler command line.
 *
 * If it is not defined,
 * a runtime check (which is usually constant folded) is used instead.
 *
 * @note
 *   This is not necessarily defined to an integer constant.
 *
 * @see XXH_isLittleEndian() for the runtime check.
 */
/*
 * Try to detect endianness automatically, to avoid the nonstandard behavior
 * in `XXH_isLittleEndian()`
 */

/* ****************************************
*  Compiler-specific Functions and Macros
******************************************/

/*
 * C23 and future versions have standard "unreachable()".
 * Once it has been implemented reliably we can add it as an
 * additional case:
 *
 * ```
 * #if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= XXH_C23_VN)
 * #  include <stddef.h>
 * #  ifdef unreachable
 * #    define XXH_UNREACHABLE() unreachable()
 * #  endif
 * #endif
 * ```
 *
 * Note C++23 also has std::unreachable() which can be detected
 * as follows:
 * ```
 * #if defined(__cpp_lib_unreachable) && (__cpp_lib_unreachable >= 202202L)
 * #  include <utility>
 * #  define XXH_UNREACHABLE() std::unreachable()
 * #endif
 * ```
 * NB: `__cpp_lib_unreachable` is defined in the `<version>` header.
 * We don't use that as including `<utility>` in `extern "C"` blocks
 * doesn't work on GCC12
 */

/*!
 * @internal
 * @def XXH_rotl32(x,r)
 * @brief 32-bit rotate left.
 *
 * @param x The 32-bit integer to be rotated.
 * @param r The number of bits to rotate.
 * @pre
 *   @p r > 0 && @p r < 32
 * @note
 *   @p x and @p r may be evaluated multiple times.
 * @return The rotated result.
 */

/*!
 * @internal
 * @fn xxh_u32 XXH_swap32(xxh_u32 x)
 * @brief A 32-bit byteswap.
 *
 * @param x The 32-bit integer to byteswap.
 * @return @p x, byteswapped.
 */

/* ***************************
*  Memory reads
*****************************/

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Enum to indicate whether a pointer is aligned.
//	 */
type XXH_alignment = int32

const XXH_aligned = 0
const /*!< Aligned */
XXH_unaligned = 1

// C documentation
//
//	/*
//	 * XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load.
//	 *
//	 * This is ideal for older compilers which don't inline memcpy.
//	 */
func XXH_readLE32(tls *libc.TLS, ptr uintptr) (r xxh_u32) {
	return XXH_read32(tls, ptr)
}

func XXH_readBE32(tls *libc.TLS, ptr uintptr) (r xxh_u32) {
	return libc.X__builtin_bswap32(tls, XXH_read32(tls, ptr))
}

func XXH_readLE32_align(tls *libc.TLS, ptr uintptr, align XXH_alignment) (r xxh_u32) {
	if align == int32(XXH_unaligned) {
		return XXH_readLE32(tls, ptr)
	} else {
		return *(*xxh_u32)(unsafe.Pointer(ptr))
	}
	return r
}

// C documentation
//
//	/* *************************************
//	*  Misc
//	***************************************/
//	/*! @ingroup public */
func XXH_INLINE_XXH_versionNumber(tls *libc.TLS) (r uint32) {
	return libc.Uint32FromInt32(libc.Int32FromInt32(XXH_VERSION_MAJOR)*libc.Int32FromInt32(100)*libc.Int32FromInt32(100) + libc.Int32FromInt32(XXH_VERSION_MINOR)*libc.Int32FromInt32(100) + libc.Int32FromInt32(XXH_VERSION_RELEASE))
}

/* *******************************************************************
*  32-bit hash functions
*********************************************************************/
/*!
 * @}
 * @defgroup XXH32_impl XXH32 implementation
 * @ingroup impl
 *
 * Details on the XXH32 implementation.
 * @{
 */
/* #define instead of static const, to be used as initializers */

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Normal stripe processing routine.
//	 *
//	 * This shuffles the bits so that any bit from @p input impacts several bits in
//	 * @p acc.
//	 *
//	 * @param acc The accumulator lane.
//	 * @param input The stripe of input to mix.
//	 * @return The mixed accumulator lane.
//	 */
func XXH32_round(tls *libc.TLS, acc xxh_u32, input xxh_u32) (r xxh_u32) {
	acc = acc + input*uint32(0x85EBCA77)
	acc = acc<<libc.Int32FromInt32(13) | acc>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(13))
	acc = acc * uint32(0x9E3779B1)
	return acc
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Mixes all bits to finalize the hash.
//	 *
//	 * The final mix ensures that all input bits have a chance to impact any bit in
//	 * the output digest, resulting in an unbiased distribution.
//	 *
//	 * @param hash The hash to avalanche.
//	 * @return The avalanched hash.
//	 */
func XXH32_avalanche(tls *libc.TLS, hash xxh_u32) (r xxh_u32) {
	hash = hash ^ hash>>int32(15)
	hash = hash * uint32(0x85EBCA77)
	hash = hash ^ hash>>int32(13)
	hash = hash * uint32(0xC2B2AE3D)
	hash = hash ^ hash>>int32(16)
	return hash
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Processes the last 0-15 bytes of @p ptr.
//	 *
//	 * There may be up to 15 bytes remaining to consume from the input.
//	 * This final stage will digest them to ensure that all input bytes are present
//	 * in the final mix.
//	 *
//	 * @param hash The hash to finalize.
//	 * @param ptr The pointer to the remaining input.
//	 * @param len The remaining length, modulo 16.
//	 * @param align Whether @p ptr is aligned.
//	 * @return The finalized hash.
//	 * @see XXH64_finalize().
//	 */
func XXH32_finalize(tls *libc.TLS, hash xxh_u32, ptr uintptr, len1 size_t, align XXH_alignment) (r xxh_u32) {
	var v1 uintptr
	_ = v1
	if ptr == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	/* Compact rerolled version; generally faster */
	if !(libc.Int32FromInt32(XXH32_ENDJMP) != 0) {
		len1 = len1 & uint64(15)
		for len1 >= uint64(4) {
			hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
			ptr = ptr + uintptr(4)
			hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
			len1 = len1 - uint64(4)
		}
		for len1 > uint64(0) {
			v1 = ptr
			ptr = ptr + 1
			hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
			hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
			len1 = len1 - 1
		}
		return XXH32_avalanche(tls, hash)
	} else {
		switch len1 & libc.Uint64FromInt32(15) {
		case uint64(12):
			goto _2
		case uint64(8):
			goto _3
		case uint64(4):
			goto _4
		case uint64(13):
			goto _5
		case uint64(9):
			goto _6
		case uint64(5):
			goto _7
		case uint64(14):
			goto _8
		case uint64(10):
			goto _9
		case uint64(6):
			goto _10
		case uint64(15):
			goto _11
		case uint64(11):
			goto _12
		case uint64(7):
			goto _13
		case uint64(3):
			goto _14
		case uint64(2):
			goto _15
		case uint64(1):
			goto _16
		case uint64(0):
			goto _17
		}
		goto _18 /* or switch(bEnd - p) */
	_2:
		;
	_21:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		goto _20
	_20:
		;
		if 0 != 0 {
			goto _21
		}
		goto _19
	_19:
		;
		/* fallthrough */
	_3:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_4:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		return XXH32_avalanche(tls, hash)
	_5:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_6:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_7:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		return XXH32_avalanche(tls, hash)
	_8:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_9:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_10:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		return XXH32_avalanche(tls, hash)
	_11:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_12:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_13:
		;
		hash = hash + XXH_readLE32_align(tls, ptr, align)*uint32(0xC2B2AE3D)
		ptr = ptr + uintptr(4)
		hash = (hash<<libc.Int32FromInt32(17) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(17))) * uint32(0x27D4EB2F)
		/* fallthrough */
	_14:
		;
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		/* fallthrough */
	_15:
		;
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		/* fallthrough */
	_16:
		;
		v1 = ptr
		ptr = ptr + 1
		hash = hash + uint32(*(*xxh_u8)(unsafe.Pointer(v1)))*uint32(0x165667B1)
		hash = (hash<<libc.Int32FromInt32(11) | hash>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(11))) * uint32(0x9E3779B1)
		/* fallthrough */
	_17:
		;
		return XXH32_avalanche(tls, hash)
	_18:
		;
		if !(libc.Int32FromInt32(0) != 0) {
		}
		return hash /* reaching this point is deemed impossible */
	}
	return r
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief The implementation for @ref XXH32().
//	 *
//	 * @param input , len , seed Directly passed from @ref XXH32().
//	 * @param align Whether @p input is aligned.
//	 * @return The calculated hash.
//	 */
func XXH32_endian_align(tls *libc.TLS, input uintptr, len1 size_t, seed xxh_u32, align XXH_alignment) (r xxh_u32) {
	var bEnd, limit uintptr
	var h32, v1, v2, v3, v4 xxh_u32
	_, _, _, _, _, _, _ = bEnd, h32, limit, v1, v2, v3, v4
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	if len1 >= uint64(16) {
		bEnd = input + uintptr(len1)
		limit = bEnd - uintptr(15)
		v1 = seed + uint32(0x9E3779B1) + uint32(0x85EBCA77)
		v2 = seed + uint32(0x85EBCA77)
		v3 = seed + uint32(0)
		v4 = seed - uint32(0x9E3779B1)
		for cond := true; cond; cond = input < limit {
			v1 = XXH32_round(tls, v1, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
			v2 = XXH32_round(tls, v2, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
			v3 = XXH32_round(tls, v3, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
			v4 = XXH32_round(tls, v4, XXH_readLE32_align(tls, input, align))
			input = input + uintptr(4)
		}
		h32 = v1<<libc.Int32FromInt32(1) | v1>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(1)) + (v2<<libc.Int32FromInt32(7) | v2>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(7))) + (v3<<libc.Int32FromInt32(12) | v3>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(12))) + (v4<<libc.Int32FromInt32(18) | v4>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(18)))
	} else {
		h32 = seed + uint32(0x165667B1)
	}
	h32 = h32 + uint32(len1)
	return XXH32_finalize(tls, h32, input, len1&uint64(15), align)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32(tls *libc.TLS, input uintptr, len1 size_t, seed XXH32_hash_t) (r XXH32_hash_t) {
	if XXH_FORCE_ALIGN_CHECK != 0 {
		if uint64(input)&uint64(3) == uint64(0) { /* Input is 4-bytes aligned, leverage the speed benefit */
			return XXH32_endian_align(tls, input, len1, seed, int32(XXH_aligned))
		}
	}
	return XXH32_endian_align(tls, input, len1, seed, int32(XXH_unaligned))
}

// C documentation
//
//	/*******   Hash streaming   *******/
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_createState(tls *libc.TLS) (r uintptr) {
	return XXH_malloc(tls, uint64(48))
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_freeState(tls *libc.TLS, statePtr uintptr) (r XXH_NAMESPACEXXH_errorcode) {
	XXH_free(tls, statePtr)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_copyState(tls *libc.TLS, dstState uintptr, srcState uintptr) {
	XXH_memcpy(tls, dstState, srcState, uint64(48))
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_reset(tls *libc.TLS, statePtr uintptr, seed XXH32_hash_t) (r XXH_NAMESPACEXXH_errorcode) {
	if !(statePtr != libc.UintptrFromInt32(0)) {
	}
	libc.Xmemset(tls, statePtr, 0, uint64(48))
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8)) = seed + uint32(0x9E3779B1) + uint32(0x85EBCA77)
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8 + 1*4)) = seed + uint32(0x85EBCA77)
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8 + 2*4)) = seed + uint32(0)
	*(*XXH32_hash_t)(unsafe.Pointer(statePtr + 8 + 3*4)) = seed - uint32(0x9E3779B1)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_update(tls *libc.TLS, state uintptr, input uintptr, len1 size_t) (r XXH_NAMESPACEXXH_errorcode) {
	var bEnd, limit, p, p32 uintptr
	_, _, _, _ = bEnd, limit, p, p32
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
		return int32(XXH_NAMESPACEXXH_OK)
	}
	p = input
	bEnd = p + uintptr(len1)
	*(*XXH32_hash_t)(unsafe.Pointer(state)) += uint32(len1)
	*(*XXH32_hash_t)(unsafe.Pointer(state + 4)) |= libc.Uint32FromInt32(libc.BoolInt32(len1 >= libc.Uint64FromInt32(16)) | libc.BoolInt32((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Ftotal_len_32 >= libc.Uint32FromInt32(16)))
	if uint64((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize)+len1 < uint64(16) { /* fill in tmp buffer */
		XXH_memcpy(tls, state+24+uintptr((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize), input, len1)
		*(*XXH32_hash_t)(unsafe.Pointer(state + 40)) += uint32(len1)
		return int32(XXH_NAMESPACEXXH_OK)
	}
	if (*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize != 0 { /* some data left from previous update */
		XXH_memcpy(tls, state+24+uintptr((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize), input, uint64(uint32(16)-(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize))
		p32 = state + 24
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE32(tls, p32))
		p32 += 4
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)), XXH_readLE32(tls, p32))
		p32 += 4
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)), XXH_readLE32(tls, p32))
		p32 += 4
		*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)), XXH_readLE32(tls, p32))
		p = p + uintptr(uint32(16)-(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize)
		(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize = uint32(0)
	}
	if p <= bEnd-uintptr(16) {
		limit = bEnd - uintptr(16)
		for cond := true; cond; cond = p <= limit {
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
			*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)) = XXH32_round(tls, *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4)), XXH_readLE32(tls, p))
			p = p + uintptr(4)
		}
	}
	if p < bEnd {
		XXH_memcpy(tls, state+24, p, libc.Uint64FromInt64(int64(bEnd)-int64(p)))
		(*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize = libc.Uint32FromInt64(int64(bEnd) - int64(p))
	}
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_digest(tls *libc.TLS, state uintptr) (r XXH32_hash_t) {
	var h32 xxh_u32
	_ = h32
	if (*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Flarge_len != 0 {
		h32 = *(*XXH32_hash_t)(unsafe.Pointer(state + 8))<<libc.Int32FromInt32(1) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(1)) + (*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4))<<libc.Int32FromInt32(7) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 1*4))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(7))) + (*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4))<<libc.Int32FromInt32(12) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(12))) + (*(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4))<<libc.Int32FromInt32(18) | *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 3*4))>>(libc.Int32FromInt32(32)-libc.Int32FromInt32(18)))
	} else {
		h32 = *(*XXH32_hash_t)(unsafe.Pointer(state + 8 + 2*4)) + uint32(0x165667B1)
	}
	h32 = h32 + (*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Ftotal_len_32
	return XXH32_finalize(tls, h32, state+24, uint64((*XXH_NAMESPACEXXH32_state_t)(unsafe.Pointer(state)).Fmemsize), int32(XXH_aligned))
}

/*******   Canonical representation   *******/

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_canonicalFromHash(tls *libc.TLS, dst uintptr, _hash XXH32_hash_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*XXH32_hash_t)(unsafe.Pointer(bp)) = _hash
	if int32(XXH_CPU_LITTLE_ENDIAN) != 0 {
		*(*XXH32_hash_t)(unsafe.Pointer(bp)) = libc.X__builtin_bswap32(tls, *(*XXH32_hash_t)(unsafe.Pointer(bp)))
	}
	XXH_memcpy(tls, dst, bp, uint64(4))
}

// C documentation
//
//	/*! @ingroup XXH32_family */
func XXH_INLINE_XXH32_hashFromCanonical(tls *libc.TLS, src uintptr) (r XXH32_hash_t) {
	return XXH_readBE32(tls, src)
}

/* *******************************************************************
*  64-bit hash functions
*********************************************************************/
/*!
 * @}
 * @ingroup impl
 * @{
 */
/*******   Memory access   *******/

type xxh_u64 = uint64

// C documentation
//
//	/*
//	 * __attribute__((aligned(1))) is supported by gcc and clang. Originally the
//	 * documentation claimed that it only increased the alignment, but actually it
//	 * can decrease it on gcc, clang, and icc:
//	 * https://gcc.gnu.org/bugzilla/show_bug.cgi?id=69502,
//	 * https://gcc.godbolt.org/z/xYez1j67Y.
//	 */
func XXH_read64(tls *libc.TLS, ptr uintptr) (r xxh_u64) {
	return *(*uint64)(unsafe.Pointer(ptr))
}

// C documentation
//
//	/* XXH_FORCE_MEMORY_ACCESS==3 is an endian-independent byteshift load. */
func XXH_readLE64(tls *libc.TLS, ptr uintptr) (r xxh_u64) {
	return XXH_read64(tls, ptr)
}

func XXH_readBE64(tls *libc.TLS, ptr uintptr) (r xxh_u64) {
	return libc.X__builtin_bswap64(tls, XXH_read64(tls, ptr))
}

func XXH_readLE64_align(tls *libc.TLS, ptr uintptr, align XXH_alignment) (r xxh_u64) {
	if align == int32(XXH_unaligned) {
		return XXH_readLE64(tls, ptr)
	} else {
		return *(*xxh_u64)(unsafe.Pointer(ptr))
	}
	return r
}

/*******   xxh64   *******/
/*!
 * @}
 * @defgroup XXH64_impl XXH64 implementation
 * @ingroup impl
 *
 * Details on the XXH64 implementation.
 * @{
 */
/* #define rather that static const, to be used as initializers */

// C documentation
//
//	/*! @copydoc XXH32_round */
func XXH64_round(tls *libc.TLS, acc xxh_u64, input xxh_u64) (r xxh_u64) {
	acc = uint64(acc + input*libc.Uint64FromUint64(0xC2B2AE3D27D4EB4F))
	acc = acc<<libc.Int32FromInt32(31) | acc>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(31))
	acc = uint64(acc * libc.Uint64FromUint64(0x9E3779B185EBCA87))
	return acc
}

func XXH64_mergeRound(tls *libc.TLS, acc xxh_u64, val xxh_u64) (r xxh_u64) {
	val = XXH64_round(tls, uint64(0), val)
	acc = acc ^ val
	acc = acc*uint64(0x9E3779B185EBCA87) + uint64(0x85EBCA77C2B2AE63)
	return acc
}

// C documentation
//
//	/*! @copydoc XXH32_avalanche */
func XXH64_avalanche(tls *libc.TLS, hash xxh_u64) (r xxh_u64) {
	hash = hash ^ hash>>int32(33)
	hash = uint64(hash * libc.Uint64FromUint64(0xC2B2AE3D27D4EB4F))
	hash = hash ^ hash>>int32(29)
	hash = uint64(hash * libc.Uint64FromUint64(0x165667B19E3779F9))
	hash = hash ^ hash>>int32(32)
	return hash
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief Processes the last 0-31 bytes of @p ptr.
//	 *
//	 * There may be up to 31 bytes remaining to consume from the input.
//	 * This final stage will digest them to ensure that all input bytes are present
//	 * in the final mix.
//	 *
//	 * @param hash The hash to finalize.
//	 * @param ptr The pointer to the remaining input.
//	 * @param len The remaining length, modulo 32.
//	 * @param align Whether @p ptr is aligned.
//	 * @return The finalized hash
//	 * @see XXH32_finalize().
//	 */
func XXH64_finalize(tls *libc.TLS, hash xxh_u64, ptr uintptr, len1 size_t, align XXH_alignment) (r xxh_u64) {
	var k1 xxh_u64
	var v1 uintptr
	_, _ = k1, v1
	if ptr == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	len1 = len1 & uint64(31)
	for len1 >= uint64(8) {
		k1 = XXH64_round(tls, uint64(0), XXH_readLE64_align(tls, ptr, align))
		ptr = ptr + uintptr(8)
		hash = hash ^ k1
		hash = uint64(hash<<libc.Int32FromInt32(27)|hash>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(27)))*uint64(0x9E3779B185EBCA87) + uint64(0x85EBCA77C2B2AE63)
		len1 = len1 - uint64(8)
	}
	if len1 >= uint64(4) {
		hash = uint64(hash ^ uint64(XXH_readLE32_align(tls, ptr, align))*libc.Uint64FromUint64(0x9E3779B185EBCA87))
		ptr = ptr + uintptr(4)
		hash = uint64(hash<<libc.Int32FromInt32(23)|hash>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(23)))*uint64(0xC2B2AE3D27D4EB4F) + uint64(0x165667B19E3779F9)
		len1 = len1 - uint64(4)
	}
	for len1 > uint64(0) {
		v1 = ptr
		ptr = ptr + 1
		hash = uint64(hash ^ uint64(*(*xxh_u8)(unsafe.Pointer(v1)))*libc.Uint64FromUint64(0x27D4EB2F165667C5))
		hash = uint64(hash<<libc.Int32FromInt32(11)|hash>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(11))) * uint64(0x9E3779B185EBCA87)
		len1 = len1 - 1
	}
	return XXH64_avalanche(tls, hash)
}

// C documentation
//
//	/*!
//	 * @internal
//	 * @brief The implementation for @ref XXH64().
//	 *
//	 * @param input , len , seed Directly passed from @ref XXH64().
//	 * @param align Whether @p input is aligned.
//	 * @return The calculated hash.
//	 */
func XXH64_endian_align(tls *libc.TLS, input uintptr, len1 size_t, seed xxh_u64, align XXH_alignment) (r xxh_u64) {
	var bEnd, limit uintptr
	var h64, v1, v2, v3, v4 xxh_u64
	_, _, _, _, _, _, _ = bEnd, h64, limit, v1, v2, v3, v4
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
	}
	if len1 >= uint64(32) {
		bEnd = input + uintptr(len1)
		limit = bEnd - uintptr(31)
		v1 = seed + uint64(0x9E3779B185EBCA87) + uint64(0xC2B2AE3D27D4EB4F)
		v2 = seed + uint64(0xC2B2AE3D27D4EB4F)
		v3 = seed + uint64(0)
		v4 = seed - uint64(0x9E3779B185EBCA87)
		for cond := true; cond; cond = input < limit {
			v1 = XXH64_round(tls, v1, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
			v2 = XXH64_round(tls, v2, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
			v3 = XXH64_round(tls, v3, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
			v4 = XXH64_round(tls, v4, XXH_readLE64_align(tls, input, align))
			input = input + uintptr(8)
		}
		h64 = v1<<libc.Int32FromInt32(1) | v1>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(1)) + (v2<<libc.Int32FromInt32(7) | v2>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(7))) + (v3<<libc.Int32FromInt32(12) | v3>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(12))) + (v4<<libc.Int32FromInt32(18) | v4>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(18)))
		h64 = XXH64_mergeRound(tls, h64, v1)
		h64 = XXH64_mergeRound(tls, h64, v2)
		h64 = XXH64_mergeRound(tls, h64, v3)
		h64 = XXH64_mergeRound(tls, h64, v4)
	} else {
		h64 = seed + uint64(0x27D4EB2F165667C5)
	}
	h64 = h64 + len1
	return XXH64_finalize(tls, h64, input, len1, align)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64(tls *libc.TLS, input uintptr, len1 size_t, seed XXH64_hash_t) (r XXH64_hash_t) {
	if XXH_FORCE_ALIGN_CHECK != 0 {
		if uint64(input)&uint64(7) == uint64(0) { /* Input is aligned, let's leverage the speed advantage */
			return XXH64_endian_align(tls, input, len1, seed, int32(XXH_aligned))
		}
	}
	return XXH64_endian_align(tls, input, len1, seed, int32(XXH_unaligned))
}

// C documentation
//
//	/*******   Hash Streaming   *******/
//	/*! @ingroup XXH64_family*/
func XXH_INLINE_XXH64_createState(tls *libc.TLS) (r uintptr) {
	return XXH_malloc(tls, uint64(88))
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_freeState(tls *libc.TLS, statePtr uintptr) (r XXH_NAMESPACEXXH_errorcode) {
	XXH_free(tls, statePtr)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_copyState(tls *libc.TLS, dstState uintptr, srcState uintptr) {
	XXH_memcpy(tls, dstState, srcState, uint64(88))
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_reset(tls *libc.TLS, statePtr uintptr, seed XXH64_hash_t) (r XXH_NAMESPACEXXH_errorcode) {
	if !(statePtr != libc.UintptrFromInt32(0)) {
	}
	libc.Xmemset(tls, statePtr, 0, uint64(88))
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8)) = seed + uint64(0x9E3779B185EBCA87) + uint64(0xC2B2AE3D27D4EB4F)
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8 + 1*8)) = seed + uint64(0xC2B2AE3D27D4EB4F)
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8 + 2*8)) = seed + uint64(0)
	*(*XXH64_hash_t)(unsafe.Pointer(statePtr + 8 + 3*8)) = seed - uint64(0x9E3779B185EBCA87)
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_update(tls *libc.TLS, state uintptr, input uintptr, len1 size_t) (r XXH_NAMESPACEXXH_errorcode) {
	var bEnd, limit, p uintptr
	_, _, _ = bEnd, limit, p
	if input == libc.UintptrFromInt32(0) {
		if !(len1 == libc.Uint64FromInt32(0)) {
		}
		return int32(XXH_NAMESPACEXXH_OK)
	}
	p = input
	bEnd = p + uintptr(len1)
	*(*XXH64_hash_t)(unsafe.Pointer(state)) += len1
	if uint64((*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize)+len1 < uint64(32) { /* fill in tmp buffer */
		XXH_memcpy(tls, state+40+uintptr((*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize), input, len1)
		*(*XXH32_hash_t)(unsafe.Pointer(state + 72)) += uint32(len1)
		return int32(XXH_NAMESPACEXXH_OK)
	}
	if (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize != 0 { /* tmp buffer is full */
		XXH_memcpy(tls, state+40+uintptr((*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize), input, uint64(uint32(32)-(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE64(tls, state+40+uintptr(0)*8))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)), XXH_readLE64(tls, state+40+uintptr(1)*8))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)), XXH_readLE64(tls, state+40+uintptr(2)*8))
		*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)), XXH_readLE64(tls, state+40+uintptr(3)*8))
		p = p + uintptr(uint32(32)-(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize)
		(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize = uint32(0)
	}
	if p+uintptr(32) <= bEnd {
		limit = bEnd - uintptr(32)
		for cond := true; cond; cond = p <= limit {
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
			*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)) = XXH64_round(tls, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)), XXH_readLE64(tls, p))
			p = p + uintptr(8)
		}
	}
	if p < bEnd {
		XXH_memcpy(tls, state+40, p, libc.Uint64FromInt64(int64(bEnd)-int64(p)))
		(*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Fmemsize = libc.Uint32FromInt64(int64(bEnd) - int64(p))
	}
	return int32(XXH_NAMESPACEXXH_OK)
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_digest(tls *libc.TLS, state uintptr) (r XXH64_hash_t) {
	var h64 xxh_u64
	_ = h64
	if (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Ftotal_len >= uint64(32) {
		h64 = *(*XXH64_hash_t)(unsafe.Pointer(state + 8))<<libc.Int32FromInt32(1) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(1)) + (*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8))<<libc.Int32FromInt32(7) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(7))) + (*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8))<<libc.Int32FromInt32(12) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(12))) + (*(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8))<<libc.Int32FromInt32(18) | *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8))>>(libc.Int32FromInt32(64)-libc.Int32FromInt32(18)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 1*8)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)))
		h64 = XXH64_mergeRound(tls, h64, *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 3*8)))
	} else {
		h64 = *(*XXH64_hash_t)(unsafe.Pointer(state + 8 + 2*8)) + uint64(0x27D4EB2F165667C5)
	}
	h64 = h64 + (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Ftotal_len
	return XXH64_finalize(tls, h64, state+40, (*XXH_NAMESPACEXXH64_state_t)(unsafe.Pointer(state)).Ftotal_len, int32(XXH_aligned))
}

/******* Canonical representation   *******/

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_canonicalFromHash(tls *libc.TLS, dst uintptr, _hash XXH64_hash_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*XXH64_hash_t)(unsafe.Pointer(bp)) = _hash
	if int32(XXH_CPU_LITTLE_ENDIAN) != 0 {
		*(*XXH64_hash_t)(unsafe.Pointer(bp)) = libc.X__builtin_bswap64(tls, *(*XXH64_hash_t)(unsafe.Pointer(bp)))
	}
	XXH_memcpy(tls, dst, bp, uint64(8))
}

// C documentation
//
//	/*! @ingroup XXH64_family */
func XXH_INLINE_XXH64_hashFromCanonical(tls *libc.TLS, src uintptr) (r XXH64_hash_t) {
	return XXH_readBE64(tls, src)
}

/*!
 * @}
 */
/**** ended inlining xxhash.h ****/
/**** start inlining zstd_trace.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* weak symbol support
 * For now, enable conservatively:
 * - Only GNUC
 * - Only ELF
 * - Only x86-64, i386, aarch64 and risc-v.
 * Also, explicitly disable on platforms known not to work so they aren't
 * forgotten in the future.
 */

/* Only enable tracing when weak symbols are available. */

/**** ended inlining zstd_trace.h ****/

/* ---- static assert (debug) --- */

/*-*************************************
*  shared macros
***************************************/

/*-*************************************
*  Common constants
***************************************/

var repStartValue = [3]U32{
	0: uint32(1),
	1: uint32(4),
	2: uint32(8),
}

var ZSTD_fcs_fieldSize = [4]size_t{
	1: uint64(2),
	2: uint64(4),
	3: uint64(8),
}
var ZSTD_did_fieldSize = [4]size_t{
	1: uint64(1),
	2: uint64(2),
	3: uint64(4),
}

var ZSTD_blockHeaderSize = uint64(ZSTD_BLOCKHEADERSIZE)

type blockType_e = int32

type SymbolEncodingType_e = int32

const set_basic = 0
const set_rle = 1
const set_compressed = 2
const set_repeat = 3

/* Each table cannot take more than #symbols * FSELog bits */

var LL_bits = [36]U8{
	16: uint8(1),
	17: uint8(1),
	18: uint8(1),
	19: uint8(1),
	20: uint8(2),
	21: uint8(2),
	22: uint8(3),
	23: uint8(3),
	24: uint8(4),
	25: uint8(6),
	26: uint8(7),
	27: uint8(8),
	28: uint8(9),
	29: uint8(10),
	30: uint8(11),
	31: uint8(12),
	32: uint8(13),
	33: uint8(14),
	34: uint8(15),
	35: uint8(16),
}
var LL_defaultNorm = [36]S16{
	0:  int16(4),
	1:  int16(3),
	2:  int16(2),
	3:  int16(2),
	4:  int16(2),
	5:  int16(2),
	6:  int16(2),
	7:  int16(2),
	8:  int16(2),
	9:  int16(2),
	10: int16(2),
	11: int16(2),
	12: int16(2),
	13: int16(1),
	14: int16(1),
	15: int16(1),
	16: int16(2),
	17: int16(2),
	18: int16(2),
	19: int16(2),
	20: int16(2),
	21: int16(2),
	22: int16(2),
	23: int16(2),
	24: int16(2),
	25: int16(3),
	26: int16(2),
	27: int16(1),
	28: int16(1),
	29: int16(1),
	30: int16(1),
	31: int16(1),
	32: int16(-int32(1)),
	33: int16(-int32(1)),
	34: int16(-int32(1)),
	35: int16(-int32(1)),
}
var LL_defaultNormLog = uint32(LL_DEFAULTNORMLOG)

var ML_bits = [53]U8{
	32: uint8(1),
	33: uint8(1),
	34: uint8(1),
	35: uint8(1),
	36: uint8(2),
	37: uint8(2),
	38: uint8(3),
	39: uint8(3),
	40: uint8(4),
	41: uint8(4),
	42: uint8(5),
	43: uint8(7),
	44: uint8(8),
	45: uint8(9),
	46: uint8(10),
	47: uint8(11),
	48: uint8(12),
	49: uint8(13),
	50: uint8(14),
	51: uint8(15),
	52: uint8(16),
}
var ML_defaultNorm = [53]S16{
	0:  int16(1),
	1:  int16(4),
	2:  int16(3),
	3:  int16(2),
	4:  int16(2),
	5:  int16(2),
	6:  int16(2),
	7:  int16(2),
	8:  int16(2),
	9:  int16(1),
	10: int16(1),
	11: int16(1),
	12: int16(1),
	13: int16(1),
	14: int16(1),
	15: int16(1),
	16: int16(1),
	17: int16(1),
	18: int16(1),
	19: int16(1),
	20: int16(1),
	21: int16(1),
	22: int16(1),
	23: int16(1),
	24: int16(1),
	25: int16(1),
	26: int16(1),
	27: int16(1),
	28: int16(1),
	29: int16(1),
	30: int16(1),
	31: int16(1),
	32: int16(1),
	33: int16(1),
	34: int16(1),
	35: int16(1),
	36: int16(1),
	37: int16(1),
	38: int16(1),
	39: int16(1),
	40: int16(1),
	41: int16(1),
	42: int16(1),
	43: int16(1),
	44: int16(1),
	45: int16(1),
	46: int16(-int32(1)),
	47: int16(-int32(1)),
	48: int16(-int32(1)),
	49: int16(-int32(1)),
	50: int16(-int32(1)),
	51: int16(-int32(1)),
	52: int16(-int32(1)),
}
var ML_defaultNormLog = uint32(ML_DEFAULTNORMLOG)

var OF_defaultNorm = [29]S16{
	0:  int16(1),
	1:  int16(1),
	2:  int16(1),
	3:  int16(1),
	4:  int16(1),
	5:  int16(1),
	6:  int16(2),
	7:  int16(2),
	8:  int16(2),
	9:  int16(1),
	10: int16(1),
	11: int16(1),
	12: int16(1),
	13: int16(1),
	14: int16(1),
	15: int16(1),
	16: int16(1),
	17: int16(1),
	18: int16(1),
	19: int16(1),
	20: int16(1),
	21: int16(1),
	22: int16(1),
	23: int16(1),
	24: int16(-int32(1)),
	25: int16(-int32(1)),
	26: int16(-int32(1)),
	27: int16(-int32(1)),
	28: int16(-int32(1)),
}
var OF_defaultNormLog = uint32(OF_DEFAULTNORMLOG)

// C documentation
//
//	/*-*******************************************
//	*  Shared functions to include for inlining
//	*********************************************/
func ZSTD_copy8(tls *libc.TLS, dst uintptr, src uintptr) {
	libc.Xmemcpy(tls, dst, src, libc.Uint64FromInt32(libc.Int32FromInt32(8)))
}

// C documentation
//
//	/* Need to use memmove here since the literal buffer can now be located within
//	   the dst buffer. In circumstances where the op "catches up" to where the
//	   literal buffer is, there can be partial overlaps in this call on the final
//	   copy if the literal is being shifted by less than 16 bytes. */
func ZSTD_copy16(tls *libc.TLS, dst uintptr, src uintptr) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* copy16_buf at bp+0 */ [16]BYTE
	libc.Xmemcpy(tls, bp, src, libc.Uint64FromInt32(libc.Int32FromInt32(16)))
	libc.Xmemcpy(tls, dst, bp, libc.Uint64FromInt32(libc.Int32FromInt32(16)))
}

type ZSTD_overlap_e = int32

const ZSTD_no_overlap = 0
const ZSTD_overlap_src_before_dst = 1

// C documentation
//
//	/*! ZSTD_wildcopy() :
//	 *  Custom version of ZSTD_memcpy(), can over read/write up to WILDCOPY_OVERLENGTH bytes (if length==0)
//	 *  @param ovtype controls the overlap detection
//	 *         - ZSTD_no_overlap: The source and destination are guaranteed to be at least WILDCOPY_VECLEN bytes apart.
//	 *         - ZSTD_overlap_src_before_dst: The src and dst may overlap, but they MUST be at least 8 bytes apart.
//	 *           The src buffer must be before the dst buffer.
//	 */
func ZSTD_wildcopy(tls *libc.TLS, dst uintptr, src uintptr, length ptrdiff_t, ovtype ZSTD_overlap_e) {
	var diff ptrdiff_t
	var ip, oend, op uintptr
	_, _, _, _ = diff, ip, oend, op
	diff = int64(dst) - int64(src)
	ip = src
	op = dst
	oend = op + uintptr(length)
	if ovtype == int32(ZSTD_overlap_src_before_dst) && diff < int64(WILDCOPY_VECLEN) {
		/* Handle short offset copies. */
		for cond := true; cond; cond = op < oend {
			ZSTD_copy8(tls, op, ip)
			op = op + uintptr(8)
			ip = ip + uintptr(8)
		}
	} else {
		/* Separate out the first COPY16() call because the copy length is
		 * almost certain to be short, so the branches have different
		 * probabilities. Since it is almost certain to be short, only do
		 * one COPY16() in the first call. Then, do two calls per loop since
		 * at that point it is more likely to have a high trip count.
		 */
		ZSTD_copy16(tls, op, ip)
		if int64(16) >= length {
			return
		}
		op = op + uintptr(16)
		ip = ip + uintptr(16)
		for cond := true; cond; cond = op < oend {
			ZSTD_copy16(tls, op, ip)
			op = op + uintptr(16)
			ip = ip + uintptr(16)
			ZSTD_copy16(tls, op, ip)
			op = op + uintptr(16)
			ip = ip + uintptr(16)
		}
	}
}

func ZSTD_limitCopy(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var length size_t
	var v1 uint64
	_, _ = length, v1
	if dstCapacity < srcSize {
		v1 = dstCapacity
	} else {
		v1 = srcSize
	}
	length = v1
	if length > uint64(0) {
		libc.Xmemcpy(tls, dst, src, length)
	}
	return length
}

/* define "workspace is too large" as this number of times larger than needed */

/* when workspace is continuously too large
 * during at least this number of times,
 * context's memory usage is considered wasteful,
 * because it's sized to handle a worst case scenario which rarely happens.
 * In which case, resize it down to free some memory */

// C documentation
//
//	/* Controls whether the input/output buffer is buffered or stable. */
type ZSTD_bufferMode_e = int32

/*-*******************************************
*  Private declarations
*********************************************/

// C documentation
//
//	/**
//	 * Contains the compressed frame size and an upper-bound for the decompressed frame size.
//	 * Note: before using `compressedSize`, check for errors using ZSTD_isError().
//	 *       similarly, before using `decompressedBound`, check for errors using:
//	 *          `decompressedBound != ZSTD_CONTENTSIZE_ERROR`
//	 */
type ZSTD_frameSizeInfo = struct {
	FnbBlocks          size_t
	FcompressedSize    size_t
	FdecompressedBound uint64
}

/* zstdmt, adaptive_compression (shouldn't get this definition from here) */

type blockProperties_t = struct {
	FblockType blockType_e
	FlastBlock U32
	ForigSize  U32
}

// C documentation
//
//	/**
//	 * @returns true iff the CPU supports dynamic BMI2 dispatch.
//	 */
func ZSTD_cpuSupportsBmi2(tls *libc.TLS) (r int32) {
	var cpuid ZSTD_cpuid_t
	_ = cpuid
	cpuid = ZSTD_cpuid(tls)
	return libc.BoolInt32(ZSTD_cpuid_bmi1(tls, cpuid) != 0 && ZSTD_cpuid_bmi2(tls, cpuid) != 0)
}

/**** ended inlining zstd_internal.h ****/

// C documentation
//
//	/*-****************************************
//	*  Version
//	******************************************/
func ZSTD_versionNumber(tls *libc.TLS) (r uint32) {
	return libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_VERSION_MAJOR)*libc.Int32FromInt32(100)*libc.Int32FromInt32(100) + libc.Int32FromInt32(ZSTD_VERSION_MINOR)*libc.Int32FromInt32(100) + libc.Int32FromInt32(ZSTD_VERSION_RELEASE))
}

func ZSTD_versionString(tls *libc.TLS) (r uintptr) {
	return __ccgo_ts + 1320
}

// C documentation
//
//	/*-****************************************
//	*  ZSTD Error Management
//	******************************************/
//	/*! ZSTD_isError() :
//	 *  tells if a return value is an error code
//	 *  symbol is required for external callers */
func ZSTD_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

// C documentation
//
//	/*! ZSTD_getErrorName() :
//	 *  provides error code string from function result (useful for debugging) */
func ZSTD_getErrorName(tls *libc.TLS, code size_t) (r uintptr) {
	return ERR_getErrorName(tls, code)
}

// C documentation
//
//	/*! ZSTD_getError() :
//	 *  convert a `size_t` function result into a proper ZSTD_errorCode enum */
func ZSTD_getErrorCode(tls *libc.TLS, code size_t) (r ZSTD_ErrorCode) {
	return ERR_getErrorCode(tls, code)
}

// C documentation
//
//	/*! ZSTD_getErrorString() :
//	 *  provides error code string from enum */
func ZSTD_getErrorString(tls *libc.TLS, code ZSTD_ErrorCode) (r uintptr) {
	return ERR_getErrorString(tls, code)
}

/**** ended inlining hist.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

/* **************************************************************
*  Templates
****************************************************************/
/*
  designed to be included
  for type-specific functions (template emulation in C)
  Objective is to write these functions only once, for improved maintenance
*/

/* safety checks */

/* Function names */

/* Function templates */

// C documentation
//
//	/* FSE_buildCTable_wksp() :
//	 * Same as FSE_buildCTable(), but using an externally allocated scratch buffer (`workSpace`).
//	 * wkspSize should be sized to handle worst case situation, which is `1<<max_tableLog * sizeof(FSE_FUNCTION_TYPE)`
//	 * workSpace must also be properly aligned with FSE_FUNCTION_TYPE requirements
//	 */
func FSE_buildCTable_wksp(tls *libc.TLS, ct uintptr, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, workSpace uintptr, wkspSize size_t) (r size_t) {
	var FSCT, cumul, ptr, spread, symbolTT, tableSymbol, tableU16, v12 uintptr
	var add, sv U64
	var freq, i, n, nbOccurrences int32
	var highThreshold, maxBitsOut, maxSV1, minStatePlus, position1, s, step, symbol, tableMask, tableSize, u, u2, v3 U32
	var pos, position, s1, u1, uPosition, unroll size_t
	var s2 BYTE
	var s3, total, v1 uint32
	var v11 U16
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = FSCT, add, cumul, freq, highThreshold, i, maxBitsOut, maxSV1, minStatePlus, n, nbOccurrences, pos, position, position1, ptr, s, s1, s2, s3, spread, step, sv, symbol, symbolTT, tableMask, tableSize, tableSymbol, tableU16, total, u, u1, u2, uPosition, unroll, v1, v11, v12, v3
	tableSize = libc.Uint32FromInt32(int32(1) << tableLog)
	tableMask = tableSize - uint32(1)
	ptr = ct
	tableU16 = ptr + uintptr(2)*2
	if tableLog != 0 {
		v1 = tableSize >> int32(1)
	} else {
		v1 = uint32(1)
	}
	FSCT = ptr + uintptr(1)*4 + uintptr(v1)*4
	symbolTT = FSCT
	step = tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3)
	maxSV1 = maxSymbolValue + uint32(1)
	cumul = workSpace                                               /* size = maxSV1 */
	tableSymbol = cumul + uintptr(maxSV1+libc.Uint32FromInt32(1))*2 /* size = tableSize */
	highThreshold = tableSize - uint32(1)
	/* Must be 2 bytes-aligned */
	if uint64(4)*((uint64(maxSymbolValue+libc.Uint32FromInt32(2))+uint64(1)<<tableLog)/uint64(2)+uint64(libc.Uint64FromInt64(8)/libc.Uint64FromInt64(4))) > wkspSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	/* CTable header */
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(2))*2)) = uint16(tableLog)
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(1))*2)) = uint16(maxSymbolValue)
	/* required for threshold strategy to work */
	/* For explanations on how to distribute symbol values over the table :
	 * https://fastcompression.blogspot.fr/2014/02/fse-distributing-symbol-values.html */
	/* symbol start positions */
	*(*U16)(unsafe.Pointer(cumul)) = uint16(0)
	u = uint32(1)
	for {
		if !(u <= maxSV1) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(u-uint32(1))*2))) == -int32(1) { /* Low proba symbol */
			*(*U16)(unsafe.Pointer(cumul + uintptr(u)*2)) = libc.Uint16FromInt32(libc.Int32FromUint16(*(*U16)(unsafe.Pointer(cumul + uintptr(u-uint32(1))*2))) + int32(1))
			v3 = highThreshold
			highThreshold = highThreshold - 1
			*(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(v3))) = uint8(u - libc.Uint32FromInt32(1))
		} else {
			*(*U16)(unsafe.Pointer(cumul + uintptr(u)*2)) = libc.Uint16FromInt32(libc.Int32FromUint16(*(*U16)(unsafe.Pointer(cumul + uintptr(u-uint32(1))*2))) + libc.Int32FromUint16(libc.Uint16FromInt16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(u-uint32(1))*2)))))
			/* no overflow */
		}
		goto _2
	_2:
		;
		u = u + 1
	}
	*(*U16)(unsafe.Pointer(cumul + uintptr(maxSV1)*2)) = uint16(tableSize + libc.Uint32FromInt32(1))
	/* Spread symbols */
	if highThreshold == tableSize-uint32(1) {
		/* Case for no low prob count symbols. Lay down 8 bytes at a time
		 * to reduce branch misses since we are operating on a small block
		 */
		spread = tableSymbol + uintptr(tableSize) /* size = tableSize + 8 (may write beyond tableSize) */
		add = uint64(0x0101010101010101)
		pos = uint64(0)
		sv = uint64(0)
		s = uint32(0)
		for {
			if !(s < maxSV1) {
				break
			}
			n = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)))
			MEM_write64(tls, spread+uintptr(pos), sv)
			i = int32(8)
			for {
				if !(i < n) {
					break
				}
				MEM_write64(tls, spread+uintptr(pos)+uintptr(i), sv)
				goto _5
			_5:
				;
				i = i + int32(8)
			}
			pos = pos + libc.Uint64FromInt32(n)
			goto _4
		_4:
			;
			s = s + 1
			sv = sv + add
		}
		/* Spread symbols across the table. Lack of lowprob symbols means that
		 * we don't need variable sized inner loop, so we can unroll the loop and
		 * reduce branch misses.
		 */
		position = uint64(0)
		unroll = uint64(2) /* Experimentally determined optimal unroll */
		/* FSE_MIN_TABLELOG is 5 */
		s1 = uint64(0)
		for {
			if !(s1 < uint64(tableSize)) {
				break
			}
			u1 = uint64(0)
			for {
				if !(u1 < unroll) {
					break
				}
				uPosition = (position + u1*uint64(step)) & uint64(tableMask)
				*(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(uPosition))) = *(*BYTE)(unsafe.Pointer(spread + uintptr(s1+u1)))
				goto _7
			_7:
				;
				u1 = u1 + 1
			}
			position = (position + unroll*uint64(step)) & uint64(tableMask)
			goto _6
		_6:
			;
			s1 = s1 + unroll
		}
		/* Must have initialized all positions */
	} else {
		position1 = uint32(0)
		symbol = uint32(0)
		for {
			if !(symbol < maxSV1) {
				break
			}
			freq = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(symbol)*2)))
			nbOccurrences = 0
			for {
				if !(nbOccurrences < freq) {
					break
				}
				*(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(position1))) = uint8(symbol)
				position1 = (position1 + step) & tableMask
				for position1 > highThreshold {
					position1 = (position1 + step) & tableMask
				} /* Low proba area */
				goto _9
			_9:
				;
				nbOccurrences = nbOccurrences + 1
			}
			goto _8
		_8:
			;
			symbol = symbol + 1
		}
		/* Must have initialized all positions */
	}
	/* Build table */
	u2 = uint32(0)
	for {
		if !(u2 < tableSize) {
			break
		}
		s2 = *(*BYTE)(unsafe.Pointer(tableSymbol + uintptr(u2))) /* note : static analyzer may not understand tableSymbol is properly initialized */
		v12 = cumul + uintptr(s2)*2
		v11 = *(*U16)(unsafe.Pointer(v12))
		*(*U16)(unsafe.Pointer(v12)) = *(*U16)(unsafe.Pointer(v12)) + 1
		*(*U16)(unsafe.Pointer(tableU16 + uintptr(v11)*2)) = uint16(tableSize + u2) /* TableU16 : sorted by symbol order; gives next state value */
		goto _10
	_10:
		;
		u2 = u2 + 1
	}
	/* Build Symbol Transformation Table */
	total = uint32(0)
	s3 = uint32(0)
	for {
		if !(s3 <= maxSymbolValue) {
			break
		}
		switch int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2))) {
		case 0:
			/* filling nonetheless, for compatibility with FSE_getMaxNbBits() */
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaNbBits = (tableLog+uint32(1))<<int32(16) - libc.Uint32FromInt32(libc.Int32FromInt32(1)<<tableLog)
		case -int32(1):
			fallthrough
		case int32(1):
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaNbBits = tableLog<<int32(16) - libc.Uint32FromInt32(libc.Int32FromInt32(1)<<tableLog)
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaFindState = libc.Int32FromUint32(total - libc.Uint32FromInt32(1))
			total = total + 1
		default:
			maxBitsOut = tableLog - ZSTD_highbit32(tls, libc.Uint32FromInt16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))-uint32(1))
			minStatePlus = libc.Uint32FromInt16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2))) << maxBitsOut
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaNbBits = maxBitsOut<<libc.Int32FromInt32(16) - minStatePlus
			(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(s3)*8))).FdeltaFindState = libc.Int32FromUint32(total - libc.Uint32FromInt16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2))))
			total = total + libc.Uint32FromInt16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))
		}
		goto _13
	_13:
		;
		s3 = s3 + 1
	}
	return uint64(0)
}

// C documentation
//
//	/*-**************************************************************
//	*  FSE NCount encoding
//	****************************************************************/
func FSE_NCountWriteBound(tls *libc.TLS, maxSymbolValue uint32, tableLog uint32) (r size_t) {
	var maxHeaderSize size_t
	var v1 uint64
	_, _ = maxHeaderSize, v1
	maxHeaderSize = uint64(((maxSymbolValue+uint32(1))*tableLog+uint32(4)+uint32(2))/uint32(8) + uint32(1) + uint32(2))
	if maxSymbolValue != 0 {
		v1 = maxHeaderSize
	} else {
		v1 = uint64(FSE_NCOUNTBOUND)
	}
	return v1 /* maxSymbolValue==0 ? use default */
}

func FSE_writeNCount_generic(tls *libc.TLS, header uintptr, headerBufferSize size_t, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32, writeIsSafe uint32) (r size_t) {
	var alphabetSize, start, symbol, v1 uint32
	var bitCount, count, max, nbBits, previousIs0, remaining, tableSize, threshold, v2 int32
	var bitStream U32
	var oend, ostart, out uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = alphabetSize, bitCount, bitStream, count, max, nbBits, oend, ostart, out, previousIs0, remaining, start, symbol, tableSize, threshold, v1, v2
	ostart = header
	out = ostart
	oend = ostart + uintptr(headerBufferSize)
	tableSize = int32(1) << tableLog
	bitStream = uint32(0)
	bitCount = 0
	symbol = uint32(0)
	alphabetSize = maxSymbolValue + uint32(1)
	previousIs0 = 0
	/* Table Size */
	bitStream = bitStream + (tableLog-uint32(FSE_MIN_TABLELOG))<<bitCount
	bitCount = bitCount + int32(4)
	/* Init */
	remaining = tableSize + int32(1) /* +1 for extra accuracy */
	threshold = tableSize
	nbBits = libc.Int32FromUint32(tableLog) + int32(1)
	for symbol < alphabetSize && remaining > int32(1) { /* stops at 1 */
		if previousIs0 != 0 {
			start = symbol
			for symbol < alphabetSize && !(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(symbol)*2)) != 0) {
				symbol = symbol + 1
			}
			if symbol == alphabetSize {
				break
			} /* incorrect distribution */
			for symbol >= start+uint32(24) {
				start = start + uint32(24)
				bitStream = bitStream + uint32(0xFFFF)<<bitCount
				if !(writeIsSafe != 0) && out > oend-uintptr(2) {
					return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
				} /* Buffer overflow */
				*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
				*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
				out = out + uintptr(2)
				bitStream = bitStream >> uint32(16)
			}
			for symbol >= start+uint32(3) {
				start = start + uint32(3)
				bitStream = bitStream + uint32(3)<<bitCount
				bitCount = bitCount + int32(2)
			}
			bitStream = bitStream + (symbol-start)<<bitCount
			bitCount = bitCount + int32(2)
			if bitCount > int32(16) {
				if !(writeIsSafe != 0) && out > oend-uintptr(2) {
					return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
				} /* Buffer overflow */
				*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
				*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
				out = out + uintptr(2)
				bitStream = bitStream >> uint32(16)
				bitCount = bitCount - int32(16)
			}
		}
		v1 = symbol
		symbol = symbol + 1
		count = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(v1)*2)))
		max = int32(2)*threshold - int32(1) - remaining
		if count < 0 {
			v2 = -count
		} else {
			v2 = count
		}
		remaining = remaining - v2
		count = count + 1 /* +1 for extra accuracy */
		if count >= threshold {
			count = count + max
		} /* [0..max[ [max..threshold[ (...) [threshold+max 2*threshold[ */
		bitStream = bitStream + libc.Uint32FromInt32(count)<<bitCount
		bitCount = bitCount + nbBits
		bitCount = bitCount - libc.BoolInt32(count < max)
		previousIs0 = libc.BoolInt32(count == int32(1))
		if remaining < int32(1) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		}
		for remaining < threshold {
			nbBits = nbBits - 1
			threshold = threshold >> int32(1)
		}
		if bitCount > int32(16) {
			if !(writeIsSafe != 0) && out > oend-uintptr(2) {
				return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
			} /* Buffer overflow */
			*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
			*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
			out = out + uintptr(2)
			bitStream = bitStream >> uint32(16)
			bitCount = bitCount - int32(16)
		}
	}
	if remaining != int32(1) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* incorrect normalized distribution */
	/* flush remaining bitStream */
	if !(writeIsSafe != 0) && out > oend-uintptr(2) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	} /* Buffer overflow */
	*(*BYTE)(unsafe.Pointer(out)) = uint8(bitStream)
	*(*BYTE)(unsafe.Pointer(out + 1)) = uint8(bitStream >> libc.Int32FromInt32(8))
	out = out + uintptr((bitCount+int32(7))/int32(8))
	return libc.Uint64FromInt64(int64(out) - int64(ostart))
}

func FSE_writeNCount(tls *libc.TLS, buffer uintptr, bufferSize size_t, normalizedCounter uintptr, maxSymbolValue uint32, tableLog uint32) (r size_t) {
	if tableLog > libc.Uint32FromInt32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	} /* Unsupported */
	if tableLog < uint32(FSE_MIN_TABLELOG) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* Unsupported */
	if bufferSize < FSE_NCountWriteBound(tls, maxSymbolValue, tableLog) {
		return FSE_writeNCount_generic(tls, buffer, bufferSize, normalizedCounter, maxSymbolValue, tableLog, uint32(0))
	}
	return FSE_writeNCount_generic(tls, buffer, bufferSize, normalizedCounter, maxSymbolValue, tableLog, uint32(1))
}

/*-**************************************************************
*  FSE Compression Code
****************************************************************/

// C documentation
//
//	/* provides the minimum logSize to safely represent a distribution */
func FSE_minTableLog(tls *libc.TLS, srcSize size_t, maxSymbolValue uint32) (r uint32) {
	var minBits, minBitsSrc, minBitsSymbols U32
	var v1 uint32
	_, _, _, _ = minBits, minBitsSrc, minBitsSymbols, v1
	minBitsSrc = ZSTD_highbit32(tls, uint32(srcSize)) + uint32(1)
	minBitsSymbols = ZSTD_highbit32(tls, maxSymbolValue) + uint32(2)
	if minBitsSrc < minBitsSymbols {
		v1 = minBitsSrc
	} else {
		v1 = minBitsSymbols
	}
	minBits = v1
	/* Not supported, RLE should be used instead */
	return minBits
}

func FSE_optimalTableLog_internal(tls *libc.TLS, maxTableLog uint32, srcSize size_t, maxSymbolValue uint32, minus uint32) (r uint32) {
	var maxBitsSrc, minBits, tableLog U32
	_, _, _ = maxBitsSrc, minBits, tableLog
	maxBitsSrc = ZSTD_highbit32(tls, uint32(srcSize-libc.Uint64FromInt32(1))) - minus
	tableLog = maxTableLog
	minBits = FSE_minTableLog(tls, srcSize, maxSymbolValue)
	/* Not supported, RLE should be used instead */
	if tableLog == uint32(0) {
		tableLog = libc.Uint32FromInt32(libc.Int32FromInt32(FSE_DEFAULT_MEMORY_USAGE) - libc.Int32FromInt32(2))
	}
	if maxBitsSrc < tableLog {
		tableLog = maxBitsSrc
	} /* Accuracy can be reduced */
	if minBits > tableLog {
		tableLog = minBits
	} /* Need a minimum to safely represent all symbol values */
	if tableLog < uint32(FSE_MIN_TABLELOG) {
		tableLog = uint32(FSE_MIN_TABLELOG)
	}
	if tableLog > libc.Uint32FromInt32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		tableLog = libc.Uint32FromInt32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE) - libc.Int32FromInt32(2))
	}
	return tableLog
}

func FSE_optimalTableLog(tls *libc.TLS, maxTableLog uint32, srcSize size_t, maxSymbolValue uint32) (r uint32) {
	return FSE_optimalTableLog_internal(tls, maxTableLog, srcSize, maxSymbolValue, uint32(2))
}

/* Secondary normalization method.
   To be used when primary method fails. */

func FSE_normalizeM2(tls *libc.TLS, norm uintptr, tableLog U32, count uintptr, total size_t, maxSymbolValue U32, lowProbCount int16) (r size_t) {
	var NOT_YET_ASSIGNED int16
	var ToDistribute, distributed, lowOne, lowThreshold, maxC, maxV, s, sEnd, sStart, weight U32
	var end, mid, rStep, tmpTotal, vStepLog U64
	var v4 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = NOT_YET_ASSIGNED, ToDistribute, distributed, end, lowOne, lowThreshold, maxC, maxV, mid, rStep, s, sEnd, sStart, tmpTotal, vStepLog, weight, v4
	NOT_YET_ASSIGNED = int16(-int32(2))
	distributed = uint32(0)
	/* Init */
	lowThreshold = uint32(total >> tableLog)
	lowOne = uint32(total * libc.Uint64FromInt32(3) >> (tableLog + libc.Uint32FromInt32(1)))
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) == uint32(0) {
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = 0
			goto _1
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowThreshold {
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = lowProbCount
			distributed = distributed + 1
			total = total - uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
			goto _1
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowOne {
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = int16(1)
			distributed = distributed + 1
			total = total - uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
			goto _1
		}
		*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = NOT_YET_ASSIGNED
		goto _1
	_1:
		;
		s = s + 1
	}
	ToDistribute = libc.Uint32FromInt32(libc.Int32FromInt32(1)<<tableLog) - distributed
	if ToDistribute == uint32(0) {
		return uint64(0)
	}
	if total/uint64(ToDistribute) > uint64(lowOne) {
		/* risk of rounding to zero */
		lowOne = uint32(total * libc.Uint64FromInt32(3) / uint64(ToDistribute*libc.Uint32FromInt32(2)))
		s = uint32(0)
		for {
			if !(s <= maxSymbolValue) {
				break
			}
			if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) == int32(NOT_YET_ASSIGNED) && *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowOne {
				*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = int16(1)
				distributed = distributed + 1
				total = total - uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
				goto _2
			}
			goto _2
		_2:
			;
			s = s + 1
		}
		ToDistribute = libc.Uint32FromInt32(libc.Int32FromInt32(1)<<tableLog) - distributed
	}
	if distributed == maxSymbolValue+uint32(1) {
		/* all values are pretty poor;
		   probably incompressible data (should have already been detected);
		   find max, then give all remaining points to max */
		maxV = uint32(0)
		maxC = uint32(0)
		s = uint32(0)
		for {
			if !(s <= maxSymbolValue) {
				break
			}
			if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) > maxC {
				maxV = s
				maxC = *(*uint32)(unsafe.Pointer(count + uintptr(s)*4))
			}
			goto _3
		_3:
			;
			s = s + 1
		}
		v4 = norm + uintptr(maxV)*2
		*(*int16)(unsafe.Pointer(v4)) = int16(int32(*(*int16)(unsafe.Pointer(v4))) + int32(libc.Int16FromUint32(ToDistribute)))
		return uint64(0)
	}
	if total == uint64(0) {
		/* all of the symbols were low enough for the lowOne or lowThreshold */
		s = uint32(0)
		for {
			if !(ToDistribute > uint32(0)) {
				break
			}
			if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) > 0 {
				ToDistribute = ToDistribute - 1
				*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = *(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) + 1
			}
			goto _5
		_5:
			;
			s = (s + uint32(1)) % (maxSymbolValue + uint32(1))
		}
		return uint64(0)
	}
	vStepLog = uint64(uint32(62) - tableLog)
	mid = uint64(uint64(1)<<(vStepLog-uint64(1)) - uint64(1))
	rStep = (libc.Uint64FromInt32(1)<<vStepLog*uint64(ToDistribute) + mid) / uint64(uint32(total)) /* scale on remaining */
	tmpTotal = mid
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) == int32(NOT_YET_ASSIGNED) {
			end = tmpTotal + uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))*rStep
			sStart = uint32(tmpTotal >> vStepLog)
			sEnd = uint32(end >> vStepLog)
			weight = sEnd - sStart
			if weight < uint32(1) {
				return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
			}
			*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)) = libc.Int16FromUint32(weight)
			tmpTotal = end
		}
		goto _6
	_6:
		;
		s = s + 1
	}
	return uint64(0)
}

func FSE_normalizeCount(tls *libc.TLS, normalizedCounter uintptr, tableLog uint32, count uintptr, total size_t, maxSymbolValue uint32, useLowProbCount uint32) (r size_t) {
	var errorCode size_t
	var largest, s uint32
	var largestP, lowProbCount, proba int16
	var lowThreshold U32
	var restToBeat, scale, step, vStep U64
	var stillToDistribute, v1 int32
	var v3 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = errorCode, largest, largestP, lowProbCount, lowThreshold, proba, restToBeat, s, scale, step, stillToDistribute, vStep, v1, v3
	/* Sanity checks */
	if tableLog == uint32(0) {
		tableLog = libc.Uint32FromInt32(libc.Int32FromInt32(FSE_DEFAULT_MEMORY_USAGE) - libc.Int32FromInt32(2))
	}
	if tableLog < uint32(FSE_MIN_TABLELOG) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* Unsupported size */
	if tableLog > libc.Uint32FromInt32(libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2)) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	} /* Unsupported size */
	if tableLog < FSE_minTableLog(tls, total, maxSymbolValue) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* Too small tableLog, compression potentially impossible */
	if useLowProbCount != 0 {
		v1 = -int32(1)
	} else {
		v1 = int32(1)
	}
	lowProbCount = int16(v1)
	scale = uint64(uint32(62) - tableLog)
	step = libc.Uint64FromInt32(1) << libc.Int32FromInt32(62) / uint64(uint32(total)) /* <== here, one division ! */
	vStep = uint64(uint64(1) << (scale - uint64(20)))
	stillToDistribute = int32(1) << tableLog
	largest = uint32(0)
	largestP = 0
	lowThreshold = uint32(total >> tableLog)
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4))) == total {
			return uint64(0)
		} /* rle special case */
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) == uint32(0) {
			*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)) = 0
			goto _2
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) <= lowThreshold {
			*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)) = lowProbCount
			stillToDistribute = stillToDistribute - 1
		} else {
			proba = libc.Int16FromUint64(uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4))) * step >> scale)
			if int32(proba) < int32(8) {
				restToBeat = vStep * uint64(rtbTable[proba])
				proba = int16(int32(proba) + libc.BoolInt32(uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))*step-libc.Uint64FromInt16(proba)<<scale > restToBeat))
			}
			if int32(proba) > int32(largestP) {
				largestP = proba
				largest = s
			}
			*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)) = proba
			stillToDistribute = stillToDistribute - int32(proba)
		}
		goto _2
	_2:
		;
		s = s + 1
	}
	if -stillToDistribute >= int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(largest)*2)))>>int32(1) {
		/* corner case, need another normalization method */
		errorCode = FSE_normalizeM2(tls, normalizedCounter, tableLog, count, total, maxSymbolValue, lowProbCount)
		if ERR_isError(tls, errorCode) != 0 {
			return errorCode
		}
	} else {
		v3 = normalizedCounter + uintptr(largest)*2
		*(*int16)(unsafe.Pointer(v3)) = int16(int32(*(*int16)(unsafe.Pointer(v3))) + int32(int16(stillToDistribute)))
	}
	return uint64(tableLog)
}

var rtbTable = [8]U32{
	1: uint32(473195),
	2: uint32(504333),
	3: uint32(520860),
	4: uint32(550000),
	5: uint32(700000),
	6: uint32(750000),
	7: uint32(830000),
}

// C documentation
//
//	/* fake FSE_CTable, for rle input (always same symbol) */
func FSE_buildCTable_rle(tls *libc.TLS, ct uintptr, symbolValue BYTE) (r size_t) {
	var FSCTptr, ptr, symbolTT, tableU16 uintptr
	_, _, _, _ = FSCTptr, ptr, symbolTT, tableU16
	ptr = ct
	tableU16 = ptr + uintptr(2)*2
	FSCTptr = ptr + uintptr(2)*4
	symbolTT = FSCTptr
	/* header */
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(2))*2)) = libc.Uint16FromInt32(0)
	*(*U16)(unsafe.Pointer(tableU16 + uintptr(-libc.Int32FromInt32(1))*2)) = uint16(symbolValue)
	/* Build table */
	*(*U16)(unsafe.Pointer(tableU16)) = uint16(0)
	*(*U16)(unsafe.Pointer(tableU16 + 1*2)) = uint16(0) /* just in case */
	/* Build Symbol Transformation Table */
	(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaNbBits = uint32(0)
	(*(*FSE_symbolCompressionTransform)(unsafe.Pointer(symbolTT + uintptr(symbolValue)*8))).FdeltaFindState = 0
	return uint64(0)
}

func FSE_compress_usingCTable_generic(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, ct uintptr, fast uint32) (r size_t) {
	bp := tls.Alloc(112)
	defer tls.Free(112)
	var iend, ip, istart, v1 uintptr
	var initError size_t
	var _ /* CState1 at bp+40 */ FSE_CState_t
	var _ /* CState2 at bp+72 */ FSE_CState_t
	var _ /* bitC at bp+0 */ BIT_CStream_t
	_, _, _, _, _ = iend, initError, ip, istart, v1
	istart = src
	iend = istart + uintptr(srcSize)
	ip = iend
	/* init */
	if srcSize <= uint64(2) {
		return uint64(0)
	}
	initError = BIT_initCStream(tls, bp, dst, dstSize)
	if ERR_isError(tls, initError) != 0 {
		return uint64(0)
	} /* not enough space available to write a bitstream */
	if srcSize&uint64(1) != 0 {
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+40, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+72, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if fast != 0 {
			BIT_flushBitsFast(tls, bp)
		} else {
			BIT_flushBits(tls, bp)
		}
	} else {
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+72, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_initCState2(tls, bp+40, ct, uint32(*(*BYTE)(unsafe.Pointer(v1))))
	}
	/* join to mod 4 */
	srcSize = srcSize - uint64(2)
	if libc.Bool(libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) > libc.Uint64FromInt32((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(4)+libc.Int32FromInt32(7))) && srcSize&uint64(2) != 0 { /* test bit 2 */
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+72, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if fast != 0 {
			BIT_flushBitsFast(tls, bp)
		} else {
			BIT_flushBits(tls, bp)
		}
	}
	/* 2 or 4 encoding per loop */
	for ip > istart {
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+72, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) < libc.Uint64FromInt32((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(2)+libc.Int32FromInt32(7)) { /* this test must be static */
			if fast != 0 {
				BIT_flushBitsFast(tls, bp)
			} else {
				BIT_flushBits(tls, bp)
			}
		}
		ip = ip - 1
		v1 = ip
		FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		if libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) > libc.Uint64FromInt32((libc.Int32FromInt32(FSE_MAX_MEMORY_USAGE)-libc.Int32FromInt32(2))*libc.Int32FromInt32(4)+libc.Int32FromInt32(7)) { /* this test must be static */
			ip = ip - 1
			v1 = ip
			FSE_encodeSymbol(tls, bp, bp+72, uint32(*(*BYTE)(unsafe.Pointer(v1))))
			ip = ip - 1
			v1 = ip
			FSE_encodeSymbol(tls, bp, bp+40, uint32(*(*BYTE)(unsafe.Pointer(v1))))
		}
		if fast != 0 {
			BIT_flushBitsFast(tls, bp)
		} else {
			BIT_flushBits(tls, bp)
		}
	}
	FSE_flushCState(tls, bp, bp+72)
	FSE_flushCState(tls, bp, bp+40)
	return BIT_closeCStream(tls, bp)
}

func FSE_compress_usingCTable(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, ct uintptr) (r size_t) {
	var fast uint32
	_ = fast
	fast = libc.BoolUint32(dstSize >= srcSize+srcSize>>libc.Int32FromInt32(7)+libc.Uint64FromInt32(4)+libc.Uint64FromInt64(8))
	if fast != 0 {
		return FSE_compress_usingCTable_generic(tls, dst, dstSize, src, srcSize, ct, uint32(1))
	} else {
		return FSE_compress_usingCTable_generic(tls, dst, dstSize, src, srcSize, ct, uint32(0))
	}
	return r
}

func FSE_compressBound(tls *libc.TLS, size size_t) (r size_t) {
	return libc.Uint64FromInt32(FSE_NCOUNTBOUND) + (size + size>>libc.Int32FromInt32(7) + libc.Uint64FromInt32(4) + libc.Uint64FromInt64(8))
}

/**** ended inlining compress/fse_compress.c ****/
/**** start inlining compress/hist.c ****/
/* ******************************************************************
 * hist : Histogram functions
 * part of Finite State Entropy project
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* --- dependencies --- */
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/debug.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: hist.h ****/

// C documentation
//
//	/* --- Error management --- */
func HIST_isError(tls *libc.TLS, code size_t) (r uint32) {
	return ERR_isError(tls, code)
}

// C documentation
//
//	/*-**************************************************************
//	 *  Histogram functions
//	 ****************************************************************/
func HIST_add(tls *libc.TLS, count uintptr, src uintptr, srcSize size_t) {
	var end, ip, v1 uintptr
	_, _, _ = end, ip, v1
	ip = src
	end = ip + uintptr(srcSize)
	for ip < end {
		v1 = ip
		ip = ip + 1
		*(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) = *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) + 1
	}
}

func HIST_count_simple(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, src uintptr, srcSize size_t) (r uint32) {
	var end, ip, v1 uintptr
	var largestCount, maxSymbolValue uint32
	var s U32
	_, _, _, _, _, _ = end, ip, largestCount, maxSymbolValue, s, v1
	ip = src
	end = ip + uintptr(srcSize)
	maxSymbolValue = *(*uint32)(unsafe.Pointer(maxSymbolValuePtr))
	largestCount = uint32(0)
	libc.Xmemset(tls, count, 0, uint64(maxSymbolValue+libc.Uint32FromInt32(1))*libc.Uint64FromInt64(4))
	if srcSize == uint64(0) {
		*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = uint32(0)
		return uint32(0)
	}
	for ip < end {
		v1 = ip
		ip = ip + 1
		*(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) = *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) + 1
	}
	for !(*(*uint32)(unsafe.Pointer(count + uintptr(maxSymbolValue)*4)) != 0) {
		maxSymbolValue = maxSymbolValue - 1
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = maxSymbolValue
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) > largestCount {
			largestCount = *(*uint32)(unsafe.Pointer(count + uintptr(s)*4))
		}
		goto _2
	_2:
		;
		s = s + 1
	}
	return largestCount
}

type HIST_checkInput_e = int32

const trustInput = 0
const checkMaxSymbolValue = 1

// C documentation
//
//	/* HIST_count_parallel_wksp() :
//	 * store histogram into 4 intermediate tables, recombined at the end.
//	 * this design makes better use of OoO cpus,
//	 * and is noticeably faster when some values are heavily repeated.
//	 * But it needs some additional workspace for intermediate tables.
//	 * `workSpace` must be a U32 table of size >= HIST_WKSP_SIZE_U32.
//	 * @return : largest histogram frequency,
//	 *           or an error code (notably when histogram's alphabet is larger than *maxSymbolValuePtr) */
func HIST_count_parallel_wksp(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t, check HIST_checkInput_e, workSpace uintptr) (r size_t) {
	var Counting1, Counting2, Counting3, Counting4, iend, ip, v1 uintptr
	var c, cached, s U32
	var countSize size_t
	var max, maxSymbolValue uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = Counting1, Counting2, Counting3, Counting4, c, cached, countSize, iend, ip, max, maxSymbolValue, s, v1
	ip = source
	iend = ip + uintptr(sourceSize)
	countSize = uint64(*(*uint32)(unsafe.Pointer(maxSymbolValuePtr))+libc.Uint32FromInt32(1)) * uint64(4)
	max = uint32(0)
	Counting1 = workSpace
	Counting2 = Counting1 + uintptr(256)*4
	Counting3 = Counting2 + uintptr(256)*4
	Counting4 = Counting3 + uintptr(256)*4
	/* safety checks */
	if !(sourceSize != 0) {
		libc.Xmemset(tls, count, 0, countSize)
		*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = uint32(0)
		return uint64(0)
	}
	libc.Xmemset(tls, workSpace, 0, libc.Uint64FromInt32(libc.Int32FromInt32(4)*libc.Int32FromInt32(256))*libc.Uint64FromInt64(4))
	/* by stripes of 16 bytes */
	cached = MEM_read32(tls, ip)
	ip = ip + uintptr(4)
	for ip < iend-uintptr(15) {
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
		c = cached
		cached = MEM_read32(tls, ip)
		ip = ip + uintptr(4)
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(uint8(c))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) = *(*U32)(unsafe.Pointer(Counting2 + uintptr(uint8(c>>libc.Int32FromInt32(8)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) = *(*U32)(unsafe.Pointer(Counting3 + uintptr(uint8(c>>libc.Int32FromInt32(16)))*4)) + 1
		*(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) = *(*U32)(unsafe.Pointer(Counting4 + uintptr(c>>int32(24))*4)) + 1
	}
	ip = ip - uintptr(4)
	/* finish last symbols */
	for ip < iend {
		v1 = ip
		ip = ip + 1
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) = *(*U32)(unsafe.Pointer(Counting1 + uintptr(*(*BYTE)(unsafe.Pointer(v1)))*4)) + 1
	}
	s = uint32(0)
	for {
		if !(s < uint32(256)) {
			break
		}
		*(*U32)(unsafe.Pointer(Counting1 + uintptr(s)*4)) += *(*U32)(unsafe.Pointer(Counting2 + uintptr(s)*4)) + *(*U32)(unsafe.Pointer(Counting3 + uintptr(s)*4)) + *(*U32)(unsafe.Pointer(Counting4 + uintptr(s)*4))
		if *(*U32)(unsafe.Pointer(Counting1 + uintptr(s)*4)) > max {
			max = *(*U32)(unsafe.Pointer(Counting1 + uintptr(s)*4))
		}
		goto _2
	_2:
		;
		s = s + 1
	}
	maxSymbolValue = uint32(255)
	for !(*(*U32)(unsafe.Pointer(Counting1 + uintptr(maxSymbolValue)*4)) != 0) {
		maxSymbolValue = maxSymbolValue - 1
	}
	if check != 0 && maxSymbolValue > *(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooSmall))
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = maxSymbolValue
	libc.Xmemmove(tls, count, Counting1, countSize) /* in case count & Counting1 are overlapping */
	return uint64(max)
}

// C documentation
//
//	/* HIST_countFast_wksp() :
//	 * Same as HIST_countFast(), but using an externally provided scratch buffer.
//	 * `workSpace` is a writable buffer which must be 4-bytes aligned,
//	 * `workSpaceSize` must be >= HIST_WKSP_SIZE
//	 */
func HIST_countFast_wksp(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t, workSpace uintptr, workSpaceSize size_t) (r size_t) {
	if sourceSize < uint64(1500) { /* heuristic threshold */
		return uint64(HIST_count_simple(tls, count, maxSymbolValuePtr, source, sourceSize))
	}
	if uint64(workSpace)&uint64(3) != 0 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* must be aligned on 4-bytes boundaries */
	if workSpaceSize < libc.Uint64FromInt32(HIST_WKSP_SIZE_U32)*libc.Uint64FromInt64(4) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_workSpace_tooSmall))
	}
	return HIST_count_parallel_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, int32(trustInput), workSpace)
}

// C documentation
//
//	/* HIST_count_wksp() :
//	 * Same as HIST_count(), but using an externally provided scratch buffer.
//	 * `workSpace` size must be table of >= HIST_WKSP_SIZE_U32 unsigned */
func HIST_count_wksp(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t, workSpace uintptr, workSpaceSize size_t) (r size_t) {
	if uint64(workSpace)&uint64(3) != 0 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* must be aligned on 4-bytes boundaries */
	if workSpaceSize < libc.Uint64FromInt32(HIST_WKSP_SIZE_U32)*libc.Uint64FromInt64(4) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_workSpace_tooSmall))
	}
	if *(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) < uint32(255) {
		return HIST_count_parallel_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, int32(checkMaxSymbolValue), workSpace)
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = uint32(255)
	return HIST_countFast_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, workSpace, workSpaceSize)
}

// C documentation
//
//	/* fast variant (unsafe : won't check if src contains values beyond count[] limit) */
func HIST_countFast(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, source uintptr, sourceSize size_t) (r size_t) {
	bp := tls.Alloc(4096)
	defer tls.Free(4096)
	var _ /* tmpCounters at bp+0 */ [1024]uint32
	return HIST_countFast_wksp(tls, count, maxSymbolValuePtr, source, sourceSize, bp, uint64(4096))
}

func HIST_count(tls *libc.TLS, count uintptr, maxSymbolValuePtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(4096)
	defer tls.Free(4096)
	var _ /* tmpCounters at bp+0 */ [1024]uint32
	return HIST_count_wksp(tls, count, maxSymbolValuePtr, src, srcSize, bp, uint64(4096))
}

/**** ended inlining compress/hist.c ****/
/**** start inlining compress/huf_compress.c ****/
/* ******************************************************************
 * Huffman encoder, part of New Generation Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Compiler specifics
****************************************************************/

/* **************************************************************
*  Includes
****************************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  Required declarations
//	****************************************************************/
type nodeElt = struct {
	Fcount  U32
	Fparent U16
	Fbyte1  BYTE
	FnbBits BYTE
}

/**** ended inlining compress/hist.c ****/
/**** start inlining compress/huf_compress.c ****/
/* ******************************************************************
 * Huffman encoder, part of New Generation Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *  - Public forum : https://groups.google.com/forum/#!forum/lz4c
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Compiler specifics
****************************************************************/

/* **************************************************************
*  Includes
****************************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Error Management
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  Required declarations
//	****************************************************************/
type nodeElt_s = nodeElt

/* **************************************************************
*  Debug Traces
****************************************************************/

/* *******************************************************
*  HUF : Huffman block compression
*********************************************************/

func HUF_alignUpWorkspace(tls *libc.TLS, workspace uintptr, workspaceSizePtr uintptr, align size_t) (r uintptr) {
	var add, mask, rem size_t
	var aligned uintptr
	_, _, _, _ = add, aligned, mask, rem
	mask = align - uint64(1)
	rem = uint64(workspace) & mask
	add = (align - rem) & mask
	aligned = workspace + uintptr(add)
	/* pow 2 */
	if *(*size_t)(unsafe.Pointer(workspaceSizePtr)) >= add {
		*(*size_t)(unsafe.Pointer(workspaceSizePtr)) -= add
		return aligned
	} else {
		*(*size_t)(unsafe.Pointer(workspaceSizePtr)) = uint64(0)
		return libc.UintptrFromInt32(0)
	}
	return r
}

/* HUF_compressWeights() :
 * Same as FSE_compress(), but dedicated to huff0's weights compression.
 * The use case needs much less stack memory.
 * Note : all elements within weightTable are supposed to be <= HUF_TABLELOG_MAX.
 */

type HUF_CompressWeightsWksp = struct {
	FCTable        [59]FSE_CTable
	FscratchBuffer [41]U32
	Fcount         [13]uint32
	Fnorm          [13]S16
}

func HUF_compressWeights(tls *libc.TLS, dst uintptr, dstSize size_t, weightTable uintptr, wtSize size_t, workspace uintptr, _workspaceSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*size_t)(unsafe.Pointer(bp)) = _workspaceSize
	var _var_err__, _var_err__1, cSize, hSize size_t
	var maxCount uint32
	var oend, op, ostart, wksp uintptr
	var tableLog U32
	var _ /* maxSymbolValue at bp+8 */ uint32
	_, _, _, _, _, _, _, _, _, _ = _var_err__, _var_err__1, cSize, hSize, maxCount, oend, op, ostart, tableLog, wksp
	ostart = dst
	op = ostart
	oend = ostart + uintptr(dstSize)
	*(*uint32)(unsafe.Pointer(bp + 8)) = uint32(HUF_TABLELOG_MAX)
	tableLog = uint32(MAX_FSE_TABLELOG_FOR_HUFF_HEADER)
	wksp = HUF_alignUpWorkspace(tls, workspace, bp, uint64(4))
	if *(*size_t)(unsafe.Pointer(bp)) < uint64(480) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	/* init conditions */
	if wtSize <= uint64(1) {
		return uint64(0)
	} /* Not compressible */
	/* Scan input and build symbol stats */
	maxCount = HIST_count_simple(tls, wksp+400, bp+8, weightTable, wtSize) /* never fails */
	if uint64(maxCount) == wtSize {
		return uint64(1)
	} /* only a single symbol in src : rle */
	if maxCount == uint32(1) {
		return uint64(0)
	} /* each symbol present maximum once => not compressible */
	tableLog = FSE_optimalTableLog(tls, tableLog, wtSize, *(*uint32)(unsafe.Pointer(bp + 8)))
	_var_err__ = FSE_normalizeCount(tls, wksp+452, tableLog, wksp+400, wtSize, *(*uint32)(unsafe.Pointer(bp + 8)), uint32(0))
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	/* Write table description header */
	hSize = FSE_writeNCount(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), wksp+452, *(*uint32)(unsafe.Pointer(bp + 8)), tableLog)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	op = op + uintptr(hSize)
	/* Compress */
	_var_err__1 = FSE_buildCTable_wksp(tls, wksp, wksp+452, *(*uint32)(unsafe.Pointer(bp + 8)), tableLog, wksp+236, uint64(164))
	if ERR_isError(tls, _var_err__1) != 0 {
		return _var_err__1
	}
	cSize = FSE_compress_usingCTable(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), weightTable, wtSize, wksp)
	if ERR_isError(tls, cSize) != 0 {
		return cSize
	}
	if cSize == uint64(0) {
		return uint64(0)
	} /* not enough space for compressed data */
	op = op + uintptr(cSize)
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func HUF_getNbBits(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt & uint64(0xFF)
}

func HUF_getNbBitsFast(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt
}

func HUF_getValue(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt & ^libc.Uint64FromInt32(0xFF)
}

func HUF_getValueFast(tls *libc.TLS, elt HUF_CElt) (r size_t) {
	return elt
}

func HUF_setNbBits(tls *libc.TLS, elt uintptr, nbBits size_t) {
	*(*HUF_CElt)(unsafe.Pointer(elt)) = nbBits
}

func HUF_setValue(tls *libc.TLS, elt uintptr, value size_t) {
	var nbBits size_t
	_ = nbBits
	nbBits = HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(elt)))
	if nbBits > uint64(0) {
		*(*HUF_CElt)(unsafe.Pointer(elt)) |= value << (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - nbBits)
	}
}

func HUF_readCTableHeader(tls *libc.TLS, ctable uintptr) (r HUF_CTableHeader) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* header at bp+0 */ HUF_CTableHeader
	libc.Xmemcpy(tls, bp, ctable, libc.Uint64FromInt64(8))
	return *(*HUF_CTableHeader)(unsafe.Pointer(bp))
}

func HUF_writeCTableHeader(tls *libc.TLS, ctable uintptr, tableLog U32, maxSymbolValue U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* header at bp+0 */ HUF_CTableHeader
	_ = libc.Uint64FromInt64(1)
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(8))
	(*(*HUF_CTableHeader)(unsafe.Pointer(bp))).FtableLog = uint8(tableLog)
	(*(*HUF_CTableHeader)(unsafe.Pointer(bp))).FmaxSymbolValue = uint8(maxSymbolValue)
	libc.Xmemcpy(tls, ctable, bp, libc.Uint64FromInt64(8))
}

type HUF_WriteCTableWksp = struct {
	Fwksp         HUF_CompressWeightsWksp
	FbitsToWeight [13]BYTE
	FhuffWeight   [255]BYTE
}

func HUF_writeCTable_wksp(tls *libc.TLS, dst uintptr, maxDstSize size_t, CTable uintptr, maxSymbolValue uint32, huffLog uint32, workspace uintptr, _workspaceSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*size_t)(unsafe.Pointer(bp)) = _workspaceSize
	var ct, op, wksp uintptr
	var hSize size_t
	var n U32
	_, _, _, _, _ = ct, hSize, n, op, wksp
	ct = CTable + uintptr(1)*8
	op = dst
	wksp = HUF_alignUpWorkspace(tls, workspace, bp, uint64(4))
	_ = libc.Uint64FromInt64(1)
	/* check conditions */
	if *(*size_t)(unsafe.Pointer(bp)) < uint64(748) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	if maxSymbolValue > uint32(HUF_SYMBOLVALUE_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	/* convert to weight */
	*(*BYTE)(unsafe.Pointer(wksp + 480)) = uint8(0)
	n = uint32(1)
	for {
		if !(n < huffLog+uint32(1)) {
			break
		}
		*(*BYTE)(unsafe.Pointer(wksp + 480 + uintptr(n))) = uint8(huffLog + libc.Uint32FromInt32(1) - n)
		goto _1
	_1:
		;
		n = n + 1
	}
	n = uint32(0)
	for {
		if !(n < maxSymbolValue) {
			break
		}
		*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(n))) = *(*BYTE)(unsafe.Pointer(wksp + 480 + uintptr(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n)*8))))))
		goto _2
	_2:
		;
		n = n + 1
	}
	/* attempt weights compression by FSE */
	if maxDstSize < uint64(1) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	hSize = HUF_compressWeights(tls, op+uintptr(1), maxDstSize-uint64(1), wksp+493, uint64(maxSymbolValue), wksp, uint64(480))
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if libc.BoolInt32(hSize > uint64(1))&libc.BoolInt32(hSize < uint64(maxSymbolValue/uint32(2))) != 0 { /* FSE compressed */
		*(*BYTE)(unsafe.Pointer(op)) = uint8(hSize)
		return hSize + uint64(1)
	}
	/* write raw values as 4-bits (max : 15) */
	if maxSymbolValue > libc.Uint32FromInt32(libc.Int32FromInt32(256)-libc.Int32FromInt32(128)) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* should not happen : likely means source cannot be compressed */
	if uint64((maxSymbolValue+uint32(1))/uint32(2)+uint32(1)) > maxDstSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	} /* not enough space within dst buffer */
	*(*BYTE)(unsafe.Pointer(op)) = uint8(libc.Uint32FromInt32(128) + (maxSymbolValue - libc.Uint32FromInt32(1)))
	*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(maxSymbolValue))) = uint8(0) /* to be sure it doesn't cause msan issue in final combination */
	n = uint32(0)
	for {
		if !(n < maxSymbolValue) {
			break
		}
		*(*BYTE)(unsafe.Pointer(op + uintptr(n/uint32(2)+uint32(1)))) = libc.Uint8FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(n))))<<libc.Int32FromInt32(4) + libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(wksp + 493 + uintptr(n+uint32(1))))))
		goto _3
	_3:
		;
		n = n + uint32(2)
	}
	return uint64((maxSymbolValue+uint32(1))/uint32(2) + uint32(1))
}

func HUF_readCTable(tls *libc.TLS, CTable uintptr, maxSymbolValuePtr uintptr, src uintptr, srcSize size_t, hasZeroWeights uintptr) (r size_t) {
	bp := tls.Alloc(352)
	defer tls.Free(352)
	var ct, v7 uintptr
	var curr, n, n1, n2, n3, n4, nextRankStart, w U32
	var min, v6 U16
	var nbPerRank [14]U16
	var readSize size_t
	var _ /* huffWeight at bp+0 */ [256]BYTE
	var _ /* nbSymbols at bp+312 */ U32
	var _ /* rankVal at bp+256 */ [13]U32
	var _ /* tableLog at bp+308 */ U32
	var _ /* valPerRank at bp+316 */ [14]U16
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = ct, curr, min, n, n1, n2, n3, n4, nbPerRank, nextRankStart, readSize, w, v6, v7 /* large enough for values from 0 to 16 */
	*(*U32)(unsafe.Pointer(bp + 308)) = uint32(0)
	*(*U32)(unsafe.Pointer(bp + 312)) = uint32(0)
	ct = CTable + uintptr(1)*8
	/* get symbol weights */
	readSize = HUF_readStats(tls, bp, libc.Uint64FromInt32(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)), bp+256, bp+312, bp+308, src, srcSize)
	if ERR_isError(tls, readSize) != 0 {
		return readSize
	}
	*(*uint32)(unsafe.Pointer(hasZeroWeights)) = libc.BoolUint32((*(*[13]U32)(unsafe.Pointer(bp + 256)))[0] > libc.Uint32FromInt32(0))
	/* check result */
	if *(*U32)(unsafe.Pointer(bp + 308)) > uint32(HUF_TABLELOG_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	if *(*U32)(unsafe.Pointer(bp + 312)) > *(*uint32)(unsafe.Pointer(maxSymbolValuePtr))+uint32(1) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooSmall))
	}
	*(*uint32)(unsafe.Pointer(maxSymbolValuePtr)) = *(*U32)(unsafe.Pointer(bp + 312)) - uint32(1)
	HUF_writeCTableHeader(tls, CTable, *(*U32)(unsafe.Pointer(bp + 308)), *(*uint32)(unsafe.Pointer(maxSymbolValuePtr)))
	/* Prepare base value per rank */
	nextRankStart = uint32(0)
	n = uint32(1)
	for {
		if !(n <= *(*U32)(unsafe.Pointer(bp + 308))) {
			break
		}
		curr = nextRankStart
		nextRankStart = nextRankStart + (*(*[13]U32)(unsafe.Pointer(bp + 256)))[n]<<(n-libc.Uint32FromInt32(1))
		(*(*[13]U32)(unsafe.Pointer(bp + 256)))[n] = curr
		goto _1
	_1:
		;
		n = n + 1
	}
	/* fill nbBits */
	n1 = uint32(0)
	for {
		if !(n1 < *(*U32)(unsafe.Pointer(bp + 312))) {
			break
		}
		w = uint32((*(*[256]BYTE)(unsafe.Pointer(bp)))[n1])
		HUF_setNbBits(tls, ct+uintptr(n1)*8, libc.Uint64FromInt32(libc.Int32FromUint8(uint8(*(*U32)(unsafe.Pointer(bp + 308))+libc.Uint32FromInt32(1)-w))&-libc.BoolInt32(w != uint32(0))))
		goto _2
	_2:
		;
		n1 = n1 + 1
	}
	/* fill val */
	nbPerRank = [14]U16{} /* support w=0=>n=tableLog+1 */
	*(*[14]U16)(unsafe.Pointer(bp + 316)) = [14]U16{}
	n2 = uint32(0)
	for {
		if !(n2 < *(*U32)(unsafe.Pointer(bp + 312))) {
			break
		}
		nbPerRank[HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n2)*8)))] = nbPerRank[HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n2)*8)))] + 1
		goto _3
	_3:
		;
		n2 = n2 + 1
	}
	/* determine stating value per rank */
	(*(*[14]U16)(unsafe.Pointer(bp + 316)))[*(*U32)(unsafe.Pointer(bp + 308))+uint32(1)] = uint16(0) /* for w==0 */
	min = uint16(0)
	n3 = *(*U32)(unsafe.Pointer(bp + 308))
	for {
		if !(n3 > uint32(0)) {
			break
		} /* start at n=tablelog <-> w=1 */
		(*(*[14]U16)(unsafe.Pointer(bp + 316)))[n3] = min /* get starting value within each rank */
		min = libc.Uint16FromInt32(int32(min) + libc.Int32FromUint16(nbPerRank[n3]))
		min = libc.Uint16FromInt32(int32(min) >> libc.Int32FromInt32(1))
		goto _4
	_4:
		;
		n3 = n3 - 1
	}
	/* assign value within rank, symbol order */
	n4 = uint32(0)
	for {
		if !(n4 < *(*U32)(unsafe.Pointer(bp + 312))) {
			break
		}
		v7 = bp + 316 + uintptr(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n4)*8))))*2
		v6 = *(*U16)(unsafe.Pointer(v7))
		*(*U16)(unsafe.Pointer(v7)) = *(*U16)(unsafe.Pointer(v7)) + 1
		HUF_setValue(tls, ct+uintptr(n4)*8, uint64(v6))
		goto _5
	_5:
		;
		n4 = n4 + 1
	}
	return readSize
}

func HUF_getNbBitsFromCTable(tls *libc.TLS, CTable uintptr, symbolValue U32) (r U32) {
	var ct uintptr
	_ = ct
	ct = CTable + uintptr(1)*8
	if symbolValue > uint32(HUF_readCTableHeader(tls, CTable).FmaxSymbolValue) {
		return uint32(0)
	}
	return uint32(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(symbolValue)*8))))
}

// C documentation
//
//	/**
//	 * HUF_setMaxHeight():
//	 * Try to enforce @targetNbBits on the Huffman tree described in @huffNode.
//	 *
//	 * It attempts to convert all nodes with nbBits > @targetNbBits
//	 * to employ @targetNbBits instead. Then it adjusts the tree
//	 * so that it remains a valid canonical Huffman tree.
//	 *
//	 * @pre               The sum of the ranks of each symbol == 2^largestBits,
//	 *                    where largestBits == huffNode[lastNonNull].nbBits.
//	 * @post              The sum of the ranks of each symbol == 2^largestBits,
//	 *                    where largestBits is the return value (expected <= targetNbBits).
//	 *
//	 * @param huffNode    The Huffman tree modified in place to enforce targetNbBits.
//	 *                    It's presumed sorted, from most frequent to rarest symbol.
//	 * @param lastNonNull The symbol with the lowest count in the Huffman tree.
//	 * @param targetNbBits  The allowed number of bits, which the Huffman tree
//	 *                    may not respect. After this function the Huffman tree will
//	 *                    respect targetNbBits.
//	 * @return            The maximum number of bits of the Huffman tree after adjustment.
//	 */
func HUF_setMaxHeight(tls *libc.TLS, huffNode uintptr, lastNonNull U32, targetNbBits U32) (r U32) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var baseCost, currentNbBits, highPos, highTotal, largestBits, lowPos, lowTotal, nBitsToDecrease, noSymbol U32
	var n, pos, totalCost int32
	var _ /* rankLast at bp+0 */ [14]U32
	_, _, _, _, _, _, _, _, _, _, _, _ = baseCost, currentNbBits, highPos, highTotal, largestBits, lowPos, lowTotal, n, nBitsToDecrease, noSymbol, pos, totalCost
	largestBits = uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lastNonNull)*8))).FnbBits)
	/* early exit : no elt > targetNbBits, so the tree is already valid. */
	if largestBits <= targetNbBits {
		return largestBits
	}
	/* there are several too large elements (at least >= 2) */
	totalCost = 0
	baseCost = libc.Uint32FromInt32(int32(1) << (largestBits - targetNbBits))
	n = libc.Int32FromUint32(lastNonNull)
	/* Adjust any ranks > targetNbBits to targetNbBits.
	 * Compute totalCost, which is how far the sum of the ranks is
	 * we are over 2^largestBits after adjust the offending ranks.
	 */
	for uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits) > targetNbBits {
		totalCost = libc.Int32FromUint32(uint32(totalCost) + (baseCost - libc.Uint32FromInt32(libc.Int32FromInt32(1)<<(largestBits-uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits)))))
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits = uint8(targetNbBits)
		n = n - 1
	}
	/* n stops at huffNode[n].nbBits <= targetNbBits */
	/* n end at index of smallest symbol using < targetNbBits */
	for uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits) == targetNbBits {
		n = n - 1
	}
	/* renorm totalCost from 2^largestBits to 2^targetNbBits
	 * note : totalCost is necessarily a multiple of baseCost */
	totalCost = libc.Int32FromUint32(uint32(totalCost) >> (largestBits - targetNbBits))
	/* repay normalized cost */
	noSymbol = uint32(0xF0F0F0F0)
	/* Get pos of last (smallest = lowest cum. count) symbol per rank */
	libc.Xmemset(tls, bp, int32(0xF0), libc.Uint64FromInt64(56))
	currentNbBits = targetNbBits
	pos = n
	for {
		if !(pos >= 0) {
			break
		}
		if uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).FnbBits) >= currentNbBits {
			goto _1
		}
		currentNbBits = uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).FnbBits) /* < targetNbBits */
		(*(*[14]U32)(unsafe.Pointer(bp)))[targetNbBits-currentNbBits] = libc.Uint32FromInt32(pos)
		goto _1
	_1:
		;
		pos = pos - 1
	}
	for totalCost > 0 {
		/* Try to reduce the next power of 2 above totalCost because we
		 * gain back half the rank.
		 */
		nBitsToDecrease = ZSTD_highbit32(tls, libc.Uint32FromInt32(totalCost)) + uint32(1)
		for {
			if !(nBitsToDecrease > uint32(1)) {
				break
			}
			highPos = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease]
			lowPos = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease-uint32(1)]
			if highPos == noSymbol {
				goto _2
			}
			/* Decrease highPos if no symbols of lowPos or if it is
			 * not cheaper to remove 2 lowPos than highPos.
			 */
			if lowPos == noSymbol {
				break
			}
			highTotal = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(highPos)*8))).Fcount
			lowTotal = uint32(2) * (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowPos)*8))).Fcount
			if highTotal <= lowTotal {
				break
			}
			goto _2
		_2:
			;
			nBitsToDecrease = nBitsToDecrease - 1
		}
		/* only triggered when no more rank 1 symbol left => find closest one (note : there is necessarily at least one !) */
		/* HUF_MAX_TABLELOG test just to please gcc 5+; but it should not be necessary */
		for nBitsToDecrease <= uint32(HUF_TABLELOG_MAX) && (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] == noSymbol {
			nBitsToDecrease = nBitsToDecrease + 1
		}
		/* Increase the number of bits to gain back half the rank cost. */
		totalCost = totalCost - int32(1)<<(nBitsToDecrease-uint32(1))
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease])*8))).FnbBits = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease])*8))).FnbBits + 1
		/* Fix up the new rank.
		 * If the new rank was empty, this symbol is now its smallest.
		 * Otherwise, this symbol will be the largest in the new rank so no adjustment.
		 */
		if (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease-uint32(1)] == noSymbol {
			(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease-uint32(1)] = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease]
		}
		/* Fix up the old rank.
		 * If the symbol was at position 0, meaning it was the highest weight symbol in the tree,
		 * it must be the only symbol in its rank, so the old rank now has no symbols.
		 * Otherwise, since the Huffman nodes are sorted by count, the previous position is now
		 * the smallest node in the rank. If the previous position belongs to a different rank,
		 * then the rank is now empty.
		 */
		if (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] == uint32(0) { /* special case, reached largest symbol */
			(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] = noSymbol
		} else {
			(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] = (*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] - 1
			if uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease])*8))).FnbBits) != targetNbBits-nBitsToDecrease {
				(*(*[14]U32)(unsafe.Pointer(bp)))[nBitsToDecrease] = noSymbol
			} /* this rank is now empty */
		}
	} /* while (totalCost > 0) */
	/* If we've removed too much weight, then we have to add it back.
	 * To avoid overshooting again, we only adjust the smallest rank.
	 * We take the largest nodes from the lowest rank 0 and move them
	 * to rank 1. There's guaranteed to be enough rank 0 symbols because
	 * TODO.
	 */
	for totalCost < 0 { /* Sometimes, cost correction overshoot */
		/* special case : no rank 1 symbol (using targetNbBits-1);
		 * let's create one from largest rank 0 (using targetNbBits).
		 */
		if (*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] == noSymbol {
			for uint32((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits) == targetNbBits {
				n = n - 1
			}
			(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n+int32(1))*8))).FnbBits = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n+int32(1))*8))).FnbBits - 1
			(*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] = libc.Uint32FromInt32(n + libc.Int32FromInt32(1))
			totalCost = totalCost + 1
			continue
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)]+uint32(1))*8))).FnbBits = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)]+uint32(1))*8))).FnbBits - 1
		(*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] = (*(*[14]U32)(unsafe.Pointer(bp)))[int32(1)] + 1
		totalCost = totalCost + 1
	}
	/* repay normalized cost */
	/* there are several too large elements (at least >= 2) */
	return targetNbBits
}

type rankPos = struct {
	Fbase U16
	Fcurr U16
}

type huffNodeTable = [512]nodeElt

/* Number of buckets available for HUF_sort() */

type HUF_buildCTable_wksp_tables = struct {
	FhuffNodeTbl  huffNodeTable
	FrankPosition [192]rankPos
}

/* RANK_POSITION_DISTINCT_COUNT_CUTOFF == Cutoff point in HUF_sort() buckets for which we use log2 bucketing.
 * Strategy is to use as many buckets as possible for representing distinct
 * counts while using the remainder to represent all "large" counts.
 *
 * To satisfy this requirement for 192 buckets, we can do the following:
 * Let buckets 0-166 represent distinct counts of [0, 166]
 * Let buckets 166 to 192 represent all remaining counts up to RANK_POSITION_MAX_COUNT_LOG using log2 bucketing.
 */

// C documentation
//
//	/* Return the appropriate bucket index for a given count. See definition of
//	 * RANK_POSITION_DISTINCT_COUNT_CUTOFF for explanation of bucketing strategy.
//	 */
func HUF_getIndex(tls *libc.TLS, count U32) (r U32) {
	var v1 uint32
	_ = v1
	if count < libc.Uint32FromInt32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1))+ZSTD_highbit32(tls, libc.Uint32FromInt32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1))) {
		v1 = count
	} else {
		v1 = ZSTD_highbit32(tls, count) + libc.Uint32FromInt32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1))
	}
	return v1
}

// C documentation
//
//	/* Helper swap function for HUF_quickSortPartition() */
func HUF_swapNodes(tls *libc.TLS, a uintptr, b uintptr) {
	var tmp nodeElt
	_ = tmp
	tmp = *(*nodeElt)(unsafe.Pointer(a))
	*(*nodeElt)(unsafe.Pointer(a)) = *(*nodeElt)(unsafe.Pointer(b))
	*(*nodeElt)(unsafe.Pointer(b)) = tmp
}

// C documentation
//
//	/* Returns 0 if the huffNode array is not sorted by descending count */
func HUF_isSorted(tls *libc.TLS, huffNode uintptr, maxSymbolValue1 U32) (r int32) {
	var i U32
	_ = i
	i = uint32(1)
	for {
		if !(i < maxSymbolValue1) {
			break
		}
		if (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(i)*8))).Fcount > (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(i-uint32(1))*8))).Fcount {
			return 0
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return int32(1)
}

// C documentation
//
//	/* Insertion sort by descending order */
func HUF_insertionSort(tls *libc.TLS, huffNode uintptr, low int32, high int32) {
	var i, j, size int32
	var key nodeElt
	_, _, _, _ = i, j, key, size
	size = high - low + int32(1)
	huffNode = huffNode + uintptr(low)*8
	i = int32(1)
	for {
		if !(i < size) {
			break
		}
		key = *(*nodeElt)(unsafe.Pointer(huffNode + uintptr(i)*8))
		j = i - int32(1)
		for j >= 0 && (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j)*8))).Fcount < key.Fcount {
			*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j+int32(1))*8)) = *(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j)*8))
			j = j - 1
		}
		*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(j+int32(1))*8)) = key
		goto _1
	_1:
		;
		i = i + 1
	}
}

// C documentation
//
//	/* Pivot helper function for quicksort. */
func HUF_quickSortPartition(tls *libc.TLS, arr uintptr, low int32, high int32) (r int32) {
	var i, j int32
	var pivot U32
	_, _, _ = i, j, pivot
	/* Simply select rightmost element as pivot. "Better" selectors like
	 * median-of-three don't experimentally appear to have any benefit.
	 */
	pivot = (*(*nodeElt)(unsafe.Pointer(arr + uintptr(high)*8))).Fcount
	i = low - int32(1)
	j = low
	for {
		if !(j < high) {
			break
		}
		if (*(*nodeElt)(unsafe.Pointer(arr + uintptr(j)*8))).Fcount > pivot {
			i = i + 1
			HUF_swapNodes(tls, arr+uintptr(i)*8, arr+uintptr(j)*8)
		}
		goto _1
	_1:
		;
		j = j + 1
	}
	HUF_swapNodes(tls, arr+uintptr(i+int32(1))*8, arr+uintptr(high)*8)
	return i + int32(1)
}

// C documentation
//
//	/* Classic quicksort by descending with partially iterative calls
//	 * to reduce worst case callstack size.
//	 */
func HUF_simpleQuickSort(tls *libc.TLS, arr uintptr, low int32, high int32) {
	var idx, kInsertionSortThreshold int32
	_, _ = idx, kInsertionSortThreshold
	kInsertionSortThreshold = int32(8)
	if high-low < kInsertionSortThreshold {
		HUF_insertionSort(tls, arr, low, high)
		return
	}
	for low < high {
		idx = HUF_quickSortPartition(tls, arr, low, high)
		if idx-low < high-idx {
			HUF_simpleQuickSort(tls, arr, low, idx-int32(1))
			low = idx + int32(1)
		} else {
			HUF_simpleQuickSort(tls, arr, idx+int32(1), high)
			high = idx - int32(1)
		}
	}
}

// C documentation
//
//	/**
//	 * HUF_sort():
//	 * Sorts the symbols [0, maxSymbolValue] by count[symbol] in decreasing order.
//	 * This is a typical bucket sorting strategy that uses either quicksort or insertion sort to sort each bucket.
//	 *
//	 * @param[out] huffNode       Sorted symbols by decreasing count. Only members `.count` and `.byte` are filled.
//	 *                            Must have (maxSymbolValue + 1) entries.
//	 * @param[in]  count          Histogram of the symbols.
//	 * @param[in]  maxSymbolValue Maximum symbol value.
//	 * @param      rankPosition   This is a scratch workspace. Must have RANK_POSITION_TABLE_SIZE entries.
//	 */
func HUF_sort(tls *libc.TLS, huffNode uintptr, count uintptr, maxSymbolValue U32, rankPosition uintptr) {
	var bucketSize int32
	var bucketStartIdx, c, lowerRank, maxSymbolValue1, n, pos, r U32
	var v3 uintptr
	var v5 U16
	_, _, _, _, _, _, _, _, _, _ = bucketSize, bucketStartIdx, c, lowerRank, maxSymbolValue1, n, pos, r, v3, v5
	maxSymbolValue1 = maxSymbolValue + uint32(1)
	/* Compute base and set curr to base.
	 * For symbol s let lowerRank = HUF_getIndex(count[n]) and rank = lowerRank + 1.
	 * See HUF_getIndex to see bucketing strategy.
	 * We attribute each symbol to lowerRank's base value, because we want to know where
	 * each rank begins in the output, so for rank R we want to count ranks R+1 and above.
	 */
	libc.Xmemset(tls, rankPosition, 0, libc.Uint64FromInt64(4)*libc.Uint64FromInt32(RANK_POSITION_TABLE_SIZE))
	n = uint32(0)
	for {
		if !(n < maxSymbolValue1) {
			break
		}
		lowerRank = HUF_getIndex(tls, *(*uint32)(unsafe.Pointer(count + uintptr(n)*4)))
		(*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(lowerRank)*4))).Fbase = (*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(lowerRank)*4))).Fbase + 1
		goto _1
	_1:
		;
		n = n + 1
	}
	/* Set up the rankPosition table */
	n = libc.Uint32FromInt32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE) - libc.Int32FromInt32(1))
	for {
		if !(n > uint32(0)) {
			break
		}
		v3 = rankPosition + uintptr(n-uint32(1))*4
		*(*U16)(unsafe.Pointer(v3)) = U16(int32(*(*U16)(unsafe.Pointer(v3))) + libc.Int32FromUint16((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fbase))
		(*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n-uint32(1))*4))).Fcurr = (*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n-uint32(1))*4))).Fbase
		goto _2
	_2:
		;
		n = n - 1
	}
	/* Insert each symbol into their appropriate bucket, setting up rankPosition table. */
	n = uint32(0)
	for {
		if !(n < maxSymbolValue1) {
			break
		}
		c = *(*uint32)(unsafe.Pointer(count + uintptr(n)*4))
		r = HUF_getIndex(tls, c) + uint32(1)
		v3 = rankPosition + uintptr(r)*4 + 2
		v5 = *(*U16)(unsafe.Pointer(v3))
		*(*U16)(unsafe.Pointer(v3)) = *(*U16)(unsafe.Pointer(v3)) + 1
		pos = uint32(v5)
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).Fcount = c
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(pos)*8))).Fbyte1 = uint8(n)
		goto _4
	_4:
		;
		n = n + 1
	}
	/* Sort each bucket. */
	n = libc.Uint32FromInt32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1)) + ZSTD_highbit32(tls, libc.Uint32FromInt32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1)-libc.Int32FromInt32(RANK_POSITION_MAX_COUNT_LOG)-libc.Int32FromInt32(1)))
	for {
		if !(n < libc.Uint32FromInt32(libc.Int32FromInt32(RANK_POSITION_TABLE_SIZE)-libc.Int32FromInt32(1))) {
			break
		}
		bucketSize = libc.Int32FromUint16((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fcurr) - libc.Int32FromUint16((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fbase)
		bucketStartIdx = uint32((*(*rankPos)(unsafe.Pointer(rankPosition + uintptr(n)*4))).Fbase)
		if bucketSize > int32(1) {
			HUF_simpleQuickSort(tls, huffNode+uintptr(bucketStartIdx)*8, 0, bucketSize-int32(1))
		}
		goto _7
	_7:
		;
		n = n + 1
	}
}

/** HUF_buildCTable_wksp() :
 *  Same as HUF_buildCTable(), but using externally allocated scratch buffer.
 *  `workSpace` must be aligned on 4-bytes boundaries, and be at least as large as sizeof(HUF_buildCTable_wksp_tables).
 */

// C documentation
//
//	/* HUF_buildTree():
//	 * Takes the huffNode array sorted by HUF_sort() and builds an unlimited-depth Huffman tree.
//	 *
//	 * @param huffNode        The array sorted by HUF_sort(). Builds the Huffman tree in this array.
//	 * @param maxSymbolValue  The maximum symbol value.
//	 * @return                The smallest node in the Huffman tree (by count).
//	 */
func HUF_buildTree(tls *libc.TLS, huffNode uintptr, maxSymbolValue U32) (r int32) {
	var huffNode0 uintptr
	var lowN, lowS, n, n1, n2, nodeNb, nodeRoot, nonNullRank, v3, v4, v5, v6, v7, v8 int32
	var v1 U16
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = huffNode0, lowN, lowS, n, n1, n2, nodeNb, nodeRoot, nonNullRank, v1, v3, v4, v5, v6, v7, v8
	huffNode0 = huffNode - uintptr(1)*8
	nodeNb = libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX) + libc.Int32FromInt32(1)
	/* init for parents */
	nonNullRank = libc.Int32FromUint32(maxSymbolValue)
	for (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nonNullRank)*8))).Fcount == uint32(0) {
		nonNullRank = nonNullRank - 1
	}
	lowS = nonNullRank
	nodeRoot = nodeNb + lowS - int32(1)
	lowN = nodeNb
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nodeNb)*8))).Fcount = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fcount + (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS-int32(1))*8))).Fcount
	v1 = libc.Uint16FromInt32(nodeNb)
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS-int32(1))*8))).Fparent = v1
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fparent = v1
	nodeNb = nodeNb + 1
	lowS = lowS - int32(2)
	n = nodeNb
	for {
		if !(n <= nodeRoot) {
			break
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fcount = libc.Uint32FromUint32(1) << libc.Int32FromInt32(30)
		goto _2
	_2:
		;
		n = n + 1
	}
	(*(*nodeElt)(unsafe.Pointer(huffNode0))).Fcount = libc.Uint32FromUint32(1) << libc.Int32FromInt32(31) /* fake entry, strong barrier */
	/* create parents */
	for nodeNb <= nodeRoot {
		if (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fcount < (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowN)*8))).Fcount {
			v4 = lowS
			lowS = lowS - 1
			v3 = v4
		} else {
			v5 = lowN
			lowN = lowN + 1
			v3 = v5
		}
		n1 = v3
		if (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowS)*8))).Fcount < (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(lowN)*8))).Fcount {
			v7 = lowS
			lowS = lowS - 1
			v6 = v7
		} else {
			v8 = lowN
			lowN = lowN + 1
			v6 = v8
		}
		n2 = v6
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nodeNb)*8))).Fcount = (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n1)*8))).Fcount + (*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n2)*8))).Fcount
		v1 = libc.Uint16FromInt32(nodeNb)
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n2)*8))).Fparent = v1
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n1)*8))).Fparent = v1
		nodeNb = nodeNb + 1
	}
	/* distribute weights (unlimited tree height) */
	(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(nodeRoot)*8))).FnbBits = uint8(0)
	n = nodeRoot - int32(1)
	for {
		if !(n >= libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)) {
			break
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits = libc.Uint8FromInt32(libc.Int32FromUint8((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fparent)*8))).FnbBits) + int32(1))
		goto _10
	_10:
		;
		n = n - 1
	}
	n = 0
	for {
		if !(n <= nonNullRank) {
			break
		}
		(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits = libc.Uint8FromInt32(libc.Int32FromUint8((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fparent)*8))).FnbBits) + int32(1))
		goto _11
	_11:
		;
		n = n + 1
	}
	return nonNullRank
}

// C documentation
//
//	/**
//	 * HUF_buildCTableFromTree():
//	 * Build the CTable given the Huffman tree in huffNode.
//	 *
//	 * @param[out] CTable         The output Huffman CTable.
//	 * @param      huffNode       The Huffman tree.
//	 * @param      nonNullRank    The last and smallest node in the Huffman tree.
//	 * @param      maxSymbolValue The maximum symbol value.
//	 * @param      maxNbBits      The exact maximum number of bits used in the Huffman tree.
//	 */
func HUF_buildCTableFromTree(tls *libc.TLS, CTable uintptr, huffNode uintptr, nonNullRank int32, maxSymbolValue U32, maxNbBits U32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var alphabetSize, n int32
	var ct, v6 uintptr
	var min, v5 U16
	var nbPerRank [13]U16
	var _ /* valPerRank at bp+0 */ [13]U16
	_, _, _, _, _, _, _ = alphabetSize, ct, min, n, nbPerRank, v5, v6
	ct = CTable + uintptr(1)*8
	nbPerRank = [13]U16{}
	*(*[13]U16)(unsafe.Pointer(bp)) = [13]U16{}
	alphabetSize = libc.Int32FromUint32(maxSymbolValue + libc.Uint32FromInt32(1))
	n = 0
	for {
		if !(n <= nonNullRank) {
			break
		}
		nbPerRank[(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits] = nbPerRank[(*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits] + 1
		goto _1
	_1:
		;
		n = n + 1
	}
	/* determine starting value per rank */
	min = uint16(0)
	n = libc.Int32FromUint32(maxNbBits)
	for {
		if !(n > 0) {
			break
		}
		(*(*[13]U16)(unsafe.Pointer(bp)))[n] = min /* get starting value within each rank */
		min = libc.Uint16FromInt32(int32(min) + libc.Int32FromUint16(nbPerRank[n]))
		min = libc.Uint16FromInt32(int32(min) >> libc.Int32FromInt32(1))
		goto _2
	_2:
		;
		n = n - 1
	}
	n = 0
	for {
		if !(n < alphabetSize) {
			break
		}
		HUF_setNbBits(tls, ct+uintptr((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).Fbyte1)*8, uint64((*(*nodeElt)(unsafe.Pointer(huffNode + uintptr(n)*8))).FnbBits))
		goto _3
	_3:
		;
		n = n + 1
	} /* push nbBits per symbol, symbol order */
	n = 0
	for {
		if !(n < alphabetSize) {
			break
		}
		v6 = bp + uintptr(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(n)*8))))*2
		v5 = *(*U16)(unsafe.Pointer(v6))
		*(*U16)(unsafe.Pointer(v6)) = *(*U16)(unsafe.Pointer(v6)) + 1
		HUF_setValue(tls, ct+uintptr(n)*8, uint64(v5))
		goto _4
	_4:
		;
		n = n + 1
	} /* assign value within rank, symbol order */
	HUF_writeCTableHeader(tls, CTable, maxNbBits, maxSymbolValue)
}

func HUF_buildCTable_wksp(tls *libc.TLS, CTable uintptr, count uintptr, maxSymbolValue U32, maxNbBits U32, workSpace uintptr, _wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*size_t)(unsafe.Pointer(bp)) = _wkspSize
	var huffNode, huffNode0, wksp_tables uintptr
	var nonNullRank int32
	_, _, _, _ = huffNode, huffNode0, nonNullRank, wksp_tables
	wksp_tables = HUF_alignUpWorkspace(tls, workSpace, bp, uint64(4))
	huffNode0 = wksp_tables
	huffNode = huffNode0 + uintptr(1)*8
	_ = libc.Uint64FromInt64(1)
	/* safety checks */
	if *(*size_t)(unsafe.Pointer(bp)) < uint64(4864) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_workSpace_tooSmall))
	}
	if maxNbBits == uint32(0) {
		maxNbBits = uint32(HUF_TABLELOG_DEFAULT)
	}
	if maxSymbolValue > uint32(HUF_SYMBOLVALUE_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	libc.Xmemset(tls, huffNode0, 0, libc.Uint64FromInt64(4096))
	/* sort, decreasing order */
	HUF_sort(tls, huffNode, count, maxSymbolValue, wksp_tables+4096)
	/* build tree */
	nonNullRank = HUF_buildTree(tls, huffNode, maxSymbolValue)
	/* determine and enforce maxTableLog */
	maxNbBits = HUF_setMaxHeight(tls, huffNode, libc.Uint32FromInt32(nonNullRank), maxNbBits)
	if maxNbBits > uint32(HUF_TABLELOG_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	} /* check fit into table */
	HUF_buildCTableFromTree(tls, CTable, huffNode, nonNullRank, maxSymbolValue, maxNbBits)
	return uint64(maxNbBits)
}

func HUF_estimateCompressedSize(tls *libc.TLS, CTable uintptr, count uintptr, maxSymbolValue uint32) (r size_t) {
	var ct uintptr
	var nbBits size_t
	var s int32
	_, _, _ = ct, nbBits, s
	ct = CTable + uintptr(1)*8
	nbBits = uint64(0)
	s = 0
	for {
		if !(s <= libc.Int32FromUint32(maxSymbolValue)) {
			break
		}
		nbBits = nbBits + HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(s)*8)))*uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))
		goto _1
	_1:
		;
		s = s + 1
	}
	return nbBits >> int32(3)
}

func HUF_validateCTable(tls *libc.TLS, CTable uintptr, count uintptr, maxSymbolValue uint32) (r int32) {
	var bad, s int32
	var ct uintptr
	var header HUF_CTableHeader
	_, _, _, _ = bad, ct, header, s
	header = HUF_readCTableHeader(tls, CTable)
	ct = CTable + uintptr(1)*8
	bad = 0
	if uint32(header.FmaxSymbolValue) < maxSymbolValue {
		return 0
	}
	s = 0
	for {
		if !(s <= libc.Int32FromUint32(maxSymbolValue)) {
			break
		}
		bad = bad | libc.BoolInt32(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) != uint32(0))&libc.BoolInt32(HUF_getNbBits(tls, *(*HUF_CElt)(unsafe.Pointer(ct + uintptr(s)*8))) == uint64(0))
		goto _1
	_1:
		;
		s = s + 1
	}
	return libc.BoolInt32(!(bad != 0))
}

func HUF_compressBound(tls *libc.TLS, size size_t) (r size_t) {
	return libc.Uint64FromInt32(HUF_CTABLEBOUND) + (size + size>>libc.Int32FromInt32(8) + libc.Uint64FromInt32(8))
}

/** HUF_CStream_t:
 * Huffman uses its own BIT_CStream_t implementation.
 * There are three major differences from BIT_CStream_t:
 *   1. HUF_addBits() takes a HUF_CElt (size_t) which is
 *      the pair (nbBits, value) in the format:
 *      format:
 *        - Bits [0, 4)            = nbBits
 *        - Bits [4, 64 - nbBits)  = 0
 *        - Bits [64 - nbBits, 64) = value
 *   2. The bitContainer is built from the upper bits and
 *      right shifted. E.g. to add a new value of N bits
 *      you right shift the bitContainer by N, then or in
 *      the new value into the N upper bits.
 *   3. The bitstream has two bit containers. You can add
 *      bits to the second container and merge them into
 *      the first container.
 */

type HUF_CStream_t = struct {
	FbitContainer [2]size_t
	FbitPos       [2]size_t
	FstartPtr     uintptr
	Fptr          uintptr
	FendPtr       uintptr
}

// C documentation
//
//	/**! HUF_initCStream():
//	 * Initializes the bitstream.
//	 * @returns 0 or an error code.
//	 */
func HUF_initCStream(tls *libc.TLS, bitC uintptr, startPtr uintptr, dstCapacity size_t) (r size_t) {
	libc.Xmemset(tls, bitC, 0, libc.Uint64FromInt64(56))
	(*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr = startPtr
	(*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr
	(*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr = (*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr + uintptr(dstCapacity) - uintptr(8)
	if dstCapacity <= uint64(8) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	return uint64(0)
}

// C documentation
//
//	/*! HUF_addBits():
//	 * Adds the symbol stored in HUF_CElt elt to the bitstream.
//	 *
//	 * @param elt   The element we're adding. This is a (nbBits, value) pair.
//	 *              See the HUF_CStream_t docs for the format.
//	 * @param idx   Insert into the bitstream at this idx.
//	 * @param kFast This is a template parameter. If the bitstream is guaranteed
//	 *              to have at least 4 unused bits after this call it may be 1,
//	 *              otherwise it must be 0. HUF_addBits() is faster when fast is set.
//	 */
func HUF_addBits(tls *libc.TLS, bitC uintptr, elt HUF_CElt, idx int32, kFast int32) {
	var v1 uint64
	_ = v1
	/* This is efficient on x86-64 with BMI2 because shrx
	 * only reads the low 6 bits of the register. The compiler
	 * knows this and elides the mask. When fast is set,
	 * every operation can use the same value loaded from elt.
	 */
	*(*size_t)(unsafe.Pointer(bitC + uintptr(idx)*8)) >>= HUF_getNbBits(tls, elt)
	if kFast != 0 {
		v1 = HUF_getValueFast(tls, elt)
	} else {
		v1 = HUF_getValue(tls, elt)
	}
	*(*size_t)(unsafe.Pointer(bitC + uintptr(idx)*8)) |= v1
	/* We only read the low 8 bits of bitC->bitPos[idx] so it
	 * doesn't matter that the high bits have noise from the value.
	 */
	*(*size_t)(unsafe.Pointer(bitC + 16 + uintptr(idx)*8)) += HUF_getNbBitsFast(tls, elt)
	/* The last 4-bits of elt are dirty if fast is set,
	 * so we must not be overwriting bits that have already been
	 * inserted into the bit container.
	 */
}

func HUF_zeroIndex1(tls *libc.TLS, bitC uintptr) {
	*(*size_t)(unsafe.Pointer(bitC + 1*8)) = uint64(0)
	*(*size_t)(unsafe.Pointer(bitC + 16 + 1*8)) = uint64(0)
}

// C documentation
//
//	/*! HUF_mergeIndex1() :
//	 * Merges the bit container @ index 1 into the bit container @ index 0
//	 * and zeros the bit container @ index 1.
//	 */
func HUF_mergeIndex1(tls *libc.TLS, bitC uintptr) {
	*(*size_t)(unsafe.Pointer(bitC)) >>= *(*size_t)(unsafe.Pointer(bitC + 16 + 1*8)) & uint64(0xFF)
	*(*size_t)(unsafe.Pointer(bitC)) |= *(*size_t)(unsafe.Pointer(bitC + 1*8))
	*(*size_t)(unsafe.Pointer(bitC + 16)) += *(*size_t)(unsafe.Pointer(bitC + 16 + 1*8))
}

// C documentation
//
//	/*! HUF_flushBits() :
//	* Flushes the bits in the bit container @ index 0.
//	*
//	* @post bitPos will be < 8.
//	* @param kFast If kFast is set then we must know a-priori that
//	*              the bit container will not overflow.
//	*/
func HUF_flushBits(tls *libc.TLS, bitC uintptr, kFast int32) {
	var bitContainer, nbBits, nbBytes size_t
	_, _, _ = bitContainer, nbBits, nbBytes
	/* The upper bits of bitPos are noisy, so we must mask by 0xFF. */
	nbBits = *(*size_t)(unsafe.Pointer(bitC + 16)) & uint64(0xFF)
	nbBytes = nbBits >> int32(3)
	/* The top nbBits bits of bitContainer are the ones we need. */
	bitContainer = *(*size_t)(unsafe.Pointer(bitC)) >> (libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) - nbBits)
	/* Mask bitPos to account for the bytes we consumed. */
	*(*size_t)(unsafe.Pointer(bitC + 16)) &= uint64(7)
	MEM_writeLEST(tls, (*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr, bitContainer)
	*(*uintptr)(unsafe.Pointer(bitC + 40)) += uintptr(nbBytes)
	if !(kFast != 0) && (*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr > (*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		(*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr = (*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr
	}
	/* bitContainer doesn't need to be modified because the leftover
	 * bits are already the top bitPos bits. And we don't care about
	 * noise in the lower values.
	 */
}

// C documentation
//
//	/*! HUF_endMark()
//	 * @returns The Huffman stream end mark: A 1-bit value = 1.
//	 */
func HUF_endMark(tls *libc.TLS) (r HUF_CElt) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* endMark at bp+0 */ HUF_CElt
	HUF_setNbBits(tls, bp, uint64(1))
	HUF_setValue(tls, bp, uint64(1))
	return *(*HUF_CElt)(unsafe.Pointer(bp))
}

// C documentation
//
//	/*! HUF_closeCStream() :
//	 *  @return Size of CStream, in bytes,
//	 *          or 0 if it could not fit into dstBuffer */
func HUF_closeCStream(tls *libc.TLS, bitC uintptr) (r size_t) {
	var nbBits size_t
	_ = nbBits
	HUF_addBits(tls, bitC, HUF_endMark(tls), 0, 0)
	HUF_flushBits(tls, bitC, 0)
	nbBits = *(*size_t)(unsafe.Pointer(bitC + 16)) & uint64(0xFF)
	if (*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr >= (*HUF_CStream_t)(unsafe.Pointer(bitC)).FendPtr {
		return uint64(0)
	} /* overflow detected */
	return libc.Uint64FromInt64(int64((*HUF_CStream_t)(unsafe.Pointer(bitC)).Fptr)-int64((*HUF_CStream_t)(unsafe.Pointer(bitC)).FstartPtr)) + libc.BoolUint64(nbBits > libc.Uint64FromInt32(0))
	return r
}

func HUF_encodeSymbol(tls *libc.TLS, bitCPtr uintptr, symbol U32, CTable uintptr, idx int32, fast int32) {
	HUF_addBits(tls, bitCPtr, *(*HUF_CElt)(unsafe.Pointer(CTable + uintptr(symbol)*8)), idx, fast)
}

func HUF_compress1X_usingCTable_internal_body_loop(tls *libc.TLS, bitC uintptr, ip uintptr, srcSize size_t, ct uintptr, kUnroll int32, kFastFlush int32, kLastFast int32) {
	var n, rem, u, u1, v2 int32
	_, _, _, _, _ = n, rem, u, u1, v2
	/* Join to kUnroll */
	n = libc.Int32FromUint64(srcSize)
	rem = n % kUnroll
	if rem > 0 {
		for {
			if !(rem > 0) {
				break
			}
			n = n - 1
			v2 = n
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(v2)))), ct, 0, 0)
			goto _1
		_1:
			;
			rem = rem - 1
		}
		HUF_flushBits(tls, bitC, kFastFlush)
	}
	/* Join to 2 * kUnroll */
	if n%(int32(2)*kUnroll) != 0 {
		u = int32(1)
		for {
			if !(u < kUnroll) {
				break
			}
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-u)))), ct, 0, int32(1))
			goto _3
		_3:
			;
			u = u + 1
		}
		HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll)))), ct, 0, kLastFast)
		HUF_flushBits(tls, bitC, kFastFlush)
		n = n - kUnroll
	}
	for {
		if !(n > 0) {
			break
		}
		u1 = int32(1)
		for {
			if !(u1 < kUnroll) {
				break
			}
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-u1)))), ct, 0, int32(1))
			goto _5
		_5:
			;
			u1 = u1 + 1
		}
		HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll)))), ct, 0, kLastFast)
		HUF_flushBits(tls, bitC, kFastFlush)
		/* Encode kUnroll symbols into the bitstream @ index 1.
		 * This allows us to start filling the bit container
		 * without any data dependencies.
		 */
		HUF_zeroIndex1(tls, bitC)
		u1 = int32(1)
		for {
			if !(u1 < kUnroll) {
				break
			}
			HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll-u1)))), ct, int32(1), int32(1))
			goto _6
		_6:
			;
			u1 = u1 + 1
		}
		HUF_encodeSymbol(tls, bitC, uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(n-kUnroll-kUnroll)))), ct, int32(1), kLastFast)
		/* Merge bitstream @ index 1 into the bitstream @ index 0 */
		HUF_mergeIndex1(tls, bitC)
		HUF_flushBits(tls, bitC, kFastFlush)
		goto _4
	_4:
		;
		n = n - int32(2)*kUnroll
	}
}

// C documentation
//
//	/**
//	 * Returns a tight upper bound on the output space needed by Huffman
//	 * with 8 bytes buffer to handle over-writes. If the output is at least
//	 * this large we don't need to do bounds checks during Huffman encoding.
//	 */
func HUF_tightCompressBound(tls *libc.TLS, srcSize size_t, tableLog size_t) (r size_t) {
	return srcSize*tableLog>>int32(3) + uint64(8)
}

func HUF_compress1X_usingCTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr) (r size_t) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var ct, ip, oend, op, ostart uintptr
	var initErr size_t
	var tableLog U32
	var v1 int32
	var _ /* bitC at bp+0 */ HUF_CStream_t
	_, _, _, _, _, _, _, _ = ct, initErr, ip, oend, op, ostart, tableLog, v1
	tableLog = uint32(HUF_readCTableHeader(tls, CTable).FtableLog)
	ct = CTable + uintptr(1)*8
	ip = src
	ostart = dst
	oend = ostart + uintptr(dstSize)
	/* init */
	if dstSize < uint64(8) {
		return uint64(0)
	} /* not enough space to compress */
	op = ostart
	initErr = HUF_initCStream(tls, bp, op, libc.Uint64FromInt64(int64(oend)-int64(op)))
	if ERR_isError(tls, initErr) != 0 {
		return uint64(0)
	}
	if dstSize < HUF_tightCompressBound(tls, srcSize, uint64(tableLog)) || tableLog > uint32(11) {
		if MEM_32bits(tls) != 0 {
			v1 = int32(2)
		} else {
			v1 = int32(4)
		}
		HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, v1, 0, 0)
	} else {
		if MEM_32bits(tls) != 0 {
			switch tableLog {
			case uint32(11):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(2), int32(1), 0)
			case uint32(10):
				fallthrough
			case uint32(9):
				fallthrough
			case uint32(8):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(2), int32(1), int32(1))
			case uint32(7):
				fallthrough
			default:
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(3), int32(1), int32(1))
				break
			}
		} else {
			switch tableLog {
			case uint32(11):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(5), int32(1), 0)
			case uint32(10):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(5), int32(1), int32(1))
			case uint32(9):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(6), int32(1), 0)
			case uint32(8):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(7), int32(1), 0)
			case uint32(7):
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(8), int32(1), 0)
			case uint32(6):
				fallthrough
			default:
				HUF_compress1X_usingCTable_internal_body_loop(tls, bp, ip, srcSize, ct, int32(9), int32(1), int32(1))
				break
			}
		}
	}
	return HUF_closeCStream(tls, bp)
}

func HUF_compress1X_usingCTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr) (r size_t) {
	return HUF_compress1X_usingCTable_internal_body(tls, dst, dstSize, src, srcSize, CTable)
}

func HUF_compress1X_usingCTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr) (r size_t) {
	return HUF_compress1X_usingCTable_internal_body(tls, dst, dstSize, src, srcSize, CTable)
}

func HUF_compress1X_usingCTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_compress1X_usingCTable_internal_bmi2(tls, dst, dstSize, src, srcSize, CTable)
	}
	return HUF_compress1X_usingCTable_internal_default(tls, dst, dstSize, src, srcSize, CTable)
}

func HUF_compress1X_usingCTable(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	return HUF_compress1X_usingCTable_internal(tls, dst, dstSize, src, srcSize, CTable, flags)
}

func HUF_compress4X_usingCTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	var cSize, cSize1, cSize2, cSize3, segmentSize size_t
	var iend, ip, oend, op, ostart uintptr
	_, _, _, _, _, _, _, _, _, _ = cSize, cSize1, cSize2, cSize3, iend, ip, oend, op, ostart, segmentSize
	segmentSize = (srcSize + uint64(3)) / uint64(4) /* first 3 segments */
	ip = src
	iend = ip + uintptr(srcSize)
	ostart = dst
	oend = ostart + uintptr(dstSize)
	op = ostart
	if dstSize < libc.Uint64FromInt32(libc.Int32FromInt32(6)+libc.Int32FromInt32(1)+libc.Int32FromInt32(1)+libc.Int32FromInt32(1)+libc.Int32FromInt32(8)) {
		return uint64(0)
	} /* minimum space to compress successfully */
	if srcSize < uint64(12) {
		return uint64(0)
	} /* no saving possible : too small input */
	op = op + uintptr(6) /* jumpTable */
	cSize = HUF_compress1X_usingCTable_internal(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, segmentSize, CTable, flags)
	if ERR_isError(tls, cSize) != 0 {
		return cSize
	}
	if cSize == uint64(0) || cSize > uint64(65535) {
		return uint64(0)
	}
	MEM_writeLE16(tls, ostart, uint16(cSize))
	op = op + uintptr(cSize)
	ip = ip + uintptr(segmentSize)
	cSize1 = HUF_compress1X_usingCTable_internal(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, segmentSize, CTable, flags)
	if ERR_isError(tls, cSize1) != 0 {
		return cSize1
	}
	if cSize1 == uint64(0) || cSize1 > uint64(65535) {
		return uint64(0)
	}
	MEM_writeLE16(tls, ostart+uintptr(2), uint16(cSize1))
	op = op + uintptr(cSize1)
	ip = ip + uintptr(segmentSize)
	cSize2 = HUF_compress1X_usingCTable_internal(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, segmentSize, CTable, flags)
	if ERR_isError(tls, cSize2) != 0 {
		return cSize2
	}
	if cSize2 == uint64(0) || cSize2 > uint64(65535) {
		return uint64(0)
	}
	MEM_writeLE16(tls, ostart+uintptr(4), uint16(cSize2))
	op = op + uintptr(cSize2)
	ip = ip + uintptr(segmentSize)
	cSize3 = HUF_compress1X_usingCTable_internal(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, libc.Uint64FromInt64(int64(iend)-int64(ip)), CTable, flags)
	if ERR_isError(tls, cSize3) != 0 {
		return cSize3
	}
	if cSize3 == uint64(0) || cSize3 > uint64(65535) {
		return uint64(0)
	}
	op = op + uintptr(cSize3)
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func HUF_compress4X_usingCTable(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, CTable uintptr, flags int32) (r size_t) {
	return HUF_compress4X_usingCTable_internal(tls, dst, dstSize, src, srcSize, CTable, flags)
}

type HUF_nbStreams_e = int32

const HUF_singleStream = 0
const HUF_fourStreams = 1

func HUF_compressCTable_internal(tls *libc.TLS, ostart uintptr, op uintptr, oend uintptr, src uintptr, srcSize size_t, nbStreams HUF_nbStreams_e, CTable uintptr, flags int32) (r size_t) {
	var cSize size_t
	var v1 uint64
	_, _ = cSize, v1
	if nbStreams == int32(HUF_singleStream) {
		v1 = HUF_compress1X_usingCTable_internal(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), src, srcSize, CTable, flags)
	} else {
		v1 = HUF_compress4X_usingCTable_internal(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), src, srcSize, CTable, flags)
	}
	cSize = v1
	if ERR_isError(tls, cSize) != 0 {
		return cSize
	}
	if cSize == uint64(0) {
		return uint64(0)
	} /* uncompressible */
	op = op + uintptr(cSize)
	/* check compressibility */
	if libc.Uint64FromInt64(int64(op)-int64(ostart)) >= srcSize-uint64(1) {
		return uint64(0)
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

type HUF_compress_tables_t = struct {
	Fcount  [256]uint32
	FCTable [257]HUF_CElt
	Fwksps  struct {
		FwriteCTable_wksp [0]HUF_WriteCTableWksp
		Fhist_wksp        [0][1024]U32
		FbuildCTable_wksp HUF_buildCTable_wksp_tables
	}
}

func HUF_cardinality(tls *libc.TLS, count uintptr, maxSymbolValue uint32) (r uint32) {
	var cardinality, i uint32
	_, _ = cardinality, i
	cardinality = uint32(0)
	i = uint32(0)
	for {
		if !(i < maxSymbolValue+uint32(1)) {
			break
		}
		if *(*uint32)(unsafe.Pointer(count + uintptr(i)*4)) != uint32(0) {
			cardinality = cardinality + uint32(1)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return cardinality
}

func HUF_minTableLog(tls *libc.TLS, symbolCardinality uint32) (r uint32) {
	var minBitsSymbols U32
	_ = minBitsSymbols
	minBitsSymbols = ZSTD_highbit32(tls, symbolCardinality) + uint32(1)
	return minBitsSymbols
}

func HUF_optimalTableLog(tls *libc.TLS, maxTableLog uint32, srcSize size_t, maxSymbolValue uint32, workSpace uintptr, wkspSize size_t, table uintptr, count uintptr, flags int32) (r uint32) {
	var dst uintptr
	var dstSize, hSize, maxBits, newSize, optSize size_t
	var minTableLog, optLog, optLogGuess, symbolCardinality uint32
	_, _, _, _, _, _, _, _, _, _ = dst, dstSize, hSize, maxBits, minTableLog, newSize, optLog, optLogGuess, optSize, symbolCardinality
	/* Not supported, RLE should be used instead */
	if !(flags&int32(HUF_flags_optimalDepth) != 0) {
		/* cheap evaluation, based on FSE */
		return FSE_optimalTableLog_internal(tls, maxTableLog, srcSize, maxSymbolValue, uint32(1))
	}
	dst = workSpace + uintptr(748)
	dstSize = wkspSize - uint64(748)
	symbolCardinality = HUF_cardinality(tls, count, maxSymbolValue)
	minTableLog = HUF_minTableLog(tls, symbolCardinality)
	optSize = libc.Uint64FromInt32(^libc.Int32FromInt32(0)) - libc.Uint64FromInt32(1)
	optLog = maxTableLog
	/* Search until size increases */
	optLogGuess = minTableLog
	for {
		if !(optLogGuess <= maxTableLog) {
			break
		}
		maxBits = HUF_buildCTable_wksp(tls, table, count, maxSymbolValue, optLogGuess, workSpace, wkspSize)
		if ERR_isError(tls, maxBits) != 0 {
			goto _1
		}
		if maxBits < uint64(optLogGuess) && optLogGuess > minTableLog {
			break
		}
		hSize = HUF_writeCTable_wksp(tls, dst, dstSize, table, maxSymbolValue, uint32(maxBits), workSpace, wkspSize)
		if ERR_isError(tls, hSize) != 0 {
			goto _1
		}
		newSize = HUF_estimateCompressedSize(tls, table, count, maxSymbolValue) + hSize
		if newSize > optSize+uint64(1) {
			break
		}
		if newSize < optSize {
			optSize = newSize
			optLog = optLogGuess
		}
		goto _1
	_1:
		;
		optLogGuess = optLogGuess + 1
	}
	return optLog
	return r
}

// C documentation
//
//	/* HUF_compress_internal() :
//	 * `workSpace_align4` must be aligned on 4-bytes boundaries,
//	 * and occupies the same space as a table of HUF_WORKSPACE_SIZE_U64 unsigned */
func HUF_compress_internal(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, _maxSymbolValue uint32, huffLog uint32, nbStreams HUF_nbStreams_e, workSpace uintptr, _wkspSize size_t, oldHufTable uintptr, repeat uintptr, flags int32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	*(*uint32)(unsafe.Pointer(bp)) = _maxSymbolValue
	*(*size_t)(unsafe.Pointer(bp + 8)) = _wkspSize
	var _var_err__, hSize, largest, largestBegin, largestEnd, largestTotal, maxBits, newSize, oldSize size_t
	var oend, op, ostart, table uintptr
	var _ /* maxSymbolValueBegin at bp+16 */ uint32
	var _ /* maxSymbolValueEnd at bp+20 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = _var_err__, hSize, largest, largestBegin, largestEnd, largestTotal, maxBits, newSize, oend, oldSize, op, ostart, table
	table = HUF_alignUpWorkspace(tls, workSpace, bp+8, uint64(8))
	ostart = dst
	oend = ostart + uintptr(dstSize)
	op = ostart
	_ = libc.Uint64FromInt64(1)
	/* checks & inits */
	if *(*size_t)(unsafe.Pointer(bp + 8)) < uint64(7944) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_workSpace_tooSmall))
	}
	if !(srcSize != 0) {
		return uint64(0)
	} /* Uncompressed */
	if !(dstSize != 0) {
		return uint64(0)
	} /* cannot fit anything within dst budget */
	if srcSize > libc.Uint64FromInt32(libc.Int32FromInt32(128)*libc.Int32FromInt32(1024)) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	} /* current block size limit */
	if huffLog > uint32(HUF_TABLELOG_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	if *(*uint32)(unsafe.Pointer(bp)) > uint32(HUF_SYMBOLVALUE_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_maxSymbolValue_tooLarge))
	}
	if !(*(*uint32)(unsafe.Pointer(bp)) != 0) {
		*(*uint32)(unsafe.Pointer(bp)) = uint32(HUF_SYMBOLVALUE_MAX)
	}
	if !(huffLog != 0) {
		huffLog = uint32(HUF_TABLELOG_DEFAULT)
	}
	/* Heuristic : If old table is valid, use it for small inputs */
	if flags&int32(HUF_flags_preferRepeat) != 0 && repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) == int32(HUF_repeat_valid) {
		return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, oldHufTable, flags)
	}
	/* If uncompressible data is suspected, do a smaller sampling first */
	_ = libc.Uint64FromInt64(1)
	if flags&int32(HUF_flags_suspectUncompressible) != 0 && srcSize >= libc.Uint64FromInt32(libc.Int32FromInt32(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)*libc.Int32FromInt32(SUSPECT_INCOMPRESSIBLE_SAMPLE_RATIO)) {
		largestTotal = uint64(0)
		*(*uint32)(unsafe.Pointer(bp + 16)) = *(*uint32)(unsafe.Pointer(bp))
		largestBegin = uint64(HIST_count_simple(tls, table, bp+16, src, uint64(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)))
		if ERR_isError(tls, largestBegin) != 0 {
			return largestBegin
		}
		largestTotal = largestTotal + largestBegin
		*(*uint32)(unsafe.Pointer(bp + 20)) = *(*uint32)(unsafe.Pointer(bp))
		largestEnd = uint64(HIST_count_simple(tls, table, bp+20, src+uintptr(srcSize)-uintptr(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE), uint64(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)))
		if ERR_isError(tls, largestEnd) != 0 {
			return largestEnd
		}
		largestTotal = largestTotal + largestEnd
		if largestTotal <= libc.Uint64FromInt32(libc.Int32FromInt32(2)*libc.Int32FromInt32(SUSPECT_INCOMPRESSIBLE_SAMPLE_SIZE)>>libc.Int32FromInt32(7)+libc.Int32FromInt32(4)) {
			return uint64(0)
		} /* heuristic : probably not compressible enough */
	}
	/* Scan input and build symbol stats */
	largest = HIST_count_wksp(tls, table, bp, src, srcSize, table+3080, uint64(4096))
	if ERR_isError(tls, largest) != 0 {
		return largest
	}
	if largest == srcSize {
		*(*BYTE)(unsafe.Pointer(ostart)) = *(*BYTE)(unsafe.Pointer(src))
		return uint64(1)
	} /* single symbol, rle */
	if largest <= srcSize>>libc.Int32FromInt32(7)+uint64(4) {
		return uint64(0)
	} /* heuristic : probably not compressible enough */
	/* Check validity of previous table */
	if repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) == int32(HUF_repeat_check) && !(HUF_validateCTable(tls, oldHufTable, table, *(*uint32)(unsafe.Pointer(bp))) != 0) {
		*(*HUF_repeat)(unsafe.Pointer(repeat)) = int32(HUF_repeat_none)
	}
	/* Heuristic : use existing table for small inputs */
	if flags&int32(HUF_flags_preferRepeat) != 0 && repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) != int32(HUF_repeat_none) {
		return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, oldHufTable, flags)
	}
	/* Build Huffman Tree */
	huffLog = HUF_optimalTableLog(tls, huffLog, srcSize, *(*uint32)(unsafe.Pointer(bp)), table+3080, uint64(4864), table+1024, table, flags)
	maxBits = HUF_buildCTable_wksp(tls, table+1024, table, *(*uint32)(unsafe.Pointer(bp)), huffLog, table+3080, uint64(4864))
	_var_err__ = maxBits
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	huffLog = uint32(maxBits)
	/* Write table description header */
	hSize = HUF_writeCTable_wksp(tls, op, dstSize, table+1024, *(*uint32)(unsafe.Pointer(bp)), huffLog, table+3080, uint64(748))
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	/* Check if using previous huffman table is beneficial */
	if repeat != 0 && *(*HUF_repeat)(unsafe.Pointer(repeat)) != int32(HUF_repeat_none) {
		oldSize = HUF_estimateCompressedSize(tls, oldHufTable, table, *(*uint32)(unsafe.Pointer(bp)))
		newSize = HUF_estimateCompressedSize(tls, table+1024, table, *(*uint32)(unsafe.Pointer(bp)))
		if oldSize <= hSize+newSize || hSize+uint64(12) >= srcSize {
			return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, oldHufTable, flags)
		}
	}
	/* Use the new huffman table */
	if hSize+uint64(12) >= srcSize {
		return uint64(0)
	}
	op = op + uintptr(hSize)
	if repeat != 0 {
		*(*HUF_repeat)(unsafe.Pointer(repeat)) = int32(HUF_repeat_none)
	}
	if oldHufTable != 0 {
		libc.Xmemcpy(tls, oldHufTable, table+1024, libc.Uint64FromInt64(2056))
	} /* Save new table */
	return HUF_compressCTable_internal(tls, ostart, op, oend, src, srcSize, nbStreams, table+1024, flags)
}

func HUF_compress1X_repeat(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, maxSymbolValue uint32, huffLog uint32, workSpace uintptr, wkspSize size_t, hufTable uintptr, repeat uintptr, flags int32) (r size_t) {
	return HUF_compress_internal(tls, dst, dstSize, src, srcSize, maxSymbolValue, huffLog, int32(HUF_singleStream), workSpace, wkspSize, hufTable, repeat, flags)
}

// C documentation
//
//	/* HUF_compress4X_repeat():
//	 * compress input using 4 streams.
//	 * consider skipping quickly
//	 * reuse an existing huffman compression table */
func HUF_compress4X_repeat(tls *libc.TLS, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, maxSymbolValue uint32, huffLog uint32, workSpace uintptr, wkspSize size_t, hufTable uintptr, repeat uintptr, flags int32) (r size_t) {
	return HUF_compress_internal(tls, dst, dstSize, src, srcSize, maxSymbolValue, huffLog, int32(HUF_fourStreams), workSpace, wkspSize, hufTable, repeat, flags)
}

/**** ended inlining compress/huf_compress.c ****/
/**** start inlining compress/zstd_compress_literals.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
 *  Dependencies
 ***************************************/
/**** start inlining zstd_compress_literals.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** start inlining zstd_compress_internal.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* This header contains definitions
 * that shall **only** be used by modules within lib/compress.
 */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: ../common/zstd_internal.h ****/
/**** start inlining zstd_cwksp.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
*  Dependencies
***************************************/
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/portability_macros.h ****/
/**** skipping file: ../common/compiler.h ****/

/*-*************************************
*  Constants
***************************************/

/* Since the workspace is effectively its own little malloc implementation /
 * arena, when we run under ASAN, we should similarly insert redzones between
 * each internal element of the workspace, so ASAN will catch overruns that
 * reach outside an object but that stay inside the workspace.
 *
 * This defines the size of that redzone.
 */

/* Set our tables and aligneds to align by 64 bytes */

// C documentation
//
//	/*-*************************************
//	*  Structures
//	***************************************/
type ZSTD_cwksp_alloc_phase_e = int32

const ZSTD_cwksp_alloc_objects = 0
const ZSTD_cwksp_alloc_aligned_init_once = 1
const ZSTD_cwksp_alloc_aligned = 2
const ZSTD_cwksp_alloc_buffers = 3

// C documentation
//
//	/**
//	 * Used to describe whether the workspace is statically allocated (and will not
//	 * necessarily ever be freed), or if it's dynamically allocated and we can
//	 * expect a well-formed caller to free this.
//	 */
type ZSTD_cwksp_static_alloc_e = int32

const ZSTD_cwksp_dynamic_alloc = 0
const ZSTD_cwksp_static_alloc = 1

// C documentation
//
//	/**
//	 * Zstd fits all its internal datastructures into a single continuous buffer,
//	 * so that it only needs to perform a single OS allocation (or so that a buffer
//	 * can be provided to it and it can perform no allocations at all). This buffer
//	 * is called the workspace.
//	 *
//	 * Several optimizations complicate that process of allocating memory ranges
//	 * from this workspace for each internal datastructure:
//	 *
//	 * - These different internal datastructures have different setup requirements:
//	 *
//	 *   - The static objects need to be cleared once and can then be trivially
//	 *     reused for each compression.
//	 *
//	 *   - Various buffers don't need to be initialized at all--they are always
//	 *     written into before they're read.
//	 *
//	 *   - The matchstate tables have a unique requirement that they don't need
//	 *     their memory to be totally cleared, but they do need the memory to have
//	 *     some bound, i.e., a guarantee that all values in the memory they've been
//	 *     allocated is less than some maximum value (which is the starting value
//	 *     for the indices that they will then use for compression). When this
//	 *     guarantee is provided to them, they can use the memory without any setup
//	 *     work. When it can't, they have to clear the area.
//	 *
//	 * - These buffers also have different alignment requirements.
//	 *
//	 * - We would like to reuse the objects in the workspace for multiple
//	 *   compressions without having to perform any expensive reallocation or
//	 *   reinitialization work.
//	 *
//	 * - We would like to be able to efficiently reuse the workspace across
//	 *   multiple compressions **even when the compression parameters change** and
//	 *   we need to resize some of the objects (where possible).
//	 *
//	 * To attempt to manage this buffer, given these constraints, the ZSTD_cwksp
//	 * abstraction was created. It works as follows:
//	 *
//	 * Workspace Layout:
//	 *
//	 * [                        ... workspace ...                           ]
//	 * [objects][tables ->] free space [<- buffers][<- aligned][<- init once]
//	 *
//	 * The various objects that live in the workspace are divided into the
//	 * following categories, and are allocated separately:
//	 *
//	 * - Static objects: this is optionally the enclosing ZSTD_CCtx or ZSTD_CDict,
//	 *   so that literally everything fits in a single buffer. Note: if present,
//	 *   this must be the first object in the workspace, since ZSTD_customFree{CCtx,
//	 *   CDict}() rely on a pointer comparison to see whether one or two frees are
//	 *   required.
//	 *
//	 * - Fixed size objects: these are fixed-size, fixed-count objects that are
//	 *   nonetheless "dynamically" allocated in the workspace so that we can
//	 *   control how they're initialized separately from the broader ZSTD_CCtx.
//	 *   Examples:
//	 *   - Entropy Workspace
//	 *   - 2 x ZSTD_compressedBlockState_t
//	 *   - CDict dictionary contents
//	 *
//	 * - Tables: these are any of several different datastructures (hash tables,
//	 *   chain tables, binary trees) that all respect a common format: they are
//	 *   uint32_t arrays, all of whose values are between 0 and (nextSrc - base).
//	 *   Their sizes depend on the cparams. These tables are 64-byte aligned.
//	 *
//	 * - Init once: these buffers require to be initialized at least once before
//	 *   use. They should be used when we want to skip memory initialization
//	 *   while not triggering memory checkers (like Valgrind) when reading from
//	 *   from this memory without writing to it first.
//	 *   These buffers should be used carefully as they might contain data
//	 *   from previous compressions.
//	 *   Buffers are aligned to 64 bytes.
//	 *
//	 * - Aligned: these buffers don't require any initialization before they're
//	 *   used. The user of the buffer should make sure they write into a buffer
//	 *   location before reading from it.
//	 *   Buffers are aligned to 64 bytes.
//	 *
//	 * - Buffers: these buffers are used for various purposes that don't require
//	 *   any alignment or initialization before they're used. This means they can
//	 *   be moved around at no cost for a new compression.
//	 *
//	 * Allocating Memory:
//	 *
//	 * The various types of objects must be allocated in order, so they can be
//	 * correctly packed into the workspace buffer. That order is:
//	 *
//	 * 1. Objects
//	 * 2. Init once / Tables
//	 * 3. Aligned / Tables
//	 * 4. Buffers / Tables
//	 *
//	 * Attempts to reserve objects of different types out of order will fail.
//	 */
type ZSTD_cwksp = struct {
	Fworkspace                  uintptr
	FworkspaceEnd               uintptr
	FobjectEnd                  uintptr
	FtableEnd                   uintptr
	FtableValidEnd              uintptr
	FallocStart                 uintptr
	FinitOnceStart              uintptr
	FallocFailed                BYTE
	FworkspaceOversizedDuration int32
	Fphase                      ZSTD_cwksp_alloc_phase_e
	FisStatic                   ZSTD_cwksp_static_alloc_e
}

func ZSTD_cwksp_assert_internal_consistency(tls *libc.TLS, ws uintptr) {
	_ = ws
}

// C documentation
//
//	/**
//	 * Align must be a power of 2.
//	 */
func ZSTD_cwksp_align(tls *libc.TLS, size size_t, align size_t) (r size_t) {
	var mask size_t
	_ = mask
	mask = align - uint64(1)
	return (size + mask) & ^mask
}

// C documentation
//
//	/**
//	 * Use this to determine how much space in the workspace we will consume to
//	 * allocate this object. (Normally it should be exactly the size of the object,
//	 * but under special conditions, like ASAN, where we pad each object, it might
//	 * be larger.)
//	 *
//	 * Since tables aren't currently redzoned, you don't need to call through this
//	 * to figure out how much space you need for the matchState tables. Everything
//	 * else is though.
//	 *
//	 * Do not use for sizing aligned buffers. Instead, use ZSTD_cwksp_aligned64_alloc_size().
//	 */
func ZSTD_cwksp_alloc_size(tls *libc.TLS, size size_t) (r size_t) {
	if size == uint64(0) {
		return uint64(0)
	}
	return size
}

func ZSTD_cwksp_aligned_alloc_size(tls *libc.TLS, size size_t, alignment size_t) (r size_t) {
	return ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, size, alignment))
}

// C documentation
//
//	/**
//	 * Returns an adjusted alloc size that is the nearest larger multiple of 64 bytes.
//	 * Used to determine the number of bytes required for a given "aligned".
//	 */
func ZSTD_cwksp_aligned64_alloc_size(tls *libc.TLS, size size_t) (r size_t) {
	return ZSTD_cwksp_aligned_alloc_size(tls, size, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES))
}

// C documentation
//
//	/**
//	 * Returns the amount of additional space the cwksp must allocate
//	 * for internal purposes (currently only alignment).
//	 */
func ZSTD_cwksp_slack_space_required(tls *libc.TLS) (r size_t) {
	var slackSpace size_t
	_ = slackSpace
	/* For alignment, the wksp will always allocate an additional 2*ZSTD_CWKSP_ALIGNMENT_BYTES
	 * bytes to align the beginning of tables section and end of buffers;
	 */
	slackSpace = libc.Uint64FromInt32(libc.Int32FromInt32(ZSTD_CWKSP_ALIGNMENT_BYTES) * libc.Int32FromInt32(2))
	return slackSpace
}

// C documentation
//
//	/**
//	 * Return the number of additional bytes required to align a pointer to the given number of bytes.
//	 * alignBytes must be a power of two.
//	 */
func ZSTD_cwksp_bytes_to_align_ptr(tls *libc.TLS, ptr uintptr, alignBytes size_t) (r size_t) {
	var alignBytesMask, bytes size_t
	_, _ = alignBytesMask, bytes
	alignBytesMask = alignBytes - uint64(1)
	bytes = (alignBytes - uint64(ptr)&alignBytesMask) & alignBytesMask
	return bytes
}

// C documentation
//
//	/**
//	 * Returns the initial value for allocStart which is used to determine the position from
//	 * which we can allocate from the end of the workspace.
//	 */
func ZSTD_cwksp_initialAllocStart(tls *libc.TLS, ws uintptr) (r uintptr) {
	var endPtr uintptr
	_ = endPtr
	endPtr = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd
	endPtr = endPtr - uintptr(uint64(endPtr)%libc.Uint64FromInt32(ZSTD_CWKSP_ALIGNMENT_BYTES))
	return endPtr
}

// C documentation
//
//	/**
//	 * Internal function. Do not use directly.
//	 * Reserves the given number of bytes within the aligned/buffer segment of the wksp,
//	 * which counts from the end of the wksp (as opposed to the object/table segment).
//	 *
//	 * Returns a pointer to the beginning of that space.
//	 */
func ZSTD_cwksp_reserve_internal_buffer_space(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alloc, bottom uintptr
	_, _ = alloc, bottom
	alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart - uintptr(bytes)
	bottom = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
	if alloc < bottom {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(1)
		return libc.UintptrFromInt32(0)
	}
	/* the area is reserved from the end of wksp.
	 * If it overlaps with tableValidEnd, it voids guarantees on values' range */
	if alloc < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = alloc
	}
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart = alloc
	return alloc
}

// C documentation
//
//	/**
//	 * Moves the cwksp to the next phase, and does any necessary allocations.
//	 * cwksp initialization must necessarily go through each phase in order.
//	 * Returns a 0 on success, or zstd error
//	 */
func ZSTD_cwksp_internal_advance_phase(tls *libc.TLS, ws uintptr, phase ZSTD_cwksp_alloc_phase_e) (r size_t) {
	var alloc, objectEnd uintptr
	var bytesToAlign size_t
	_, _, _ = alloc, bytesToAlign, objectEnd
	if phase > (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase {
		/* Going from allocating objects to allocating initOnce / tables */
		if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase < int32(ZSTD_cwksp_alloc_aligned_init_once) && phase >= int32(ZSTD_cwksp_alloc_aligned_init_once) {
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart = ZSTD_cwksp_initialAllocStart(tls, ws)
			/* Align the start of the tables to 64 bytes. Use [0, 63] bytes */
			alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
			bytesToAlign = ZSTD_cwksp_bytes_to_align_ptr(tls, alloc, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES))
			objectEnd = alloc + uintptr(bytesToAlign)
			if objectEnd > (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1326, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
			}
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd = objectEnd
			(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = objectEnd /* table area starts being empty */
			if (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd {
				(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
			}
		}
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase = phase
		ZSTD_cwksp_assert_internal_consistency(tls, ws)
	}
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Returns whether this object/buffer/etc was allocated in this workspace.
//	 */
func ZSTD_cwksp_owns_buffer(tls *libc.TLS, ws uintptr, ptr uintptr) (r int32) {
	return libc.BoolInt32(ptr != libc.UintptrFromInt32(0) && (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace <= ptr && ptr < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd)
}

// C documentation
//
//	/**
//	 * Internal function. Do not use directly.
//	 */
func ZSTD_cwksp_reserve_internal(tls *libc.TLS, ws uintptr, bytes size_t, phase ZSTD_cwksp_alloc_phase_e) (r uintptr) {
	var alloc uintptr
	_ = alloc
	if ZSTD_isError(tls, ZSTD_cwksp_internal_advance_phase(tls, ws, phase)) != 0 || bytes == uint64(0) {
		return libc.UintptrFromInt32(0)
	}
	alloc = ZSTD_cwksp_reserve_internal_buffer_space(tls, ws, bytes)
	return alloc
}

// C documentation
//
//	/**
//	 * Reserves and returns unaligned memory.
//	 */
func ZSTD_cwksp_reserve_buffer(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	return ZSTD_cwksp_reserve_internal(tls, ws, bytes, int32(ZSTD_cwksp_alloc_buffers))
}

// C documentation
//
//	/**
//	 * Reserves and returns memory sized on and aligned on ZSTD_CWKSP_ALIGNMENT_BYTES (64 bytes).
//	 * This memory has been initialized at least once in the past.
//	 * This doesn't mean it has been initialized this time, and it might contain data from previous
//	 * operations.
//	 * The main usage is for algorithms that might need read access into uninitialized memory.
//	 * The algorithm must maintain safety under these conditions and must make sure it doesn't
//	 * leak any of the past data (directly or in side channels).
//	 */
func ZSTD_cwksp_reserve_aligned_init_once(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alignedBytes size_t
	var ptr uintptr
	var v1 uint64
	_, _, _ = alignedBytes, ptr, v1
	alignedBytes = ZSTD_cwksp_align(tls, bytes, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES))
	ptr = ZSTD_cwksp_reserve_internal(tls, ws, alignedBytes, int32(ZSTD_cwksp_alloc_aligned_init_once))
	if ptr != 0 && ptr < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart {
		/* We assume the memory following the current allocation is either:
		 * 1. Not usable as initOnce memory (end of workspace)
		 * 2. Another initOnce buffer that has been allocated before (and so was previously memset)
		 * 3. An ASAN redzone, in which case we don't want to write on it
		 * For these reasons it should be fine to not explicitly zero every byte up to ws->initOnceStart.
		 * Note that we assume here that MSAN and ASAN cannot run in the same time. */
		if libc.Uint64FromInt64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart)-int64(ptr)) < alignedBytes {
			v1 = libc.Uint64FromInt64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart) - int64(ptr))
		} else {
			v1 = alignedBytes
		}
		libc.Xmemset(tls, ptr, 0, v1)
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart = ptr
	}
	return ptr
}

// C documentation
//
//	/**
//	 * Reserves and returns memory sized on and aligned on ZSTD_CWKSP_ALIGNMENT_BYTES (64 bytes).
//	 */
func ZSTD_cwksp_reserve_aligned64(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var ptr uintptr
	_ = ptr
	ptr = ZSTD_cwksp_reserve_internal(tls, ws, ZSTD_cwksp_align(tls, bytes, uint64(ZSTD_CWKSP_ALIGNMENT_BYTES)), int32(ZSTD_cwksp_alloc_aligned))
	return ptr
}

// C documentation
//
//	/**
//	 * Aligned on 64 bytes. These buffers have the special property that
//	 * their values remain constrained, allowing us to reuse them without
//	 * memset()-ing them.
//	 */
func ZSTD_cwksp_reserve_table(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alloc, end, top uintptr
	var phase ZSTD_cwksp_alloc_phase_e
	_, _, _, _ = alloc, end, phase, top
	phase = int32(ZSTD_cwksp_alloc_aligned_init_once)
	/* We can only start allocating tables after we are done reserving space for objects at the
	 * start of the workspace */
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase < phase {
		if ZSTD_isError(tls, ZSTD_cwksp_internal_advance_phase(tls, ws, phase)) != 0 {
			return libc.UintptrFromInt32(0)
		}
	}
	alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
	end = alloc + uintptr(bytes)
	top = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
	if end > top {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(1)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = end
	return alloc
}

// C documentation
//
//	/**
//	 * Aligned on sizeof(void*).
//	 * Note : should happen only once, at workspace first initialization
//	 */
func ZSTD_cwksp_reserve_object(tls *libc.TLS, ws uintptr, bytes size_t) (r uintptr) {
	var alloc, end uintptr
	var roundedBytes size_t
	_, _, _ = alloc, end, roundedBytes
	roundedBytes = ZSTD_cwksp_align(tls, bytes, uint64(8))
	alloc = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	end = alloc + uintptr(roundedBytes)
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
	/* we must be in the first phase, no advance is possible */
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase != int32(ZSTD_cwksp_alloc_objects) || end > (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(1)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd = end
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = end
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = end
	return alloc
}

// C documentation
//
//	/**
//	 * with alignment control
//	 * Note : should happen only once, at workspace first initialization
//	 */
func ZSTD_cwksp_reserve_object_aligned(tls *libc.TLS, ws uintptr, byteSize size_t, alignment size_t) (r uintptr) {
	var mask, surplus size_t
	var start uintptr
	var v1 uint64
	_, _, _, _ = mask, start, surplus, v1
	mask = alignment - uint64(1)
	if alignment > uint64(8) {
		v1 = alignment - uint64(8)
	} else {
		v1 = uint64(0)
	}
	surplus = v1
	start = ZSTD_cwksp_reserve_object(tls, ws, byteSize+surplus)
	if start == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	if surplus == uint64(0) {
		return start
	}
	return uintptr((uint64(start) + surplus) & ^mask)
}

func ZSTD_cwksp_mark_tables_dirty(tls *libc.TLS, ws uintptr) {
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

func ZSTD_cwksp_mark_tables_clean(tls *libc.TLS, ws uintptr) {
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd
	}
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

// C documentation
//
//	/**
//	 * Zero the part of the allocated tables not already marked clean.
//	 */
func ZSTD_cwksp_clean_tables(tls *libc.TLS, ws uintptr) {
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd < (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd {
		libc.Xmemset(tls, (*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd, 0, libc.Uint64FromInt64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd)-int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd)))
	}
	ZSTD_cwksp_mark_tables_clean(tls, ws)
}

// C documentation
//
//	/**
//	 * Invalidates table allocations.
//	 * All other allocations remain valid.
//	 */
func ZSTD_cwksp_clear_tables(tls *libc.TLS, ws uintptr) {
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

// C documentation
//
//	/**
//	 * Invalidates all buffer, aligned, and table allocations.
//	 * Object allocations remain valid.
//	 */
func ZSTD_cwksp_clear(tls *libc.TLS, ws uintptr) {
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart = ZSTD_cwksp_initialAllocStart(tls, ws)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed = uint8(0)
	if (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase > int32(ZSTD_cwksp_alloc_aligned_init_once) {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase = int32(ZSTD_cwksp_alloc_aligned_init_once)
	}
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

func ZSTD_cwksp_sizeof(tls *libc.TLS, ws uintptr) (r size_t) {
	return libc.Uint64FromInt64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd) - int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace))
}

func ZSTD_cwksp_used(tls *libc.TLS, ws uintptr) (r size_t) {
	return libc.Uint64FromInt64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd)-int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace)) + libc.Uint64FromInt64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd)-int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart))
}

// C documentation
//
//	/**
//	 * The provided workspace takes ownership of the buffer [start, start+size).
//	 * Any existing values in the workspace are ignored (the previously managed
//	 * buffer, if present, must be separately freed).
//	 */
func ZSTD_cwksp_init(tls *libc.TLS, ws uintptr, start uintptr, size size_t, isStatic ZSTD_cwksp_static_alloc_e) {
	/* ensure correct alignment */
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace = start
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceEnd = start + uintptr(size)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableValidEnd = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FobjectEnd
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FinitOnceStart = ZSTD_cwksp_initialAllocStart(tls, ws)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).Fphase = int32(ZSTD_cwksp_alloc_objects)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FisStatic = isStatic
	ZSTD_cwksp_clear(tls, ws)
	(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration = 0
	ZSTD_cwksp_assert_internal_consistency(tls, ws)
}

func ZSTD_cwksp_create(tls *libc.TLS, ws uintptr, size size_t, customMem ZSTD_customMem) (r size_t) {
	var workspace uintptr
	_ = workspace
	workspace = ZSTD_customMalloc(tls, size, customMem)
	if workspace == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	ZSTD_cwksp_init(tls, ws, workspace, size, int32(ZSTD_cwksp_dynamic_alloc))
	return uint64(0)
}

func ZSTD_cwksp_free(tls *libc.TLS, ws uintptr, customMem ZSTD_customMem) {
	var ptr uintptr
	_ = ptr
	ptr = (*ZSTD_cwksp)(unsafe.Pointer(ws)).Fworkspace
	libc.Xmemset(tls, ws, 0, libc.Uint64FromInt64(72))
	ZSTD_customFree(tls, ptr, customMem)
}

// C documentation
//
//	/**
//	 * Moves the management of a workspace from one cwksp to another. The src cwksp
//	 * is left in an invalid state (src must be re-init()'ed before it's used again).
//	 */
func ZSTD_cwksp_move(tls *libc.TLS, dst uintptr, src uintptr) {
	*(*ZSTD_cwksp)(unsafe.Pointer(dst)) = *(*ZSTD_cwksp)(unsafe.Pointer(src))
	libc.Xmemset(tls, src, 0, libc.Uint64FromInt64(72))
}

func ZSTD_cwksp_reserve_failed(tls *libc.TLS, ws uintptr) (r int32) {
	return libc.Int32FromUint8((*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocFailed)
}

/*-*************************************
*  Functions Checking Free Space
***************************************/

// C documentation
//
//	/* ZSTD_alignmentSpaceWithinBounds() :
//	 * Returns if the estimated space needed for a wksp is within an acceptable limit of the
//	 * actual amount of space used.
//	 */
func ZSTD_cwksp_estimated_space_within_bounds(tls *libc.TLS, ws uintptr, estimatedSpace size_t) (r int32) {
	/* We have an alignment space between objects and tables between tables and buffers, so we can have up to twice
	 * the alignment bytes difference between estimation and actual usage */
	return libc.BoolInt32(estimatedSpace-ZSTD_cwksp_slack_space_required(tls) <= ZSTD_cwksp_used(tls, ws) && ZSTD_cwksp_used(tls, ws) <= estimatedSpace)
}

func ZSTD_cwksp_available_space(tls *libc.TLS, ws uintptr) (r size_t) {
	return libc.Uint64FromInt64(int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FallocStart) - int64((*ZSTD_cwksp)(unsafe.Pointer(ws)).FtableEnd))
}

func ZSTD_cwksp_check_available(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) (r int32) {
	return libc.BoolInt32(ZSTD_cwksp_available_space(tls, ws) >= additionalNeededSpace)
}

func ZSTD_cwksp_check_too_large(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) (r int32) {
	return ZSTD_cwksp_check_available(tls, ws, additionalNeededSpace*uint64(ZSTD_WORKSPACETOOLARGE_FACTOR))
}

func ZSTD_cwksp_check_wasteful(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) (r int32) {
	return libc.BoolInt32(ZSTD_cwksp_check_too_large(tls, ws, additionalNeededSpace) != 0 && (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration > int32(ZSTD_WORKSPACETOOLARGE_MAXDURATION))
}

func ZSTD_cwksp_bump_oversized_duration(tls *libc.TLS, ws uintptr, additionalNeededSpace size_t) {
	if ZSTD_cwksp_check_too_large(tls, ws, additionalNeededSpace) != 0 {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration = (*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration + 1
	} else {
		(*ZSTD_cwksp)(unsafe.Pointer(ws)).FworkspaceOversizedDuration = 0
	}
}

/**** ended inlining zstd_cwksp.h ****/
/**** start inlining zstdmt_compress.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ===   Dependencies   === */
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/

/* Note : This is an internal API.
 *        These APIs used to be exposed with ZSTDLIB_API,
 *        because it used to be the only way to invoke MT compression.
 *        Now, you must use ZSTD_compress2 and ZSTD_compressStream2() instead.
 *
 *        This API requires ZSTD_MULTITHREAD to be defined during compilation,
 *        otherwise ZSTDMT_createCCtx*() will fail.
 */

/* ===   Constants   === */

/* ========================================================
 * ===  Private interface, for use by ZSTD_compress.c   ===
 * ===  Not exposed in libzstd. Never invoke directly   ===
 * ======================================================== */

// C documentation
//
//	/* ===   Memory management   === */
type ZSTDMT_CCtx = struct {
	Ffactory           uintptr
	Fjobs              uintptr
	FbufPool           uintptr
	FcctxPool          uintptr
	FseqPool           uintptr
	Fparams            ZSTD_CCtx_params
	FtargetSectionSize size_t
	FtargetPrefixSize  size_t
	FjobReady          int32
	FinBuff            InBuff_t
	FroundBuff         RoundBuff_t
	Fserial            SerialState
	Frsync             RSyncState_t
	FjobIDMask         uint32
	FdoneJobID         uint32
	FnextJobID         uint32
	FframeEnded        uint32
	FallJobsCompleted  uint32
	FframeContentSize  uint64
	Fconsumed          uint64
	Fproduced          uint64
	FcMem              ZSTD_customMem
	FcdictLocal        uintptr
	Fcdict             uintptr
	F__ccgo3112        uint8
}

/**** ended inlining zstd_cwksp.h ****/
/**** start inlining zstdmt_compress.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ===   Dependencies   === */
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/

/* Note : This is an internal API.
 *        These APIs used to be exposed with ZSTDLIB_API,
 *        because it used to be the only way to invoke MT compression.
 *        Now, you must use ZSTD_compress2 and ZSTD_compressStream2() instead.
 *
 *        This API requires ZSTD_MULTITHREAD to be defined during compilation,
 *        otherwise ZSTDMT_createCCtx*() will fail.
 */

/* ===   Constants   === */

/* ========================================================
 * ===  Private interface, for use by ZSTD_compress.c   ===
 * ===  Not exposed in libzstd. Never invoke directly   ===
 * ======================================================== */

// C documentation
//
//	/* ===   Memory management   === */
type ZSTDMT_CCtx_s = ZSTDMT_CCtx

/**** ended inlining zstd_preSplit.h ****/

/*-*************************************
*  Constants
***************************************/

// C documentation
//
//	/*-*************************************
//	*  Context memory management
//	***************************************/
type ZSTD_compressionStage_e = int32

type ZSTD_cStreamStage = int32

type ZSTD_prefixDict = struct {
	Fdict            uintptr
	FdictSize        size_t
	FdictContentType ZSTD_dictContentType_e
}

type ZSTD_prefixDict_s = ZSTD_prefixDict

type ZSTD_localDict = struct {
	FdictBuffer      uintptr
	Fdict            uintptr
	FdictSize        size_t
	FdictContentType ZSTD_dictContentType_e
	Fcdict           uintptr
}

type ZSTD_hufCTables_t = struct {
	FCTable     [257]HUF_CElt
	FrepeatMode HUF_repeat
}

type ZSTD_fseCTables_t = struct {
	FoffcodeCTable          [193]FSE_CTable
	FmatchlengthCTable      [363]FSE_CTable
	FlitlengthCTable        [329]FSE_CTable
	Foffcode_repeatMode     FSE_repeat
	Fmatchlength_repeatMode FSE_repeat
	Flitlength_repeatMode   FSE_repeat
}

type ZSTD_entropyCTables_t = struct {
	Fhuf ZSTD_hufCTables_t
	Ffse ZSTD_fseCTables_t
}

// C documentation
//
//	/***********************************************
//	*  Sequences *
//	***********************************************/
type SeqDef = struct {
	FoffBase   U32
	FlitLength U16
	FmlBase    U16
}

// C documentation
//
//	/***********************************************
//	*  Sequences *
//	***********************************************/
type SeqDef_s = SeqDef

// C documentation
//
//	/* Controls whether seqStore has a single "long" litLength or matchLength. See SeqStore_t. */
type ZSTD_longLengthType_e = int32

const ZSTD_llt_none = 0
const /* no longLengthType */
ZSTD_llt_literalLength = 1
const /* represents a long literal */
ZSTD_llt_matchLength = 2

type SeqStore_t = struct {
	FsequencesStart uintptr
	Fsequences      uintptr
	FlitStart       uintptr
	Flit            uintptr
	FllCode         uintptr
	FmlCode         uintptr
	FofCode         uintptr
	FmaxNbSeq       size_t
	FmaxNbLit       size_t
	FlongLengthType ZSTD_longLengthType_e
	FlongLengthPos  U32
}

type ZSTD_SequenceLength = struct {
	FlitLength   U32
	FmatchLength U32
}

// C documentation
//
//	/**
//	 * Returns the ZSTD_SequenceLength for the given sequences. It handles the decoding of long sequences
//	 * indicated by longLengthPos and longLengthType, and adds MINMATCH back to matchLength.
//	 */
func ZSTD_getSequenceLength(tls *libc.TLS, seqStore uintptr, seq uintptr) (r ZSTD_SequenceLength) {
	var seqLen ZSTD_SequenceLength
	_ = seqLen
	seqLen.FlitLength = uint32((*SeqDef)(unsafe.Pointer(seq)).FlitLength)
	seqLen.FmatchLength = libc.Uint32FromInt32(libc.Int32FromUint16((*SeqDef)(unsafe.Pointer(seq)).FmlBase) + int32(MINMATCH))
	if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos == libc.Uint32FromInt64((int64(seq)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart))/8) {
		if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
			seqLen.FlitLength += uint32(0x10000)
		}
		if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_matchLength) {
			seqLen.FmatchLength += uint32(0x10000)
		}
	}
	return seqLen
}

/* compress, dictBuilder, decodeCorpus (shouldn't get its definition from here) */

// C documentation
//
//	/***********************************************
//	*  Entropy buffer statistics structs and funcs *
//	***********************************************/
//	/** ZSTD_hufCTablesMetadata_t :
//	 *  Stores Literals Block Type for a super-block in hType, and
//	 *  huffman tree description in hufDesBuffer.
//	 *  hufDesSize refers to the size of huffman tree description in bytes.
//	 *  This metadata is populated in ZSTD_buildBlockEntropyStats_literals() */
type ZSTD_hufCTablesMetadata_t = struct {
	FhType        SymbolEncodingType_e
	FhufDesBuffer [128]BYTE
	FhufDesSize   size_t
}

// C documentation
//
//	/** ZSTD_fseCTablesMetadata_t :
//	 *  Stores symbol compression modes for a super-block in {ll, ol, ml}Type, and
//	 *  fse tables in fseTablesBuffer.
//	 *  fseTablesSize refers to the size of fse tables in bytes.
//	 *  This metadata is populated in ZSTD_buildBlockEntropyStats_sequences() */
type ZSTD_fseCTablesMetadata_t = struct {
	FllType          SymbolEncodingType_e
	FofType          SymbolEncodingType_e
	FmlType          SymbolEncodingType_e
	FfseTablesBuffer [133]BYTE
	FfseTablesSize   size_t
	FlastCountSize   size_t
}

type ZSTD_entropyCTablesMetadata_t = struct {
	FhufMetadata ZSTD_hufCTablesMetadata_t
	FfseMetadata ZSTD_fseCTablesMetadata_t
}

/*********************************
*  Compression internals structs *
*********************************/

type ZSTD_match_t = struct {
	Foff  U32
	Flen1 U32
}

type rawSeq = struct {
	Foffset      U32
	FlitLength   U32
	FmatchLength U32
}

type RawSeqStore_t = struct {
	Fseq           uintptr
	Fpos           size_t
	FposInSequence size_t
	Fsize          size_t
	Fcapacity      size_t
}

var kNullRawSeqStore = RawSeqStore_t{}

type ZSTD_optimal_t = struct {
	Fprice  int32
	Foff    U32
	Fmlen   U32
	Flitlen U32
	Frep    [3]U32
}

type ZSTD_OptPrice_e = int32

const zop_dynamic = 0
const zop_predef = 1

type optState_t = struct {
	FlitFreq                 uintptr
	FlitLengthFreq           uintptr
	FmatchLengthFreq         uintptr
	FoffCodeFreq             uintptr
	FmatchTable              uintptr
	FpriceTable              uintptr
	FlitSum                  U32
	FlitLengthSum            U32
	FmatchLengthSum          U32
	FoffCodeSum              U32
	FlitSumBasePrice         U32
	FlitLengthSumBasePrice   U32
	FmatchLengthSumBasePrice U32
	FoffCodeSumBasePrice     U32
	FpriceType               ZSTD_OptPrice_e
	FsymbolCosts             uintptr
	FliteralCompressionMode  ZSTD_ParamSwitch_e
}

type ZSTD_compressedBlockState_t = struct {
	Fentropy ZSTD_entropyCTables_t
	Frep     [3]U32
}

type ZSTD_window_t = struct {
	FnextSrc               uintptr
	Fbase                  uintptr
	FdictBase              uintptr
	FdictLimit             U32
	FlowLimit              U32
	FnbOverflowCorrections U32
}

type ZSTD_MatchState_t = struct {
	Fwindow              ZSTD_window_t
	FloadedDictEnd       U32
	FnextToUpdate        U32
	FhashLog3            U32
	FrowHashLog          U32
	FtagTable            uintptr
	FhashCache           [8]U32
	FhashSalt            U64
	FhashSaltEntropy     U32
	FhashTable           uintptr
	FhashTable3          uintptr
	FchainTable          uintptr
	FforceNonContiguous  int32
	FdedicatedDictSearch int32
	Fopt                 optState_t
	FdictMatchState      uintptr
	FcParams             ZSTD_compressionParameters
	FldmSeqStore         uintptr
	FprefetchCDictTables int32
	FlazySkipping        int32
}

type ZSTD_blockState_t = struct {
	FprevCBlock uintptr
	FnextCBlock uintptr
	FmatchState ZSTD_MatchState_t
}

type ldmEntry_t = struct {
	Foffset   U32
	Fchecksum U32
}

type ldmMatchCandidate_t = struct {
	Fsplit    uintptr
	Fhash     U32
	Fchecksum U32
	Fbucket   uintptr
}

type ldmState_t = struct {
	Fwindow          ZSTD_window_t
	FhashTable       uintptr
	FloadedDictEnd   U32
	FbucketOffsets   uintptr
	FsplitIndices    [64]size_t
	FmatchCandidates [64]ldmMatchCandidate_t
}

type ldmParams_t = struct {
	FenableLdm      ZSTD_ParamSwitch_e
	FhashLog        U32
	FbucketSizeLog  U32
	FminMatchLength U32
	FhashRateLog    U32
	FwindowLog      U32
}

type SeqCollector = struct {
	FcollectSequences int32
	FseqStart         uintptr
	FseqIndex         size_t
	FmaxSequences     size_t
}

/* typedef'd to ZSTD_CCtx_params within "zstd.h" */

// C documentation
//
//	/**
//	 * Indicates whether this compression proceeds directly from user-provided
//	 * source buffer to user-provided destination buffer (ZSTDb_not_buffered), or
//	 * whether the context needs to buffer the input/output (ZSTDb_buffered).
//	 */
type ZSTD_buffered_policy_e = int32

// C documentation
//
//	/**
//	 * Struct that contains all elements of block splitter that should be allocated
//	 * in a wksp.
//	 */
type ZSTD_blockSplitCtx = struct {
	FfullSeqStoreChunk  SeqStore_t
	FfirstHalfSeqStore  SeqStore_t
	FsecondHalfSeqStore SeqStore_t
	FcurrSeqStore       SeqStore_t
	FnextSeqStore       SeqStore_t
	Fpartitions         [196]U32
	FentropyMetadata    ZSTD_entropyCTablesMetadata_t
}

type ZSTD_dictTableLoadMethod_e = int32

const ZSTD_dtlm_fast = 0
const ZSTD_dtlm_full = 1

type ZSTD_tableFillPurpose_e = int32

const ZSTD_tfp_forCCtx = 0
const ZSTD_tfp_forCDict = 1

type ZSTD_dictMode_e = int32

const ZSTD_noDict = 0
const ZSTD_extDict = 1
const ZSTD_dictMatchState = 2
const ZSTD_dedicatedDictSearch = 3

type ZSTD_CParamMode_e = int32

const ZSTD_cpm_noAttachDict = 0
const /* Compression with ZSTD_noDict or ZSTD_extDict.
 * In this mode we use both the srcSize and the dictSize
 * when selecting and adjusting parameters.
 */
ZSTD_cpm_attachDict = 1
const /* Compression with ZSTD_dictMatchState or ZSTD_dedicatedDictSearch.
 * In this mode we only take the srcSize into account when selecting
 * and adjusting parameters.
 */
ZSTD_cpm_createCDict = 2
const /* Creating a CDict.
 * In this mode we take both the source size and the dictionary size
 * into account when selecting and adjusting the parameters.
 */
ZSTD_cpm_unknown = 3

type ZSTD_BlockCompressor_f = uintptr

func ZSTD_LLcode(tls *libc.TLS, litLength U32) (r U32) {
	var v1 uint32
	_ = v1
	if litLength > uint32(63) {
		v1 = ZSTD_highbit32(tls, litLength) + LL_deltaCode
	} else {
		v1 = uint32(LL_Code[litLength])
	}
	return v1
}

var LL_Code = [64]BYTE{
	1:  uint8(1),
	2:  uint8(2),
	3:  uint8(3),
	4:  uint8(4),
	5:  uint8(5),
	6:  uint8(6),
	7:  uint8(7),
	8:  uint8(8),
	9:  uint8(9),
	10: uint8(10),
	11: uint8(11),
	12: uint8(12),
	13: uint8(13),
	14: uint8(14),
	15: uint8(15),
	16: uint8(16),
	17: uint8(16),
	18: uint8(17),
	19: uint8(17),
	20: uint8(18),
	21: uint8(18),
	22: uint8(19),
	23: uint8(19),
	24: uint8(20),
	25: uint8(20),
	26: uint8(20),
	27: uint8(20),
	28: uint8(21),
	29: uint8(21),
	30: uint8(21),
	31: uint8(21),
	32: uint8(22),
	33: uint8(22),
	34: uint8(22),
	35: uint8(22),
	36: uint8(22),
	37: uint8(22),
	38: uint8(22),
	39: uint8(22),
	40: uint8(23),
	41: uint8(23),
	42: uint8(23),
	43: uint8(23),
	44: uint8(23),
	45: uint8(23),
	46: uint8(23),
	47: uint8(23),
	48: uint8(24),
	49: uint8(24),
	50: uint8(24),
	51: uint8(24),
	52: uint8(24),
	53: uint8(24),
	54: uint8(24),
	55: uint8(24),
	56: uint8(24),
	57: uint8(24),
	58: uint8(24),
	59: uint8(24),
	60: uint8(24),
	61: uint8(24),
	62: uint8(24),
	63: uint8(24),
}

var LL_deltaCode = uint32(19)

// C documentation
//
//	/* ZSTD_MLcode() :
//	 * note : mlBase = matchLength - MINMATCH;
//	 *        because it's the format it's stored in seqStore->sequences */
func ZSTD_MLcode(tls *libc.TLS, mlBase U32) (r U32) {
	var v1 uint32
	_ = v1
	if mlBase > uint32(127) {
		v1 = ZSTD_highbit32(tls, mlBase) + ML_deltaCode
	} else {
		v1 = uint32(ML_Code[mlBase])
	}
	return v1
}

var ML_Code = [128]BYTE{
	1:   uint8(1),
	2:   uint8(2),
	3:   uint8(3),
	4:   uint8(4),
	5:   uint8(5),
	6:   uint8(6),
	7:   uint8(7),
	8:   uint8(8),
	9:   uint8(9),
	10:  uint8(10),
	11:  uint8(11),
	12:  uint8(12),
	13:  uint8(13),
	14:  uint8(14),
	15:  uint8(15),
	16:  uint8(16),
	17:  uint8(17),
	18:  uint8(18),
	19:  uint8(19),
	20:  uint8(20),
	21:  uint8(21),
	22:  uint8(22),
	23:  uint8(23),
	24:  uint8(24),
	25:  uint8(25),
	26:  uint8(26),
	27:  uint8(27),
	28:  uint8(28),
	29:  uint8(29),
	30:  uint8(30),
	31:  uint8(31),
	32:  uint8(32),
	33:  uint8(32),
	34:  uint8(33),
	35:  uint8(33),
	36:  uint8(34),
	37:  uint8(34),
	38:  uint8(35),
	39:  uint8(35),
	40:  uint8(36),
	41:  uint8(36),
	42:  uint8(36),
	43:  uint8(36),
	44:  uint8(37),
	45:  uint8(37),
	46:  uint8(37),
	47:  uint8(37),
	48:  uint8(38),
	49:  uint8(38),
	50:  uint8(38),
	51:  uint8(38),
	52:  uint8(38),
	53:  uint8(38),
	54:  uint8(38),
	55:  uint8(38),
	56:  uint8(39),
	57:  uint8(39),
	58:  uint8(39),
	59:  uint8(39),
	60:  uint8(39),
	61:  uint8(39),
	62:  uint8(39),
	63:  uint8(39),
	64:  uint8(40),
	65:  uint8(40),
	66:  uint8(40),
	67:  uint8(40),
	68:  uint8(40),
	69:  uint8(40),
	70:  uint8(40),
	71:  uint8(40),
	72:  uint8(40),
	73:  uint8(40),
	74:  uint8(40),
	75:  uint8(40),
	76:  uint8(40),
	77:  uint8(40),
	78:  uint8(40),
	79:  uint8(40),
	80:  uint8(41),
	81:  uint8(41),
	82:  uint8(41),
	83:  uint8(41),
	84:  uint8(41),
	85:  uint8(41),
	86:  uint8(41),
	87:  uint8(41),
	88:  uint8(41),
	89:  uint8(41),
	90:  uint8(41),
	91:  uint8(41),
	92:  uint8(41),
	93:  uint8(41),
	94:  uint8(41),
	95:  uint8(41),
	96:  uint8(42),
	97:  uint8(42),
	98:  uint8(42),
	99:  uint8(42),
	100: uint8(42),
	101: uint8(42),
	102: uint8(42),
	103: uint8(42),
	104: uint8(42),
	105: uint8(42),
	106: uint8(42),
	107: uint8(42),
	108: uint8(42),
	109: uint8(42),
	110: uint8(42),
	111: uint8(42),
	112: uint8(42),
	113: uint8(42),
	114: uint8(42),
	115: uint8(42),
	116: uint8(42),
	117: uint8(42),
	118: uint8(42),
	119: uint8(42),
	120: uint8(42),
	121: uint8(42),
	122: uint8(42),
	123: uint8(42),
	124: uint8(42),
	125: uint8(42),
	126: uint8(42),
	127: uint8(42),
}

var ML_deltaCode = uint32(36)

// C documentation
//
//	/* ZSTD_cParam_withinBounds:
//	 * @return 1 if value is within cParam bounds,
//	 * 0 otherwise */
func ZSTD_cParam_withinBounds(tls *libc.TLS, cParam ZSTD_cParameter, value int32) (r int32) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_cParam_getBounds(tls, cParam)
	if ZSTD_isError(tls, bounds.Ferror1) != 0 {
		return 0
	}
	if value < bounds.FlowerBound {
		return 0
	}
	if value > bounds.FupperBound {
		return 0
	}
	return int32(1)
}

// C documentation
//
//	/* ZSTD_selectAddr:
//	 * @return index >= lowLimit ? candidate : backup,
//	 * tries to force branchless codegen. */
func ZSTD_selectAddr(tls *libc.TLS, index U32, lowLimit U32, candidate uintptr, backup uintptr) (r uintptr) {
	var v1 uintptr
	_ = v1
	if index >= lowLimit {
		v1 = candidate
	} else {
		v1 = backup
	}
	return v1
}

// C documentation
//
//	/* ZSTD_noCompressBlock() :
//	 * Writes uncompressed block to dst buffer from given src.
//	 * Returns the size of the block */
func ZSTD_noCompressBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32) (r size_t) {
	var cBlockHeader24 U32
	_ = cBlockHeader24
	cBlockHeader24 = lastBlock + uint32(bt_raw)<<libc.Int32FromInt32(1) + uint32(srcSize<<libc.Int32FromInt32(3))
	if srcSize+ZSTD_blockHeaderSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1391, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	MEM_writeLE24(tls, dst, cBlockHeader24)
	libc.Xmemcpy(tls, dst+uintptr(ZSTD_blockHeaderSize), src, srcSize)
	return ZSTD_blockHeaderSize + srcSize
}

func ZSTD_rleCompressBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src BYTE, srcSize size_t, lastBlock U32) (r size_t) {
	var cBlockHeader U32
	var op uintptr
	_, _ = cBlockHeader, op
	op = dst
	cBlockHeader = lastBlock + uint32(bt_rle)<<libc.Int32FromInt32(1) + uint32(srcSize<<libc.Int32FromInt32(3))
	if dstCapacity < uint64(4) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	MEM_writeLE24(tls, op, cBlockHeader)
	*(*BYTE)(unsafe.Pointer(op + 3)) = src
	return uint64(4)
}

// C documentation
//
//	/* ZSTD_minGain() :
//	 * minimum compression required
//	 * to generate a compress block or a compressed literals section.
//	 * note : use same formula for both situations */
func ZSTD_minGain(tls *libc.TLS, srcSize size_t, strat ZSTD_strategy) (r size_t) {
	var minlog U32
	var v1 uint32
	_, _ = minlog, v1
	if strat >= int32(ZSTD_btultra) {
		v1 = libc.Uint32FromInt32(strat) - uint32(1)
	} else {
		v1 = uint32(6)
	}
	minlog = v1
	_ = libc.Uint64FromInt64(1)
	return srcSize>>minlog + uint64(2)
}

func ZSTD_literalsCompressionIsDisabled(tls *libc.TLS, cctxParams uintptr) (r int32) {
	switch (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FliteralCompressionMode {
	case int32(ZSTD_ps_enable):
		return 0
	case int32(ZSTD_ps_disable):
		return int32(1)
	default:
		fallthrough
	case int32(ZSTD_ps_auto):
		return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy == int32(ZSTD_fast) && (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.FtargetLength > uint32(0))
	}
	return r
}

// C documentation
//
//	/*! ZSTD_safecopyLiterals() :
//	 *  memcpy() function that won't read beyond more than WILDCOPY_OVERLENGTH bytes past ilimit_w.
//	 *  Only called when the sequence ends past ilimit_w, so it only needs to be optimized for single
//	 *  large copies.
//	 */
func ZSTD_safecopyLiterals(tls *libc.TLS, op uintptr, ip uintptr, iend uintptr, ilimit_w uintptr) {
	var v1, v2 uintptr
	_, _ = v1, v2
	if ip <= ilimit_w {
		ZSTD_wildcopy(tls, op, ip, int64(ilimit_w)-int64(ip), int32(ZSTD_no_overlap))
		op = op + uintptr(int64(ilimit_w)-int64(ip))
		ip = ilimit_w
	}
	for ip < iend {
		v1 = op
		op = op + 1
		v2 = ip
		ip = ip + 1
		*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
	}
}

// C documentation
//
//	/*! ZSTD_storeSeqOnly() :
//	 *  Store a sequence (litlen, litPtr, offBase and matchLength) into SeqStore_t.
//	 *  Literals themselves are not copied, but @litPtr is updated.
//	 *  @offBase : Users should employ macros REPCODE_TO_OFFBASE() and OFFSET_TO_OFFBASE().
//	 *  @matchLength : must be >= MINMATCH
//	*/
func ZSTD_storeSeqOnly(tls *libc.TLS, seqStorePtr uintptr, litLength size_t, offBase U32, matchLength size_t) {
	var mlBase size_t
	_ = mlBase
	/* literal Length */
	if libc.BoolInt64(litLength > libc.Uint64FromInt32(0xFFFF)) != 0 {
		/* there can only be a single long length */
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType = int32(ZSTD_llt_literalLength)
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos = libc.Uint32FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	}
	(*(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences))).FlitLength = uint16(litLength)
	/* match offset */
	(*(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences))).FoffBase = offBase
	/* match Length */
	mlBase = matchLength - uint64(MINMATCH)
	if libc.BoolInt64(mlBase > libc.Uint64FromInt32(0xFFFF)) != 0 {
		/* there can only be a single long length */
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType = int32(ZSTD_llt_matchLength)
		(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos = libc.Uint32FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	}
	(*(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences))).FmlBase = uint16(mlBase)
	(*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences += 8
}

// C documentation
//
//	/*! ZSTD_storeSeq() :
//	 *  Store a sequence (litlen, litPtr, offBase and matchLength) into SeqStore_t.
//	 *  @offBase : Users should employ macros REPCODE_TO_OFFBASE() and OFFSET_TO_OFFBASE().
//	 *  @matchLength : must be >= MINMATCH
//	 *  Allowed to over-read literals up to litLimit.
//	*/
func ZSTD_storeSeq(tls *libc.TLS, seqStorePtr uintptr, litLength size_t, literals uintptr, litLimit uintptr, offBase U32, matchLength size_t) {
	var litEnd, litLimit_w uintptr
	_, _ = litEnd, litLimit_w
	litLimit_w = litLimit - uintptr(WILDCOPY_OVERLENGTH)
	litEnd = literals + uintptr(litLength)
	/* copy Literals */
	if litEnd <= litLimit_w {
		/* Common case we can use wildcopy.
		 * First copy 16 bytes, because literals are likely short.
		 */
		_ = libc.Uint64FromInt64(1)
		ZSTD_copy16(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit, literals)
		if litLength > uint64(16) {
			ZSTD_wildcopy(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit+uintptr(16), literals+uintptr(16), libc.Int64FromUint64(litLength)-int64(16), int32(ZSTD_no_overlap))
		}
	} else {
		ZSTD_safecopyLiterals(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit, literals, litEnd, litLimit_w)
	}
	*(*uintptr)(unsafe.Pointer(seqStorePtr + 24)) += uintptr(litLength)
	ZSTD_storeSeqOnly(tls, seqStorePtr, litLength, offBase, matchLength)
}

// C documentation
//
//	/* ZSTD_updateRep() :
//	 * updates in-place @rep (array of repeat offsets)
//	 * @offBase : sum-type, using numeric representation of ZSTD_storeSeq()
//	 */
func ZSTD_updateRep(tls *libc.TLS, rep uintptr, offBase U32, ll0 U32) {
	var currentOffset, repCode U32
	var v1, v2 uint32
	_, _, _, _ = currentOffset, repCode, v1, v2
	if offBase > uint32(ZSTD_REP_NUM) { /* full offset */
		*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep + 1*4))
		*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
		*(*U32)(unsafe.Pointer(rep)) = offBase - libc.Uint32FromInt32(ZSTD_REP_NUM)
	} else { /* repcode */
		repCode = offBase - uint32(1) + ll0
		if repCode > uint32(0) {
			if repCode == uint32(ZSTD_REP_NUM) {
				v1 = *(*U32)(unsafe.Pointer(rep)) - uint32(1)
			} else {
				v1 = *(*U32)(unsafe.Pointer(rep + uintptr(repCode)*4))
			} /* note : if repCode==0, no change */
			currentOffset = v1
			if repCode >= uint32(2) {
				v2 = *(*U32)(unsafe.Pointer(rep + 1*4))
			} else {
				v2 = *(*U32)(unsafe.Pointer(rep + 2*4))
			}
			*(*U32)(unsafe.Pointer(rep + 2*4)) = v2
			*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
			*(*U32)(unsafe.Pointer(rep)) = currentOffset
		} else { /* repCode == 0 */
			/* nothing to do */
		}
	}
}

type Repcodes_t = struct {
	Frep [3]U32
}

type repcodes_s = Repcodes_t

func ZSTD_newRep(tls *libc.TLS, rep uintptr, offBase U32, ll0 U32) (r Repcodes_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* newReps at bp+0 */ Repcodes_t
	libc.Xmemcpy(tls, bp, rep, libc.Uint64FromInt64(12))
	ZSTD_updateRep(tls, bp, offBase, ll0)
	return *(*Repcodes_t)(unsafe.Pointer(bp))
}

// C documentation
//
//	/*-*************************************
//	*  Match length counter
//	***************************************/
func ZSTD_count(tls *libc.TLS, pIn uintptr, pMatch uintptr, pInLimit uintptr) (r size_t) {
	var diff, diff1 size_t
	var pInLoopLimit, pStart uintptr
	_, _, _, _ = diff, diff1, pInLoopLimit, pStart
	pStart = pIn
	pInLoopLimit = pInLimit - uintptr(libc.Uint64FromInt64(8)-libc.Uint64FromInt32(1))
	if pIn < pInLoopLimit {
		diff = MEM_readST(tls, pMatch) ^ MEM_readST(tls, pIn)
		if diff != 0 {
			return uint64(ZSTD_NbCommonBytes(tls, diff))
		}
		pIn = pIn + uintptr(8)
		pMatch = pMatch + uintptr(8)
		for pIn < pInLoopLimit {
			diff1 = MEM_readST(tls, pMatch) ^ MEM_readST(tls, pIn)
			if !(diff1 != 0) {
				pIn = pIn + uintptr(8)
				pMatch = pMatch + uintptr(8)
				continue
			}
			pIn = pIn + uintptr(ZSTD_NbCommonBytes(tls, diff1))
			return libc.Uint64FromInt64(int64(pIn) - int64(pStart))
		}
	}
	if MEM_64bits(tls) != 0 && pIn < pInLimit-libc.UintptrFromInt32(3) && MEM_read32(tls, pMatch) == MEM_read32(tls, pIn) {
		pIn = pIn + uintptr(4)
		pMatch = pMatch + uintptr(4)
	}
	if pIn < pInLimit-libc.UintptrFromInt32(1) && libc.Int32FromUint16(MEM_read16(tls, pMatch)) == libc.Int32FromUint16(MEM_read16(tls, pIn)) {
		pIn = pIn + uintptr(2)
		pMatch = pMatch + uintptr(2)
	}
	if pIn < pInLimit && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(pMatch))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(pIn))) {
		pIn = pIn + 1
	}
	return libc.Uint64FromInt64(int64(pIn) - int64(pStart))
}

// C documentation
//
//	/** ZSTD_count_2segments() :
//	 *  can count match length with `ip` & `match` in 2 different segments.
//	 *  convention : on reaching mEnd, match count continue starting from iStart
//	 */
func ZSTD_count_2segments(tls *libc.TLS, ip uintptr, match uintptr, iEnd uintptr, mEnd uintptr, iStart uintptr) (r size_t) {
	var matchLength size_t
	var vEnd, v1 uintptr
	_, _, _ = matchLength, vEnd, v1
	if ip+uintptr(int64(mEnd)-int64(match)) < iEnd {
		v1 = ip + uintptr(int64(mEnd)-int64(match))
	} else {
		v1 = iEnd
	}
	vEnd = v1
	matchLength = ZSTD_count(tls, ip, match, vEnd)
	if match+uintptr(matchLength) != mEnd {
		return matchLength
	}
	return matchLength + ZSTD_count(tls, ip+uintptr(matchLength), iStart, iEnd)
}

// C documentation
//
//	/*-*************************************
//	 *  Hashes
//	 ***************************************/
var prime3bytes = uint32(506832829)

func ZSTD_hash3(tls *libc.TLS, u U32, h U32, s U32) (r U32) {
	return (u<<(libc.Int32FromInt32(32)-libc.Int32FromInt32(24))*prime3bytes ^ s) >> (uint32(32) - h)
}

func ZSTD_hash3Ptr(tls *libc.TLS, ptr uintptr, h U32) (r size_t) {
	return uint64(ZSTD_hash3(tls, MEM_readLE32(tls, ptr), h, uint32(0)))
}

/* only in zstd_opt.h */

func ZSTD_hash3PtrS(tls *libc.TLS, ptr uintptr, h U32, s U32) (r size_t) {
	return uint64(ZSTD_hash3(tls, MEM_readLE32(tls, ptr), h, s))
}

var prime4bytes = uint32(2654435761)

func ZSTD_hash4(tls *libc.TLS, u U32, h U32, s U32) (r U32) {
	return (u*prime4bytes ^ s) >> (uint32(32) - h)
}

func ZSTD_hash4Ptr(tls *libc.TLS, ptr uintptr, h U32) (r size_t) {
	return uint64(ZSTD_hash4(tls, MEM_readLE32(tls, ptr), h, uint32(0)))
}

func ZSTD_hash4PtrS(tls *libc.TLS, ptr uintptr, h U32, s U32) (r size_t) {
	return uint64(ZSTD_hash4(tls, MEM_readLE32(tls, ptr), h, s))
}

var prime5bytes = uint64(889523592379)

func ZSTD_hash5(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u<<(libc.Int32FromInt32(64)-libc.Int32FromInt32(40))*prime5bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash5Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash5(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash5PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash5(tls, MEM_readLE64(tls, p), h, s)
}

var prime6bytes = uint64(227718039650203)

func ZSTD_hash6(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u<<(libc.Int32FromInt32(64)-libc.Int32FromInt32(48))*prime6bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash6Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash6(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash6PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash6(tls, MEM_readLE64(tls, p), h, s)
}

var prime7bytes = uint64(58295818150454627)

func ZSTD_hash7(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u<<(libc.Int32FromInt32(64)-libc.Int32FromInt32(56))*prime7bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash7Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash7(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash7PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash7(tls, MEM_readLE64(tls, p), h, s)
}

var prime8bytes = uint64(0xCF1BBCDCB7A56463)

func ZSTD_hash8(tls *libc.TLS, u U64, h U32, s U64) (r size_t) {
	return (u*prime8bytes ^ s) >> (libc.Uint32FromInt32(64) - h)
}

func ZSTD_hash8Ptr(tls *libc.TLS, p uintptr, h U32) (r size_t) {
	return ZSTD_hash8(tls, MEM_readLE64(tls, p), h, uint64(0))
}

func ZSTD_hash8PtrS(tls *libc.TLS, p uintptr, h U32, s U64) (r size_t) {
	return ZSTD_hash8(tls, MEM_readLE64(tls, p), h, s)
}

func ZSTD_hashPtr(tls *libc.TLS, p uintptr, hBits U32, mls U32) (r size_t) {
	/* Although some of these hashes do support hBits up to 64, some do not.
	 * To be on the safe side, always avoid hBits > 32. */
	switch mls {
	default:
		fallthrough
	case uint32(4):
		return ZSTD_hash4Ptr(tls, p, hBits)
	case uint32(5):
		return ZSTD_hash5Ptr(tls, p, hBits)
	case uint32(6):
		return ZSTD_hash6Ptr(tls, p, hBits)
	case uint32(7):
		return ZSTD_hash7Ptr(tls, p, hBits)
	case uint32(8):
		return ZSTD_hash8Ptr(tls, p, hBits)
	}
	return r
}

func ZSTD_hashPtrSalted(tls *libc.TLS, p uintptr, hBits U32, mls U32, hashSalt U64) (r size_t) {
	/* Although some of these hashes do support hBits up to 64, some do not.
	 * To be on the safe side, always avoid hBits > 32. */
	switch mls {
	default:
		fallthrough
	case uint32(4):
		return ZSTD_hash4PtrS(tls, p, hBits, uint32(hashSalt))
	case uint32(5):
		return ZSTD_hash5PtrS(tls, p, hBits, hashSalt)
	case uint32(6):
		return ZSTD_hash6PtrS(tls, p, hBits, hashSalt)
	case uint32(7):
		return ZSTD_hash7PtrS(tls, p, hBits, hashSalt)
	case uint32(8):
		return ZSTD_hash8PtrS(tls, p, hBits, hashSalt)
	}
	return r
}

// C documentation
//
//	/** ZSTD_ipow() :
//	 * Return base^exponent.
//	 */
func ZSTD_ipow(tls *libc.TLS, base U64, exponent U64) (r U64) {
	var power U64
	_ = power
	power = uint64(1)
	for exponent != 0 {
		if exponent&uint64(1) != 0 {
			power = power * base
		}
		exponent = exponent >> uint64(1)
		base = base * base
	}
	return power
}

// C documentation
//
//	/** ZSTD_rollingHash_append() :
//	 * Add the buffer to the hash value.
//	 */
func ZSTD_rollingHash_append(tls *libc.TLS, hash U64, buf uintptr, size size_t) (r U64) {
	var istart uintptr
	var pos size_t
	_, _ = istart, pos
	istart = buf
	pos = uint64(0)
	for {
		if !(pos < size) {
			break
		}
		hash = hash * prime8bytes
		hash = hash + libc.Uint64FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart + uintptr(pos))))+int32(ZSTD_ROLL_HASH_CHAR_OFFSET))
		goto _1
	_1:
		;
		pos = pos + 1
	}
	return hash
}

// C documentation
//
//	/** ZSTD_rollingHash_compute() :
//	 * Compute the rolling hash value of the buffer.
//	 */
func ZSTD_rollingHash_compute(tls *libc.TLS, buf uintptr, size size_t) (r U64) {
	return ZSTD_rollingHash_append(tls, uint64(0), buf, size)
}

// C documentation
//
//	/** ZSTD_rollingHash_primePower() :
//	 * Compute the primePower to be passed to ZSTD_rollingHash_rotate() for a hash
//	 * over a window of length bytes.
//	 */
func ZSTD_rollingHash_primePower(tls *libc.TLS, length U32) (r U64) {
	return ZSTD_ipow(tls, prime8bytes, uint64(length-uint32(1)))
}

// C documentation
//
//	/** ZSTD_rollingHash_rotate() :
//	 * Rotate the rolling hash by one byte.
//	 */
func ZSTD_rollingHash_rotate(tls *libc.TLS, hash U64, toRemove BYTE, toAdd BYTE, primePower U64) (r U64) {
	hash = hash - libc.Uint64FromInt32(libc.Int32FromUint8(toRemove)+libc.Int32FromInt32(ZSTD_ROLL_HASH_CHAR_OFFSET))*primePower
	hash = hash * prime8bytes
	hash = hash + libc.Uint64FromInt32(libc.Int32FromUint8(toAdd)+int32(ZSTD_ROLL_HASH_CHAR_OFFSET))
	return hash
}

/*-*************************************
*  Round buffer management
***************************************/
/* Max @current value allowed:
 * In 32-bit mode: we want to avoid crossing the 2 GB limit,
 *                 reducing risks of side effects in case of signed operations on indexes.
 * In 64-bit mode: we want to ensure that adding the maximum job size (512 MB)
 *                 doesn't overflow U32 index capacity (4 GB) */
/* Maximum chunk size before overflow correction needs to be called again */

// C documentation
//
//	/**
//	 * ZSTD_window_clear():
//	 * Clears the window containing the history by simply setting it to empty.
//	 */
func ZSTD_window_clear(tls *libc.TLS, window uintptr) {
	var end U32
	var endT size_t
	_, _ = end, endT
	endT = libc.Uint64FromInt64(int64((*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	end = uint32(endT)
	(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = end
	(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = end
}

func ZSTD_window_isEmpty(tls *libc.TLS, window ZSTD_window_t) (r U32) {
	return libc.BoolUint32(window.FdictLimit == uint32(ZSTD_WINDOW_START_INDEX) && window.FlowLimit == uint32(ZSTD_WINDOW_START_INDEX) && int64(window.FnextSrc)-int64(window.Fbase) == int64(ZSTD_WINDOW_START_INDEX))
}

// C documentation
//
//	/**
//	 * ZSTD_window_hasExtDict():
//	 * Returns non-zero if the window has a non-empty extDict.
//	 */
func ZSTD_window_hasExtDict(tls *libc.TLS, window ZSTD_window_t) (r U32) {
	return libc.BoolUint32(window.FlowLimit < window.FdictLimit)
}

// C documentation
//
//	/**
//	 * ZSTD_matchState_dictMode():
//	 * Inspects the provided matchState and figures out what dictMode should be
//	 * passed to the compressor.
//	 */
func ZSTD_matchState_dictMode(tls *libc.TLS, ms uintptr) (r ZSTD_dictMode_e) {
	var v1, v2, v3 int32
	_, _, _ = v1, v2, v3
	if ZSTD_window_hasExtDict(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow) != 0 {
		v1 = int32(ZSTD_extDict)
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState != libc.UintptrFromInt32(0) {
			if (*ZSTD_MatchState_t)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState)).FdedicatedDictSearch != 0 {
				v3 = int32(ZSTD_dedicatedDictSearch)
			} else {
				v3 = int32(ZSTD_dictMatchState)
			}
			v2 = v3
		} else {
			v2 = int32(ZSTD_noDict)
		}
		v1 = v2
	}
	return v1
}

/* Defining this macro to non-zero tells zstd to run the overflow correction
 * code much more frequently. This is very inefficient, and should only be
 * used for tests and fuzzers.
 */

// C documentation
//
//	/**
//	 * ZSTD_window_canOverflowCorrect():
//	 * Returns non-zero if the indices are large enough for overflow correction
//	 * to work correctly without impacting compression ratio.
//	 */
func ZSTD_window_canOverflowCorrect(tls *libc.TLS, window ZSTD_window_t, cycleLog U32, maxDist U32, loadedDictEnd U32, src uintptr) (r U32) {
	var adjustedIndex, adjustment, curr, cycleSize, dictionaryInvalidated, indexLargeEnough, minIndexToOverflowCorrect U32
	var v1, v2 uint32
	_, _, _, _, _, _, _, _, _ = adjustedIndex, adjustment, curr, cycleSize, dictionaryInvalidated, indexLargeEnough, minIndexToOverflowCorrect, v1, v2
	cycleSize = uint32(1) << cycleLog
	curr = libc.Uint32FromInt64(int64(src) - int64(window.Fbase))
	if maxDist > cycleSize {
		v1 = maxDist
	} else {
		v1 = cycleSize
	}
	minIndexToOverflowCorrect = cycleSize + v1 + uint32(ZSTD_WINDOW_START_INDEX)
	/* Adjust the min index to backoff the overflow correction frequency,
	 * so we don't waste too much CPU in overflow correction. If this
	 * computation overflows we don't really care, we just need to make
	 * sure it is at least minIndexToOverflowCorrect.
	 */
	adjustment = window.FnbOverflowCorrections + uint32(1)
	if minIndexToOverflowCorrect*adjustment > minIndexToOverflowCorrect {
		v2 = minIndexToOverflowCorrect * adjustment
	} else {
		v2 = minIndexToOverflowCorrect
	}
	adjustedIndex = v2
	indexLargeEnough = libc.BoolUint32(curr > adjustedIndex)
	/* Only overflow correct early if the dictionary is invalidated already,
	 * so we don't hurt compression ratio.
	 */
	dictionaryInvalidated = libc.BoolUint32(curr > maxDist+loadedDictEnd)
	return libc.BoolUint32(indexLargeEnough != 0 && dictionaryInvalidated != 0)
}

// C documentation
//
//	/**
//	 * ZSTD_window_needOverflowCorrection():
//	 * Returns non-zero if the indices are getting too large and need overflow
//	 * protection.
//	 */
func ZSTD_window_needOverflowCorrection(tls *libc.TLS, window ZSTD_window_t, cycleLog U32, maxDist U32, loadedDictEnd U32, src uintptr, srcEnd uintptr) (r U32) {
	var curr U32
	var v1 uint32
	_, _ = curr, v1
	curr = libc.Uint32FromInt64(int64(srcEnd) - int64(window.Fbase))
	if ZSTD_WINDOW_OVERFLOW_CORRECT_FREQUENTLY != 0 {
		if ZSTD_window_canOverflowCorrect(tls, window, cycleLog, maxDist, loadedDictEnd, src) != 0 {
			return uint32(1)
		}
	}
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	return libc.BoolUint32(curr > v1)
}

// C documentation
//
//	/**
//	 * ZSTD_window_correctOverflow():
//	 * Reduces the indices to protect from index overflow.
//	 * Returns the correction made to the indices, which must be applied to every
//	 * stored index.
//	 *
//	 * The least significant cycleLog bits of the indices must remain the same,
//	 * which may be 0. Every index up to maxDist in the past must be valid.
//	 */
func ZSTD_window_correctOverflow(tls *libc.TLS, window uintptr, cycleLog U32, maxDist U32, src uintptr) (r U32) {
	var correction, curr, currentCycle, currentCycleCorrection, cycleMask, cycleSize, newCurrent U32
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _ = correction, curr, currentCycle, currentCycleCorrection, cycleMask, cycleSize, newCurrent, v1, v2, v3
	/* preemptive overflow correction:
	 * 1. correction is large enough:
	 *    lowLimit > (3<<29) ==> current > 3<<29 + 1<<windowLog
	 *    1<<windowLog <= newCurrent < 1<<chainLog + 1<<windowLog
	 *
	 *    current - newCurrent
	 *    > (3<<29 + 1<<windowLog) - (1<<windowLog + 1<<chainLog)
	 *    > (3<<29) - (1<<chainLog)
	 *    > (3<<29) - (1<<30)             (NOTE: chainLog <= 30)
	 *    > 1<<29
	 *
	 * 2. (ip+ZSTD_CHUNKSIZE_MAX - cctx->base) doesn't overflow:
	 *    After correction, current is less than (1<<chainLog + 1<<windowLog).
	 *    In 64-bit mode we are safe, because we have 64-bit ptrdiff_t.
	 *    In 32-bit mode we are safe, because (chainLog <= 29), so
	 *    ip+ZSTD_CHUNKSIZE_MAX - cctx->base < 1<<32.
	 * 3. (cctx->lowLimit + 1<<windowLog) < 1<<32:
	 *    windowLog <= 31 ==> 3<<29 + 1<<windowLog < 7<<29 < 1<<32.
	 */
	cycleSize = uint32(1) << cycleLog
	cycleMask = cycleSize - uint32(1)
	curr = libc.Uint32FromInt64(int64(src) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	currentCycle = curr & cycleMask
	if currentCycle < uint32(ZSTD_WINDOW_START_INDEX) {
		if cycleSize > libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_WINDOW_START_INDEX)) {
			v2 = cycleSize
		} else {
			v2 = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_WINDOW_START_INDEX))
		}
		v1 = v2
	} else {
		v1 = uint32(0)
	}
	/* Ensure newCurrent - maxDist >= ZSTD_WINDOW_START_INDEX. */
	currentCycleCorrection = v1
	if maxDist > cycleSize {
		v3 = maxDist
	} else {
		v3 = cycleSize
	}
	newCurrent = currentCycle + currentCycleCorrection + v3
	correction = curr - newCurrent
	/* maxDist must be a power of two so that:
	 *   (newCurrent & cycleMask) == (curr & cycleMask)
	 * This is required to not corrupt the chains / binary tree.
	 */
	if !(libc.Int32FromInt32(ZSTD_WINDOW_OVERFLOW_CORRECT_FREQUENTLY) != 0) {
		/* Loose bound, should be around 1<<29 (see above) */
	}
	*(*uintptr)(unsafe.Pointer(window + 8)) += uintptr(correction)
	*(*uintptr)(unsafe.Pointer(window + 16)) += uintptr(correction)
	if (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit < correction+uint32(ZSTD_WINDOW_START_INDEX) {
		(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = uint32(ZSTD_WINDOW_START_INDEX)
	} else {
		*(*U32)(unsafe.Pointer(window + 28)) -= correction
	}
	if (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit < correction+uint32(ZSTD_WINDOW_START_INDEX) {
		(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = uint32(ZSTD_WINDOW_START_INDEX)
	} else {
		*(*U32)(unsafe.Pointer(window + 24)) -= correction
	}
	/* Ensure we can still reference the full window. */
	/* Ensure that lowLimit and dictLimit didn't underflow. */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnbOverflowCorrections = (*ZSTD_window_t)(unsafe.Pointer(window)).FnbOverflowCorrections + 1
	return correction
}

// C documentation
//
//	/**
//	 * ZSTD_window_enforceMaxDist():
//	 * Updates lowLimit so that:
//	 *    (srcEnd - base) - lowLimit == maxDist + loadedDictEnd
//	 *
//	 * It ensures index is valid as long as index >= lowLimit.
//	 * This must be called before a block compression call.
//	 *
//	 * loadedDictEnd is only defined if a dictionary is in use for current compression.
//	 * As the name implies, loadedDictEnd represents the index at end of dictionary.
//	 * The value lies within context's referential, it can be directly compared to blockEndIdx.
//	 *
//	 * If loadedDictEndPtr is NULL, no dictionary is in use, and we use loadedDictEnd == 0.
//	 * If loadedDictEndPtr is not NULL, we set it to zero after updating lowLimit.
//	 * This is because dictionaries are allowed to be referenced fully
//	 * as long as the last byte of the dictionary is in the window.
//	 * Once input has progressed beyond window size, dictionary cannot be referenced anymore.
//	 *
//	 * In normal dict mode, the dictionary lies between lowLimit and dictLimit.
//	 * In dictMatchState mode, lowLimit and dictLimit are the same,
//	 * and the dictionary is below them.
//	 * forceWindow and dictMatchState are therefore incompatible.
//	 */
func ZSTD_window_enforceMaxDist(tls *libc.TLS, window uintptr, blockEnd uintptr, maxDist U32, loadedDictEndPtr uintptr, dictMatchStatePtr uintptr) {
	var blockEndIdx, loadedDictEnd, newLowLimit U32
	var v1 uint32
	_, _, _, _ = blockEndIdx, loadedDictEnd, newLowLimit, v1
	blockEndIdx = libc.Uint32FromInt64(int64(blockEnd) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	if loadedDictEndPtr != libc.UintptrFromInt32(0) {
		v1 = *(*U32)(unsafe.Pointer(loadedDictEndPtr))
	} else {
		v1 = uint32(0)
	}
	loadedDictEnd = v1
	/* - When there is no dictionary : loadedDictEnd == 0.
	     In which case, the test (blockEndIdx > maxDist) is merely to avoid
	     overflowing next operation `newLowLimit = blockEndIdx - maxDist`.
	   - When there is a standard dictionary :
	     Index referential is copied from the dictionary,
	     which means it starts from 0.
	     In which case, loadedDictEnd == dictSize,
	     and it makes sense to compare `blockEndIdx > maxDist + dictSize`
	     since `blockEndIdx` also starts from zero.
	   - When there is an attached dictionary :
	     loadedDictEnd is expressed within the referential of the context,
	     so it can be directly compared against blockEndIdx.
	*/
	if blockEndIdx > maxDist+loadedDictEnd {
		newLowLimit = blockEndIdx - maxDist
		if (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit < newLowLimit {
			(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = newLowLimit
		}
		if (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit < (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit {
			(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = (*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit
		}
		/* On reaching window size, dictionaries are invalidated */
		if loadedDictEndPtr != 0 {
			*(*U32)(unsafe.Pointer(loadedDictEndPtr)) = uint32(0)
		}
		if dictMatchStatePtr != 0 {
			*(*uintptr)(unsafe.Pointer(dictMatchStatePtr)) = libc.UintptrFromInt32(0)
		}
	}
}

// C documentation
//
//	/* Similar to ZSTD_window_enforceMaxDist(),
//	 * but only invalidates dictionary
//	 * when input progresses beyond window size.
//	 * assumption : loadedDictEndPtr and dictMatchStatePtr are valid (non NULL)
//	 *              loadedDictEnd uses same referential as window->base
//	 *              maxDist is the window size */
func ZSTD_checkDictValidity(tls *libc.TLS, window uintptr, blockEnd uintptr, maxDist U32, loadedDictEndPtr uintptr, dictMatchStatePtr uintptr) {
	var blockEndIdx, loadedDictEnd U32
	_, _ = blockEndIdx, loadedDictEnd
	blockEndIdx = libc.Uint32FromInt64(int64(blockEnd) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
	loadedDictEnd = *(*U32)(unsafe.Pointer(loadedDictEndPtr))
	if blockEndIdx > loadedDictEnd+maxDist || loadedDictEnd != (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit {
		/* On reaching window size, dictionaries are invalidated.
		 * For simplification, if window size is reached anywhere within next block,
		 * the dictionary is invalidated for the full block.
		 *
		 * We also have to invalidate the dictionary if ZSTD_window_update() has detected
		 * non-contiguous segments, which means that loadedDictEnd != window->dictLimit.
		 * loadedDictEnd may be 0, if forceWindow is true, but in that case we never use
		 * dictMatchState, so setting it to NULL is not a problem.
		 */
		*(*U32)(unsafe.Pointer(loadedDictEndPtr)) = uint32(0)
		*(*uintptr)(unsafe.Pointer(dictMatchStatePtr)) = libc.UintptrFromInt32(0)
	} else {
		if *(*U32)(unsafe.Pointer(loadedDictEndPtr)) != uint32(0) {
		}
	}
}

func ZSTD_window_init(tls *libc.TLS, window uintptr) {
	libc.Xmemset(tls, window, 0, libc.Uint64FromInt64(40))
	(*ZSTD_window_t)(unsafe.Pointer(window)).Fbase = __ccgo_ts + 1432
	(*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase = __ccgo_ts + 1432
	_ = libc.Uint64FromInt64(1)                                                                                                           /* Start above ZSTD_DUBT_UNSORTED_MARK */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = uint32(ZSTD_WINDOW_START_INDEX)                                                 /* start from >0, so that 1st position is valid */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = uint32(ZSTD_WINDOW_START_INDEX)                                                  /* it ensures first and later CCtx usages compress the same */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc = (*ZSTD_window_t)(unsafe.Pointer(window)).Fbase + uintptr(ZSTD_WINDOW_START_INDEX) /* see issue #1241 */
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnbOverflowCorrections = uint32(0)
}

// C documentation
//
//	/**
//	 * ZSTD_window_update():
//	 * Updates the window by appending [src, src + srcSize) to the window.
//	 * If it is not contiguous, the current prefix becomes the extDict, and we
//	 * forget about the extDict. Handles overlap of the prefix and extDict.
//	 * Returns non-zero if the segment is contiguous.
//	 */
func ZSTD_window_update(tls *libc.TLS, window uintptr, src uintptr, srcSize size_t, forceNonContiguous int32) (r U32) {
	var contiguous, lowLimitMax U32
	var distanceFromBase, highInputIdx size_t
	var ip uintptr
	var v1 uint32
	_, _, _, _, _, _ = contiguous, distanceFromBase, highInputIdx, ip, lowLimitMax, v1
	ip = src
	contiguous = uint32(1)
	if srcSize == uint64(0) {
		return contiguous
	}
	/* Check if blocks follow each other */
	if src != (*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc || forceNonContiguous != 0 {
		/* not contiguous */
		distanceFromBase = libc.Uint64FromInt64(int64((*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).Fbase))
		(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit
		/* should never overflow */
		(*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit = uint32(distanceFromBase)
		(*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase = (*ZSTD_window_t)(unsafe.Pointer(window)).Fbase
		(*ZSTD_window_t)(unsafe.Pointer(window)).Fbase = ip - uintptr(distanceFromBase)
		/* ms->nextToUpdate = window->dictLimit; */
		if (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit-(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit < uint32(HASH_READ_SIZE) {
			(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit
		} /* too small extDict */
		contiguous = uint32(0)
	}
	(*ZSTD_window_t)(unsafe.Pointer(window)).FnextSrc = ip + uintptr(srcSize)
	/* if input and dictionary overlap : reduce dictionary (area presumed modified by input) */
	if libc.BoolInt32(ip+uintptr(srcSize) > (*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase+uintptr((*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit))&libc.BoolInt32(ip < (*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase+uintptr((*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit)) != 0 {
		highInputIdx = libc.Uint64FromInt64(int64(ip+uintptr(srcSize)) - int64((*ZSTD_window_t)(unsafe.Pointer(window)).FdictBase))
		if highInputIdx > uint64((*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit) {
			v1 = (*ZSTD_window_t)(unsafe.Pointer(window)).FdictLimit
		} else {
			v1 = uint32(highInputIdx)
		}
		lowLimitMax = v1
		(*ZSTD_window_t)(unsafe.Pointer(window)).FlowLimit = lowLimitMax
	}
	return contiguous
}

// C documentation
//
//	/**
//	 * Returns the lowest allowed match index. It may either be in the ext-dict or the prefix.
//	 */
func ZSTD_getLowestMatchIndex(tls *libc.TLS, ms uintptr, curr U32, windowLog uint32) (r U32) {
	var isDictionary, lowestValid, matchLowest, maxDistance, withinWindow U32
	var v1, v2 uint32
	_, _, _, _, _, _, _ = isDictionary, lowestValid, matchLowest, maxDistance, withinWindow, v1, v2
	maxDistance = uint32(1) << windowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinWindow = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinWindow
	}
	/* When using a dictionary the entire dictionary is valid if a single byte of the dictionary
	 * is within the window. We invalidate the dictionary (and set loadedDictEnd to 0) when it isn't
	 * valid for the entire block. So this check is sufficient to find the lowest valid match index.
	 */
	matchLowest = v2
	return matchLowest
}

// C documentation
//
//	/**
//	 * Returns the lowest allowed match index in the prefix.
//	 */
func ZSTD_getLowestPrefixIndex(tls *libc.TLS, ms uintptr, curr U32, windowLog uint32) (r U32) {
	var isDictionary, lowestValid, matchLowest, maxDistance, withinWindow U32
	var v1, v2 uint32
	_, _, _, _, _, _, _ = isDictionary, lowestValid, matchLowest, maxDistance, withinWindow, v1, v2
	maxDistance = uint32(1) << windowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinWindow = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinWindow
	}
	/* When computing the lowest prefix index we need to take the dictionary into account to handle
	 * the edge case where the dictionary and the source are contiguous in memory.
	 */
	matchLowest = v2
	return matchLowest
}

// C documentation
//
//	/* index_safety_check:
//	 * intentional underflow : ensure repIndex isn't overlapping dict + prefix
//	 * @return 1 if values are not overlapping,
//	 * 0 otherwise */
func ZSTD_index_overlap_check(tls *libc.TLS, prefixLowestIndex U32, repIndex U32) (r int32) {
	return libc.BoolInt32(prefixLowestIndex-libc.Uint32FromInt32(1)-repIndex >= libc.Uint32FromInt32(3))
}

/* debug functions */

/* Short Cache */

/* Normally, zstd matchfinders follow this flow:
 *     1. Compute hash at ip
 *     2. Load index from hashTable[hash]
 *     3. Check if *ip == *(base + index)
 * In dictionary compression, loading *(base + index) is often an L2 or even L3 miss.
 *
 * Short cache is an optimization which allows us to avoid step 3 most of the time
 * when the data doesn't actually match. With short cache, the flow becomes:
 *     1. Compute (hash, currentTag) at ip. currentTag is an 8-bit independent hash at ip.
 *     2. Load (index, matchTag) from hashTable[hash]. See ZSTD_writeTaggedIndex to understand how this works.
 *     3. Only if currentTag == matchTag, check *ip == *(base + index). Otherwise, continue.
 *
 * Currently, short cache is only implemented in CDict hashtables. Thus, its use is limited to
 * dictMatchState matchfinders.
 */

// C documentation
//
//	/* Helper function for ZSTD_fillHashTable and ZSTD_fillDoubleHashTable.
//	 * Unpacks hashAndTag into (hash, tag), then packs (index, tag) into hashTable[hash]. */
func ZSTD_writeTaggedIndex(tls *libc.TLS, hashTable uintptr, hashAndTag size_t, index U32) {
	var hash size_t
	var tag U32
	_, _ = hash, tag
	hash = hashAndTag >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
	tag = uint32(hashAndTag & uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)-libc.Uint32FromInt32(1)))
	*(*U32)(unsafe.Pointer(hashTable + uintptr(hash)*4)) = index<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS) | tag
}

// C documentation
//
//	/* Helper function for short cache matchfinders.
//	 * Unpacks tag1 and tag2 from lower bits of packedTag1 and packedTag2, then checks if the tags match. */
func ZSTD_comparePackedTags(tls *libc.TLS, packedTag1 size_t, packedTag2 size_t) (r int32) {
	var tag1, tag2 U32
	_, _ = tag1, tag2
	tag1 = uint32(packedTag1 & uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)-libc.Uint32FromInt32(1)))
	tag2 = uint32(packedTag2 & uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)-libc.Uint32FromInt32(1)))
	return libc.BoolInt32(tag1 == tag2)
}

type ZSTD_SequencePosition = struct {
	Fidx           U32
	FposInSequence U32
	FposInSrc      size_t
}

type BlockSummary = struct {
	FnbSequences size_t
	FblockSize   size_t
	FlitSize     size_t
}

// C documentation
//
//	/* Returns 1 if an external sequence producer is registered, otherwise returns 0. */
func ZSTD_hasExtSeqProd(tls *libc.TLS, params uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdFunc != libc.UintptrFromInt32(0))
}

/**** ended inlining zstd_compress_literals.h ****/

/* **************************************************************
*  Debug Traces
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  Literals compression - special cases
//	****************************************************************/
func ZSTD_noCompressLiterals(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var flSize U32
	var ostart uintptr
	_, _ = flSize, ostart
	ostart = dst
	flSize = libc.Uint32FromInt32(int32(1) + libc.BoolInt32(srcSize > uint64(31)) + libc.BoolInt32(srcSize > uint64(4095)))
	if srcSize+uint64(flSize) > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	switch flSize {
	case uint32(1): /* 2 - 1 - 5 */
		*(*BYTE)(unsafe.Pointer(ostart)) = uint8(uint64(uint32(set_basic)) + srcSize<<libc.Int32FromInt32(3))
	case uint32(2): /* 2 - 2 - 12 */
		MEM_writeLE16(tls, ostart, uint16(uint64(uint32(set_basic)+libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	case uint32(3): /* 2 - 2 - 20 */
		MEM_writeLE32(tls, ostart, uint32(uint64(uint32(set_basic)+libc.Uint32FromInt32(libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	default: /* not necessary : flSize is {1,2,3} */
	}
	libc.Xmemcpy(tls, ostart+uintptr(flSize), src, srcSize)
	return srcSize + uint64(flSize)
}

func allBytesIdentical(tls *libc.TLS, src uintptr, srcSize size_t) (r int32) {
	var b BYTE
	var p size_t
	_, _ = b, p
	b = *(*BYTE)(unsafe.Pointer(src))
	p = uint64(1)
	for {
		if !(p < srcSize) {
			break
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(src + uintptr(p)))) != libc.Int32FromUint8(b) {
			return 0
		}
		goto _1
	_1:
		;
		p = p + 1
	}
	return int32(1)
	return r
}

func ZSTD_compressRleLiteralsBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var flSize U32
	var ostart uintptr
	_, _ = flSize, ostart
	ostart = dst
	flSize = libc.Uint32FromInt32(int32(1) + libc.BoolInt32(srcSize > uint64(31)) + libc.BoolInt32(srcSize > uint64(4095)))
	_ = dstCapacity
	switch flSize {
	case uint32(1): /* 2 - 1 - 5 */
		*(*BYTE)(unsafe.Pointer(ostart)) = uint8(uint64(uint32(set_rle)) + srcSize<<libc.Int32FromInt32(3))
	case uint32(2): /* 2 - 2 - 12 */
		MEM_writeLE16(tls, ostart, uint16(uint64(uint32(set_rle)+libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	case uint32(3): /* 2 - 2 - 20 */
		MEM_writeLE32(tls, ostart, uint32(uint64(uint32(set_rle)+libc.Uint32FromInt32(libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)))+srcSize<<libc.Int32FromInt32(4)))
	default: /* not necessary : flSize is {1,2,3} */
	}
	*(*BYTE)(unsafe.Pointer(ostart + uintptr(flSize))) = *(*BYTE)(unsafe.Pointer(src))
	return uint64(flSize + uint32(1))
}

// C documentation
//
//	/* ZSTD_minLiteralsToCompress() :
//	 * returns minimal amount of literals
//	 * for literal compression to even be attempted.
//	 * Minimum is made tighter as compression strategy increases.
//	 */
func ZSTD_minLiteralsToCompress(tls *libc.TLS, strategy ZSTD_strategy, huf_repeat HUF_repeat) (r size_t) {
	var mintc size_t
	var shift, v1 int32
	var v2 uint64
	_, _, _, _ = mintc, shift, v1, v2
	/* btultra2 : min 8 bytes;
	 * then 2x larger for each successive compression strategy
	 * max threshold 64 bytes */
	if int32(9)-strategy < int32(3) {
		v1 = int32(9) - strategy
	} else {
		v1 = int32(3)
	}
	shift = v1
	if huf_repeat == int32(HUF_repeat_valid) {
		v2 = uint64(6)
	} else {
		v2 = libc.Uint64FromInt32(8) << shift
	}
	mintc = v2
	return mintc
	return r
}

func ZSTD_compressLiterals(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, entropyWorkspace uintptr, entropyWorkspaceSize size_t, prevHuf uintptr, nextHuf uintptr, strategy ZSTD_strategy, disableLiteralCompression int32, suspectUncompressible int32, bmi2 int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cLitSize, lhSize, minGain size_t
	var flags, v1, v2, v3, v4 int32
	var hType SymbolEncodingType_e
	var huf_compress, ostart, v5 uintptr
	var lhc, lhc1, lhc2, singleStream U32
	var _ /* repeat at bp+0 */ HUF_repeat
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cLitSize, flags, hType, huf_compress, lhSize, lhc, lhc1, lhc2, minGain, ostart, singleStream, v1, v2, v3, v4, v5
	lhSize = libc.Uint64FromInt32(int32(3) + libc.BoolInt32(srcSize >= libc.Uint64FromInt32(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(srcSize >= libc.Uint64FromInt32(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	ostart = dst
	singleStream = libc.BoolUint32(srcSize < uint64(256))
	hType = int32(set_compressed)
	/* Prepare nextEntropy assuming reusing the existing table */
	libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
	if disableLiteralCompression != 0 {
		return ZSTD_noCompressLiterals(tls, dst, dstCapacity, src, srcSize)
	}
	/* if too small, don't even attempt compression (speed opt) */
	if srcSize < ZSTD_minLiteralsToCompress(tls, strategy, (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode) {
		return ZSTD_noCompressLiterals(tls, dst, dstCapacity, src, srcSize)
	}
	if dstCapacity < lhSize+uint64(1) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1434, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	*(*HUF_repeat)(unsafe.Pointer(bp)) = (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode
	if bmi2 != 0 {
		v1 = int32(HUF_flags_bmi2)
	} else {
		v1 = 0
	}
	if strategy < int32(ZSTD_lazy) && srcSize <= uint64(1024) {
		v2 = int32(HUF_flags_preferRepeat)
	} else {
		v2 = 0
	}
	if strategy >= int32(ZSTD_btultra) {
		v3 = int32(HUF_flags_optimalDepth)
	} else {
		v3 = 0
	}
	if suspectUncompressible != 0 {
		v4 = int32(HUF_flags_suspectUncompressible)
	} else {
		v4 = 0
	}
	flags = 0 | v1 | v2 | v3 | v4
	if *(*HUF_repeat)(unsafe.Pointer(bp)) == int32(HUF_repeat_valid) && lhSize == uint64(3) {
		singleStream = uint32(1)
	}
	if singleStream != 0 {
		v5 = __ccgo_fp(HUF_compress1X_repeat)
	} else {
		v5 = __ccgo_fp(HUF_compress4X_repeat)
	}
	huf_compress = v5
	cLitSize = (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uint32, uint32, uintptr, size_t, uintptr, uintptr, int32) size_t)(unsafe.Pointer(&struct{ uintptr }{huf_compress})))(tls, ostart+uintptr(lhSize), dstCapacity-lhSize, src, srcSize, uint32(HUF_SYMBOLVALUE_MAX), uint32(LitHufLog), entropyWorkspace, entropyWorkspaceSize, nextHuf, bp, flags)
	if *(*HUF_repeat)(unsafe.Pointer(bp)) != int32(HUF_repeat_none) {
		/* reused the existing table */
		hType = int32(set_repeat)
	}
	minGain = ZSTD_minGain(tls, srcSize, strategy)
	if cLitSize == uint64(0) || cLitSize >= srcSize-minGain || ERR_isError(tls, cLitSize) != 0 {
		libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
		return ZSTD_noCompressLiterals(tls, dst, dstCapacity, src, srcSize)
	}
	if cLitSize == uint64(1) {
		/* A return value of 1 signals that the alphabet consists of a single symbol.
		 * However, in some rare circumstances, it could be the compressed size (a single byte).
		 * For that outcome to have a chance to happen, it's necessary that `srcSize < 8`.
		 * (it's also necessary to not generate statistics).
		 * Therefore, in such a case, actively check that all bytes are identical. */
		if srcSize >= uint64(8) || allBytesIdentical(tls, src, srcSize) != 0 {
			libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
			return ZSTD_compressRleLiteralsBlock(tls, dst, dstCapacity, src, srcSize)
		}
	}
	if hType == int32(set_compressed) {
		/* using a newly constructed table */
		(*ZSTD_hufCTables_t)(unsafe.Pointer(nextHuf)).FrepeatMode = int32(HUF_repeat_check)
	}
	/* Build header */
	switch lhSize {
	case uint64(3): /* 2 - 2 - 10 - 10 */
		if !(singleStream != 0) {
		}
		lhc = libc.Uint32FromInt32(hType) + libc.BoolUint32(!(singleStream != 0))<<libc.Int32FromInt32(2) + uint32(srcSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(14)
		MEM_writeLE24(tls, ostart, lhc)
	case uint64(4): /* 2 - 2 - 14 - 14 */
		lhc1 = libc.Uint32FromInt32(hType+libc.Int32FromInt32(2)<<libc.Int32FromInt32(2)) + uint32(srcSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(18)
		MEM_writeLE32(tls, ostart, lhc1)
	case uint64(5): /* 2 - 2 - 18 - 18 */
		lhc2 = libc.Uint32FromInt32(hType+libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)) + uint32(srcSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(22)
		MEM_writeLE32(tls, ostart, lhc2)
		*(*BYTE)(unsafe.Pointer(ostart + 4)) = uint8(cLitSize >> libc.Int32FromInt32(10))
	default: /* not possible : lhSize is {3,4,5} */
	}
	return lhSize + cLitSize
}

/**** ended inlining compress/zstd_compress_literals.c ****/
/**** start inlining compress/zstd_compress_sequences.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
 *  Dependencies
 ***************************************/
/**** start inlining zstd_compress_sequences.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/zstd_internal.h ****/

type ZSTD_DefaultPolicy_e = int32

const ZSTD_defaultDisallowed = 0
const ZSTD_defaultAllowed = 1

/**** ended inlining zstd_compress_sequences.h ****/

// C documentation
//
//	/**
//	 * -log2(x / 256) lookup table for x in [0, 256).
//	 * If x == 0: Return 0
//	 * Else: Return floor(-log2(x / 256) * 256)
//	 */
var kInverseProbabilityLog256 = [256]uint32{
	1:   uint32(2048),
	2:   uint32(1792),
	3:   uint32(1642),
	4:   uint32(1536),
	5:   uint32(1453),
	6:   uint32(1386),
	7:   uint32(1329),
	8:   uint32(1280),
	9:   uint32(1236),
	10:  uint32(1197),
	11:  uint32(1162),
	12:  uint32(1130),
	13:  uint32(1100),
	14:  uint32(1073),
	15:  uint32(1047),
	16:  uint32(1024),
	17:  uint32(1001),
	18:  uint32(980),
	19:  uint32(960),
	20:  uint32(941),
	21:  uint32(923),
	22:  uint32(906),
	23:  uint32(889),
	24:  uint32(874),
	25:  uint32(859),
	26:  uint32(844),
	27:  uint32(830),
	28:  uint32(817),
	29:  uint32(804),
	30:  uint32(791),
	31:  uint32(779),
	32:  uint32(768),
	33:  uint32(756),
	34:  uint32(745),
	35:  uint32(734),
	36:  uint32(724),
	37:  uint32(714),
	38:  uint32(704),
	39:  uint32(694),
	40:  uint32(685),
	41:  uint32(676),
	42:  uint32(667),
	43:  uint32(658),
	44:  uint32(650),
	45:  uint32(642),
	46:  uint32(633),
	47:  uint32(626),
	48:  uint32(618),
	49:  uint32(610),
	50:  uint32(603),
	51:  uint32(595),
	52:  uint32(588),
	53:  uint32(581),
	54:  uint32(574),
	55:  uint32(567),
	56:  uint32(561),
	57:  uint32(554),
	58:  uint32(548),
	59:  uint32(542),
	60:  uint32(535),
	61:  uint32(529),
	62:  uint32(523),
	63:  uint32(517),
	64:  uint32(512),
	65:  uint32(506),
	66:  uint32(500),
	67:  uint32(495),
	68:  uint32(489),
	69:  uint32(484),
	70:  uint32(478),
	71:  uint32(473),
	72:  uint32(468),
	73:  uint32(463),
	74:  uint32(458),
	75:  uint32(453),
	76:  uint32(448),
	77:  uint32(443),
	78:  uint32(438),
	79:  uint32(434),
	80:  uint32(429),
	81:  uint32(424),
	82:  uint32(420),
	83:  uint32(415),
	84:  uint32(411),
	85:  uint32(407),
	86:  uint32(402),
	87:  uint32(398),
	88:  uint32(394),
	89:  uint32(390),
	90:  uint32(386),
	91:  uint32(382),
	92:  uint32(377),
	93:  uint32(373),
	94:  uint32(370),
	95:  uint32(366),
	96:  uint32(362),
	97:  uint32(358),
	98:  uint32(354),
	99:  uint32(350),
	100: uint32(347),
	101: uint32(343),
	102: uint32(339),
	103: uint32(336),
	104: uint32(332),
	105: uint32(329),
	106: uint32(325),
	107: uint32(322),
	108: uint32(318),
	109: uint32(315),
	110: uint32(311),
	111: uint32(308),
	112: uint32(305),
	113: uint32(302),
	114: uint32(298),
	115: uint32(295),
	116: uint32(292),
	117: uint32(289),
	118: uint32(286),
	119: uint32(282),
	120: uint32(279),
	121: uint32(276),
	122: uint32(273),
	123: uint32(270),
	124: uint32(267),
	125: uint32(264),
	126: uint32(261),
	127: uint32(258),
	128: uint32(256),
	129: uint32(253),
	130: uint32(250),
	131: uint32(247),
	132: uint32(244),
	133: uint32(241),
	134: uint32(239),
	135: uint32(236),
	136: uint32(233),
	137: uint32(230),
	138: uint32(228),
	139: uint32(225),
	140: uint32(222),
	141: uint32(220),
	142: uint32(217),
	143: uint32(215),
	144: uint32(212),
	145: uint32(209),
	146: uint32(207),
	147: uint32(204),
	148: uint32(202),
	149: uint32(199),
	150: uint32(197),
	151: uint32(194),
	152: uint32(192),
	153: uint32(190),
	154: uint32(187),
	155: uint32(185),
	156: uint32(182),
	157: uint32(180),
	158: uint32(178),
	159: uint32(175),
	160: uint32(173),
	161: uint32(171),
	162: uint32(168),
	163: uint32(166),
	164: uint32(164),
	165: uint32(162),
	166: uint32(159),
	167: uint32(157),
	168: uint32(155),
	169: uint32(153),
	170: uint32(151),
	171: uint32(149),
	172: uint32(146),
	173: uint32(144),
	174: uint32(142),
	175: uint32(140),
	176: uint32(138),
	177: uint32(136),
	178: uint32(134),
	179: uint32(132),
	180: uint32(130),
	181: uint32(128),
	182: uint32(126),
	183: uint32(123),
	184: uint32(121),
	185: uint32(119),
	186: uint32(117),
	187: uint32(115),
	188: uint32(114),
	189: uint32(112),
	190: uint32(110),
	191: uint32(108),
	192: uint32(106),
	193: uint32(104),
	194: uint32(102),
	195: uint32(100),
	196: uint32(98),
	197: uint32(96),
	198: uint32(94),
	199: uint32(93),
	200: uint32(91),
	201: uint32(89),
	202: uint32(87),
	203: uint32(85),
	204: uint32(83),
	205: uint32(82),
	206: uint32(80),
	207: uint32(78),
	208: uint32(76),
	209: uint32(74),
	210: uint32(73),
	211: uint32(71),
	212: uint32(69),
	213: uint32(67),
	214: uint32(66),
	215: uint32(64),
	216: uint32(62),
	217: uint32(61),
	218: uint32(59),
	219: uint32(57),
	220: uint32(55),
	221: uint32(54),
	222: uint32(52),
	223: uint32(50),
	224: uint32(49),
	225: uint32(47),
	226: uint32(46),
	227: uint32(44),
	228: uint32(42),
	229: uint32(41),
	230: uint32(39),
	231: uint32(37),
	232: uint32(36),
	233: uint32(34),
	234: uint32(33),
	235: uint32(31),
	236: uint32(30),
	237: uint32(28),
	238: uint32(26),
	239: uint32(25),
	240: uint32(23),
	241: uint32(22),
	242: uint32(20),
	243: uint32(19),
	244: uint32(17),
	245: uint32(16),
	246: uint32(14),
	247: uint32(13),
	248: uint32(11),
	249: uint32(10),
	250: uint32(8),
	251: uint32(7),
	252: uint32(5),
	253: uint32(4),
	254: uint32(2),
	255: uint32(1),
}

func ZSTD_getFSEMaxSymbolValue(tls *libc.TLS, ctable uintptr) (r uint32) {
	var maxSymbolValue U32
	var ptr, u16ptr uintptr
	_, _, _ = maxSymbolValue, ptr, u16ptr
	ptr = ctable
	u16ptr = ptr
	maxSymbolValue = uint32(MEM_read16(tls, u16ptr+uintptr(1)*2))
	return maxSymbolValue
}

// C documentation
//
//	/**
//	 * Returns true if we should use ncount=-1 else we should
//	 * use ncount=1 for low probability symbols instead.
//	 */
func ZSTD_useLowProbCount(tls *libc.TLS, nbSeq size_t) (r uint32) {
	/* Heuristic: This should cover most blocks <= 16K and
	 * start to fade out after 16K to about 32K depending on
	 * compressibility.
	 */
	return libc.BoolUint32(nbSeq >= uint64(2048))
}

// C documentation
//
//	/**
//	 * Returns the cost in bytes of encoding the normalized count header.
//	 * Returns an error if any of the helper functions return an error.
//	 */
func ZSTD_NCountCost(tls *libc.TLS, count uintptr, max uint32, nbSeq size_t, FSELog uint32) (r size_t) {
	bp := tls.Alloc(624)
	defer tls.Free(624)
	var err_code size_t
	var tableLog U32
	var _ /* norm at bp+512 */ [53]S16
	var _ /* wksp at bp+0 */ [512]BYTE
	_, _ = err_code, tableLog
	tableLog = FSE_optimalTableLog(tls, FSELog, nbSeq, max)
	err_code = FSE_normalizeCount(tls, bp+512, tableLog, count, nbSeq, max, ZSTD_useLowProbCount(tls, nbSeq))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return FSE_writeNCount(tls, bp, uint64(512), bp+512, max, tableLog)
}

// C documentation
//
//	/**
//	 * Returns the cost in bits of encoding the distribution described by count
//	 * using the entropy bound.
//	 */
func ZSTD_entropyCost(tls *libc.TLS, count uintptr, max uint32, total size_t) (r size_t) {
	var cost, norm, s uint32
	_, _, _ = cost, norm, s
	cost = uint32(0)
	s = uint32(0)
	for {
		if !(s <= max) {
			break
		}
		norm = uint32(uint64(libc.Uint32FromInt32(256)**(*uint32)(unsafe.Pointer(count + uintptr(s)*4))) / total)
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) != uint32(0) && norm == uint32(0) {
			norm = uint32(1)
		}
		cost = cost + *(*uint32)(unsafe.Pointer(count + uintptr(s)*4))*kInverseProbabilityLog256[norm]
		goto _1
	_1:
		;
		s = s + 1
	}
	return uint64(cost >> int32(8))
}

// C documentation
//
//	/**
//	 * Returns the cost in bits of encoding the distribution in count using ctable.
//	 * Returns an error if ctable cannot represent all the symbols in count.
//	 */
func ZSTD_fseBitCost(tls *libc.TLS, ctable uintptr, count uintptr, max uint32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var badCost, bitCost, kAccuracyLog, s, tableLog uint32
	var cost size_t
	var _ /* cstate at bp+0 */ FSE_CState_t
	_, _, _, _, _, _ = badCost, bitCost, cost, kAccuracyLog, s, tableLog
	kAccuracyLog = uint32(8)
	cost = uint64(0)
	FSE_initCState(tls, bp, ctable)
	if ZSTD_getFSEMaxSymbolValue(tls, ctable) < max {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	s = uint32(0)
	for {
		if !(s <= max) {
			break
		}
		tableLog = (*(*FSE_CState_t)(unsafe.Pointer(bp))).FstateLog
		badCost = (tableLog + uint32(1)) << kAccuracyLog
		bitCost = FSE_bitCost(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp))).FsymbolTT, tableLog, s, kAccuracyLog)
		if *(*uint32)(unsafe.Pointer(count + uintptr(s)*4)) == uint32(0) {
			goto _1
		}
		if bitCost >= badCost {
			return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		}
		cost = cost + uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4)))*uint64(bitCost)
		goto _1
	_1:
		;
		s = s + 1
	}
	return cost >> kAccuracyLog
}

// C documentation
//
//	/**
//	 * Returns the cost in bits of encoding the distribution in count using the
//	 * table described by norm. The max symbol support by norm is assumed >= max.
//	 * norm must be valid for every symbol with non-zero probability in count.
//	 */
func ZSTD_crossEntropyCost(tls *libc.TLS, norm uintptr, accuracyLog uint32, count uintptr, max uint32) (r size_t) {
	var cost size_t
	var norm256, normAcc, s, shift, v2 uint32
	_, _, _, _, _, _ = cost, norm256, normAcc, s, shift, v2
	shift = uint32(8) - accuracyLog
	cost = uint64(0)
	s = uint32(0)
	for {
		if !(s <= max) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2))) != -int32(1) {
			v2 = libc.Uint32FromInt16(*(*int16)(unsafe.Pointer(norm + uintptr(s)*2)))
		} else {
			v2 = uint32(1)
		}
		normAcc = v2
		norm256 = normAcc << shift
		cost = cost + uint64(*(*uint32)(unsafe.Pointer(count + uintptr(s)*4))*kInverseProbabilityLog256[norm256])
		goto _1
	_1:
		;
		s = s + 1
	}
	return cost >> int32(8)
}

func ZSTD_selectEncodingType(tls *libc.TLS, repeatMode uintptr, count uintptr, max uint32, mostFrequent size_t, nbSeq size_t, FSELog uint32, prevCTable uintptr, defaultNorm uintptr, defaultNormLog U32, isDefaultAllowed ZSTD_DefaultPolicy_e, strategy ZSTD_strategy) (r SymbolEncodingType_e) {
	var NCountCost, baseLog, basicCost, compressedCost, dynamicFse_nbSeq_min, mult, repeatCost, staticFse_nbSeq_max size_t
	var v1, v2 uint64
	_, _, _, _, _, _, _, _, _, _ = NCountCost, baseLog, basicCost, compressedCost, dynamicFse_nbSeq_min, mult, repeatCost, staticFse_nbSeq_max, v1, v2
	_ = libc.Uint64FromInt64(1)
	if mostFrequent == nbSeq {
		*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_none)
		if isDefaultAllowed != 0 && nbSeq <= uint64(2) {
			/* Prefer set_basic over set_rle when there are 2 or fewer symbols,
			 * since RLE uses 1 byte, but set_basic uses 5-6 bits per symbol.
			 * If basic encoding isn't possible, always choose RLE.
			 */
			return int32(set_basic)
		}
		return int32(set_rle)
	}
	if strategy < int32(ZSTD_lazy) {
		if isDefaultAllowed != 0 {
			staticFse_nbSeq_max = uint64(1000)
			mult = libc.Uint64FromInt32(int32(10) - strategy)
			baseLog = uint64(3)
			dynamicFse_nbSeq_min = libc.Uint64FromInt32(1) << defaultNormLog * mult >> baseLog /* 28-36 for offset, 56-72 for lengths */
			/* xx_DEFAULTNORMLOG */
			if *(*FSE_repeat)(unsafe.Pointer(repeatMode)) == int32(FSE_repeat_valid) && nbSeq < staticFse_nbSeq_max {
				return int32(set_repeat)
			}
			if nbSeq < dynamicFse_nbSeq_min || mostFrequent < nbSeq>>(defaultNormLog-libc.Uint32FromInt32(1)) {
				/* The format allows default tables to be repeated, but it isn't useful.
				 * When using simple heuristics to select encoding type, we don't want
				 * to confuse these tables with dictionaries. When running more careful
				 * analysis, we don't need to waste time checking both repeating tables
				 * and default tables.
				 */
				*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_none)
				return int32(set_basic)
			}
		}
	} else {
		if isDefaultAllowed != 0 {
			v1 = ZSTD_crossEntropyCost(tls, defaultNorm, defaultNormLog, count, max)
		} else {
			v1 = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		}
		basicCost = v1
		if *(*FSE_repeat)(unsafe.Pointer(repeatMode)) != int32(FSE_repeat_none) {
			v2 = ZSTD_fseBitCost(tls, prevCTable, count, max)
		} else {
			v2 = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		}
		repeatCost = v2
		NCountCost = ZSTD_NCountCost(tls, count, max, nbSeq, FSELog)
		compressedCost = NCountCost<<libc.Int32FromInt32(3) + ZSTD_entropyCost(tls, count, max, nbSeq)
		if isDefaultAllowed != 0 {
		}
		if basicCost <= repeatCost && basicCost <= compressedCost {
			*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_none)
			return int32(set_basic)
		}
		if repeatCost <= compressedCost {
			return int32(set_repeat)
		}
	}
	*(*FSE_repeat)(unsafe.Pointer(repeatMode)) = int32(FSE_repeat_check)
	return int32(set_compressed)
}

type ZSTD_BuildCTableWksp = struct {
	Fnorm [53]S16
	Fwksp [285]U32
}

func ZSTD_buildCTable(tls *libc.TLS, dst uintptr, dstCapacity size_t, nextCTable uintptr, FSELog U32, type1 SymbolEncodingType_e, count uintptr, max U32, codeTable uintptr, nbSeq size_t, defaultNorm uintptr, defaultNormLog U32, defaultMax U32, prevCTable uintptr, prevCTableSize size_t, entropyWorkspace uintptr, entropyWorkspaceSize size_t) (r size_t) {
	var NCountSize, err_code, err_code1, err_code2, err_code3, err_code4, nbSeq_1 size_t
	var oend, op, wksp uintptr
	var tableLog U32
	_, _, _, _, _, _, _, _, _, _, _ = NCountSize, err_code, err_code1, err_code2, err_code3, err_code4, nbSeq_1, oend, op, tableLog, wksp
	op = dst
	oend = op + uintptr(dstCapacity)
	switch type1 {
	case int32(set_rle):
		goto _1
	case int32(set_repeat):
		goto _2
	case int32(set_basic):
		goto _3
	case int32(set_compressed):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
_9:
	;
	err_code = FSE_buildCTable_rle(tls, nextCTable, uint8(max))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	goto _8
_8:
	;
	if 0 != 0 {
		goto _9
	}
	goto _7
_7:
	;
	if dstCapacity == uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1467, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	*(*BYTE)(unsafe.Pointer(op)) = *(*BYTE)(unsafe.Pointer(codeTable))
	return uint64(1)
_2:
	;
	libc.Xmemcpy(tls, nextCTable, prevCTable, prevCTableSize)
	return uint64(0)
_3:
	;
	err_code1 = FSE_buildCTable_wksp(tls, nextCTable, defaultNorm, defaultMax, defaultNormLog, entropyWorkspace, entropyWorkspaceSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	} /* note : could be pre-calculated */
	return uint64(0)
_4:
	;
	wksp = entropyWorkspace
	nbSeq_1 = nbSeq
	tableLog = FSE_optimalTableLog(tls, FSELog, nbSeq, max)
	if *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(codeTable + uintptr(nbSeq-uint64(1)))))*4)) > uint32(1) {
		*(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(codeTable + uintptr(nbSeq-uint64(1)))))*4)) = *(*uint32)(unsafe.Pointer(count + uintptr(*(*BYTE)(unsafe.Pointer(codeTable + uintptr(nbSeq-uint64(1)))))*4)) - 1
		nbSeq_1 = nbSeq_1 - 1
	}
	_ = entropyWorkspaceSize
	err_code2 = FSE_normalizeCount(tls, wksp, tableLog, count, nbSeq_1, max, ZSTD_useLowProbCount(tls, nbSeq_1))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1484, 0)
		}
		return err_code2
	}
	NCountSize = FSE_writeNCount(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), wksp, max, tableLog) /* overflow protected */
	err_code3 = NCountSize
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1510, 0)
		}
		return err_code3
	}
	err_code4 = FSE_buildCTable_wksp(tls, nextCTable, wksp, max, tableLog, wksp+108, uint64(1140))
	if ERR_isError(tls, err_code4) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1533, 0)
		}
		return err_code4
	}
	return NCountSize
_5:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1561, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
_6:
	;
	return r
}

func ZSTD_encodeSequences_body(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	var extraBits, extraBits1, v1 uint32
	var llBits, mlBits, ofBits, ofBits1 U32
	var llCode, mlCode, ofCode BYTE
	var n, streamSize size_t
	var v2, v3 int32
	var _ /* blockStream at bp+0 */ BIT_CStream_t
	var _ /* stateLitLength at bp+104 */ FSE_CState_t
	var _ /* stateMatchLength at bp+40 */ FSE_CState_t
	var _ /* stateOffsetBits at bp+72 */ FSE_CState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = extraBits, extraBits1, llBits, llCode, mlBits, mlCode, n, ofBits, ofBits1, ofCode, streamSize, v1, v2, v3
	if ERR_isError(tls, BIT_initCStream(tls, bp, dst, dstCapacity)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1581, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* first symbols */
	FSE_initCState2(tls, bp+40, CTable_MatchLength, uint32(*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(nbSeq-uint64(1))))))
	FSE_initCState2(tls, bp+72, CTable_OffsetBits, uint32(*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(nbSeq-uint64(1))))))
	FSE_initCState2(tls, bp+104, CTable_LitLength, uint32(*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(nbSeq-uint64(1))))))
	BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FlitLength), uint32(LL_bits[*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(nbSeq-uint64(1))))]))
	if MEM_32bits(tls) != 0 {
		BIT_flushBits(tls, bp)
	}
	BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FmlBase), uint32(ML_bits[*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(nbSeq-uint64(1))))]))
	if MEM_32bits(tls) != 0 {
		BIT_flushBits(tls, bp)
	}
	if longOffsets != 0 {
		ofBits = uint32(*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(nbSeq-uint64(1)))))
		if MEM_32bits(tls) != 0 {
			v2 = int32(STREAM_ACCUMULATOR_MIN_32)
		} else {
			v2 = int32(STREAM_ACCUMULATOR_MIN_64)
		}
		if ofBits < libc.Uint32FromInt32(v2)-uint32(1) {
			v1 = ofBits
		} else {
			if MEM_32bits(tls) != 0 {
				v3 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v3 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
			v1 = libc.Uint32FromInt32(v3) - uint32(1)
		}
		extraBits = ofBits - v1
		if extraBits != 0 {
			BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FoffBase), extraBits)
			BIT_flushBits(tls, bp)
		}
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FoffBase>>extraBits), ofBits-extraBits)
	} else {
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(nbSeq-uint64(1))*8))).FoffBase), uint32(*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(nbSeq-uint64(1))))))
	}
	BIT_flushBits(tls, bp)
	n = nbSeq - uint64(2)
	for {
		if !(n < nbSeq) {
			break
		} /* intentional underflow */
		llCode = *(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(n)))
		ofCode = *(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(n)))
		mlCode = *(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(n)))
		llBits = uint32(LL_bits[llCode])
		ofBits1 = uint32(ofCode)
		mlBits = uint32(ML_bits[mlCode])
		/* 32b*/ /* 64b*/
		/* (7)*/                                         /* (7)*/
		FSE_encodeSymbol(tls, bp, bp+72, uint32(ofCode)) /* 15 */ /* 15 */
		FSE_encodeSymbol(tls, bp, bp+40, uint32(mlCode)) /* 24 */ /* 24 */
		if MEM_32bits(tls) != 0 {
			BIT_flushBits(tls, bp)
		} /* (7)*/
		FSE_encodeSymbol(tls, bp, bp+104, uint32(llCode)) /* 16 */ /* 33 */
		if MEM_32bits(tls) != 0 || ofBits1+mlBits+llBits >= libc.Uint32FromInt32(libc.Int32FromInt32(64)-libc.Int32FromInt32(7)-(libc.Int32FromInt32(LLFSELog)+libc.Int32FromInt32(MLFSELog)+libc.Int32FromInt32(OffFSELog))) {
			BIT_flushBits(tls, bp)
		} /* (7)*/
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FlitLength), llBits)
		if MEM_32bits(tls) != 0 && llBits+mlBits > uint32(24) {
			BIT_flushBits(tls, bp)
		}
		BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FmlBase), mlBits)
		if MEM_32bits(tls) != 0 || ofBits1+mlBits+llBits > uint32(56) {
			BIT_flushBits(tls, bp)
		}
		if longOffsets != 0 {
			if MEM_32bits(tls) != 0 {
				v2 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v2 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
			if ofBits1 < libc.Uint32FromInt32(v2)-uint32(1) {
				v1 = ofBits1
			} else {
				if MEM_32bits(tls) != 0 {
					v3 = int32(STREAM_ACCUMULATOR_MIN_32)
				} else {
					v3 = int32(STREAM_ACCUMULATOR_MIN_64)
				}
				v1 = libc.Uint32FromInt32(v3) - uint32(1)
			}
			extraBits1 = ofBits1 - v1
			if extraBits1 != 0 {
				BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FoffBase), extraBits1)
				BIT_flushBits(tls, bp) /* (7)*/
			}
			BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FoffBase>>extraBits1), ofBits1-extraBits1) /* 31 */
		} else {
			BIT_addBits(tls, bp, uint64((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(n)*8))).FoffBase), ofBits1) /* 31 */
		}
		BIT_flushBits(tls, bp) /* (7)*/
		goto _4
	_4:
		;
		n = n - 1
	}
	FSE_flushCState(tls, bp, bp+40)
	FSE_flushCState(tls, bp, bp+72)
	FSE_flushCState(tls, bp, bp+104)
	streamSize = BIT_closeCStream(tls, bp)
	if streamSize == uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1467, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	return streamSize
	return r
}

func ZSTD_encodeSequences_default(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32) (r size_t) {
	return ZSTD_encodeSequences_body(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
}

func ZSTD_encodeSequences_bmi2(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32) (r size_t) {
	return ZSTD_encodeSequences_body(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
}

func ZSTD_encodeSequences(tls *libc.TLS, dst uintptr, dstCapacity size_t, CTable_MatchLength uintptr, mlCodeTable uintptr, CTable_OffsetBits uintptr, ofCodeTable uintptr, CTable_LitLength uintptr, llCodeTable uintptr, sequences uintptr, nbSeq size_t, longOffsets int32, bmi2 int32) (r size_t) {
	if bmi2 != 0 {
		return ZSTD_encodeSequences_bmi2(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
	}
	_ = bmi2
	return ZSTD_encodeSequences_default(tls, dst, dstCapacity, CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets)
}

/**** ended inlining zstd_compress_superblock.h ****/

/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_compress_sequences.h ****/
/**** skipping file: zstd_compress_literals.h ****/

// C documentation
//
//	/** ZSTD_compressSubBlock_literal() :
//	 *  Compresses literals section for a sub-block.
//	 *  When we have to write the Huffman table we will sometimes choose a header
//	 *  size larger than necessary. This is because we have to pick the header size
//	 *  before we know the table size + compressed size, so we have a bound on the
//	 *  table size. If we guessed incorrectly, we fall back to uncompressed literals.
//	 *
//	 *  We write the header when writeEntropy=1 and set entropyWritten=1 when we succeeded
//	 *  in writing the header, otherwise it is set to 0.
//	 *
//	 *  hufMetadata->hType has literals block type info.
//	 *      If it is set_basic, all sub-blocks literals section will be Raw_Literals_Block.
//	 *      If it is set_rle, all sub-blocks literals section will be RLE_Literals_Block.
//	 *      If it is set_compressed, first sub-block's literals section will be Compressed_Literals_Block
//	 *      If it is set_compressed, first sub-block's literals section will be Treeless_Literals_Block
//	 *      and the following sub-blocks' literals sections will be Treeless_Literals_Block.
//	 *  @return : compressed size of literals section of a sub-block
//	 *            Or 0 if unable to compress.
//	 *            Or error code */
func ZSTD_compressSubBlock_literal(tls *libc.TLS, hufTable uintptr, hufMetadata uintptr, literals uintptr, litSize size_t, dst uintptr, dstSize size_t, bmi2 int32, writeEntropy int32, entropyWritten uintptr) (r size_t) {
	var cLitSize, cSize, header, lhSize size_t
	var flags, v1, v2 int32
	var hType SymbolEncodingType_e
	var lhc, lhc1, lhc2, singleStream U32
	var oend, op, ostart uintptr
	var v4 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cLitSize, cSize, flags, hType, header, lhSize, lhc, lhc1, lhc2, oend, op, ostart, singleStream, v1, v2, v4
	if writeEntropy != 0 {
		v1 = int32(200)
	} else {
		v1 = 0
	}
	header = libc.Uint64FromInt32(v1)
	lhSize = libc.Uint64FromInt32(int32(3) + libc.BoolInt32(litSize >= libc.Uint64FromInt32(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))-header) + libc.BoolInt32(litSize >= libc.Uint64FromInt32(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))-header))
	ostart = dst
	oend = ostart + uintptr(dstSize)
	op = ostart + uintptr(lhSize)
	singleStream = libc.BoolUint32(lhSize == uint64(3))
	if writeEntropy != 0 {
		v2 = (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType
	} else {
		v2 = int32(set_repeat)
	}
	hType = v2
	cLitSize = uint64(0)
	*(*int32)(unsafe.Pointer(entropyWritten)) = 0
	if litSize == uint64(0) || (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_basic) {
		return ZSTD_noCompressLiterals(tls, dst, dstSize, literals, litSize)
	} else {
		if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_rle) {
			return ZSTD_compressRleLiteralsBlock(tls, dst, dstSize, literals, litSize)
		}
	}
	if writeEntropy != 0 && (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_compressed) {
		libc.Xmemcpy(tls, op, hufMetadata+4, (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize)
		op = op + uintptr((*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize)
		cLitSize = cLitSize + (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize
	}
	if bmi2 != 0 {
		v1 = int32(HUF_flags_bmi2)
	} else {
		v1 = 0
	}
	flags = v1
	if singleStream != 0 {
		v4 = HUF_compress1X_usingCTable(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), literals, litSize, hufTable, flags)
	} else {
		v4 = HUF_compress4X_usingCTable(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), literals, litSize, hufTable, flags)
	}
	cSize = v4
	op = op + uintptr(cSize)
	cLitSize = cLitSize + cSize
	if cSize == uint64(0) || ERR_isError(tls, cSize) != 0 {
		return uint64(0)
	}
	/* If we expand and we aren't writing a header then emit uncompressed */
	if !(writeEntropy != 0) && cLitSize >= litSize {
		return ZSTD_noCompressLiterals(tls, dst, dstSize, literals, litSize)
	}
	/* If we are writing headers then allow expansion that doesn't change our header size. */
	if lhSize < libc.Uint64FromInt32(libc.Int32FromInt32(3)+libc.BoolInt32(cLitSize >= libc.Uint64FromInt32(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))))+libc.BoolInt32(cLitSize >= libc.Uint64FromInt32(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))))) {
		return ZSTD_noCompressLiterals(tls, dst, dstSize, literals, litSize)
	}
	/* Build header */
	switch lhSize {
	case uint64(3): /* 2 - 2 - 10 - 10 */
		lhc = libc.Uint32FromInt32(hType) + libc.BoolUint32(!(singleStream != 0))<<libc.Int32FromInt32(2) + uint32(litSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(14)
		MEM_writeLE24(tls, ostart, lhc)
	case uint64(4): /* 2 - 2 - 14 - 14 */
		lhc1 = libc.Uint32FromInt32(hType+libc.Int32FromInt32(2)<<libc.Int32FromInt32(2)) + uint32(litSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(18)
		MEM_writeLE32(tls, ostart, lhc1)
	case uint64(5): /* 2 - 2 - 18 - 18 */
		lhc2 = libc.Uint32FromInt32(hType+libc.Int32FromInt32(3)<<libc.Int32FromInt32(2)) + uint32(litSize)<<libc.Int32FromInt32(4) + uint32(cLitSize)<<libc.Int32FromInt32(22)
		MEM_writeLE32(tls, ostart, lhc2)
		*(*BYTE)(unsafe.Pointer(ostart + 4)) = uint8(cLitSize >> libc.Int32FromInt32(10))
	default: /* not possible : lhSize is {3,4,5} */
	}
	*(*int32)(unsafe.Pointer(entropyWritten)) = int32(1)
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_seqDecompressedSize(tls *libc.TLS, seqStore uintptr, sequences uintptr, nbSeqs size_t, litSize size_t, lastSubBlock int32) (r size_t) {
	var litLengthSum, matchLengthSum, n size_t
	var seqLen ZSTD_SequenceLength
	_, _, _, _ = litLengthSum, matchLengthSum, n, seqLen
	matchLengthSum = uint64(0)
	litLengthSum = uint64(0)
	n = uint64(0)
	for {
		if !(n < nbSeqs) {
			break
		}
		seqLen = ZSTD_getSequenceLength(tls, seqStore, sequences+uintptr(n)*8)
		litLengthSum = litLengthSum + uint64(seqLen.FlitLength)
		matchLengthSum = matchLengthSum + uint64(seqLen.FmatchLength)
		goto _1
	_1:
		;
		n = n + 1
	}
	if !(lastSubBlock != 0) {
	} else {
	}
	_ = litLengthSum
	return matchLengthSum + litSize
}

// C documentation
//
//	/** ZSTD_compressSubBlock_sequences() :
//	 *  Compresses sequences section for a sub-block.
//	 *  fseMetadata->llType, fseMetadata->ofType, and fseMetadata->mlType have
//	 *  symbol compression modes for the super-block.
//	 *  The first successfully compressed block will have these in its header.
//	 *  We set entropyWritten=1 when we succeed in compressing the sequences.
//	 *  The following sub-blocks will always have repeat mode.
//	 *  @return : compressed size of sequences section of a sub-block
//	 *            Or 0 if it is unable to compress
//	 *            Or error code. */
func ZSTD_compressSubBlock_sequences(tls *libc.TLS, fseTables uintptr, fseMetadata uintptr, sequences uintptr, nbSeq size_t, llCode uintptr, mlCode uintptr, ofCode uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, bmi2 int32, writeEntropy int32, entropyWritten uintptr) (r size_t) {
	var LLtype, MLtype, Offtype, repeat U32
	var bitstreamSize, err_code size_t
	var longOffsets, v1 int32
	var oend, op, ostart, seqHead, v2 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _ = LLtype, MLtype, Offtype, bitstreamSize, err_code, longOffsets, oend, op, ostart, repeat, seqHead, v1, v2
	if MEM_32bits(tls) != 0 {
		v1 = int32(STREAM_ACCUMULATOR_MIN_32)
	} else {
		v1 = int32(STREAM_ACCUMULATOR_MIN_64)
	}
	longOffsets = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.FwindowLog > libc.Uint32FromInt32(v1))
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart
	*(*int32)(unsafe.Pointer(entropyWritten)) = 0
	/* Sequences Header */
	if int64(oend)-int64(op) < int64(libc.Int32FromInt32(3)+libc.Int32FromInt32(1)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbSeq < uint64(128) {
		v2 = op
		op = op + 1
		*(*BYTE)(unsafe.Pointer(v2)) = uint8(nbSeq)
	} else {
		if nbSeq < uint64(LONGNBSEQ) {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(nbSeq>>libc.Int32FromInt32(8) + libc.Uint64FromInt32(0x80))
			*(*BYTE)(unsafe.Pointer(op + 1)) = uint8(nbSeq)
			op = op + uintptr(2)
		} else {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(0xFF)
			MEM_writeLE16(tls, op+uintptr(1), uint16(nbSeq-libc.Uint64FromInt32(LONGNBSEQ)))
			op = op + uintptr(3)
		}
	}
	if nbSeq == uint64(0) {
		return libc.Uint64FromInt64(int64(op) - int64(ostart))
	}
	/* seqHead : flags for FSE encoding type */
	v2 = op
	op = op + 1
	seqHead = v2
	if writeEntropy != 0 {
		LLtype = libc.Uint32FromInt32((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType)
		Offtype = libc.Uint32FromInt32((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType)
		MLtype = libc.Uint32FromInt32((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType)
		*(*BYTE)(unsafe.Pointer(seqHead)) = uint8(LLtype<<libc.Int32FromInt32(6) + Offtype<<libc.Int32FromInt32(4) + MLtype<<libc.Int32FromInt32(2))
		libc.Xmemcpy(tls, op, fseMetadata+12, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize)
		op = op + uintptr((*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize)
	} else {
		repeat = uint32(set_repeat)
		*(*BYTE)(unsafe.Pointer(seqHead)) = uint8(repeat<<libc.Int32FromInt32(6) + repeat<<libc.Int32FromInt32(4) + repeat<<libc.Int32FromInt32(2))
	}
	bitstreamSize = ZSTD_encodeSequences(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), fseTables+772, mlCode, fseTables, ofCode, fseTables+2224, llCode, sequences, nbSeq, longOffsets, bmi2)
	err_code = bitstreamSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1608, 0)
		}
		return err_code
	}
	op = op + uintptr(bitstreamSize)
	/* zstd versions <= 1.3.4 mistakenly report corruption when
	 * FSE_readNCount() receives a buffer < 4 bytes.
	 * Fixed by https://github.com/facebook/zstd/pull/1146.
	 * This can happen when the last set_compressed table present is 2
	 * bytes and the bitstream is only one byte.
	 * In this exceedingly rare case, we will simply emit an uncompressed
	 * block, since it isn't worth optimizing.
	 */
	if writeEntropy != 0 && (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FlastCountSize != 0 && (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FlastCountSize+bitstreamSize < uint64(4) {
		/* NCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */
		return uint64(0)
	}
	/* zstd versions <= 1.4.0 mistakenly report error when
	 * sequences section body size is less than 3 bytes.
	 * Fixed by https://github.com/facebook/zstd/pull/1664.
	 * This can happen when the previous sequences section block is compressed
	 * with rle mode and the current block's sequences section is compressed
	 * with repeat mode where sequences section body size can be 1 byte.
	 */
	if int64(op)-int64(seqHead) < int64(4) {
		return uint64(0)
	}
	*(*int32)(unsafe.Pointer(entropyWritten)) = int32(1)
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

// C documentation
//
//	/** ZSTD_compressSubBlock() :
//	 *  Compresses a single sub-block.
//	 *  @return : compressed size of the sub-block
//	 *            Or 0 if it failed to compress. */
func ZSTD_compressSubBlock(tls *libc.TLS, entropy uintptr, entropyMetadata uintptr, sequences uintptr, nbSeq size_t, literals uintptr, litSize size_t, llCode uintptr, mlCode uintptr, ofCode uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, bmi2 int32, writeLitEntropy int32, writeSeqEntropy int32, litEntropyWritten uintptr, seqEntropyWritten uintptr, lastBlock U32) (r size_t) {
	var cBlockHeader24 U32
	var cLitSize, cSeqSize, cSize, err_code, err_code1 size_t
	var oend, op, ostart uintptr
	_, _, _, _, _, _, _, _, _ = cBlockHeader24, cLitSize, cSeqSize, cSize, err_code, err_code1, oend, op, ostart
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart + uintptr(ZSTD_blockHeaderSize)
	cLitSize = ZSTD_compressSubBlock_literal(tls, entropy, entropyMetadata, literals, litSize, op, libc.Uint64FromInt64(int64(oend)-int64(op)), bmi2, writeLitEntropy, litEntropyWritten)
	err_code = cLitSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1636, 0)
		}
		return err_code
	}
	if cLitSize == uint64(0) {
		return uint64(0)
	}
	op = op + uintptr(cLitSize)
	cSeqSize = ZSTD_compressSubBlock_sequences(tls, entropy+2064, entropyMetadata+144, sequences, nbSeq, llCode, mlCode, ofCode, cctxParams, op, libc.Uint64FromInt64(int64(oend)-int64(op)), bmi2, writeSeqEntropy, seqEntropyWritten)
	err_code1 = cSeqSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1673, 0)
		}
		return err_code1
	}
	if cSeqSize == uint64(0) {
		return uint64(0)
	}
	op = op + uintptr(cSeqSize)
	/* Write block header */
	cSize = libc.Uint64FromInt64(int64(op)-int64(ostart)) - ZSTD_blockHeaderSize
	cBlockHeader24 = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(cSize<<libc.Int32FromInt32(3))
	MEM_writeLE24(tls, ostart, cBlockHeader24)
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_estimateSubBlockSize_literal(tls *libc.TLS, literals uintptr, litSize size_t, huf uintptr, hufMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cLitSizeEstimate, largest, literalSectionHeaderSize size_t
	var countWksp uintptr
	var _ /* maxSymbolValue at bp+0 */ uint32
	_, _, _, _ = cLitSizeEstimate, countWksp, largest, literalSectionHeaderSize
	countWksp = workspace
	*(*uint32)(unsafe.Pointer(bp)) = uint32(255)
	literalSectionHeaderSize = uint64(3) /* Use hard coded size of 3 bytes */
	if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_basic) {
		return litSize
	} else {
		if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_rle) {
			return uint64(1)
		} else {
			if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_compressed) || (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_repeat) {
				largest = HIST_count_wksp(tls, countWksp, bp, literals, litSize, workspace, wkspSize)
				if ZSTD_isError(tls, largest) != 0 {
					return litSize
				}
				cLitSizeEstimate = HUF_estimateCompressedSize(tls, huf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
				if writeEntropy != 0 {
					cLitSizeEstimate = cLitSizeEstimate + (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize
				}
				return cLitSizeEstimate + literalSectionHeaderSize
			}
		}
	}
	/* impossible */
	return uint64(0)
}

func ZSTD_estimateSubBlockSize_symbolType(tls *libc.TLS, type1 SymbolEncodingType_e, codeTable uintptr, maxCode uint32, nbSeq size_t, fseCTable uintptr, additionalBits uintptr, defaultNorm uintptr, defaultNormLog U32, defaultMax U32, workspace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cSymbolTypeSizeEstimateInBits size_t
	var countWksp, ctEnd, ctStart, ctp uintptr
	var v1 uint64
	var _ /* max at bp+0 */ uint32
	_, _, _, _, _, _ = cSymbolTypeSizeEstimateInBits, countWksp, ctEnd, ctStart, ctp, v1
	countWksp = workspace
	ctp = codeTable
	ctStart = ctp
	ctEnd = ctStart + uintptr(nbSeq)
	cSymbolTypeSizeEstimateInBits = uint64(0)
	*(*uint32)(unsafe.Pointer(bp)) = maxCode
	HIST_countFast_wksp(tls, countWksp, bp, codeTable, nbSeq, workspace, wkspSize) /* can't fail */
	if type1 == int32(set_basic) {
		/* We selected this encoding type, so it must be valid. */
		if *(*uint32)(unsafe.Pointer(bp)) <= defaultMax {
			v1 = ZSTD_crossEntropyCost(tls, defaultNorm, defaultNormLog, countWksp, *(*uint32)(unsafe.Pointer(bp)))
		} else {
			v1 = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		}
		cSymbolTypeSizeEstimateInBits = v1
	} else {
		if type1 == int32(set_rle) {
			cSymbolTypeSizeEstimateInBits = uint64(0)
		} else {
			if type1 == int32(set_compressed) || type1 == int32(set_repeat) {
				cSymbolTypeSizeEstimateInBits = ZSTD_fseBitCost(tls, fseCTable, countWksp, *(*uint32)(unsafe.Pointer(bp)))
			}
		}
	}
	if ZSTD_isError(tls, cSymbolTypeSizeEstimateInBits) != 0 {
		return nbSeq * uint64(10)
	}
	for ctp < ctEnd {
		if additionalBits != 0 {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*U8)(unsafe.Pointer(additionalBits + uintptr(*(*BYTE)(unsafe.Pointer(ctp))))))
		} else {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*BYTE)(unsafe.Pointer(ctp)))
		} /* for offset, offset code is also the number of additional bits */
		ctp = ctp + 1
	}
	return cSymbolTypeSizeEstimateInBits / uint64(8)
}

func ZSTD_estimateSubBlockSize_sequences(tls *libc.TLS, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, fseTables uintptr, fseMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	var cSeqSizeEstimate, sequencesSectionHeaderSize size_t
	_, _ = cSeqSizeEstimate, sequencesSectionHeaderSize
	sequencesSectionHeaderSize = uint64(3) /* Use hard coded size of 3 bytes */
	cSeqSizeEstimate = uint64(0)
	if nbSeq == uint64(0) {
		return sequencesSectionHeaderSize
	}
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateSubBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType, ofCodeTable, uint32(MaxOff), nbSeq, fseTables, libc.UintptrFromInt32(0), uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, uint32(DefaultMaxOff), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateSubBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType, llCodeTable, uint32(MaxLL), nbSeq, fseTables+2224, uintptr(unsafe.Pointer(&LL_bits)), uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, uint32(MaxLL), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateSubBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType, mlCodeTable, uint32(MaxML), nbSeq, fseTables+772, uintptr(unsafe.Pointer(&ML_bits)), uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, uint32(MaxML), workspace, wkspSize)
	if writeEntropy != 0 {
		cSeqSizeEstimate = cSeqSizeEstimate + (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize
	}
	return cSeqSizeEstimate + sequencesSectionHeaderSize
}

type EstimatedBlockSize = struct {
	FestLitSize   size_t
	FestBlockSize size_t
}

func ZSTD_estimateSubBlockSize(tls *libc.TLS, literals uintptr, litSize size_t, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, entropy uintptr, entropyMetadata uintptr, workspace uintptr, wkspSize size_t, writeLitEntropy int32, writeSeqEntropy int32) (r EstimatedBlockSize) {
	var ebs EstimatedBlockSize
	_ = ebs
	ebs.FestLitSize = ZSTD_estimateSubBlockSize_literal(tls, literals, litSize, entropy, entropyMetadata, workspace, wkspSize, writeLitEntropy)
	ebs.FestBlockSize = ZSTD_estimateSubBlockSize_sequences(tls, ofCodeTable, llCodeTable, mlCodeTable, nbSeq, entropy+2064, entropyMetadata+144, workspace, wkspSize, writeSeqEntropy)
	ebs.FestBlockSize += ebs.FestLitSize + ZSTD_blockHeaderSize
	return ebs
}

func ZSTD_needSequenceEntropyTables(tls *libc.TLS, fseMetadata uintptr) (r int32) {
	if (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType == int32(set_compressed) || (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType == int32(set_rle) {
		return int32(1)
	}
	if (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType == int32(set_compressed) || (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType == int32(set_rle) {
		return int32(1)
	}
	if (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType == int32(set_compressed) || (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType == int32(set_rle) {
		return int32(1)
	}
	return 0
}

func countLiterals(tls *libc.TLS, seqStore uintptr, sp uintptr, seqCount size_t) (r size_t) {
	var n, total size_t
	_, _ = n, total
	total = uint64(0)
	n = uint64(0)
	for {
		if !(n < seqCount) {
			break
		}
		total = total + uint64(ZSTD_getSequenceLength(tls, seqStore, sp+uintptr(n)*8).FlitLength)
		goto _1
	_1:
		;
		n = n + 1
	}
	return total
}

func sizeBlockSequences(tls *libc.TLS, sp uintptr, nbSeqs size_t, targetBudget size_t, avgLitCost size_t, avgSeqCost size_t, firstSubBlock int32) (r size_t) {
	var budget, currentCost, headerSize, inSize, n size_t
	_, _, _, _, _ = budget, currentCost, headerSize, inSize, n
	budget = uint64(0)
	inSize = uint64(0)
	/* entropy headers */
	headerSize = libc.Uint64FromInt32(firstSubBlock) * uint64(120) * uint64(BYTESCALE) /* generous estimate */
	budget = budget + headerSize
	/* first sequence => at least one sequence*/
	budget = budget + (uint64((*(*SeqDef)(unsafe.Pointer(sp))).FlitLength)*avgLitCost + avgSeqCost)
	if budget > targetBudget {
		return uint64(1)
	}
	inSize = libc.Uint64FromInt32(libc.Int32FromUint16((*(*SeqDef)(unsafe.Pointer(sp))).FlitLength) + (libc.Int32FromUint16((*(*SeqDef)(unsafe.Pointer(sp))).FmlBase) + int32(MINMATCH)))
	/* loop over sequences */
	n = uint64(1)
	for {
		if !(n < nbSeqs) {
			break
		}
		currentCost = uint64((*(*SeqDef)(unsafe.Pointer(sp + uintptr(n)*8))).FlitLength)*avgLitCost + avgSeqCost
		budget = budget + currentCost
		inSize = inSize + libc.Uint64FromInt32(libc.Int32FromUint16((*(*SeqDef)(unsafe.Pointer(sp + uintptr(n)*8))).FlitLength)+(libc.Int32FromUint16((*(*SeqDef)(unsafe.Pointer(sp + uintptr(n)*8))).FmlBase)+int32(MINMATCH)))
		/* stop when sub-block budget is reached */
		if budget > targetBudget && budget < inSize*uint64(BYTESCALE) {
			break
		}
		goto _1
	_1:
		;
		n = n + 1
	}
	return n
}

// C documentation
//
//	/** ZSTD_compressSubBlock_multi() :
//	 *  Breaks super-block into multiple sub-blocks and compresses them.
//	 *  Entropy will be written into the first block.
//	 *  The following blocks use repeat_mode to compress.
//	 *  Sub-blocks are all compressed, except the last one when beneficial.
//	 *  @return : compressed size of the super block (which features multiple ZSTD blocks)
//	 *            or 0 if it failed to compress. */
func ZSTD_compressSubBlock_multi(tls *libc.TLS, seqStorePtr uintptr, prevCBlock uintptr, nextCBlock uintptr, entropyMetadata uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, bmi2 int32, lastBlock U32, workspace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var avgBlockBudget, avgLitCost, avgSeqCost, blockBudgetSupp, cSize, cSize1, cSize2, decompressedSize, decompressedSize1, err_code, err_code1, err_code2, litSize, litSize1, minTarget, n, nbLiterals, nbSeqs, nbSubBlocks, rSize, seqCount, seqCount1, targetCBlockSize size_t
	var ebs EstimatedBlockSize
	var iend, ip, lend, llCodePtr, lp, lstart, mlCodePtr, oend, ofCodePtr, op, ostart, send, seq, sp, sstart uintptr
	var writeLitEntropy, writeSeqEntropy int32
	var v1, v2 uint64
	var _ /* litEntropyWritten at bp+0 */ int32
	var _ /* litEntropyWritten at bp+8 */ int32
	var _ /* rep at bp+16 */ Repcodes_t
	var _ /* seqEntropyWritten at bp+12 */ int32
	var _ /* seqEntropyWritten at bp+4 */ int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = avgBlockBudget, avgLitCost, avgSeqCost, blockBudgetSupp, cSize, cSize1, cSize2, decompressedSize, decompressedSize1, ebs, err_code, err_code1, err_code2, iend, ip, lend, litSize, litSize1, llCodePtr, lp, lstart, minTarget, mlCodePtr, n, nbLiterals, nbSeqs, nbSubBlocks, oend, ofCodePtr, op, ostart, rSize, send, seq, seqCount, seqCount1, sp, sstart, targetCBlockSize, writeLitEntropy, writeSeqEntropy, v1, v2
	sstart = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
	send = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences
	sp = sstart /* tracks progresses within seqStorePtr->sequences */
	nbSeqs = libc.Uint64FromInt64((int64(send) - int64(sstart)) / 8)
	lstart = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart
	lend = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit
	lp = lstart
	nbLiterals = libc.Uint64FromInt64(int64(lend) - int64(lstart))
	ip = src
	iend = ip + uintptr(srcSize)
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart
	llCodePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	mlCodePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	ofCodePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	minTarget = uint64(ZSTD_TARGETCBLOCKSIZE_MIN)
	if minTarget > (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FtargetCBlockSize {
		v1 = minTarget
	} else {
		v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FtargetCBlockSize
	} /* enforce minimum size, to reduce undesirable side effects */
	targetCBlockSize = v1
	writeLitEntropy = libc.BoolInt32((*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhType == int32(set_compressed))
	writeSeqEntropy = int32(1)
	/* let's start by a general estimation for the full block */
	if nbSeqs > uint64(0) {
		ebs = ZSTD_estimateSubBlockSize(tls, lp, nbLiterals, ofCodePtr, llCodePtr, mlCodePtr, nbSeqs, nextCBlock, entropyMetadata, workspace, wkspSize, writeLitEntropy, writeSeqEntropy)
		if nbLiterals != 0 {
			v1 = ebs.FestLitSize * uint64(BYTESCALE) / nbLiterals
		} else {
			v1 = uint64(BYTESCALE)
		}
		/* quick estimation */
		avgLitCost = v1
		avgSeqCost = (ebs.FestBlockSize - ebs.FestLitSize) * uint64(BYTESCALE) / nbSeqs
		if (ebs.FestBlockSize+targetCBlockSize/uint64(2))/targetCBlockSize > libc.Uint64FromInt32(libc.Int32FromInt32(1)) {
			v2 = (ebs.FestBlockSize + targetCBlockSize/uint64(2)) / targetCBlockSize
		} else {
			v2 = libc.Uint64FromInt32(libc.Int32FromInt32(1))
		}
		nbSubBlocks = v2
		blockBudgetSupp = uint64(0)
		avgBlockBudget = ebs.FestBlockSize * uint64(BYTESCALE) / nbSubBlocks
		/* simplification: if estimates states that the full superblock doesn't compress, just bail out immediately
		 * this will result in the production of a single uncompressed block covering @srcSize.*/
		if ebs.FestBlockSize > srcSize {
			return uint64(0)
		}
		/* compress and write sub-blocks */
		n = uint64(0)
		for {
			if !(n < nbSubBlocks-uint64(1)) {
				break
			}
			/* determine nb of sequences for current sub-block + nbLiterals from next sequence */
			seqCount = sizeBlockSequences(tls, sp, libc.Uint64FromInt64((int64(send)-int64(sp))/8), avgBlockBudget+blockBudgetSupp, avgLitCost, avgSeqCost, libc.BoolInt32(n == uint64(0)))
			/* if reached last sequence : break to last sub-block (simplification) */
			if sp+uintptr(seqCount)*8 == send {
				break
			}
			/* compress sub-block */
			*(*int32)(unsafe.Pointer(bp)) = 0
			*(*int32)(unsafe.Pointer(bp + 4)) = 0
			litSize = countLiterals(tls, seqStorePtr, sp, seqCount)
			decompressedSize = ZSTD_seqDecompressedSize(tls, seqStorePtr, sp, seqCount, litSize, 0)
			cSize = ZSTD_compressSubBlock(tls, nextCBlock, entropyMetadata, sp, seqCount, lp, litSize, llCodePtr, mlCodePtr, ofCodePtr, cctxParams, op, libc.Uint64FromInt64(int64(oend)-int64(op)), bmi2, writeLitEntropy, writeSeqEntropy, bp, bp+4, uint32(0))
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1712, 0)
				}
				return err_code
			}
			/* check compressibility, update state components */
			if cSize > uint64(0) && cSize < decompressedSize {
				ip = ip + uintptr(decompressedSize)
				lp = lp + uintptr(litSize)
				op = op + uintptr(cSize)
				llCodePtr = llCodePtr + uintptr(seqCount)
				mlCodePtr = mlCodePtr + uintptr(seqCount)
				ofCodePtr = ofCodePtr + uintptr(seqCount)
				/* Entropy only needs to be written once */
				if *(*int32)(unsafe.Pointer(bp)) != 0 {
					writeLitEntropy = 0
				}
				if *(*int32)(unsafe.Pointer(bp + 4)) != 0 {
					writeSeqEntropy = 0
				}
				sp = sp + uintptr(seqCount)*8
				blockBudgetSupp = uint64(0)
			}
			/* otherwise : do not compress yet, coalesce current sub-block with following one */
			goto _4
		_4:
			;
			n = n + 1
		}
	} /* if (nbSeqs > 0) */
	/* write last block */
	*(*int32)(unsafe.Pointer(bp + 8)) = 0
	*(*int32)(unsafe.Pointer(bp + 12)) = 0
	litSize1 = libc.Uint64FromInt64(int64(lend) - int64(lp))
	seqCount1 = libc.Uint64FromInt64((int64(send) - int64(sp)) / 8)
	decompressedSize1 = ZSTD_seqDecompressedSize(tls, seqStorePtr, sp, seqCount1, litSize1, int32(1))
	cSize1 = ZSTD_compressSubBlock(tls, nextCBlock, entropyMetadata, sp, seqCount1, lp, litSize1, llCodePtr, mlCodePtr, ofCodePtr, cctxParams, op, libc.Uint64FromInt64(int64(oend)-int64(op)), bmi2, writeLitEntropy, writeSeqEntropy, bp+8, bp+12, lastBlock)
	err_code1 = cSize1
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1712, 0)
		}
		return err_code1
	}
	/* update pointers, the nb of literals borrowed from next sequence must be preserved */
	if cSize1 > uint64(0) && cSize1 < decompressedSize1 {
		ip = ip + uintptr(decompressedSize1)
		lp = lp + uintptr(litSize1)
		op = op + uintptr(cSize1)
		llCodePtr = llCodePtr + uintptr(seqCount1)
		mlCodePtr = mlCodePtr + uintptr(seqCount1)
		ofCodePtr = ofCodePtr + uintptr(seqCount1)
		/* Entropy only needs to be written once */
		if *(*int32)(unsafe.Pointer(bp + 8)) != 0 {
			writeLitEntropy = 0
		}
		if *(*int32)(unsafe.Pointer(bp + 12)) != 0 {
			writeSeqEntropy = 0
		}
		sp = sp + uintptr(seqCount1)*8
	}
	if writeLitEntropy != 0 {
		libc.Xmemcpy(tls, nextCBlock, prevCBlock, libc.Uint64FromInt64(2064))
	}
	if writeSeqEntropy != 0 && ZSTD_needSequenceEntropyTables(tls, entropyMetadata+144) != 0 {
		/* If we haven't written our entropy tables, then we've violated our contract and
		 * must emit an uncompressed block.
		 */
		return uint64(0)
	}
	if ip < iend {
		/* some data left : last part of the block sent uncompressed */
		rSize = libc.Uint64FromInt64(int64(iend) - int64(ip))
		cSize2 = ZSTD_noCompressBlock(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, rSize, lastBlock)
		err_code2 = cSize2
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1741, 0)
			}
			return err_code2
		}
		op = op + uintptr(cSize2)
		/* We have to regenerate the repcodes because we've skipped some sequences */
		if sp < send {
			libc.Xmemcpy(tls, bp+16, prevCBlock+5616, libc.Uint64FromInt64(12))
			seq = sstart
			for {
				if !(seq < sp) {
					break
				}
				ZSTD_updateRep(tls, bp+16, (*SeqDef)(unsafe.Pointer(seq)).FoffBase, libc.BoolUint32(ZSTD_getSequenceLength(tls, seqStorePtr, seq).FlitLength == uint32(0)))
				goto _5
			_5:
				;
				seq += 8
			}
			libc.Xmemcpy(tls, nextCBlock+5616, bp+16, libc.Uint64FromInt64(12))
		}
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_compressSuperBlock(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock uint32) (r size_t) {
	bp := tls.Alloc(320)
	defer tls.Free(320)
	var err_code size_t
	var _ /* entropyMetadata at bp+0 */ ZSTD_entropyCTablesMetadata_t
	_ = err_code
	err_code = ZSTD_buildBlockEntropyStats(tls, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, bp, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_compressSubBlock_multi(tls, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, bp, zc+240, dst, dstCapacity, src, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).Fbmi2, lastBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize)
}

/**** ended inlining compress/zstd_compress_superblock.c ****/
/**** start inlining compress/zstd_preSplit.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: zstd_preSplit.h ****/

// C documentation
//
//	/* for hashLog > 8, hash 2 bytes.
//	 * for hashLog == 8, just take the byte, no hashing.
//	 * The speed of this method relies on compile-time constant propagation */
func hash2(tls *libc.TLS, p uintptr, hashLog uint32) (r uint32) {
	if hashLog == uint32(8) {
		return uint32(*(*BYTE)(unsafe.Pointer(p)))
	}
	return uint32(MEM_read16(tls, p)) * uint32(KNUTH) >> (uint32(32) - hashLog)
}

type Fingerprint = struct {
	Fevents   [1024]uint32
	FnbEvents size_t
}

type FPStats = struct {
	FpastEvents Fingerprint
	FnewEvents  Fingerprint
}

func initStats(tls *libc.TLS, fpstats uintptr) {
	libc.Xmemset(tls, fpstats, 0, libc.Uint64FromInt64(8208))
}

func addEvents_generic(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t, samplingRate size_t, hashLog uint32) {
	var limit, n size_t
	var p uintptr
	_, _, _ = limit, n, p
	p = src
	limit = srcSize - uint64(HASHLENGTH) + uint64(1)
	n = uint64(0)
	for {
		if !(n < limit) {
			break
		}
		*(*uint32)(unsafe.Pointer(fp + uintptr(hash2(tls, p+uintptr(n), hashLog))*4)) = *(*uint32)(unsafe.Pointer(fp + uintptr(hash2(tls, p+uintptr(n), hashLog))*4)) + 1
		goto _1
	_1:
		;
		n = n + samplingRate
	}
	*(*size_t)(unsafe.Pointer(fp + 4096)) += limit / samplingRate
}

func recordFingerprint_generic(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t, samplingRate size_t, hashLog uint32) {
	libc.Xmemset(tls, fp, 0, libc.Uint64FromInt64(4)*(libc.Uint64FromInt32(1)<<hashLog))
	(*Fingerprint)(unsafe.Pointer(fp)).FnbEvents = uint64(0)
	addEvents_generic(tls, fp, src, srcSize, samplingRate, hashLog)
}

type RecordEvents_f = uintptr

func ZSTD_recordFingerprint_1(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(1), uint32(10))
}

func ZSTD_recordFingerprint_5(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(5), uint32(10))
}

func ZSTD_recordFingerprint_11(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(11), uint32(9))
}

func ZSTD_recordFingerprint_43(tls *libc.TLS, fp uintptr, src uintptr, srcSize size_t) {
	recordFingerprint_generic(tls, fp, src, srcSize, uint64(43), uint32(8))
}

func abs64(tls *libc.TLS, s64 S64) (r U64) {
	var v1 int64
	_ = v1
	if s64 < 0 {
		v1 = -s64
	} else {
		v1 = s64
	}
	return libc.Uint64FromInt64(v1)
}

func fpDistance(tls *libc.TLS, fp1 uintptr, fp2 uintptr, hashLog uint32) (r U64) {
	var distance U64
	var n size_t
	_, _ = distance, n
	distance = uint64(0)
	n = uint64(0)
	for {
		if !(n < libc.Uint64FromInt32(1)<<hashLog) {
			break
		}
		distance = distance + abs64(tls, libc.Int64FromUint32(*(*uint32)(unsafe.Pointer(fp1 + uintptr(n)*4)))*libc.Int64FromUint64((*Fingerprint)(unsafe.Pointer(fp2)).FnbEvents)-libc.Int64FromUint32(*(*uint32)(unsafe.Pointer(fp2 + uintptr(n)*4)))*libc.Int64FromUint64((*Fingerprint)(unsafe.Pointer(fp1)).FnbEvents))
		goto _1
	_1:
		;
		n = n + 1
	}
	return distance
}

// C documentation
//
//	/* Compare newEvents with pastEvents
//	 * return 1 when considered "too different"
//	 */
func compareFingerprints(tls *libc.TLS, ref uintptr, newfp uintptr, penalty int32, hashLog uint32) (r int32) {
	var deviation, p50, threshold U64
	_, _, _ = deviation, p50, threshold
	p50 = (*Fingerprint)(unsafe.Pointer(ref)).FnbEvents * (*Fingerprint)(unsafe.Pointer(newfp)).FnbEvents
	deviation = fpDistance(tls, ref, newfp, hashLog)
	threshold = p50 * libc.Uint64FromInt32(libc.Int32FromInt32(THRESHOLD_PENALTY_RATE)-libc.Int32FromInt32(2)+penalty) / uint64(THRESHOLD_PENALTY_RATE)
	return libc.BoolInt32(deviation >= threshold)
	return r
}

func mergeEvents(tls *libc.TLS, acc uintptr, newfp uintptr) {
	var n size_t
	_ = n
	n = uint64(0)
	for {
		if !(n < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(HASHLOG_MAX))) {
			break
		}
		*(*uint32)(unsafe.Pointer(acc + uintptr(n)*4)) += *(*uint32)(unsafe.Pointer(newfp + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	*(*size_t)(unsafe.Pointer(acc + 4096)) += (*Fingerprint)(unsafe.Pointer(newfp)).FnbEvents
}

func flushEvents(tls *libc.TLS, fpstats uintptr) {
	var n size_t
	_ = n
	n = uint64(0)
	for {
		if !(n < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(HASHLOG_MAX))) {
			break
		}
		*(*uint32)(unsafe.Pointer(fpstats + uintptr(n)*4)) = *(*uint32)(unsafe.Pointer(fpstats + 4104 + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	(*FPStats)(unsafe.Pointer(fpstats)).FpastEvents.FnbEvents = (*FPStats)(unsafe.Pointer(fpstats)).FnewEvents.FnbEvents
	libc.Xmemset(tls, fpstats+4104, 0, libc.Uint64FromInt64(4104))
}

func removeEvents(tls *libc.TLS, acc uintptr, slice uintptr) {
	var n size_t
	_ = n
	n = uint64(0)
	for {
		if !(n < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(HASHLOG_MAX))) {
			break
		}
		*(*uint32)(unsafe.Pointer(acc + uintptr(n)*4)) -= *(*uint32)(unsafe.Pointer(slice + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	*(*size_t)(unsafe.Pointer(acc + 4096)) -= (*Fingerprint)(unsafe.Pointer(slice)).FnbEvents
}

func ZSTD_splitBlock_byChunks(tls *libc.TLS, blockStart uintptr, blockSize size_t, level int32, workspace uintptr, wkspSize size_t) (r size_t) {
	var fpstats, p uintptr
	var penalty int32
	var pos size_t
	var record_f RecordEvents_f
	_, _, _, _, _ = fpstats, p, penalty, pos, record_f
	record_f = records_fs[level]
	fpstats = workspace
	p = blockStart
	penalty = int32(THRESHOLD_PENALTY)
	pos = uint64(0)
	_ = libc.Uint64FromInt64(1)
	_ = wkspSize
	initStats(tls, fpstats)
	(*(*func(*libc.TLS, uintptr, uintptr, size_t))(unsafe.Pointer(&struct{ uintptr }{record_f})))(tls, fpstats, p, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)))
	pos = libc.Uint64FromInt32(libc.Int32FromInt32(8) << libc.Int32FromInt32(10))
	for {
		if !(pos <= blockSize-libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10))) {
			break
		}
		(*(*func(*libc.TLS, uintptr, uintptr, size_t))(unsafe.Pointer(&struct{ uintptr }{record_f})))(tls, fpstats+4104, p+uintptr(pos), libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)))
		if compareFingerprints(tls, fpstats, fpstats+4104, penalty, hashParams[level]) != 0 {
			return pos
		} else {
			mergeEvents(tls, fpstats, fpstats+4104)
			if penalty > 0 {
				penalty = penalty - 1
			}
		}
		goto _1
	_1:
		;
		pos = pos + libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10))
	}
	return blockSize
	_ = __ccgo_fp(flushEvents)
	_ = __ccgo_fp(removeEvents)
	return r
}

var records_fs = [4]RecordEvents_f{}

func init() {
	p := unsafe.Pointer(&records_fs)
	*(*uintptr)(unsafe.Add(p, 0)) = __ccgo_fp(ZSTD_recordFingerprint_43)
	*(*uintptr)(unsafe.Add(p, 8)) = __ccgo_fp(ZSTD_recordFingerprint_11)
	*(*uintptr)(unsafe.Add(p, 16)) = __ccgo_fp(ZSTD_recordFingerprint_5)
	*(*uintptr)(unsafe.Add(p, 24)) = __ccgo_fp(ZSTD_recordFingerprint_1)
}

var hashParams = [4]uint32{
	0: uint32(8),
	1: uint32(9),
	2: uint32(10),
	3: uint32(10),
}

// C documentation
//
//	/* ZSTD_splitBlock_fromBorders(): very fast strategy :
//	 * compare fingerprint from beginning and end of the block,
//	 * derive from their difference if it's preferable to split in the middle,
//	 * repeat the process a second time, for finer grained decision.
//	 * 3 times did not brought improvements, so I stopped at 2.
//	 * Benefits are good enough for a cheap heuristic.
//	 * More accurate splitting saves more, but speed impact is also more perceptible.
//	 * For better accuracy, use more elaborate variant *_byChunks.
//	 */
func ZSTD_splitBlock_fromBorders(tls *libc.TLS, blockStart uintptr, blockSize size_t, workspace uintptr, wkspSize size_t) (r size_t) {
	var distFromBegin, distFromEnd, minDistance U64
	var fpstats, middleEvents uintptr
	var v1 size_t
	var v2 int32
	_, _, _, _, _, _, _ = distFromBegin, distFromEnd, fpstats, middleEvents, minDistance, v1, v2
	fpstats = workspace
	middleEvents = workspace + uintptr(libc.Uint64FromInt32(512)*libc.Uint64FromInt64(4))
	_ = libc.Uint64FromInt64(1)
	_ = wkspSize
	initStats(tls, fpstats)
	HIST_add(tls, fpstats, blockStart, uint64(SEGMENT_SIZE))
	HIST_add(tls, fpstats+4104, blockStart+uintptr(blockSize)-uintptr(SEGMENT_SIZE), uint64(SEGMENT_SIZE))
	v1 = libc.Uint64FromInt32(SEGMENT_SIZE)
	(*FPStats)(unsafe.Pointer(fpstats)).FnewEvents.FnbEvents = v1
	(*FPStats)(unsafe.Pointer(fpstats)).FpastEvents.FnbEvents = v1
	if !(compareFingerprints(tls, fpstats, fpstats+4104, 0, uint32(8)) != 0) {
		return blockSize
	}
	HIST_add(tls, middleEvents, blockStart+uintptr(blockSize/uint64(2))-uintptr(libc.Int32FromInt32(SEGMENT_SIZE)/libc.Int32FromInt32(2)), uint64(SEGMENT_SIZE))
	(*Fingerprint)(unsafe.Pointer(middleEvents)).FnbEvents = uint64(SEGMENT_SIZE)
	distFromBegin = fpDistance(tls, fpstats, middleEvents, uint32(8))
	distFromEnd = fpDistance(tls, fpstats+4104, middleEvents, uint32(8))
	minDistance = libc.Uint64FromInt32(libc.Int32FromInt32(SEGMENT_SIZE) * libc.Int32FromInt32(SEGMENT_SIZE) / libc.Int32FromInt32(3))
	if abs64(tls, libc.Int64FromUint64(distFromBegin)-libc.Int64FromUint64(distFromEnd)) < minDistance {
		return libc.Uint64FromInt32(libc.Int32FromInt32(64) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	if distFromBegin > distFromEnd {
		v2 = libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))
	} else {
		v2 = libc.Int32FromInt32(96) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))
	}
	return libc.Uint64FromInt32(v2)
	return r
}

func ZSTD_splitBlock(tls *libc.TLS, blockStart uintptr, blockSize size_t, level int32, workspace uintptr, wkspSize size_t) (r size_t) {
	if level == 0 {
		return ZSTD_splitBlock_fromBorders(tls, blockStart, blockSize, workspace, wkspSize)
	}
	/* level >= 1*/
	return ZSTD_splitBlock_byChunks(tls, blockStart, blockSize, level-int32(1), workspace, wkspSize)
}

/**** ended inlining zstd_ldm.h ****/
/**** skipping file: zstd_compress_superblock.h ****/
/**** skipping file: ../common/bits.h ****/

/* ***************************************************************
*  Tuning parameters
*****************************************************************/
/*!
 * COMPRESS_HEAPMODE :
 * Select how default decompression function ZSTD_compress() allocates its context,
 * on stack (0, default), or into heap (1).
 * Note that functions with explicit context such as ZSTD_compressCCtx() are unaffected.
 */

/*!
 * ZSTD_HASHLOG3_MAX :
 * Maximum size of the hash table dedicated to find 3-bytes matches,
 * in log format, aka 17 => 1 << 17 == 128Ki positions.
 * This structure is only used in zstd_opt.
 * Since allocation is centralized for all strategies, it has to be known here.
 * The actual (selected) size of the hash table is then stored in ZSTD_MatchState_t.hashLog3,
 * so that zstd_opt.c doesn't need to know about this constant.
 */

// C documentation
//
//	/*-*************************************
//	*  Helper functions
//	***************************************/
//	/* ZSTD_compressBound()
//	 * Note that the result from this function is only valid for
//	 * the one-pass compression functions.
//	 * When employing the streaming mode,
//	 * if flushes are frequently altering the size of blocks,
//	 * the overhead from block headers can make the compressed data larger
//	 * than the return value of ZSTD_compressBound().
//	 */
func ZSTD_compressBound(tls *libc.TLS, srcSize size_t) (r1 size_t) {
	var r size_t
	var v1, v2 uint64
	_, _, _ = r, v1, v2
	if srcSize >= uint64(0xFF00FF00FF00FF00) {
		v1 = uint64(0)
	} else {
		if srcSize < libc.Uint64FromInt32(libc.Int32FromInt32(128)<<libc.Int32FromInt32(10)) {
			v2 = (libc.Uint64FromInt32(libc.Int32FromInt32(128)<<libc.Int32FromInt32(10)) - srcSize) >> int32(11)
		} else {
			v2 = uint64(0)
		}
		v1 = srcSize + srcSize>>libc.Int32FromInt32(8) + v2
	}
	r = v1
	if r == uint64(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	return r
}

/* typedef'd to ZSTD_CDict within "zstd.h" */

func ZSTD_createCCtx(tls *libc.TLS) (r uintptr) {
	return ZSTD_createCCtx_advanced(tls, ZSTD_defaultCMem)
}

func ZSTD_initCCtx(tls *libc.TLS, cctx uintptr, memManager ZSTD_customMem) {
	var err size_t
	_ = err
	libc.Xmemset(tls, cctx, 0, libc.Uint64FromInt64(5280))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem = memManager
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2 = ZSTD_cpuSupportsBmi2(tls)
	err = ZSTD_CCtx_reset(tls, cctx, int32(ZSTD_reset_parameters))
	_ = err
}

func ZSTD_createCCtx_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var cctx uintptr
	_ = cctx
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	cctx = ZSTD_customMalloc(tls, uint64(5280), customMem)
	if !(cctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_initCCtx(tls, cctx, customMem)
	return cctx
	return r
}

func ZSTD_initStaticCCtx(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	bp := tls.Alloc(80)
	defer tls.Free(80)
	var cctx uintptr
	var _ /* ws at bp+0 */ ZSTD_cwksp
	_ = cctx
	if workspaceSize <= uint64(5280) {
		return libc.UintptrFromInt32(0)
	} /* minimum size */
	if uint64(workspace)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* must be 8-aligned */
	ZSTD_cwksp_init(tls, bp, workspace, workspaceSize, int32(ZSTD_cwksp_static_alloc))
	cctx = ZSTD_cwksp_reserve_object(tls, bp, uint64(5280))
	if cctx == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	libc.Xmemset(tls, cctx, 0, libc.Uint64FromInt64(5280))
	ZSTD_cwksp_move(tls, cctx+704, bp)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize = workspaceSize
	/* statically sized space. tmpWorkspace never moves (but prev/next block swap places) */
	if !(ZSTD_cwksp_check_available(tls, cctx+704, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2))+libc.Uint64FromInt32(2)*libc.Uint64FromInt64(5632)) != 0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock = ZSTD_cwksp_reserve_object(tls, cctx+704, uint64(5632))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock = ZSTD_cwksp_reserve_object(tls, cctx+704, uint64(5632))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace = ZSTD_cwksp_reserve_object(tls, cctx+704, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2)))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize = libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)) + libc.Uint64FromInt64(4)*libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2 = ZSTD_cpuid_bmi2(tls, ZSTD_cpuid(tls))
	return cctx
}

// C documentation
//
//	/**
//	 * Clears and frees all of the dictionaries in the CCtx.
//	 */
func ZSTD_clearAllDicts(tls *libc.TLS, cctx uintptr) {
	ZSTD_customFree(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictBuffer, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
	ZSTD_freeCDict(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fcdict)
	libc.Xmemset(tls, cctx+3688, 0, libc.Uint64FromInt64(40))
	libc.Xmemset(tls, cctx+3736, 0, libc.Uint64FromInt64(24))
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict = libc.UintptrFromInt32(0)
}

func ZSTD_sizeof_localDict(tls *libc.TLS, dict ZSTD_localDict) (r size_t) {
	var bufferSize, cdictSize size_t
	var v1 uint64
	_, _, _ = bufferSize, cdictSize, v1
	if dict.FdictBuffer != libc.UintptrFromInt32(0) {
		v1 = dict.FdictSize
	} else {
		v1 = uint64(0)
	}
	bufferSize = v1
	cdictSize = ZSTD_sizeof_CDict(tls, dict.Fcdict)
	return bufferSize + cdictSize
}

func ZSTD_freeCCtxContent(tls *libc.TLS, cctx uintptr) {
	ZSTD_clearAllDicts(tls, cctx)
	ZSTDMT_freeCCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx = libc.UintptrFromInt32(0)
	ZSTD_cwksp_free(tls, cctx+704, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
}

func ZSTD_freeCCtx(tls *libc.TLS, cctx uintptr) (r size_t) {
	var cctxInWorkspace int32
	_ = cctxInWorkspace
	if cctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1769, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	cctxInWorkspace = ZSTD_cwksp_owns_buffer(tls, cctx+704, cctx)
	ZSTD_freeCCtxContent(tls, cctx)
	if !(cctxInWorkspace != 0) {
		ZSTD_customFree(tls, cctx, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
	}
	return uint64(0)
}

func ZSTD_sizeof_mtctx(tls *libc.TLS, cctx uintptr) (r size_t) {
	return ZSTDMT_sizeof_CCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
}

func ZSTD_sizeof_CCtx(tls *libc.TLS, cctx uintptr) (r size_t) {
	var v1 uint64
	_ = v1
	if cctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof on NULL */
	/* cctx may be in the workspace */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fworkspace.Fworkspace == cctx {
		v1 = uint64(0)
	} else {
		v1 = uint64(5280)
	}
	return v1 + ZSTD_cwksp_sizeof(tls, cctx+704) + ZSTD_sizeof_localDict(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict) + ZSTD_sizeof_mtctx(tls, cctx)
}

func ZSTD_sizeof_CStream(tls *libc.TLS, zcs uintptr) (r size_t) {
	return ZSTD_sizeof_CCtx(tls, zcs) /* same object */
}

// C documentation
//
//	/* private API call, for dictBuilder only */
func ZSTD_getSeqStore(tls *libc.TLS, ctx uintptr) (r uintptr) {
	return ctx + 976
}

// C documentation
//
//	/* Returns true if the strategy supports using a row based matchfinder */
func ZSTD_rowMatchFinderSupported(tls *libc.TLS, strategy ZSTD_strategy) (r int32) {
	return libc.BoolInt32(strategy >= int32(ZSTD_greedy) && strategy <= int32(ZSTD_lazy2))
}

// C documentation
//
//	/* Returns true if the strategy and useRowMatchFinder mode indicate that we will use the row based matchfinder
//	 * for this compression.
//	 */
func ZSTD_rowMatchFinderUsed(tls *libc.TLS, strategy ZSTD_strategy, mode ZSTD_ParamSwitch_e) (r int32) {
	return libc.BoolInt32(ZSTD_rowMatchFinderSupported(tls, strategy) != 0 && mode == int32(ZSTD_ps_enable))
}

// C documentation
//
//	/* Returns row matchfinder usage given an initial mode and cParams */
func ZSTD_resolveRowMatchFinderMode(tls *libc.TLS, mode ZSTD_ParamSwitch_e, cParams uintptr) (r ZSTD_ParamSwitch_e) {
	if mode != int32(ZSTD_ps_auto) {
		return mode
	} /* if requested enabled, but no SIMD, we still will use row matchfinder */
	mode = int32(ZSTD_ps_disable)
	if !(ZSTD_rowMatchFinderSupported(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy) != 0) {
		return mode
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog > uint32(14) {
		mode = int32(ZSTD_ps_enable)
	}
	return mode
}

// C documentation
//
//	/* Returns block splitter usage (generally speaking, when using slower/stronger compression modes) */
func ZSTD_resolveBlockSplitterMode(tls *libc.TLS, mode ZSTD_ParamSwitch_e, cParams uintptr) (r ZSTD_ParamSwitch_e) {
	var v1 int32
	_ = v1
	if mode != int32(ZSTD_ps_auto) {
		return mode
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog >= uint32(17) {
		v1 = int32(ZSTD_ps_enable)
	} else {
		v1 = int32(ZSTD_ps_disable)
	}
	return v1
}

// C documentation
//
//	/* Returns 1 if the arguments indicate that we should allocate a chainTable, 0 otherwise */
func ZSTD_allocateChainTable(tls *libc.TLS, strategy ZSTD_strategy, useRowMatchFinder ZSTD_ParamSwitch_e, forDDSDict U32) (r int32) {
	/* We always should allocate a chaintable if we are allocating a matchstate for a DDS dictionary matchstate.
	 * We do not allocate a chaintable if we are using ZSTD_fast, or are using the row-based matchfinder.
	 */
	return libc.BoolInt32(forDDSDict != 0 || strategy != int32(ZSTD_fast) && !(ZSTD_rowMatchFinderUsed(tls, strategy, useRowMatchFinder) != 0))
}

// C documentation
//
//	/* Returns ZSTD_ps_enable if compression parameters are such that we should
//	 * enable long distance matching (wlog >= 27, strategy >= btopt).
//	 * Returns ZSTD_ps_disable otherwise.
//	 */
func ZSTD_resolveEnableLdm(tls *libc.TLS, mode ZSTD_ParamSwitch_e, cParams uintptr) (r ZSTD_ParamSwitch_e) {
	var v1 int32
	_ = v1
	if mode != int32(ZSTD_ps_auto) {
		return mode
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog >= uint32(27) {
		v1 = int32(ZSTD_ps_enable)
	} else {
		v1 = int32(ZSTD_ps_disable)
	}
	return v1
}

func ZSTD_resolveExternalSequenceValidation(tls *libc.TLS, mode int32) (r int32) {
	return mode
}

// C documentation
//
//	/* Resolves maxBlockSize to the default if no value is present. */
func ZSTD_resolveMaxBlockSize(tls *libc.TLS, maxBlockSize size_t) (r size_t) {
	if maxBlockSize == uint64(0) {
		return libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	} else {
		return maxBlockSize
	}
	return r
}

func ZSTD_resolveExternalRepcodeSearch(tls *libc.TLS, value ZSTD_ParamSwitch_e, cLevel int32) (r ZSTD_ParamSwitch_e) {
	if value != int32(ZSTD_ps_auto) {
		return value
	}
	if cLevel < int32(10) {
		return int32(ZSTD_ps_disable)
	} else {
		return int32(ZSTD_ps_enable)
	}
	return r
}

// C documentation
//
//	/* Returns 1 if compression parameters are such that CDict hashtable and chaintable indices are tagged.
//	 * If so, the tags need to be removed in ZSTD_resetCCtx_byCopyingCDict. */
func ZSTD_CDictIndicesAreTagged(tls *libc.TLS, cParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy == int32(ZSTD_fast) || (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy == int32(ZSTD_dfast))
}

func ZSTD_makeCCtxParamsFromCParams(tls *libc.TLS, _cParams ZSTD_compressionParameters) (r ZSTD_CCtx_params) {
	bp := tls.Alloc(256)
	defer tls.Free(256)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var _ /* cctxParams at bp+32 */ ZSTD_CCtx_params
	/* should not matter, as all cParams are presumed properly defined */
	ZSTD_CCtxParams_init(tls, bp+32, int32(ZSTD_CLEVEL_DEFAULT))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(bp))
	/* Adjust advanced params according to cParams */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FldmParams.FenableLdm = ZSTD_resolveEnableLdm(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FldmParams.FenableLdm, bp)
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		ZSTD_ldm_adjustParameters(tls, bp+32+96, bp)
	}
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FpostBlockSplitter = ZSTD_resolveBlockSplitterMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FpostBlockSplitter, bp)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FuseRowMatchFinder, bp)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FvalidateSequences = ZSTD_resolveExternalSequenceValidation(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FvalidateSequences)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FmaxBlockSize = ZSTD_resolveMaxBlockSize(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FmaxBlockSize)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FsearchForExternalRepcodes = ZSTD_resolveExternalRepcodeSearch(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FsearchForExternalRepcodes, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FcompressionLevel)
	return *(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))
}

func ZSTD_createCCtxParams_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var params uintptr
	_ = params
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	params = ZSTD_customCalloc(tls, uint64(224), customMem)
	if !(params != 0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_CCtxParams_init(tls, params, int32(ZSTD_CLEVEL_DEFAULT))
	(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcustomMem = customMem
	return params
}

func ZSTD_createCCtxParams(tls *libc.TLS) (r uintptr) {
	return ZSTD_createCCtxParams_advanced(tls, ZSTD_defaultCMem)
}

func ZSTD_freeCCtxParams(tls *libc.TLS, params uintptr) (r size_t) {
	if params == libc.UintptrFromInt32(0) {
		return uint64(0)
	}
	ZSTD_customFree(tls, params, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcustomMem)
	return uint64(0)
}

func ZSTD_CCtxParams_reset(tls *libc.TLS, params uintptr) (r size_t) {
	return ZSTD_CCtxParams_init(tls, params, int32(ZSTD_CLEVEL_DEFAULT))
}

func ZSTD_CCtxParams_init(tls *libc.TLS, cctxParams uintptr, compressionLevel int32) (r size_t) {
	if !(cctxParams != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	libc.Xmemset(tls, cctxParams, 0, libc.Uint64FromInt64(224))
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel = compressionLevel
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FfParams.FcontentSizeFlag = int32(1)
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Initializes `cctxParams` from `params` and `compressionLevel`.
//	 * @param compressionLevel If params are derived from a compression level then that compression level, otherwise ZSTD_NO_CLEVEL.
//	 */
func ZSTD_CCtxParams_init_internal(tls *libc.TLS, cctxParams uintptr, params uintptr, compressionLevel int32) {
	libc.Xmemset(tls, cctxParams, 0, libc.Uint64FromInt64(224))
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FcParams
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FfParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FfParams
	/* Should not matter, as all cParams are presumed properly defined.
	 * But, set it for tracing anyway.
	 */
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel = compressionLevel
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FuseRowMatchFinder, params)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FpostBlockSplitter = ZSTD_resolveBlockSplitterMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FpostBlockSplitter, params)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FldmParams.FenableLdm = ZSTD_resolveEnableLdm(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FldmParams.FenableLdm, params)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FvalidateSequences = ZSTD_resolveExternalSequenceValidation(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FvalidateSequences)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FmaxBlockSize = ZSTD_resolveMaxBlockSize(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FmaxBlockSize)
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FsearchForExternalRepcodes = ZSTD_resolveExternalRepcodeSearch(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FsearchForExternalRepcodes, compressionLevel)
}

func ZSTD_CCtxParams_init_advanced(tls *libc.TLS, cctxParams uintptr, _params ZSTD_parameters) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var err_code size_t
	_ = err_code
	if !(cctxParams != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	err_code = ZSTD_checkCParams(tls, (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ZSTD_CCtxParams_init_internal(tls, cctxParams, bp, ZSTD_NO_CLEVEL)
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Sets cctxParams' cParams and fParams from params, but otherwise leaves them alone.
//	 * @param params Validated zstd parameters.
//	 */
func ZSTD_CCtxParams_setZstdParams(tls *libc.TLS, cctxParams uintptr, params uintptr) {
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FcParams
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FfParams = (*ZSTD_parameters)(unsafe.Pointer(params)).FfParams
	/* Should not matter, as all cParams are presumed properly defined.
	 * But, set it for tracing anyway.
	 */
	(*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel = ZSTD_NO_CLEVEL
}

func ZSTD_cParam_getBounds(tls *libc.TLS, param ZSTD_cParameter) (r ZSTD_bounds) {
	var bounds ZSTD_bounds
	var v1 int32
	_, _ = bounds, v1
	bounds = ZSTD_bounds{}
	switch param {
	case int32(ZSTD_c_compressionLevel):
		bounds.FlowerBound = ZSTD_minCLevel(tls)
		bounds.FupperBound = ZSTD_maxCLevel(tls)
		return bounds
	case int32(ZSTD_c_windowLog):
		bounds.FlowerBound = int32(ZSTD_WINDOWLOG_MIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)
		return bounds
	case int32(ZSTD_c_hashLog):
		bounds.FlowerBound = int32(ZSTD_HASHLOG_MIN)
		bounds.FupperBound = int32(30)
		return bounds
	case int32(ZSTD_c_chainLog):
		bounds.FlowerBound = int32(ZSTD_HASHLOG_MIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_CHAINLOG_MAX_64)
		return bounds
	case int32(ZSTD_c_searchLog):
		bounds.FlowerBound = int32(ZSTD_SEARCHLOG_MIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64) - libc.Int32FromInt32(1)
		return bounds
	case int32(ZSTD_c_minMatch):
		bounds.FlowerBound = int32(ZSTD_MINMATCH_MIN)
		bounds.FupperBound = int32(ZSTD_MINMATCH_MAX)
		return bounds
	case int32(ZSTD_c_targetLength):
		bounds.FlowerBound = ZSTD_TARGETLENGTH_MIN
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_strategy):
		bounds.FlowerBound = int32(ZSTD_fast)
		bounds.FupperBound = int32(ZSTD_btultra2)
		return bounds
	case int32(ZSTD_c_contentSizeFlag):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_checksumFlag):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_dictIDFlag):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_nbWorkers):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(256)
		return bounds
	case int32(ZSTD_c_jobSize):
		bounds.FlowerBound = 0
		if MEM_32bits(tls) != 0 {
			v1 = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		} else {
			v1 = libc.Int32FromInt32(1024) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		}
		bounds.FupperBound = v1
		return bounds
	case int32(ZSTD_c_overlapLog):
		bounds.FlowerBound = ZSTD_OVERLAPLOG_MIN
		bounds.FupperBound = int32(ZSTD_OVERLAPLOG_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam8):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_enableLongDistanceMatching):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_ldmHashLog):
		bounds.FlowerBound = int32(ZSTD_HASHLOG_MIN)
		bounds.FupperBound = int32(30)
		return bounds
	case int32(ZSTD_c_ldmMinMatch):
		bounds.FlowerBound = int32(ZSTD_LDM_MINMATCH_MIN)
		bounds.FupperBound = int32(ZSTD_LDM_MINMATCH_MAX)
		return bounds
	case int32(ZSTD_c_ldmBucketSizeLog):
		bounds.FlowerBound = int32(ZSTD_LDM_BUCKETSIZELOG_MIN)
		bounds.FupperBound = int32(ZSTD_LDM_BUCKETSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_ldmHashRateLog):
		bounds.FlowerBound = ZSTD_LDM_HASHRATELOG_MIN
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64) - libc.Int32FromInt32(ZSTD_HASHLOG_MIN)
		return bounds
		/* experimental parameters */
		fallthrough
	case int32(ZSTD_c_experimentalParam1):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam3):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam2):
		_ = libc.Uint64FromInt64(1)
		bounds.FlowerBound = int32(ZSTD_f_zstd1)
		bounds.FupperBound = int32(ZSTD_f_zstd1_magicless) /* note : how to ensure at compile time that this is the highest value enum ? */
		return bounds
	case int32(ZSTD_c_experimentalParam4):
		_ = libc.Uint64FromInt64(1)
		bounds.FlowerBound = int32(ZSTD_dictDefaultAttach)
		bounds.FupperBound = int32(ZSTD_dictForceLoad) /* note : how to ensure at compile time that this is the highest value enum ? */
		return bounds
	case int32(ZSTD_c_experimentalParam5):
		_ = libc.Uint64FromInt64(1)
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_targetCBlockSize):
		bounds.FlowerBound = int32(ZSTD_TARGETCBLOCKSIZE_MIN)
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam7):
		bounds.FlowerBound = ZSTD_SRCSIZEHINT_MIN
		bounds.FupperBound = int32(INT_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam9):
		fallthrough
	case int32(ZSTD_c_experimentalParam10):
		bounds.FlowerBound = int32(ZSTD_bm_buffered)
		bounds.FupperBound = int32(ZSTD_bm_stable)
		return bounds
	case int32(ZSTD_c_experimentalParam11):
		bounds.FlowerBound = int32(ZSTD_sf_noBlockDelimiters)
		bounds.FupperBound = int32(ZSTD_sf_explicitBlockDelimiters)
		return bounds
	case int32(ZSTD_c_experimentalParam12):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam13):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_experimentalParam20):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(ZSTD_BLOCKSPLITTER_LEVEL_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam14):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_experimentalParam15):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam16):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	case int32(ZSTD_c_experimentalParam17):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_c_experimentalParam18):
		bounds.FlowerBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(10)
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	case int32(ZSTD_c_experimentalParam19):
		bounds.FlowerBound = int32(ZSTD_ps_auto)
		bounds.FupperBound = int32(ZSTD_ps_disable)
		return bounds
	default:
		bounds.Ferror1 = libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
		return bounds
	}
	return r
}

// C documentation
//
//	/* ZSTD_cParam_clampBounds:
//	 * Clamps the value into the bounded range.
//	 */
func ZSTD_cParam_clampBounds(tls *libc.TLS, cParam ZSTD_cParameter, value uintptr) (r size_t) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_cParam_getBounds(tls, cParam)
	if ZSTD_isError(tls, bounds.Ferror1) != 0 {
		return bounds.Ferror1
	}
	if *(*int32)(unsafe.Pointer(value)) < bounds.FlowerBound {
		*(*int32)(unsafe.Pointer(value)) = bounds.FlowerBound
	}
	if *(*int32)(unsafe.Pointer(value)) > bounds.FupperBound {
		*(*int32)(unsafe.Pointer(value)) = bounds.FupperBound
	}
	return uint64(0)
}

func ZSTD_isUpdateAuthorized(tls *libc.TLS, param ZSTD_cParameter) (r int32) {
	switch param {
	case int32(ZSTD_c_compressionLevel):
		fallthrough
	case int32(ZSTD_c_hashLog):
		fallthrough
	case int32(ZSTD_c_chainLog):
		fallthrough
	case int32(ZSTD_c_searchLog):
		fallthrough
	case int32(ZSTD_c_minMatch):
		fallthrough
	case int32(ZSTD_c_targetLength):
		fallthrough
	case int32(ZSTD_c_strategy):
		fallthrough
	case int32(ZSTD_c_experimentalParam20):
		return int32(1)
	case int32(ZSTD_c_experimentalParam2):
		fallthrough
	case int32(ZSTD_c_windowLog):
		fallthrough
	case int32(ZSTD_c_contentSizeFlag):
		fallthrough
	case int32(ZSTD_c_checksumFlag):
		fallthrough
	case int32(ZSTD_c_dictIDFlag):
		fallthrough
	case int32(ZSTD_c_experimentalParam3):
		fallthrough
	case int32(ZSTD_c_nbWorkers):
		fallthrough
	case int32(ZSTD_c_jobSize):
		fallthrough
	case int32(ZSTD_c_overlapLog):
		fallthrough
	case int32(ZSTD_c_experimentalParam1):
		fallthrough
	case int32(ZSTD_c_experimentalParam8):
		fallthrough
	case int32(ZSTD_c_enableLongDistanceMatching):
		fallthrough
	case int32(ZSTD_c_ldmHashLog):
		fallthrough
	case int32(ZSTD_c_ldmMinMatch):
		fallthrough
	case int32(ZSTD_c_ldmBucketSizeLog):
		fallthrough
	case int32(ZSTD_c_ldmHashRateLog):
		fallthrough
	case int32(ZSTD_c_experimentalParam4):
		fallthrough
	case int32(ZSTD_c_experimentalParam5):
		fallthrough
	case int32(ZSTD_c_targetCBlockSize):
		fallthrough
	case int32(ZSTD_c_experimentalParam7):
		fallthrough
	case int32(ZSTD_c_experimentalParam9):
		fallthrough
	case int32(ZSTD_c_experimentalParam10):
		fallthrough
	case int32(ZSTD_c_experimentalParam11):
		fallthrough
	case int32(ZSTD_c_experimentalParam12):
		fallthrough
	case int32(ZSTD_c_experimentalParam13):
		fallthrough
	case int32(ZSTD_c_experimentalParam14):
		fallthrough
	case int32(ZSTD_c_experimentalParam15):
		fallthrough
	case int32(ZSTD_c_experimentalParam16):
		fallthrough
	case int32(ZSTD_c_experimentalParam17):
		fallthrough
	case int32(ZSTD_c_experimentalParam18):
		fallthrough
	case int32(ZSTD_c_experimentalParam19):
		fallthrough
	default:
		return 0
	}
	return r
}

func ZSTD_CCtx_setParameter(tls *libc.TLS, cctx uintptr, param ZSTD_cParameter, value int32) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if ZSTD_isUpdateAuthorized(tls, param) != 0 {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcParamsChanged = int32(1)
		} else {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1801, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
		}
	}
	switch param {
	case int32(ZSTD_c_nbWorkers):
		goto _1
	case int32(ZSTD_c_experimentalParam19):
		goto _2
	case int32(ZSTD_c_experimentalParam18):
		goto _3
	case int32(ZSTD_c_experimentalParam17):
		goto _4
	case int32(ZSTD_c_experimentalParam16):
		goto _5
	case int32(ZSTD_c_experimentalParam15):
		goto _6
	case int32(ZSTD_c_experimentalParam14):
		goto _7
	case int32(ZSTD_c_experimentalParam20):
		goto _8
	case int32(ZSTD_c_experimentalParam13):
		goto _9
	case int32(ZSTD_c_experimentalParam12):
		goto _10
	case int32(ZSTD_c_experimentalParam11):
		goto _11
	case int32(ZSTD_c_experimentalParam10):
		goto _12
	case int32(ZSTD_c_experimentalParam9):
		goto _13
	case int32(ZSTD_c_experimentalParam7):
		goto _14
	case int32(ZSTD_c_targetCBlockSize):
		goto _15
	case int32(ZSTD_c_ldmBucketSizeLog):
		goto _16
	case int32(ZSTD_c_ldmMinMatch):
		goto _17
	case int32(ZSTD_c_ldmHashLog):
		goto _18
	case int32(ZSTD_c_enableLongDistanceMatching):
		goto _19
	case int32(ZSTD_c_experimentalParam8):
		goto _20
	case int32(ZSTD_c_experimentalParam1):
		goto _21
	case int32(ZSTD_c_overlapLog):
		goto _22
	case int32(ZSTD_c_jobSize):
		goto _23
	case int32(ZSTD_c_experimentalParam5):
		goto _24
	case int32(ZSTD_c_experimentalParam4):
		goto _25
	case int32(ZSTD_c_experimentalParam3):
		goto _26
	case int32(ZSTD_c_dictIDFlag):
		goto _27
	case int32(ZSTD_c_checksumFlag):
		goto _28
	case int32(ZSTD_c_contentSizeFlag):
		goto _29
	case int32(ZSTD_c_experimentalParam2):
		goto _30
	case int32(ZSTD_c_ldmHashRateLog):
		goto _31
	case int32(ZSTD_c_strategy):
		goto _32
	case int32(ZSTD_c_targetLength):
		goto _33
	case int32(ZSTD_c_minMatch):
		goto _34
	case int32(ZSTD_c_searchLog):
		goto _35
	case int32(ZSTD_c_chainLog):
		goto _36
	case int32(ZSTD_c_hashLog):
		goto _37
	case int32(ZSTD_c_windowLog):
		goto _38
	case int32(ZSTD_c_compressionLevel):
		goto _39
	default:
		goto _40
	}
	goto _41
_1:
	;
_44:
	;
	if value != 0 && (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1840, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	}
	goto _43
_43:
	;
	if 0 != 0 {
		goto _44
	}
	goto _42
_42:
	;
	goto _41
_39:
	;
_38:
	;
_37:
	;
_36:
	;
_35:
	;
_34:
	;
_33:
	;
_32:
	;
_31:
	;
_30:
	;
_29:
	;
_28:
	;
_27:
	;
_26:
	;
_25:
	;
_24:
	;
_23:
	;
_22:
	;
_21:
	;
_20:
	;
_19:
	;
_18:
	;
_17:
	;
_16:
	;
_15:
	;
_14:
	;
_13:
	;
_12:
	;
_11:
	;
_10:
	;
_9:
	;
_8:
	;
_7:
	;
_6:
	;
_5:
	;
_4:
	;
_3:
	;
_2:
	;
	goto _41
_40:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1876, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
_41:
	;
	return ZSTD_CCtxParams_setParameter(tls, cctx+16, param, value)
}

func ZSTD_CCtxParams_setParameter(tls *libc.TLS, CCtxParams uintptr, param ZSTD_cParameter, _value int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*int32)(unsafe.Pointer(bp)) = _value
	var err_code, err_code1, err_code2, err_code3, err_code4 size_t
	var lcm ZSTD_ParamSwitch_e
	var pref ZSTD_dictAttachPref_e
	var v45 int32
	_, _, _, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, err_code4, lcm, pref, v45
	switch param {
	case int32(ZSTD_c_experimentalParam2):
		goto _1
	case int32(ZSTD_c_compressionLevel):
		goto _2
	case int32(ZSTD_c_windowLog):
		goto _3
	case int32(ZSTD_c_hashLog):
		goto _4
	case int32(ZSTD_c_chainLog):
		goto _5
	case int32(ZSTD_c_searchLog):
		goto _6
	case int32(ZSTD_c_minMatch):
		goto _7
	case int32(ZSTD_c_targetLength):
		goto _8
	case int32(ZSTD_c_strategy):
		goto _9
	case int32(ZSTD_c_contentSizeFlag):
		goto _10
	case int32(ZSTD_c_checksumFlag):
		goto _11
	case int32(ZSTD_c_dictIDFlag):
		goto _12
	case int32(ZSTD_c_experimentalParam3):
		goto _13
	case int32(ZSTD_c_experimentalParam4):
		goto _14
	case int32(ZSTD_c_experimentalParam5):
		goto _15
	case int32(ZSTD_c_nbWorkers):
		goto _16
	case int32(ZSTD_c_jobSize):
		goto _17
	case int32(ZSTD_c_overlapLog):
		goto _18
	case int32(ZSTD_c_experimentalParam1):
		goto _19
	case int32(ZSTD_c_experimentalParam8):
		goto _20
	case int32(ZSTD_c_enableLongDistanceMatching):
		goto _21
	case int32(ZSTD_c_ldmHashLog):
		goto _22
	case int32(ZSTD_c_ldmMinMatch):
		goto _23
	case int32(ZSTD_c_ldmBucketSizeLog):
		goto _24
	case int32(ZSTD_c_ldmHashRateLog):
		goto _25
	case int32(ZSTD_c_targetCBlockSize):
		goto _26
	case int32(ZSTD_c_experimentalParam7):
		goto _27
	case int32(ZSTD_c_experimentalParam9):
		goto _28
	case int32(ZSTD_c_experimentalParam10):
		goto _29
	case int32(ZSTD_c_experimentalParam11):
		goto _30
	case int32(ZSTD_c_experimentalParam12):
		goto _31
	case int32(ZSTD_c_experimentalParam13):
		goto _32
	case int32(ZSTD_c_experimentalParam20):
		goto _33
	case int32(ZSTD_c_experimentalParam14):
		goto _34
	case int32(ZSTD_c_experimentalParam15):
		goto _35
	case int32(ZSTD_c_experimentalParam16):
		goto _36
	case int32(ZSTD_c_experimentalParam17):
		goto _37
	case int32(ZSTD_c_experimentalParam18):
		goto _38
	case int32(ZSTD_c_experimentalParam19):
		goto _39
	default:
		goto _40
	}
	goto _41
_1:
	;
_44:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam2), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	goto _43
_43:
	;
	if 0 != 0 {
		goto _44
	}
	goto _42
_42:
	;
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Fformat = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Fformat)
_2:
	;
	err_code = ZSTD_cParam_clampBounds(tls, param, bp)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if *(*int32)(unsafe.Pointer(bp)) == 0 {
		(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel = *(*int32)(unsafe.Pointer(bp))
	}
	if (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel >= 0 {
		return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel)
	}
	return uint64(0) /* return type (size_t) cannot represent negative values */
_3:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_windowLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FwindowLog = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FwindowLog)
_4:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_hashLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FhashLog = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FhashLog)
_5:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_chainLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FchainLog = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FchainLog)
_6:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_searchLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FsearchLog = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return libc.Uint64FromInt32(*(*int32)(unsafe.Pointer(bp)))
_7:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_minMatch), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FminMatch = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FminMatch)
_8:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_targetLength), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FtargetLength = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FtargetLength)
_9:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 => use default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_strategy), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.Fstrategy = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.Fstrategy)
_10:
	;
	/* Content size written in frame header _when known_ (default:1) */
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FcontentSizeFlag = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FcontentSizeFlag)
_11:
	;
	/* A 32-bits content checksum will be calculated and written at end of frame (default:0) */
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FchecksumFlag = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FchecksumFlag)
_12:
	; /* When applicable, dictionary's dictID is provided in frame header (default:1) */
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FnoDictIDFlag = libc.BoolInt32(!(*(*int32)(unsafe.Pointer(bp)) != 0))
	return libc.BoolUint64(!((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FnoDictIDFlag != 0))
_13:
	;
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FforceWindow = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FforceWindow)
_14:
	;
	pref = *(*int32)(unsafe.Pointer(bp))
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam4), pref) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FattachDictPref = pref
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FattachDictPref)
_15:
	;
	lcm = *(*int32)(unsafe.Pointer(bp))
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam5), lcm) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FliteralCompressionMode = lcm
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FliteralCompressionMode)
_16:
	;
	err_code1 = ZSTD_cParam_clampBounds(tls, param, bp)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FnbWorkers = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FnbWorkers)
_17:
	;
	/* Adjust to the minimum non-default value. */
	if *(*int32)(unsafe.Pointer(bp)) != 0 && *(*int32)(unsafe.Pointer(bp)) < libc.Int32FromInt32(512)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)) {
		*(*int32)(unsafe.Pointer(bp)) = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))
	}
	err_code2 = ZSTD_cParam_clampBounds(tls, param, bp)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FjobSize = libc.Uint64FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FjobSize
_18:
	;
	err_code3 = ZSTD_cParam_clampBounds(tls, int32(ZSTD_c_overlapLog), bp)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoverlapLog = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoverlapLog)
_19:
	;
	err_code4 = ZSTD_cParam_clampBounds(tls, int32(ZSTD_c_overlapLog), bp)
	if ERR_isError(tls, err_code4) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code4
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Frsyncable = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Frsyncable)
_20:
	;
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableDedicatedDictSearch = libc.BoolInt32(*(*int32)(unsafe.Pointer(bp)) != 0)
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableDedicatedDictSearch)
_21:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_enableLongDistanceMatching), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm)
_22:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> auto */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmHashLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashLog = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashLog)
_23:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmMinMatch), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FminMatchLength = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FminMatchLength)
_24:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmBucketSizeLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FbucketSizeLog = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FbucketSizeLog)
_25:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_ldmHashRateLog), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashRateLog = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return uint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashRateLog)
_26:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if *(*int32)(unsafe.Pointer(bp)) > int32(ZSTD_TARGETCBLOCKSIZE_MIN) {
			v45 = *(*int32)(unsafe.Pointer(bp))
		} else {
			v45 = int32(ZSTD_TARGETCBLOCKSIZE_MIN)
		}
		*(*int32)(unsafe.Pointer(bp)) = v45
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_targetCBlockSize), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FtargetCBlockSize = uint64(libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(bp))))
	return (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FtargetCBlockSize
_27:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam7), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint)
_28:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam9), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FinBufferMode = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FinBufferMode)
_29:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam10), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoutBufferMode = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoutBufferMode)
_30:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam11), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FblockDelimiters = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FblockDelimiters)
_31:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam12), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FvalidateSequences = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FvalidateSequences)
_32:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam13), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpostBlockSplitter = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpostBlockSplitter)
_33:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam20), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpreBlockSplitter_level = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpreBlockSplitter_level)
_34:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam14), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder)
_35:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam15), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FdeterministicRefPrefix = libc.BoolInt32(!!(*(*int32)(unsafe.Pointer(bp)) != 0))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FdeterministicRefPrefix)
_36:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam16), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FprefetchCDictTables = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FprefetchCDictTables)
_37:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam17), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableMatchFinderFallback = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableMatchFinderFallback)
_38:
	;
	if *(*int32)(unsafe.Pointer(bp)) != 0 { /* 0 ==> default */
		if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam18), *(*int32)(unsafe.Pointer(bp))) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1894, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FmaxBlockSize = libc.Uint64FromInt32(*(*int32)(unsafe.Pointer(bp)))
	return (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FmaxBlockSize
_39:
	;
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_experimentalParam19), *(*int32)(unsafe.Pointer(bp))) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsearchForExternalRepcodes = *(*int32)(unsafe.Pointer(bp))
	return libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsearchForExternalRepcodes)
_40:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1876, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
_41:
	;
	return r
}

func ZSTD_CCtx_getParameter(tls *libc.TLS, cctx uintptr, param ZSTD_cParameter, value uintptr) (r size_t) {
	return ZSTD_CCtxParams_getParameter(tls, cctx+16, param, value)
}

func ZSTD_CCtxParams_getParameter(tls *libc.TLS, CCtxParams uintptr, param ZSTD_cParameter, value uintptr) (r size_t) {
	switch param {
	case int32(ZSTD_c_experimentalParam2):
		goto _1
	case int32(ZSTD_c_compressionLevel):
		goto _2
	case int32(ZSTD_c_windowLog):
		goto _3
	case int32(ZSTD_c_hashLog):
		goto _4
	case int32(ZSTD_c_chainLog):
		goto _5
	case int32(ZSTD_c_searchLog):
		goto _6
	case int32(ZSTD_c_minMatch):
		goto _7
	case int32(ZSTD_c_targetLength):
		goto _8
	case int32(ZSTD_c_strategy):
		goto _9
	case int32(ZSTD_c_contentSizeFlag):
		goto _10
	case int32(ZSTD_c_checksumFlag):
		goto _11
	case int32(ZSTD_c_dictIDFlag):
		goto _12
	case int32(ZSTD_c_experimentalParam3):
		goto _13
	case int32(ZSTD_c_experimentalParam4):
		goto _14
	case int32(ZSTD_c_experimentalParam5):
		goto _15
	case int32(ZSTD_c_nbWorkers):
		goto _16
	case int32(ZSTD_c_jobSize):
		goto _17
	case int32(ZSTD_c_overlapLog):
		goto _18
	case int32(ZSTD_c_experimentalParam1):
		goto _19
	case int32(ZSTD_c_experimentalParam8):
		goto _20
	case int32(ZSTD_c_enableLongDistanceMatching):
		goto _21
	case int32(ZSTD_c_ldmHashLog):
		goto _22
	case int32(ZSTD_c_ldmMinMatch):
		goto _23
	case int32(ZSTD_c_ldmBucketSizeLog):
		goto _24
	case int32(ZSTD_c_ldmHashRateLog):
		goto _25
	case int32(ZSTD_c_targetCBlockSize):
		goto _26
	case int32(ZSTD_c_experimentalParam7):
		goto _27
	case int32(ZSTD_c_experimentalParam9):
		goto _28
	case int32(ZSTD_c_experimentalParam10):
		goto _29
	case int32(ZSTD_c_experimentalParam11):
		goto _30
	case int32(ZSTD_c_experimentalParam12):
		goto _31
	case int32(ZSTD_c_experimentalParam13):
		goto _32
	case int32(ZSTD_c_experimentalParam20):
		goto _33
	case int32(ZSTD_c_experimentalParam14):
		goto _34
	case int32(ZSTD_c_experimentalParam15):
		goto _35
	case int32(ZSTD_c_experimentalParam16):
		goto _36
	case int32(ZSTD_c_experimentalParam17):
		goto _37
	case int32(ZSTD_c_experimentalParam18):
		goto _38
	case int32(ZSTD_c_experimentalParam19):
		goto _39
	default:
		goto _40
	}
	goto _41
_1:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Fformat
	goto _41
_2:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel
	goto _41
_3:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FwindowLog)
	goto _41
_4:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FhashLog)
	goto _41
_5:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FchainLog)
	goto _41
_6:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FsearchLog)
	goto _41
_7:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FminMatch)
	goto _41
_8:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.FtargetLength)
	goto _41
_9:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcParams.Fstrategy
	goto _41
_10:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FcontentSizeFlag
	goto _41
_11:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FchecksumFlag
	goto _41
_12:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.BoolInt32(!((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FfParams.FnoDictIDFlag != 0))
	goto _41
_13:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FforceWindow
	goto _41
_14:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FattachDictPref
	goto _41
_15:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FliteralCompressionMode
	goto _41
_16:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FnbWorkers
	goto _41
_17:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FjobSize)
	goto _41
_18:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoverlapLog
	goto _41
_19:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).Frsyncable
	goto _41
_20:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableDedicatedDictSearch
	goto _41
_21:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm
	goto _41
_22:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashLog)
	goto _41
_23:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FminMatchLength)
	goto _41
_24:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FbucketSizeLog)
	goto _41
_25:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FhashRateLog)
	goto _41
_26:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FtargetCBlockSize)
	goto _41
_27:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint
	goto _41
_28:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FinBufferMode
	goto _41
_29:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FoutBufferMode
	goto _41
_30:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FblockDelimiters
	goto _41
_31:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FvalidateSequences
	goto _41
_32:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpostBlockSplitter
	goto _41
_33:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FpreBlockSplitter_level
	goto _41
_34:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder
	goto _41
_35:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FdeterministicRefPrefix
	goto _41
_36:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FprefetchCDictTables
	goto _41
_37:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FenableMatchFinderFallback
	goto _41
_38:
	;
	*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint64((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FmaxBlockSize)
	goto _41
_39:
	;
	*(*int32)(unsafe.Pointer(value)) = (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsearchForExternalRepcodes
	goto _41
_40:
	;
_44:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1876, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	goto _43
_43:
	;
	if 0 != 0 {
		goto _44
	}
	goto _42
_42:
	;
_41:
	;
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_CCtx_setParametersUsingCCtxParams() :
//	 *  just applies `params` into `cctx`
//	 *  no action is performed, parameters are merely stored.
//	 *  If ZSTDMT is enabled, parameters are pushed to cctx->mtctx.
//	 *    This is possible even if a compression is ongoing.
//	 *    In which case, new parameters will be applied on the fly, starting with next compression job.
//	 */
func ZSTD_CCtx_setParametersUsingCCtxParams(tls *libc.TLS, cctx uintptr, params uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1914, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1949, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(params))
	return uint64(0)
}

func ZSTD_CCtx_setCParams(tls *libc.TLS, cctx uintptr, cparams ZSTD_compressionParameters) (r size_t) {
	var err_code, err_code1, err_code2, err_code3, err_code4, err_code5, err_code6, err_code7 size_t
	_, _, _, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, err_code4, err_code5, err_code6, err_code7
	_ = libc.Uint64FromInt64(1)
	/* only update if all parameters are valid */
	err_code = ZSTD_checkCParams(tls, cparams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_windowLog), libc.Int32FromUint32(cparams.FwindowLog))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_chainLog), libc.Int32FromUint32(cparams.FchainLog))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	err_code3 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_hashLog), libc.Int32FromUint32(cparams.FhashLog))
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	err_code4 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_searchLog), libc.Int32FromUint32(cparams.FsearchLog))
	if ERR_isError(tls, err_code4) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code4
	}
	err_code5 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_minMatch), libc.Int32FromUint32(cparams.FminMatch))
	if ERR_isError(tls, err_code5) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code5
	}
	err_code6 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_targetLength), libc.Int32FromUint32(cparams.FtargetLength))
	if ERR_isError(tls, err_code6) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code6
	}
	err_code7 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_strategy), cparams.Fstrategy)
	if ERR_isError(tls, err_code7) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code7
	}
	return uint64(0)
}

func ZSTD_CCtx_setFParams(tls *libc.TLS, cctx uintptr, fparams ZSTD_frameParameters) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	_ = libc.Uint64FromInt64(1)
	err_code = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_contentSizeFlag), libc.BoolInt32(fparams.FcontentSizeFlag != 0))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_checksumFlag), libc.BoolInt32(fparams.FchecksumFlag != 0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, cctx, int32(ZSTD_c_dictIDFlag), libc.BoolInt32(fparams.FnoDictIDFlag == 0))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

func ZSTD_CCtx_setParams(tls *libc.TLS, cctx uintptr, params ZSTD_parameters) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	/* First check cParams, because we want to update all or none. */
	err_code = ZSTD_checkCParams(tls, params.FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	/* Next set fParams, because this could fail if the cctx isn't in init stage. */
	err_code1 = ZSTD_CCtx_setFParams(tls, cctx, params.FfParams)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	/* Finally set cParams, which should succeed. */
	err_code2 = ZSTD_CCtx_setCParams(tls, cctx, params.FcParams)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

func ZSTD_CCtx_setPledgedSrcSize(tls *libc.TLS, cctx uintptr, pledgedSrcSize uint64) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2036, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne = pledgedSrcSize + uint64(1)
	return uint64(0)
}

// C documentation
//
//	/**
//	 * Initializes the local dictionary using requested parameters.
//	 * NOTE: Initialization does not employ the pledged src size,
//	 * because the dictionary may be used for multiple compressions.
//	 */
func ZSTD_initLocalDict(tls *libc.TLS, cctx uintptr) (r size_t) {
	var dl uintptr
	_ = dl
	dl = cctx + 3688
	if (*ZSTD_localDict)(unsafe.Pointer(dl)).Fdict == libc.UintptrFromInt32(0) {
		/* No local dictionary. */
		return uint64(0)
	}
	if (*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict != libc.UintptrFromInt32(0) {
		/* Local dictionary already initialized. */
		return uint64(0)
	}
	(*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict = ZSTD_createCDict_advanced2(tls, (*ZSTD_localDict)(unsafe.Pointer(dl)).Fdict, (*ZSTD_localDict)(unsafe.Pointer(dl)).FdictSize, int32(ZSTD_dlm_byRef), (*ZSTD_localDict)(unsafe.Pointer(dl)).FdictContentType, cctx+16, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
	if !((*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2085, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict = (*ZSTD_localDict)(unsafe.Pointer(dl)).Fcdict
	return uint64(0)
}

func ZSTD_CCtx_loadDictionary_advanced(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var dictBuffer uintptr
	_ = dictBuffer
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2118, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearAllDicts(tls, cctx)                                  /* erase any previously set dictionary */
	if dict == libc.UintptrFromInt32(0) || dictSize == uint64(0) { /* no dictionary */
		return uint64(0)
	}
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fdict = dict
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstaticSize != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2174, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		dictBuffer = ZSTD_customMalloc(tls, dictSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem)
		if dictBuffer == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2236, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		libc.Xmemcpy(tls, dictBuffer, dict, dictSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictBuffer = dictBuffer /* owned ptr to free */
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fdict = dictBuffer       /* read-only reference */
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictSize = dictSize
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.FdictContentType = dictContentType
	return uint64(0)
}

func ZSTD_CCtx_loadDictionary_byReference(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_CCtx_loadDictionary_advanced(tls, cctx, dict, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto))
}

func ZSTD_CCtx_loadDictionary(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_CCtx_loadDictionary_advanced(tls, cctx, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto))
}

func ZSTD_CCtx_refCDict(tls *libc.TLS, cctx uintptr, cdict uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2277, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	/* Free the existing local cdict (if any) to save memory. */
	ZSTD_clearAllDicts(tls, cctx)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict = cdict
	return uint64(0)
}

func ZSTD_CCtx_refThreadPool(tls *libc.TLS, cctx uintptr, pool uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2322, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fpool = pool
	return uint64(0)
}

func ZSTD_CCtx_refPrefix(tls *libc.TLS, cctx uintptr, prefix uintptr, prefixSize size_t) (r size_t) {
	return ZSTD_CCtx_refPrefix_advanced(tls, cctx, prefix, prefixSize, int32(ZSTD_dct_rawContent))
}

func ZSTD_CCtx_refPrefix_advanced(tls *libc.TLS, cctx uintptr, prefix uintptr, prefixSize size_t, dictContentType ZSTD_dictContentType_e) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2367, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearAllDicts(tls, cctx)
	if prefix != libc.UintptrFromInt32(0) && prefixSize > uint64(0) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.Fdict = prefix
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictSize = prefixSize
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictContentType = dictContentType
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_CCtx_reset() :
//	 *  Also dumps dictionary */
func ZSTD_CCtx_reset(tls *libc.TLS, cctx uintptr, reset ZSTD_ResetDirective) (r size_t) {
	if reset == int32(ZSTD_reset_session_only) || reset == int32(ZSTD_reset_session_and_parameters) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage = int32(zcss_init)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne = uint64(0)
	}
	if reset == int32(ZSTD_reset_parameters) || reset == int32(ZSTD_reset_session_and_parameters) {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage != int32(zcss_init) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2414, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
		}
		ZSTD_clearAllDicts(tls, cctx)
		return ZSTD_CCtxParams_reset(tls, cctx+16)
	}
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_checkCParams() :
//	    control CParam values remain within authorized range.
//	    @return : 0, or an error code if one value is beyond authorized range */
func ZSTD_checkCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r size_t) {
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_windowLog), libc.Int32FromUint32(cParams.FwindowLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_chainLog), libc.Int32FromUint32(cParams.FchainLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_hashLog), libc.Int32FromUint32(cParams.FhashLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_searchLog), libc.Int32FromUint32(cParams.FsearchLog)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_minMatch), libc.Int32FromUint32(cParams.FminMatch)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_targetLength), libc.Int32FromUint32(cParams.FtargetLength)) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if !(ZSTD_cParam_withinBounds(tls, int32(ZSTD_c_strategy), cParams.Fstrategy) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1894, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_clampCParams() :
//	 *  make CParam values within valid range.
//	 *  @return : valid CParams */
func ZSTD_clampCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r ZSTD_compressionParameters) {
	var bounds, bounds1, bounds2, bounds3, bounds4, bounds5, bounds6 ZSTD_bounds
	_, _, _, _, _, _, _ = bounds, bounds1, bounds2, bounds3, bounds4, bounds5, bounds6
	bounds = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_windowLog))
	if libc.Int32FromUint32(cParams.FwindowLog) < bounds.FlowerBound {
		cParams.FwindowLog = libc.Uint32FromInt32(bounds.FlowerBound)
	} else {
		if libc.Int32FromUint32(cParams.FwindowLog) > bounds.FupperBound {
			cParams.FwindowLog = libc.Uint32FromInt32(bounds.FupperBound)
		}
	}
	bounds1 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_chainLog))
	if libc.Int32FromUint32(cParams.FchainLog) < bounds1.FlowerBound {
		cParams.FchainLog = libc.Uint32FromInt32(bounds1.FlowerBound)
	} else {
		if libc.Int32FromUint32(cParams.FchainLog) > bounds1.FupperBound {
			cParams.FchainLog = libc.Uint32FromInt32(bounds1.FupperBound)
		}
	}
	bounds2 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_hashLog))
	if libc.Int32FromUint32(cParams.FhashLog) < bounds2.FlowerBound {
		cParams.FhashLog = libc.Uint32FromInt32(bounds2.FlowerBound)
	} else {
		if libc.Int32FromUint32(cParams.FhashLog) > bounds2.FupperBound {
			cParams.FhashLog = libc.Uint32FromInt32(bounds2.FupperBound)
		}
	}
	bounds3 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_searchLog))
	if libc.Int32FromUint32(cParams.FsearchLog) < bounds3.FlowerBound {
		cParams.FsearchLog = libc.Uint32FromInt32(bounds3.FlowerBound)
	} else {
		if libc.Int32FromUint32(cParams.FsearchLog) > bounds3.FupperBound {
			cParams.FsearchLog = libc.Uint32FromInt32(bounds3.FupperBound)
		}
	}
	bounds4 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_minMatch))
	if libc.Int32FromUint32(cParams.FminMatch) < bounds4.FlowerBound {
		cParams.FminMatch = libc.Uint32FromInt32(bounds4.FlowerBound)
	} else {
		if libc.Int32FromUint32(cParams.FminMatch) > bounds4.FupperBound {
			cParams.FminMatch = libc.Uint32FromInt32(bounds4.FupperBound)
		}
	}
	bounds5 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_targetLength))
	if libc.Int32FromUint32(cParams.FtargetLength) < bounds5.FlowerBound {
		cParams.FtargetLength = libc.Uint32FromInt32(bounds5.FlowerBound)
	} else {
		if libc.Int32FromUint32(cParams.FtargetLength) > bounds5.FupperBound {
			cParams.FtargetLength = libc.Uint32FromInt32(bounds5.FupperBound)
		}
	}
	bounds6 = ZSTD_cParam_getBounds(tls, int32(ZSTD_c_strategy))
	if cParams.Fstrategy < bounds6.FlowerBound {
		cParams.Fstrategy = bounds6.FlowerBound
	} else {
		if cParams.Fstrategy > bounds6.FupperBound {
			cParams.Fstrategy = bounds6.FupperBound
		}
	}
	return cParams
}

// C documentation
//
//	/** ZSTD_cycleLog() :
//	 *  condition for correct operation : hashLog > 1 */
func ZSTD_cycleLog(tls *libc.TLS, hashLog U32, strat ZSTD_strategy) (r U32) {
	var btScale U32
	_ = btScale
	btScale = libc.BoolUint32(libc.Uint32FromInt32(strat) >= uint32(ZSTD_btlazy2))
	return hashLog - btScale
}

// C documentation
//
//	/** ZSTD_dictAndWindowLog() :
//	 * Returns an adjusted window log that is large enough to fit the source and the dictionary.
//	 * The zstd format says that the entire dictionary is valid if one byte of the dictionary
//	 * is within the window. So the hashLog and chainLog should be large enough to reference both
//	 * the dictionary and the window. So we must use this adjusted dictAndWindowLog when downsizing
//	 * the hashLog and windowLog.
//	 * NOTE: srcSize must not be ZSTD_CONTENTSIZE_UNKNOWN.
//	 */
func ZSTD_dictAndWindowLog(tls *libc.TLS, windowLog U32, srcSize U64, dictSize U64) (r U32) {
	var dictAndWindowSize, maxWindowSize, windowSize U64
	_, _, _ = dictAndWindowSize, maxWindowSize, windowSize
	maxWindowSize = uint64(libc.Uint64FromUint64(1) << libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64))
	/* No dictionary ==> No change */
	if dictSize == uint64(0) {
		return windowLog
	}
	/* Handled in ZSTD_adjustCParams_internal() */
	windowSize = uint64(uint64(1) << windowLog)
	dictAndWindowSize = dictSize + windowSize
	/* If the window size is already large enough to fit both the source and the dictionary
	 * then just use the window size. Otherwise adjust so that it fits the dictionary and
	 * the window.
	 */
	if windowSize >= dictSize+srcSize {
		return windowLog /* Window size large enough already */
	} else {
		if dictAndWindowSize >= maxWindowSize {
			return libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)) /* Larger than max window log */
		} else {
			return ZSTD_highbit32(tls, uint32(dictAndWindowSize)-uint32(1)) + uint32(1)
		}
	}
	return r
}

// C documentation
//
//	/** ZSTD_adjustCParams_internal() :
//	 *  optimize `cPar` for a specified input (`srcSize` and `dictSize`).
//	 *  mostly downsize to reduce memory consumption and initialization latency.
//	 * `srcSize` can be ZSTD_CONTENTSIZE_UNKNOWN when not known.
//	 * `mode` is the mode for parameter adjustment. See docs for `ZSTD_CParamMode_e`.
//	 *  note : `srcSize==0` means 0!
//	 *  condition : cPar is presumed validated (can be checked using ZSTD_checkCParams()). */
func ZSTD_adjustCParams_internal(tls *libc.TLS, _cPar ZSTD_compressionParameters, srcSize uint64, dictSize size_t, mode ZSTD_CParamMode_e, useRowMatchFinder ZSTD_ParamSwitch_e) (r ZSTD_compressionParameters) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cPar
	var cycleLog, dictAndWindowLog, maxHashLog, maxRowHashLog, maxShortCacheHashLog, rowLog, srcLog, tSize U32
	var maxWindowResize, minSrcSize U64
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = cycleLog, dictAndWindowLog, maxHashLog, maxRowHashLog, maxShortCacheHashLog, maxWindowResize, minSrcSize, rowLog, srcLog, tSize, v1, v2, v3
	minSrcSize = uint64(513) /* (1<<9) + 1 */
	maxWindowResize = uint64(libc.Uint64FromUint64(1) << (libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64) - libc.Int32FromInt32(1)))
	/* Cascade the selected strategy down to the next-highest one built into
	 * this binary. */
	switch mode {
	case int32(ZSTD_cpm_unknown):
		fallthrough
	case int32(ZSTD_cpm_noAttachDict):
		/* If we don't know the source size, don't make any
		 * assumptions about it. We will already have selected
		 * smaller parameters if a dictionary is in use.
		 */
	case int32(ZSTD_cpm_createCDict):
		/* Assume a small source size when creating a dictionary
		 * with an unknown source size.
		 */
		if dictSize != 0 && srcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
			srcSize = minSrcSize
		}
	case int32(ZSTD_cpm_attachDict):
		/* Dictionary has its own dedicated parameters which have
		 * already been selected. We are selecting parameters
		 * for only the source.
		 */
		dictSize = uint64(0)
	default:
		break
	}
	/* resize windowLog if input is small enough, to use less memory */
	if srcSize <= maxWindowResize && dictSize <= maxWindowResize {
		tSize = uint32(srcSize + dictSize)
		if tSize < hashSizeMin {
			v1 = uint32(ZSTD_HASHLOG_MIN)
		} else {
			v1 = ZSTD_highbit32(tls, tSize-uint32(1)) + uint32(1)
		}
		srcLog = v1
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog > srcLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog = srcLog
		}
	}
	if srcSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		dictAndWindowLog = ZSTD_dictAndWindowLog(tls, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog, srcSize, dictSize)
		cycleLog = ZSTD_cycleLog(tls, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).Fstrategy)
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog > dictAndWindowLog+uint32(1) {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog = dictAndWindowLog + uint32(1)
		}
		if cycleLog > dictAndWindowLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog -= cycleLog - dictAndWindowLog
		}
	}
	if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog < uint32(ZSTD_WINDOWLOG_ABSOLUTEMIN) {
		(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog = uint32(ZSTD_WINDOWLOG_ABSOLUTEMIN)
	} /* minimum wlog required for valid frame header */
	/* We can't use more than 32 bits of hash in total, so that means that we require:
	 * (hashLog + 8) <= 32 && (chainLog + 8) <= 32
	 */
	if mode == int32(ZSTD_cpm_createCDict) && ZSTD_CDictIndicesAreTagged(tls, bp) != 0 {
		maxShortCacheHashLog = libc.Uint32FromInt32(libc.Int32FromInt32(32) - libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS))
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog > maxShortCacheHashLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog = maxShortCacheHashLog
		}
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog > maxShortCacheHashLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FchainLog = maxShortCacheHashLog
		}
	}
	/* At this point, we aren't 100% sure if we are using the row match finder.
	 * Unless it is explicitly disabled, conservatively assume that it is enabled.
	 * In this case it will only be disabled for small sources, so shrinking the
	 * hash log a little bit shouldn't result in any ratio loss.
	 */
	if useRowMatchFinder == int32(ZSTD_ps_auto) {
		useRowMatchFinder = int32(ZSTD_ps_enable)
	}
	/* We can't hash more than 32-bits in total. So that means that we require:
	 * (hashLog - rowLog + 8) <= 32
	 */
	if ZSTD_rowMatchFinderUsed(tls, (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).Fstrategy, useRowMatchFinder) != 0 {
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v2 = (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog
		} else {
			v2 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		if libc.Uint32FromInt32(libc.Int32FromInt32(4)) > v2 {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
		} else {
			if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
				v3 = (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FsearchLog
			} else {
				v3 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
			}
			v1 = v3
		}
		/* Switch to 32-entry rows if searchLog is 5 (or more) */
		rowLog = v1
		maxRowHashLog = libc.Uint32FromInt32(libc.Int32FromInt32(32) - libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS))
		maxHashLog = maxRowHashLog + rowLog
		if (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog > maxHashLog {
			(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FhashLog = maxHashLog
		}
	}
	return *(*ZSTD_compressionParameters)(unsafe.Pointer(bp))
}

var hashSizeMin = libc.Uint32FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_HASHLOG_MIN))

func ZSTD_adjustCParams(tls *libc.TLS, cPar ZSTD_compressionParameters, srcSize uint64, dictSize size_t) (r ZSTD_compressionParameters) {
	cPar = ZSTD_clampCParams(tls, cPar) /* resulting cPar is necessarily valid (all parameters within range) */
	if srcSize == uint64(0) {
		srcSize = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	return ZSTD_adjustCParams_internal(tls, cPar, srcSize, dictSize, int32(ZSTD_cpm_unknown), int32(ZSTD_ps_auto))
}

func ZSTD_overrideCParams(tls *libc.TLS, cParams uintptr, overrides uintptr) {
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FwindowLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FwindowLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FhashLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FhashLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FchainLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FchainLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FsearchLog != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FsearchLog
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FminMatch != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FminMatch
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FtargetLength != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).FtargetLength
	}
	if (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).Fstrategy != 0 {
		(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy = (*ZSTD_compressionParameters)(unsafe.Pointer(overrides)).Fstrategy
	}
}

func ZSTD_getCParamsFromCCtxParams(tls *libc.TLS, CCtxParams uintptr, srcSizeHint U64, dictSize size_t, mode ZSTD_CParamMode_e) (r ZSTD_compressionParameters) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _ /* cParams at bp+0 */ ZSTD_compressionParameters
	if srcSizeHint == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint > 0 {
		srcSizeHint = libc.Uint64FromInt32((*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FsrcSizeHint)
	}
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = ZSTD_getCParams_internal(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FcompressionLevel, srcSizeHint, dictSize, mode)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog = uint32(ZSTD_WINDOWLOG_LIMIT_DEFAULT)
	}
	ZSTD_overrideCParams(tls, bp, CCtxParams+4)
	/* srcSizeHint == 0 means 0 */
	return ZSTD_adjustCParams_internal(tls, *(*ZSTD_compressionParameters)(unsafe.Pointer(bp)), srcSizeHint, dictSize, mode, (*ZSTD_CCtx_params)(unsafe.Pointer(CCtxParams)).FuseRowMatchFinder)
}

func ZSTD_sizeof_matchState(tls *libc.TLS, cParams uintptr, useRowMatchFinder ZSTD_ParamSwitch_e, enableDedicatedDictSearch int32, forCCtx U32) (r size_t) {
	var chainSize, h3Size, hSize, lazyAdditionalSpace, optPotentialSpace, optSpace, slackSpace, tableSpace size_t
	var hashLog3 U32
	var v1, v4, v5, v6 uint64
	var v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = chainSize, h3Size, hSize, hashLog3, lazyAdditionalSpace, optPotentialSpace, optSpace, slackSpace, tableSpace, v1, v2, v3, v4, v5, v6
	if ZSTD_allocateChainTable(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder, libc.BoolUint32(enableDedicatedDictSearch != 0 && !(forCCtx != 0))) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	} else {
		v1 = uint64(0)
	}
	/* chain table size should be 0 for fast or row-hash strategies */
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	if forCCtx != 0 && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch == uint32(3) {
		if libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX)) < (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog {
			v3 = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX))
		} else {
			v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
		}
		v2 = v3
	} else {
		v2 = uint32(0)
	}
	hashLog3 = v2
	if hashLog3 != 0 {
		v4 = libc.Uint64FromInt32(1) << hashLog3
	} else {
		v4 = uint64(0)
	}
	h3Size = v4
	/* We don't use ZSTD_cwksp_alloc_size() here because the tables aren't
	 * surrounded by redzones in ASAN. */
	tableSpace = chainSize*uint64(4) + hSize*uint64(4) + h3Size*uint64(4)
	optPotentialSpace = ZSTD_cwksp_aligned64_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(MaxLL)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(MaxOff)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits))*libc.Uint64FromInt64(4)) + ZSTD_cwksp_aligned64_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(8)) + ZSTD_cwksp_aligned64_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(28))
	if ZSTD_rowMatchFinderUsed(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder) != 0 {
		v5 = ZSTD_cwksp_aligned64_alloc_size(tls, hSize)
	} else {
		v5 = uint64(0)
	}
	lazyAdditionalSpace = v5
	if forCCtx != 0 && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) {
		v6 = optPotentialSpace
	} else {
		v6 = uint64(0)
	}
	optSpace = v6
	slackSpace = ZSTD_cwksp_slack_space_required(tls)
	/* tables are guaranteed to be sized in multiples of 64 bytes (or 16 uint32_t) */
	_ = libc.Uint64FromInt64(1)
	return tableSpace + optSpace + slackSpace + lazyAdditionalSpace
}

// C documentation
//
//	/* Helper function for calculating memory requirements.
//	 * Gives a tighter bound than ZSTD_sequenceBound() by taking minMatch into account. */
func ZSTD_maxNbSeq(tls *libc.TLS, blockSize size_t, minMatch uint32, useSequenceProducer int32) (r size_t) {
	var divider U32
	var v1 int32
	_, _ = divider, v1
	if minMatch == uint32(3) || useSequenceProducer != 0 {
		v1 = int32(3)
	} else {
		v1 = int32(4)
	}
	divider = libc.Uint32FromInt32(v1)
	return blockSize / uint64(divider)
}

func ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls *libc.TLS, cParams uintptr, ldmParams uintptr, isStatic int32, useRowMatchFinder ZSTD_ParamSwitch_e, buffInSize size_t, buffOutSize size_t, pledgedSrcSize U64, useSequenceProducer int32, maxBlockSize size_t) (r size_t) {
	var blockSize, blockStateSpace, bufferSpace, cctxSpace, externalSeqSpace, ldmSeqSpace, ldmSpace, matchStateSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, tmpWorkSpace, tokenSpace, windowSize size_t
	var v1, v2, v3, v4, v5, v6, v7 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, blockStateSpace, bufferSpace, cctxSpace, externalSeqSpace, ldmSeqSpace, ldmSpace, matchStateSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, tmpWorkSpace, tokenSpace, windowSize, v1, v2, v3, v4, v5, v6, v7
	if uint64(1)<<(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog < pledgedSrcSize {
		v2 = uint64(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	} else {
		v2 = pledgedSrcSize
	}
	if uint64(1) > v2 {
		v1 = uint64(1)
	} else {
		if uint64(1)<<(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog < pledgedSrcSize {
			v3 = uint64(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
		} else {
			v3 = pledgedSrcSize
		}
		v1 = v3
	}
	windowSize = v1
	if ZSTD_resolveMaxBlockSize(tls, maxBlockSize) < windowSize {
		v4 = ZSTD_resolveMaxBlockSize(tls, maxBlockSize)
	} else {
		v4 = windowSize
	}
	blockSize = v4
	maxNbSeq = ZSTD_maxNbSeq(tls, blockSize, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch, useSequenceProducer)
	tokenSpace = ZSTD_cwksp_alloc_size(tls, uint64(WILDCOPY_OVERLENGTH)+blockSize) + ZSTD_cwksp_aligned64_alloc_size(tls, maxNbSeq*uint64(8)) + uint64(3)*ZSTD_cwksp_alloc_size(tls, maxNbSeq*uint64(1))
	tmpWorkSpace = ZSTD_cwksp_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2)))
	blockStateSpace = uint64(2) * ZSTD_cwksp_alloc_size(tls, uint64(5632))
	matchStateSize = ZSTD_sizeof_matchState(tls, cParams, useRowMatchFinder, 0, uint32(1))
	ldmSpace = ZSTD_ldm_getTableSize(tls, *(*ldmParams_t)(unsafe.Pointer(ldmParams)))
	maxNbLdmSeq = ZSTD_ldm_getMaxNbSeq(tls, *(*ldmParams_t)(unsafe.Pointer(ldmParams)), blockSize)
	if (*ldmParams_t)(unsafe.Pointer(ldmParams)).FenableLdm == int32(ZSTD_ps_enable) {
		v5 = ZSTD_cwksp_aligned64_alloc_size(tls, maxNbLdmSeq*uint64(12))
	} else {
		v5 = uint64(0)
	}
	ldmSeqSpace = v5
	bufferSpace = ZSTD_cwksp_alloc_size(tls, buffInSize) + ZSTD_cwksp_alloc_size(tls, buffOutSize)
	if isStatic != 0 {
		v6 = ZSTD_cwksp_alloc_size(tls, uint64(5280))
	} else {
		v6 = uint64(0)
	}
	cctxSpace = v6
	maxNbExternalSeq = ZSTD_sequenceBound(tls, blockSize)
	if useSequenceProducer != 0 {
		v7 = ZSTD_cwksp_aligned64_alloc_size(tls, maxNbExternalSeq*uint64(16))
	} else {
		v7 = uint64(0)
	}
	externalSeqSpace = v7
	neededSpace = cctxSpace + tmpWorkSpace + blockStateSpace + ldmSpace + ldmSeqSpace + matchStateSize + tokenSpace + bufferSpace + externalSeqSpace
	return neededSpace
}

func ZSTD_estimateCCtxSize_usingCCtxParams(tls *libc.TLS, params uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var useRowMatchFinder ZSTD_ParamSwitch_e
	var _ /* cParams at bp+0 */ ZSTD_compressionParameters
	_ = useRowMatchFinder
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = ZSTD_getCParamsFromCCtxParams(tls, params, uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), uint64(0), int32(ZSTD_cpm_noAttachDict))
	useRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, bp)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FnbWorkers > 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2467, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	/* estimateCCtxSize is for one-shot compression. So no buffers should
	 * be needed. However, we still allocate two 0-sized buffers, which can
	 * take space under ASAN. */
	return ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls, bp, params+96, int32(1), useRowMatchFinder, uint64(0), uint64(0), uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), ZSTD_hasExtSeqProd(tls, params), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
}

func ZSTD_estimateCCtxSize_usingCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var noRowCCtxSize, rowCCtxSize size_t
	var v1 uint64
	var _ /* initialParams at bp+0 */ ZSTD_CCtx_params
	_, _, _ = noRowCCtxSize, rowCCtxSize, v1
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = ZSTD_makeCCtxParamsFromCParams(tls, cParams)
	if ZSTD_rowMatchFinderSupported(tls, cParams.Fstrategy) != 0 {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_disable)
		noRowCCtxSize = ZSTD_estimateCCtxSize_usingCCtxParams(tls, bp)
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_enable)
		rowCCtxSize = ZSTD_estimateCCtxSize_usingCCtxParams(tls, bp)
		if noRowCCtxSize > rowCCtxSize {
			v1 = noRowCCtxSize
		} else {
			v1 = rowCCtxSize
		}
		return v1
	} else {
		return ZSTD_estimateCCtxSize_usingCCtxParams(tls, bp)
	}
	return r
}

func ZSTD_estimateCCtxSize_internal(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var cParams ZSTD_compressionParameters
	var largestSize size_t
	var tier int32
	var v2 uint64
	_, _, _, _ = cParams, largestSize, tier, v2
	tier = 0
	largestSize = uint64(0)
	for {
		if !(tier < int32(4)) {
			break
		}
		/* Choose the set of cParams for a given level across all srcSizes that give the largest cctxSize */
		cParams = ZSTD_getCParams_internal(tls, compressionLevel, srcSizeTiers[tier], uint64(0), int32(ZSTD_cpm_noAttachDict))
		if ZSTD_estimateCCtxSize_usingCParams(tls, cParams) > largestSize {
			v2 = ZSTD_estimateCCtxSize_usingCParams(tls, cParams)
		} else {
			v2 = largestSize
		}
		largestSize = v2
		goto _1
	_1:
		;
		tier = tier + 1
	}
	return largestSize
}

var srcSizeTiers = [4]uint64{
	0: libc.Uint64FromInt32(libc.Int32FromInt32(16) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	1: libc.Uint64FromInt32(libc.Int32FromInt32(128) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	2: libc.Uint64FromInt32(libc.Int32FromInt32(256) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	3: libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1),
}

func ZSTD_estimateCCtxSize(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var level, v2 int32
	var memBudget, newMB size_t
	_, _, _, _ = level, memBudget, newMB, v2
	memBudget = uint64(0)
	if compressionLevel < int32(1) {
		v2 = compressionLevel
	} else {
		v2 = int32(1)
	}
	level = v2
	for {
		if !(level <= compressionLevel) {
			break
		}
		/* Ensure monotonically increasing memory usage as compression level increases */
		newMB = ZSTD_estimateCCtxSize_internal(tls, level)
		if newMB > memBudget {
			memBudget = newMB
		}
		goto _1
	_1:
		;
		level = level + 1
	}
	return memBudget
}

func ZSTD_estimateCStreamSize_usingCCtxParams(tls *libc.TLS, params uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var blockSize, inBuffSize, outBuffSize size_t
	var useRowMatchFinder ZSTD_ParamSwitch_e
	var v1, v2, v3 uint64
	var _ /* cParams at bp+0 */ ZSTD_compressionParameters
	_, _, _, _, _, _, _ = blockSize, inBuffSize, outBuffSize, useRowMatchFinder, v1, v2, v3
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FnbWorkers > 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2467, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = ZSTD_getCParamsFromCCtxParams(tls, params, uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), uint64(0), int32(ZSTD_cpm_noAttachDict))
	if ZSTD_resolveMaxBlockSize(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize) < libc.Uint64FromInt32(1)<<(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog {
		v1 = ZSTD_resolveMaxBlockSize(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
	} else {
		v1 = libc.Uint64FromInt32(1) << (*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog
	}
	blockSize = v1
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FinBufferMode == int32(ZSTD_bm_buffered) {
		v2 = libc.Uint64FromInt32(1)<<(*(*ZSTD_compressionParameters)(unsafe.Pointer(bp))).FwindowLog + blockSize
	} else {
		v2 = uint64(0)
	}
	inBuffSize = v2
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FoutBufferMode == int32(ZSTD_bm_buffered) {
		v3 = ZSTD_compressBound(tls, blockSize) + uint64(1)
	} else {
		v3 = uint64(0)
	}
	outBuffSize = v3
	useRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, params+4)
	return ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls, bp, params+96, int32(1), useRowMatchFinder, inBuffSize, outBuffSize, uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), ZSTD_hasExtSeqProd(tls, params), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
	return r
}

func ZSTD_estimateCStreamSize_usingCParams(tls *libc.TLS, cParams ZSTD_compressionParameters) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var noRowCCtxSize, rowCCtxSize size_t
	var v1 uint64
	var _ /* initialParams at bp+0 */ ZSTD_CCtx_params
	_, _, _ = noRowCCtxSize, rowCCtxSize, v1
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = ZSTD_makeCCtxParamsFromCParams(tls, cParams)
	if ZSTD_rowMatchFinderSupported(tls, cParams.Fstrategy) != 0 {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_disable)
		noRowCCtxSize = ZSTD_estimateCStreamSize_usingCCtxParams(tls, bp)
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = int32(ZSTD_ps_enable)
		rowCCtxSize = ZSTD_estimateCStreamSize_usingCCtxParams(tls, bp)
		if noRowCCtxSize > rowCCtxSize {
			v1 = noRowCCtxSize
		} else {
			v1 = rowCCtxSize
		}
		return v1
	} else {
		return ZSTD_estimateCStreamSize_usingCCtxParams(tls, bp)
	}
	return r
}

func ZSTD_estimateCStreamSize_internal(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var cParams ZSTD_compressionParameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), uint64(0), int32(ZSTD_cpm_noAttachDict))
	return ZSTD_estimateCStreamSize_usingCParams(tls, cParams)
}

func ZSTD_estimateCStreamSize(tls *libc.TLS, compressionLevel int32) (r size_t) {
	var level, v2 int32
	var memBudget, newMB size_t
	_, _, _, _ = level, memBudget, newMB, v2
	memBudget = uint64(0)
	if compressionLevel < int32(1) {
		v2 = compressionLevel
	} else {
		v2 = int32(1)
	}
	level = v2
	for {
		if !(level <= compressionLevel) {
			break
		}
		newMB = ZSTD_estimateCStreamSize_internal(tls, level)
		if newMB > memBudget {
			memBudget = newMB
		}
		goto _1
	_1:
		;
		level = level + 1
	}
	return memBudget
}

// C documentation
//
//	/* ZSTD_getFrameProgression():
//	 * tells how much data has been consumed (input) and produced (output) for current frame.
//	 * able to count progression inside worker threads (non-blocking mode).
//	 */
func ZSTD_getFrameProgression(tls *libc.TLS, cctx uintptr) (r ZSTD_frameProgression) {
	var buffered size_t
	var fp ZSTD_frameProgression
	var v1 uint64
	_, _, _ = buffered, fp, v1
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers > 0 {
		return ZSTDMT_getFrameProgression(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuff == libc.UintptrFromInt32(0) {
		v1 = uint64(0)
	} else {
		v1 = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffPos - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinToCompress
	}
	buffered = v1
	if buffered != 0 {
	}
	fp.Fingested = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize + buffered
	fp.Fconsumed = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize
	fp.Fproduced = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize
	fp.Fflushed = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize /* simplified; some data might still be left within streaming output buffer */
	fp.FcurrentJobID = uint32(0)
	fp.FnbActiveWorkers = uint32(0)
	return fp
	return r
}

// C documentation
//
//	/*! ZSTD_toFlushNow()
//	 *  Only useful for multithreading scenarios currently (nbWorkers >= 1).
//	 */
func ZSTD_toFlushNow(tls *libc.TLS, cctx uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers > 0 {
		return ZSTDMT_toFlushNow(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	}
	_ = cctx
	return uint64(0) /* over-simplification; could also check if context is currently running in streaming mode, and in which case, report how many bytes are left to be flushed within output buffer */
}

func ZSTD_assertEqualCParams(tls *libc.TLS, cParams1 ZSTD_compressionParameters, cParams2 ZSTD_compressionParameters) {
	_ = cParams1
	_ = cParams2
}

func ZSTD_reset_compressedBlockState(tls *libc.TLS, bs uintptr) {
	var i int32
	_ = i
	i = 0
	for {
		if !(i < int32(ZSTD_REP_NUM)) {
			break
		}
		*(*U32)(unsafe.Pointer(bs + 5616 + uintptr(i)*4)) = repStartValue[i]
		goto _1
	_1:
		;
		i = i + 1
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Fhuf.FrepeatMode = int32(HUF_repeat_none)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Fmatchlength_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Flitlength_repeatMode = int32(FSE_repeat_none)
}

// C documentation
//
//	/*! ZSTD_invalidateMatchState()
//	 *  Invalidate all the matches in the match finder tables.
//	 *  Requires nextSrc and base to be set (can be NULL).
//	 */
func ZSTD_invalidateMatchState(tls *libc.TLS, ms uintptr) {
	ZSTD_window_clear(tls, ms)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd = uint32(0)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitLengthSum = uint32(0) /* force reset of btopt stats */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState = libc.UintptrFromInt32(0)
}

// C documentation
//
//	/**
//	 * Controls, for this matchState reset, whether the tables need to be cleared /
//	 * prepared for the coming compression (ZSTDcrp_makeClean), or whether the
//	 * tables can be left unclean (ZSTDcrp_leaveDirty), because we know that a
//	 * subsequent operation will overwrite the table space anyways (e.g., copying
//	 * the matchState contents in from a CDict).
//	 */
type ZSTD_compResetPolicy_e = int32

const ZSTDcrp_makeClean = 0
const ZSTDcrp_leaveDirty = 1

// C documentation
//
//	/**
//	 * Controls, for this matchState reset, whether indexing can continue where it
//	 * left off (ZSTDirp_continue), or whether it needs to be restarted from zero
//	 * (ZSTDirp_reset).
//	 */
type ZSTD_indexResetPolicy_e = int32

const ZSTDirp_continue = 0
const ZSTDirp_reset = 1

type ZSTD_resetTarget_e = int32

const ZSTD_resetTarget_CDict = 0
const ZSTD_resetTarget_CCtx = 1

// C documentation
//
//	/* Mixes bits in a 64 bits in a value, based on XXH3_rrmxmx */
func ZSTD_bitmix(tls *libc.TLS, val U64, len1 U64) (r U64) {
	val = val ^ (ZSTD_rotateRight_U64(tls, val, uint32(49)) ^ ZSTD_rotateRight_U64(tls, val, uint32(24)))
	val = uint64(val * libc.Uint64FromUint64(0x9FB21C651E98DF25))
	val = val ^ (val>>libc.Int32FromInt32(35) + len1)
	val = uint64(val * libc.Uint64FromUint64(0x9FB21C651E98DF25))
	return val ^ val>>libc.Int32FromInt32(28)
}

// C documentation
//
//	/* Mixes in the hashSalt and hashSaltEntropy to create a new hashSalt */
func ZSTD_advanceHashSalt(tls *libc.TLS, ms uintptr) {
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt = ZSTD_bitmix(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt, uint64(8)) ^ ZSTD_bitmix(tls, uint64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSaltEntropy), uint64(4))
}

func ZSTD_reset_matchState(tls *libc.TLS, ms uintptr, ws uintptr, cParams uintptr, useRowMatchFinder ZSTD_ParamSwitch_e, crp ZSTD_compResetPolicy_e, forceResetIndex ZSTD_indexResetPolicy_e, forWho ZSTD_resetTarget_e) (r size_t) {
	var chainSize, h3Size, hSize, tagTableSize size_t
	var hashLog3, rowLog U32
	var v1, v4 uint64
	var v2, v3, v5 uint32
	_, _, _, _, _, _, _, _, _, _, _ = chainSize, h3Size, hSize, hashLog3, rowLog, tagTableSize, v1, v2, v3, v4, v5
	if ZSTD_allocateChainTable(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder, libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdedicatedDictSearch != 0 && forWho == int32(ZSTD_resetTarget_CDict))) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	} else {
		v1 = uint64(0)
	}
	/* disable chain table allocation for fast or row-based strategies */
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	if forWho == int32(ZSTD_resetTarget_CCtx) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch == uint32(3) {
		if libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX)) < (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog {
			v3 = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HASHLOG3_MAX))
		} else {
			v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
		}
		v2 = v3
	} else {
		v2 = uint32(0)
	}
	hashLog3 = v2
	if hashLog3 != 0 {
		v4 = libc.Uint64FromInt32(1) << hashLog3
	} else {
		v4 = uint64(0)
	}
	h3Size = v4
	if forceResetIndex == int32(ZSTDirp_reset) {
		ZSTD_window_init(tls, ms)
		ZSTD_cwksp_mark_tables_dirty(tls, ws)
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3 = hashLog3
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
	ZSTD_invalidateMatchState(tls, ms)
	/* check that allocation hasn't already failed */
	ZSTD_cwksp_clear_tables(tls, ws)
	/* table Space */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable = ZSTD_cwksp_reserve_table(tls, ws, hSize*uint64(4))
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable = ZSTD_cwksp_reserve_table(tls, ws, chainSize*uint64(4))
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable3 = ZSTD_cwksp_reserve_table(tls, ws, h3Size*uint64(4))
	if ZSTD_cwksp_reserve_failed(tls, ws) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2537, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	if crp != int32(ZSTDcrp_leaveDirty) {
		/* reset tables only */
		ZSTD_cwksp_clean_tables(tls, ws)
	}
	if ZSTD_rowMatchFinderUsed(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder) != 0 {
		/* Row match finder needs an additional table of hashes ("tags") */
		tagTableSize = hSize
		/* We want to generate a new salt in case we reset a Cctx, but we always want to use
		 * 0 when we reset a Cdict */
		if forWho == int32(ZSTD_resetTarget_CCtx) {
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable = ZSTD_cwksp_reserve_aligned_init_once(tls, ws, tagTableSize)
			ZSTD_advanceHashSalt(tls, ms)
		} else {
			/* When we are not salting we want to always memset the memory */
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable = ZSTD_cwksp_reserve_aligned64(tls, ws, tagTableSize)
			libc.Xmemset(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable, 0, tagTableSize)
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt = uint64(0)
		}
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
		} else {
			v3 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		if libc.Uint32FromInt32(libc.Int32FromInt32(4)) > v3 {
			v2 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
		} else {
			if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
				v5 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
			} else {
				v5 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
			}
			v2 = v5
		} /* Switch to 32-entry rows if searchLog is 5 (or more) */
		rowLog = v2
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog - rowLog
	}
	/* opt parser space */
	if forWho == int32(ZSTD_resetTarget_CCtx) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) {
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitLengthFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, libc.Uint64FromInt32(libc.Int32FromInt32(MaxLL)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FmatchLengthFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FoffCodeFreq = ZSTD_cwksp_reserve_aligned64(tls, ws, libc.Uint64FromInt32(libc.Int32FromInt32(MaxOff)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FmatchTable = ZSTD_cwksp_reserve_aligned64(tls, ws, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(8))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FpriceTable = ZSTD_cwksp_reserve_aligned64(tls, ws, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)+libc.Int32FromInt32(3))*libc.Uint64FromInt64(28))
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(cParams))
	if ZSTD_cwksp_reserve_failed(tls, ws) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2537, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_indexTooCloseToMax() :
//	 * minor optimization : prefer memset() rather than reduceIndex()
//	 * which is measurably slow in some circumstances (reported for Visual Studio).
//	 * Works when re-using a context for a lot of smallish inputs :
//	 * if all inputs are smaller than ZSTD_INDEXOVERFLOW_MARGIN,
//	 * memset() will be triggered before reduceIndex().
//	 */
func ZSTD_indexTooCloseToMax(tls *libc.TLS, w ZSTD_window_t) (r int32) {
	var v1 uint32
	_ = v1
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	return libc.BoolInt32(libc.Uint64FromInt64(int64(w.FnextSrc)-int64(w.Fbase)) > uint64(v1-libc.Uint32FromInt32(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20)))))
}

// C documentation
//
//	/** ZSTD_dictTooBig():
//	 * When dictionaries are larger than ZSTD_CHUNKSIZE_MAX they can't be loaded in
//	 * one go generically. So we ensure that in that case we reset the tables to zero,
//	 * so that we can load as much of the dictionary as possible.
//	 */
func ZSTD_dictTooBig(tls *libc.TLS, loadedDictSize size_t) (r int32) {
	var v1 uint32
	_ = v1
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	return libc.BoolInt32(loadedDictSize > uint64(libc.Uint32FromInt32(-libc.Int32FromInt32(1))-v1))
}

// C documentation
//
//	/*! ZSTD_resetCCtx_internal() :
//	 * @param loadedDictSize The size of the dictionary to be loaded
//	 * into the context, if any. If no dictionary is used, or the
//	 * dictionary is being attached / copied, then pass 0.
//	 * note : `params` are assumed fully validated at this stage.
//	 */
func ZSTD_resetCCtx_internal(tls *libc.TLS, zc uintptr, params uintptr, pledgedSrcSize U64, loadedDictSize size_t, crp ZSTD_compResetPolicy_e, zbuff ZSTD_buffered_policy_e) (r size_t) {
	var blockSize, buffInSize, buffOutSize, err_code, err_code1, err_code2, ldmHSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, numBuckets, windowSize size_t
	var dictTooBig, indexTooClose, resizeWorkspace, workspaceTooSmall, workspaceWasteful, v7 int32
	var needsIndexReset ZSTD_indexResetPolicy_e
	var ws uintptr
	var v1, v2, v3, v4, v5, v6 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, buffInSize, buffOutSize, dictTooBig, err_code, err_code1, err_code2, indexTooClose, ldmHSize, maxNbExternalSeq, maxNbLdmSeq, maxNbSeq, neededSpace, needsIndexReset, numBuckets, resizeWorkspace, windowSize, workspaceTooSmall, workspaceWasteful, ws, v1, v2, v3, v4, v5, v6, v7
	ws = zc + 704
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock = int32(1)
	/* Set applied params early so we can modify them for LDM,
	 * and point params at the applied params.
	 */
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(params))
	params = zc + 240
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* Adjust long distance matching parameters */
		ZSTD_ldm_adjustParameters(tls, zc+240+96, params+4)
	}
	if libc.Uint64FromInt32(1)<<(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog < pledgedSrcSize {
		v2 = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
	} else {
		v2 = pledgedSrcSize
	}
	if libc.Uint64FromInt32(libc.Int32FromInt32(1)) > v2 {
		v1 = libc.Uint64FromInt32(libc.Int32FromInt32(1))
	} else {
		if libc.Uint64FromInt32(1)<<(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog < pledgedSrcSize {
			v3 = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
		} else {
			v3 = pledgedSrcSize
		}
		v1 = v3
	}
	windowSize = v1
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize < windowSize {
		v4 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize
	} else {
		v4 = windowSize
	}
	blockSize = v4
	maxNbSeq = ZSTD_maxNbSeq(tls, blockSize, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FminMatch, ZSTD_hasExtSeqProd(tls, params))
	if zbuff == int32(ZSTDb_buffered) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FoutBufferMode == int32(ZSTD_bm_buffered) {
		v5 = ZSTD_compressBound(tls, blockSize) + uint64(1)
	} else {
		v5 = uint64(0)
	}
	buffOutSize = v5
	if zbuff == int32(ZSTDb_buffered) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FinBufferMode == int32(ZSTD_bm_buffered) {
		v6 = windowSize + blockSize
	} else {
		v6 = uint64(0)
	}
	buffInSize = v6
	maxNbLdmSeq = ZSTD_ldm_getMaxNbSeq(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams, blockSize)
	indexTooClose = ZSTD_indexTooCloseToMax(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FmatchState.Fwindow)
	dictTooBig = ZSTD_dictTooBig(tls, loadedDictSize)
	if indexTooClose != 0 || dictTooBig != 0 || !((*ZSTD_CCtx)(unsafe.Pointer(zc)).Finitialized != 0) {
		v7 = int32(ZSTDirp_reset)
	} else {
		v7 = int32(ZSTDirp_continue)
	}
	needsIndexReset = v7
	neededSpace = ZSTD_estimateCCtxSize_usingCCtxParams_internal(tls, params+4, params+96, libc.BoolInt32((*ZSTD_CCtx)(unsafe.Pointer(zc)).FstaticSize != uint64(0)), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, buffInSize, buffOutSize, pledgedSrcSize, ZSTD_hasExtSeqProd(tls, params), (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FmaxBlockSize)
	err_code = neededSpace
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2592, 0)
		}
		return err_code
	}
	if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FstaticSize != 0) {
		ZSTD_cwksp_bump_oversized_duration(tls, ws, uint64(0))
	}
	/* Check if workspace is large enough, alloc a new one if needed */
	workspaceTooSmall = libc.BoolInt32(ZSTD_cwksp_sizeof(tls, ws) < neededSpace)
	workspaceWasteful = ZSTD_cwksp_check_wasteful(tls, ws, neededSpace)
	resizeWorkspace = libc.BoolInt32(workspaceTooSmall != 0 || workspaceWasteful != 0)
	if resizeWorkspace != 0 {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FstaticSize != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2619, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		needsIndexReset = int32(ZSTDirp_reset)
		ZSTD_cwksp_free(tls, ws, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FcustomMem)
		err_code1 = ZSTD_cwksp_create(tls, ws, neededSpace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FcustomMem)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code1
		}
		/* Statically sized space.
		 * tmpWorkspace never moves,
		 * though prev/next block swap places */
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock = ZSTD_cwksp_reserve_object(tls, ws, uint64(5632))
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2643, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock = ZSTD_cwksp_reserve_object(tls, ws, uint64(5632))
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2672, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace = ZSTD_cwksp_reserve_object(tls, ws, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))+libc.Uint64FromInt64(4)*libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2)))
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+2701, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize = libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)) + libc.Uint64FromInt64(4)*libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(2))
	}
	ZSTD_cwksp_clear(tls, ws)
	/* init params */
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FmatchState.FcParams = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FmatchState.FprefetchCDictTables = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FprefetchCDictTables == int32(ZSTD_ps_enable))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FpledgedSrcSizePlusOne = pledgedSrcSize + uint64(1)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FconsumedSrcSize = uint64(0)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FproducedCSize = uint64(0)
	if pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FfParams.FcontentSizeFlag = 0
	}
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockSizeMax = blockSize
	XXH_INLINE_XXH64_reset(tls, zc+808, uint64(0))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).Fstage = int32(ZSTDcs_init)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FdictID = uint32(0)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FdictContentSize = uint64(0)
	ZSTD_reset_compressedBlockState(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)
	err_code2 = ZSTD_reset_matchState(tls, zc+3224+16, ws, params+4, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, crp, needsIndexReset, int32(ZSTD_resetTarget_CCtx))
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FsequencesStart = ZSTD_cwksp_reserve_aligned64(tls, ws, maxNbSeq*uint64(8))
	/* ldm hash table */
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* TODO: avoid memset? */
		ldmHSize = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FhashLog
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FhashTable = ZSTD_cwksp_reserve_aligned64(tls, ws, ldmHSize*uint64(8))
		libc.Xmemset(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FhashTable, 0, ldmHSize*libc.Uint64FromInt64(8))
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmSequences = ZSTD_cwksp_reserve_aligned64(tls, ws, maxNbLdmSeq*uint64(12))
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FmaxNbLdmSequences = maxNbLdmSeq
		ZSTD_window_init(tls, zc+1056)
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FloadedDictEnd = uint32(0)
	}
	/* reserve space for block-level external sequences */
	if ZSTD_hasExtSeqProd(tls, params) != 0 {
		maxNbExternalSeq = ZSTD_sequenceBound(tls, blockSize)
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBufCapacity = maxNbExternalSeq
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf = ZSTD_cwksp_reserve_aligned64(tls, ws, maxNbExternalSeq*uint64(16))
	}
	/* buffers */
	/* ZSTD_wildcopy() is used to copy into the literals buffer,
	 * so we have to oversize the buffer by WILDCOPY_OVERLENGTH bytes.
	 */
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FlitStart = ZSTD_cwksp_reserve_buffer(tls, ws, blockSize+uint64(WILDCOPY_OVERLENGTH))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FmaxNbLit = blockSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FbufferedPolicy = zbuff
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FinBuffSize = buffInSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FinBuff = ZSTD_cwksp_reserve_buffer(tls, ws, buffInSize)
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FoutBuffSize = buffOutSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FoutBuff = ZSTD_cwksp_reserve_buffer(tls, ws, buffOutSize)
	/* ldm bucketOffsets table */
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* TODO: avoid memset? */
		numBuckets = libc.Uint64FromInt32(1) << ((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FhashLog - (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FbucketSizeLog)
		(*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FbucketOffsets = ZSTD_cwksp_reserve_buffer(tls, ws, numBuckets)
		libc.Xmemset(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmState.FbucketOffsets, 0, numBuckets)
	}
	/* sequences storage */
	ZSTD_referenceExternalSequences(tls, zc, libc.UintptrFromInt32(0), uint64(0))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FmaxNbSeq = maxNbSeq
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FllCode = ZSTD_cwksp_reserve_buffer(tls, ws, maxNbSeq*uint64(1))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FmlCode = ZSTD_cwksp_reserve_buffer(tls, ws, maxNbSeq*uint64(1))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FofCode = ZSTD_cwksp_reserve_buffer(tls, ws, maxNbSeq*uint64(1))
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).Finitialized = int32(1)
	return uint64(0)
	return r
}

// C documentation
//
//	/* ZSTD_invalidateRepCodes() :
//	 * ensures next compression will not use repcodes from previous block.
//	 * Note : only works with regular variant;
//	 *        do not use with extDict variant ! */
func ZSTD_invalidateRepCodes(tls *libc.TLS, cctx uintptr) {
	var i int32
	_ = i
	i = 0
	for {
		if !(i < int32(ZSTD_REP_NUM)) {
			break
		}
		*(*U32)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock + 5616 + uintptr(i)*4)) = uint32(0)
		goto _1
	_1:
		;
		i = i + 1
	}
}

// C documentation
//
//	/* These are the approximate sizes for each strategy past which copying the
//	 * dictionary tables into the working context is faster than using them
//	 * in-place.
//	 */
var attachDictSizeCutoffs = [10]size_t{
	0: libc.Uint64FromInt32(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	1: libc.Uint64FromInt32(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	2: libc.Uint64FromInt32(libc.Int32FromInt32(16) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	3: libc.Uint64FromInt32(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	4: libc.Uint64FromInt32(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	5: libc.Uint64FromInt32(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	6: libc.Uint64FromInt32(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	7: libc.Uint64FromInt32(libc.Int32FromInt32(32) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	8: libc.Uint64FromInt32(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
	9: libc.Uint64FromInt32(libc.Int32FromInt32(8) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10))),
}

func ZSTD_shouldAttachDict(tls *libc.TLS, cdict uintptr, params uintptr, pledgedSrcSize U64) (r int32) {
	var cutoff size_t
	var dedicatedDictSearch int32
	_, _ = cutoff, dedicatedDictSearch
	cutoff = attachDictSizeCutoffs[(*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams.Fstrategy]
	dedicatedDictSearch = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FdedicatedDictSearch
	return libc.BoolInt32(dedicatedDictSearch != 0 || (pledgedSrcSize <= cutoff || pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) || (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FattachDictPref == int32(ZSTD_dictForceAttach)) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FattachDictPref != int32(ZSTD_dictForceCopy) && !((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FforceWindow != 0)) /* dictMatchState isn't correctly
	 * handled in _enforceMaxDist */
}

func ZSTD_resetCCtx_byAttachingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr, _params ZSTD_CCtx_params, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	bp := tls.Alloc(256)
	defer tls.Free(256)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var cdictEnd, cdictLen U32
	var err_code size_t
	var windowLog uint32
	var _ /* adjusted_cdict_cParams at bp+224 */ ZSTD_compressionParameters
	_, _, _, _ = cdictEnd, cdictLen, err_code, windowLog
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams
	windowLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog
	/* Resize working context table params for input only, since the dict
	 * has its own tables. */
	/* pledgedSrcSize == 0 means 0! */
	if (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FdedicatedDictSearch != 0 {
		ZSTD_dedicatedDictSearch_revertCParams(tls, bp+224)
	}
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = ZSTD_adjustCParams_internal(tls, *(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)), pledgedSrcSize, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize, int32(ZSTD_cpm_attachDict), (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog = windowLog
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder /* cdict overrides */
	err_code = ZSTD_resetCCtx_internal(tls, cctx, bp, pledgedSrcSize, uint64(0), int32(ZSTDcrp_makeClean), zbuff)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	cdictEnd = libc.Uint32FromInt64(int64((*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.Fwindow.FnextSrc) - int64((*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.Fwindow.Fbase))
	cdictLen = cdictEnd - (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.Fwindow.FdictLimit
	if cdictLen == uint32(0) {
		/* don't even attach dictionaries with no contents */
	} else {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FdictMatchState = cdict + 104
		/* prep working match state so dict matches never have negative indices
		 * when they are translated to the working context's index space. */
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.FdictLimit < cdictEnd {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.FnextSrc = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.Fbase + uintptr(cdictEnd)
			ZSTD_window_clear(tls, cctx+3224+16)
		}
		/* loadedDictEnd is expressed within the referential of the active context */
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FloadedDictEnd = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.Fwindow.FdictLimit
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize
	/* copy block state */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cdict+408, libc.Uint64FromInt64(5632))
	return uint64(0)
}

func ZSTD_copyCDictTableIntoCCtx(tls *libc.TLS, dst uintptr, src uintptr, tableSize size_t, cParams uintptr) {
	var i size_t
	var index, taggedIndex U32
	_, _, _ = i, index, taggedIndex
	if ZSTD_CDictIndicesAreTagged(tls, cParams) != 0 {
		i = uint64(0)
		for {
			if !(i < tableSize) {
				break
			}
			taggedIndex = *(*U32)(unsafe.Pointer(src + uintptr(i)*4))
			index = taggedIndex >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
			*(*U32)(unsafe.Pointer(dst + uintptr(i)*4)) = index
			goto _1
		_1:
			;
			i = i + 1
		}
	} else {
		libc.Xmemcpy(tls, dst, src, tableSize*libc.Uint64FromInt64(4))
	}
}

func ZSTD_resetCCtx_byCopyingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr, _params ZSTD_CCtx_params, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var cdict_cParams, dstMatchState, srcMatchState uintptr
	var chainSize, err_code, h3Size, hSize, tagTableSize size_t
	var h3log U32
	var windowLog uint32
	var v1 uint64
	_, _, _, _, _, _, _, _, _, _, _ = cdict_cParams, chainSize, dstMatchState, err_code, h3Size, h3log, hSize, srcMatchState, tagTableSize, windowLog, v1
	cdict_cParams = cdict + 104 + 256
	windowLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog
	/* Copy only compression parameters related to tables. */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog = windowLog
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder
	err_code = ZSTD_resetCCtx_internal(tls, cctx, bp, pledgedSrcSize, uint64(0), int32(ZSTDcrp_leaveDirty), zbuff)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ZSTD_cwksp_mark_tables_dirty(tls, cctx+704)
	/* copy tables */
	if ZSTD_allocateChainTable(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).Fstrategy, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder, uint32(0)) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).FchainLog
	} else {
		v1 = uint64(0)
	}
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).FhashLog
	ZSTD_copyCDictTableIntoCCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashTable, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FhashTable, hSize, cdict_cParams)
	/* Do not copy cdict's chainTable if cctx has parameters such that it would not use chainTable */
	if ZSTD_allocateChainTable(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FuseRowMatchFinder, uint32(0)) != 0 {
		ZSTD_copyCDictTableIntoCCtx(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FchainTable, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FchainTable, chainSize, cdict_cParams)
	}
	/* copy tag table */
	if ZSTD_rowMatchFinderUsed(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cdict_cParams)).Fstrategy, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder) != 0 {
		tagTableSize = hSize
		libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FtagTable, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FtagTable, tagTableSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashSalt = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FhashSalt
	}
	/* Zero the hashTable3, since the cdict never fills it */
	h3log = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashLog3
	if h3log != 0 {
		v1 = libc.Uint64FromInt32(1) << h3log
	} else {
		v1 = uint64(0)
	}
	h3Size = v1
	libc.Xmemset(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FmatchState.FhashTable3, 0, h3Size*libc.Uint64FromInt64(4))
	ZSTD_cwksp_mark_tables_clean(tls, cctx+704)
	/* copy dictionary offsets */
	srcMatchState = cdict + 104
	dstMatchState = cctx + 3224 + 16
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).Fwindow = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).Fwindow
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FnextToUpdate
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FloadedDictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FloadedDictEnd
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize
	/* copy block state */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cdict+408, libc.Uint64FromInt64(5632))
	return uint64(0)
}

// C documentation
//
//	/* We have a choice between copying the dictionary context into the working
//	 * context, or referencing the dictionary context from the working context
//	 * in-place. We decide here which strategy to use. */
func ZSTD_resetCCtx_usingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr, params uintptr, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	if ZSTD_shouldAttachDict(tls, cdict, params, pledgedSrcSize) != 0 {
		return ZSTD_resetCCtx_byAttachingCDict(tls, cctx, cdict, *(*ZSTD_CCtx_params)(unsafe.Pointer(params)), pledgedSrcSize, zbuff)
	} else {
		return ZSTD_resetCCtx_byCopyingCDict(tls, cctx, cdict, *(*ZSTD_CCtx_params)(unsafe.Pointer(params)), pledgedSrcSize, zbuff)
	}
	return r
}

// C documentation
//
//	/*! ZSTD_copyCCtx_internal() :
//	 *  Duplicate an existing context `srcCCtx` into another one `dstCCtx`.
//	 *  Only works during stage ZSTDcs_init (i.e. after creation, but before first call to ZSTD_compressContinue()).
//	 *  The "context", in this case, refers to the hash and chain tables,
//	 *  entropy tables, and dictionary references.
//	 * `windowLog` value is enforced if != 0, otherwise value is copied from srcCCtx.
//	 * @return : 0, or an error code */
func ZSTD_copyCCtx_internal(tls *libc.TLS, dstCCtx uintptr, srcCCtx uintptr, fParams ZSTD_frameParameters, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var chainSize, h3Size, hSize size_t
	var dstMatchState, srcMatchState uintptr
	var h3log U32
	var v1, v2 uint64
	var _ /* params at bp+0 */ ZSTD_CCtx_params
	_, _, _, _, _, _, _, _ = chainSize, dstMatchState, h3Size, h3log, hSize, srcMatchState, v1, v2
	if (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).Fstage != int32(ZSTDcs_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2732, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	libc.Xmemcpy(tls, dstCCtx+896, srcCCtx+896, libc.Uint64FromInt64(24))
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FrequestedParams
	/* Copy only compression parameters related to tables. */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FuseRowMatchFinder
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FpostBlockSplitter = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FpostBlockSplitter
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FldmParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams = fParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FmaxBlockSize = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FmaxBlockSize
	ZSTD_resetCCtx_internal(tls, dstCCtx, bp, pledgedSrcSize, uint64(0), int32(ZSTDcrp_leaveDirty), zbuff)
	ZSTD_cwksp_mark_tables_dirty(tls, dstCCtx+704)
	/* copy tables */
	if ZSTD_allocateChainTable(tls, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FuseRowMatchFinder, uint32(0)) != 0 {
		v1 = libc.Uint64FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams.FchainLog
	} else {
		v1 = uint64(0)
	}
	chainSize = v1
	hSize = libc.Uint64FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FappliedParams.FcParams.FhashLog
	h3log = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FhashLog3
	if h3log != 0 {
		v2 = libc.Uint64FromInt32(1) << h3log
	} else {
		v2 = uint64(0)
	}
	h3Size = v2
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FmatchState.FhashTable, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FhashTable, hSize*libc.Uint64FromInt64(4))
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FmatchState.FchainTable, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FchainTable, chainSize*libc.Uint64FromInt64(4))
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FmatchState.FhashTable3, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FmatchState.FhashTable3, h3Size*libc.Uint64FromInt64(4))
	ZSTD_cwksp_mark_tables_clean(tls, dstCCtx+704)
	/* copy dictionary offsets */
	srcMatchState = srcCCtx + 3224 + 16
	dstMatchState = dstCCtx + 3224 + 16
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).Fwindow = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).Fwindow
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FnextToUpdate
	(*ZSTD_MatchState_t)(unsafe.Pointer(dstMatchState)).FloadedDictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(srcMatchState)).FloadedDictEnd
	(*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FdictID = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FdictID
	(*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FdictContentSize = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FdictContentSize
	/* copy block state */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(dstCCtx)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FblockState.FprevCBlock, libc.Uint64FromInt64(5632))
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_copyCCtx() :
//	 *  Duplicate an existing context `srcCCtx` into another one `dstCCtx`.
//	 *  Only works during stage ZSTDcs_init (i.e. after creation, but before first call to ZSTD_compressContinue()).
//	 *  pledgedSrcSize==0 means "unknown".
//	*   @return : 0, or an error code */
func ZSTD_copyCCtx(tls *libc.TLS, dstCCtx uintptr, srcCCtx uintptr, pledgedSrcSize uint64) (r size_t) {
	var fParams ZSTD_frameParameters
	var zbuff ZSTD_buffered_policy_e
	_, _ = fParams, zbuff
	fParams = ZSTD_frameParameters{
		FcontentSizeFlag: int32(1),
	}
	zbuff = (*ZSTD_CCtx)(unsafe.Pointer(srcCCtx)).FbufferedPolicy
	_ = libc.Uint64FromInt64(1)
	if pledgedSrcSize == uint64(0) {
		pledgedSrcSize = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	fParams.FcontentSizeFlag = libc.BoolInt32(pledgedSrcSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
	return ZSTD_copyCCtx_internal(tls, dstCCtx, srcCCtx, fParams, pledgedSrcSize, zbuff)
}

// C documentation
//
//	/*! ZSTD_reduceTable() :
//	 *  reduce table indexes by `reducerValue`, or squash to zero.
//	 *  PreserveMark preserves "unsorted mark" for btlazy2 strategy.
//	 *  It must be set to a clear 0/1 value, to remove branch during inlining.
//	 *  Presume table size is a multiple of ZSTD_ROWSIZE
//	 *  to help auto-vectorization */
func ZSTD_reduceTable_internal(tls *libc.TLS, table uintptr, size U32, reducerValue U32, preserveMark int32) {
	var cellNb, column, nbRows, rowNb int32
	var newVal, reducerThreshold U32
	_, _, _, _, _, _ = cellNb, column, nbRows, newVal, reducerThreshold, rowNb
	nbRows = libc.Int32FromUint32(size) / int32(ZSTD_ROWSIZE)
	cellNb = 0
	/* Protect special index values < ZSTD_WINDOW_START_INDEX. */
	reducerThreshold = reducerValue + uint32(ZSTD_WINDOW_START_INDEX)
	/* multiple of ZSTD_ROWSIZE */
	/* can be cast to int */
	rowNb = 0
	for {
		if !(rowNb < nbRows) {
			break
		}
		column = 0
		for {
			if !(column < int32(ZSTD_ROWSIZE)) {
				break
			}
			if preserveMark != 0 && *(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) == uint32(ZSTD_DUBT_UNSORTED_MARK) {
				/* This write is pointless, but is required(?) for the compiler
				 * to auto-vectorize the loop. */
				newVal = uint32(ZSTD_DUBT_UNSORTED_MARK)
			} else {
				if *(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) < reducerThreshold {
					newVal = uint32(0)
				} else {
					newVal = *(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) - reducerValue
				}
			}
			*(*U32)(unsafe.Pointer(table + uintptr(cellNb)*4)) = newVal
			cellNb = cellNb + 1
			goto _2
		_2:
			;
			column = column + 1
		}
		goto _1
	_1:
		;
		rowNb = rowNb + 1
	}
}

func ZSTD_reduceTable(tls *libc.TLS, table uintptr, size U32, reducerValue U32) {
	ZSTD_reduceTable_internal(tls, table, size, reducerValue, 0)
}

func ZSTD_reduceTable_btlazy2(tls *libc.TLS, table uintptr, size U32, reducerValue U32) {
	ZSTD_reduceTable_internal(tls, table, size, reducerValue, int32(1))
}

// C documentation
//
//	/*! ZSTD_reduceIndex() :
//	*   rescale all indexes to avoid future overflow (indexes are U32) */
func ZSTD_reduceIndex(tls *libc.TLS, ms uintptr, params uintptr, reducerValue U32) {
	var chainSize, h3Size, hSize U32
	_, _, _ = chainSize, h3Size, hSize
	hSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog
	ZSTD_reduceTable(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable, hSize, reducerValue)
	if ZSTD_allocateChainTable(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder, libc.Uint32FromInt32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdedicatedDictSearch)) != 0 {
		chainSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy == int32(ZSTD_btlazy2) {
			ZSTD_reduceTable_btlazy2(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable, chainSize, reducerValue)
		} else {
			ZSTD_reduceTable(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable, chainSize, reducerValue)
		}
	}
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3 != 0 {
		h3Size = libc.Uint32FromInt32(1) << (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3
		ZSTD_reduceTable(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable3, h3Size, reducerValue)
	}
}

/*-*******************************************************
*  Block entropic compression
*********************************************************/

/* See doc/zstd_compression_format.md for detailed format description */

func ZSTD_seqToCodes(tls *libc.TLS, seqStorePtr uintptr) (r int32) {
	var llCodeTable, mlCodeTable, ofCodeTable, sequences uintptr
	var llv, mlv, nbSeq, ofCode, u U32
	var longOffsets, v2 int32
	var v3 bool
	_, _, _, _, _, _, _, _, _, _, _, _ = llCodeTable, llv, longOffsets, mlCodeTable, mlv, nbSeq, ofCode, ofCodeTable, sequences, u, v2, v3
	sequences = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
	llCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	ofCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	mlCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	nbSeq = libc.Uint32FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	longOffsets = 0
	u = uint32(0)
	for {
		if !(u < nbSeq) {
			break
		}
		llv = uint32((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(u)*8))).FlitLength)
		ofCode = ZSTD_highbit32(tls, (*(*SeqDef)(unsafe.Pointer(sequences + uintptr(u)*8))).FoffBase)
		mlv = uint32((*(*SeqDef)(unsafe.Pointer(sequences + uintptr(u)*8))).FmlBase)
		*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr(u))) = uint8(ZSTD_LLcode(tls, llv))
		*(*BYTE)(unsafe.Pointer(ofCodeTable + uintptr(u))) = uint8(ofCode)
		*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr(u))) = uint8(ZSTD_MLcode(tls, mlv))
		if v3 = MEM_32bits(tls) != 0; v3 {
			if MEM_32bits(tls) != 0 {
				v2 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v2 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
		}
		if v3 && ofCode >= libc.Uint32FromInt32(v2) {
			longOffsets = int32(1)
		}
		goto _1
	_1:
		;
		u = u + 1
	}
	if (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType == int32(ZSTD_llt_literalLength) {
		*(*BYTE)(unsafe.Pointer(llCodeTable + uintptr((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos))) = uint8(MaxLL)
	}
	if (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthType == int32(ZSTD_llt_matchLength) {
		*(*BYTE)(unsafe.Pointer(mlCodeTable + uintptr((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlongLengthPos))) = uint8(MaxML)
	}
	return longOffsets
}

// C documentation
//
//	/* ZSTD_useTargetCBlockSize():
//	 * Returns if target compressed block size param is being used.
//	 * If used, compression will do best effort to make a compressed block size to be around targetCBlockSize.
//	 * Returns 1 if true, 0 otherwise. */
func ZSTD_useTargetCBlockSize(tls *libc.TLS, cctxParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FtargetCBlockSize != uint64(0))
}

// C documentation
//
//	/* ZSTD_blockSplitterEnabled():
//	 * Returns if block splitting param is being used
//	 * If used, compression will do best effort to split a block in order to improve compression ratio.
//	 * At the time this function is called, the parameter must be finalized.
//	 * Returns 1 if true, 0 otherwise. */
func ZSTD_blockSplitterEnabled(tls *libc.TLS, cctxParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FpostBlockSplitter == int32(ZSTD_ps_enable))
}

// C documentation
//
//	/* Type returned by ZSTD_buildSequencesStatistics containing finalized symbol encoding types
//	 * and size of the sequences statistics
//	 */
type ZSTD_symbolEncodingTypeStats_t = struct {
	FLLtype        U32
	FOfftype       U32
	FMLtype        U32
	Fsize          size_t
	FlastCountSize size_t
	FlongOffsets   int32
}

// C documentation
//
//	/* ZSTD_buildSequencesStatistics():
//	 * Returns a ZSTD_symbolEncodingTypeStats_t, or a zstd error code in the `size` field.
//	 * Modifies `nextEntropy` to have the appropriate values as a side effect.
//	 * nbSeq must be greater than 0.
//	 *
//	 * entropyWkspSize must be of size at least ENTROPY_WORKSPACE_SIZE - (MaxSeq + 1)*sizeof(U32)
//	 */
func ZSTD_buildSequencesStatistics(tls *libc.TLS, seqStorePtr uintptr, nbSeq size_t, prevEntropy uintptr, nextEntropy uintptr, dst uintptr, dstEnd uintptr, strategy ZSTD_strategy, countWorkspace uintptr, entropyWorkspace uintptr, entropyWkspSize size_t) (r ZSTD_symbolEncodingTypeStats_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, llCodeTable, mlCodeTable, oend, ofCodeTable, op, ostart uintptr
	var countSize, countSize1, countSize2, mostFrequent, mostFrequent1, mostFrequent2 size_t
	var defaultPolicy ZSTD_DefaultPolicy_e
	var stats ZSTD_symbolEncodingTypeStats_t
	var v1 int32
	var _ /* max at bp+0 */ uint32
	var _ /* max at bp+4 */ uint32
	var _ /* max at bp+8 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, countSize, countSize1, countSize2, defaultPolicy, llCodeTable, mlCodeTable, mostFrequent, mostFrequent1, mostFrequent2, oend, ofCodeTable, op, ostart, stats, v1
	ostart = dst
	oend = dstEnd
	op = ostart
	CTable_LitLength = nextEntropy + 2224
	CTable_OffsetBits = nextEntropy
	CTable_MatchLength = nextEntropy + 772
	ofCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	llCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	mlCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	stats.FlastCountSize = uint64(0)
	/* convert length/distances into codes */
	stats.FlongOffsets = ZSTD_seqToCodes(tls, seqStorePtr)
	/* ZSTD_selectEncodingType() divides by nbSeq */
	/* build CTable for Literal Lengths */
	*(*uint32)(unsafe.Pointer(bp)) = uint32(MaxLL)
	mostFrequent = HIST_countFast_wksp(tls, countWorkspace, bp, llCodeTable, nbSeq, entropyWorkspace, entropyWkspSize) /* can't fail */
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Flitlength_repeatMode = (*ZSTD_fseCTables_t)(unsafe.Pointer(prevEntropy)).Flitlength_repeatMode
	stats.FLLtype = libc.Uint32FromInt32(ZSTD_selectEncodingType(tls, nextEntropy+3548, countWorkspace, *(*uint32)(unsafe.Pointer(bp)), mostFrequent, nbSeq, uint32(LLFSELog), prevEntropy+2224, uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, int32(ZSTD_defaultAllowed), strategy))
	/* We don't copy tables */
	countSize = ZSTD_buildCTable(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), CTable_LitLength, uint32(LLFSELog), libc.Int32FromUint32(stats.FLLtype), countWorkspace, *(*uint32)(unsafe.Pointer(bp)), llCodeTable, nbSeq, uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, uint32(MaxLL), prevEntropy+2224, uint64(1316), entropyWorkspace, entropyWkspSize)
	if ZSTD_isError(tls, countSize) != 0 {
		stats.Fsize = countSize
		return stats
	}
	if stats.FLLtype == uint32(set_compressed) {
		stats.FlastCountSize = countSize
	}
	op = op + uintptr(countSize)
	/* build CTable for Offsets */
	*(*uint32)(unsafe.Pointer(bp + 4)) = uint32(MaxOff)
	mostFrequent1 = HIST_countFast_wksp(tls, countWorkspace, bp+4, ofCodeTable, nbSeq, entropyWorkspace, entropyWkspSize)
	if *(*uint32)(unsafe.Pointer(bp + 4)) <= uint32(DefaultMaxOff) {
		v1 = int32(ZSTD_defaultAllowed)
	} else {
		v1 = int32(ZSTD_defaultDisallowed)
	} /* can't fail */
	/* We can only use the basic table if max <= DefaultMaxOff, otherwise the offsets are too large */
	defaultPolicy = v1
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Foffcode_repeatMode = (*ZSTD_fseCTables_t)(unsafe.Pointer(prevEntropy)).Foffcode_repeatMode
	stats.FOfftype = libc.Uint32FromInt32(ZSTD_selectEncodingType(tls, nextEntropy+3540, countWorkspace, *(*uint32)(unsafe.Pointer(bp + 4)), mostFrequent1, nbSeq, uint32(OffFSELog), prevEntropy, uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, defaultPolicy, strategy))
	/* We don't copy tables */
	countSize1 = ZSTD_buildCTable(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), CTable_OffsetBits, uint32(OffFSELog), libc.Int32FromUint32(stats.FOfftype), countWorkspace, *(*uint32)(unsafe.Pointer(bp + 4)), ofCodeTable, nbSeq, uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, uint32(DefaultMaxOff), prevEntropy, uint64(772), entropyWorkspace, entropyWkspSize)
	if ZSTD_isError(tls, countSize1) != 0 {
		stats.Fsize = countSize1
		return stats
	}
	if stats.FOfftype == uint32(set_compressed) {
		stats.FlastCountSize = countSize1
	}
	op = op + uintptr(countSize1)
	/* build CTable for MatchLengths */
	*(*uint32)(unsafe.Pointer(bp + 8)) = uint32(MaxML)
	mostFrequent2 = HIST_countFast_wksp(tls, countWorkspace, bp+8, mlCodeTable, nbSeq, entropyWorkspace, entropyWkspSize) /* can't fail */
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Fmatchlength_repeatMode = (*ZSTD_fseCTables_t)(unsafe.Pointer(prevEntropy)).Fmatchlength_repeatMode
	stats.FMLtype = libc.Uint32FromInt32(ZSTD_selectEncodingType(tls, nextEntropy+3544, countWorkspace, *(*uint32)(unsafe.Pointer(bp + 8)), mostFrequent2, nbSeq, uint32(MLFSELog), prevEntropy+772, uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, int32(ZSTD_defaultAllowed), strategy))
	/* We don't copy tables */
	countSize2 = ZSTD_buildCTable(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), CTable_MatchLength, uint32(MLFSELog), libc.Int32FromUint32(stats.FMLtype), countWorkspace, *(*uint32)(unsafe.Pointer(bp + 8)), mlCodeTable, nbSeq, uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, uint32(MaxML), prevEntropy+772, uint64(1452), entropyWorkspace, entropyWkspSize)
	if ZSTD_isError(tls, countSize2) != 0 {
		stats.Fsize = countSize2
		return stats
	}
	if stats.FMLtype == uint32(set_compressed) {
		stats.FlastCountSize = countSize2
	}
	op = op + uintptr(countSize2)
	stats.Fsize = libc.Uint64FromInt64(int64(op) - int64(ostart))
	return stats
}

// C documentation
//
//	/* ZSTD_entropyCompressSeqStore_internal():
//	 * compresses both literals and sequences
//	 * Returns compressed size of block, or a zstd error.
//	 */
func ZSTD_entropyCompressSeqStore_internal(tls *libc.TLS, dst uintptr, dstCapacity size_t, literals uintptr, litSize size_t, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, entropyWorkspace uintptr, entropyWkspSize size_t, bmi2 int32) (r size_t) {
	var CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, count, llCodeTable, mlCodeTable, oend, ofCodeTable, op, ostart, seqHead, sequences, v1 uintptr
	var bitstreamSize, cSize, err_code, err_code1, err_code2, lastCountSize, nbSeq, numSequences size_t
	var longOffsets, suspectUncompressible int32
	var stats ZSTD_symbolEncodingTypeStats_t
	var strategy ZSTD_strategy
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = CTable_LitLength, CTable_MatchLength, CTable_OffsetBits, bitstreamSize, cSize, count, err_code, err_code1, err_code2, lastCountSize, llCodeTable, longOffsets, mlCodeTable, nbSeq, numSequences, oend, ofCodeTable, op, ostart, seqHead, sequences, stats, strategy, suspectUncompressible, v1
	strategy = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy
	count = entropyWorkspace
	CTable_LitLength = nextEntropy + 2064 + 2224
	CTable_OffsetBits = nextEntropy + 2064
	CTable_MatchLength = nextEntropy + 2064 + 772
	sequences = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
	nbSeq = libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	ofCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
	llCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
	mlCodeTable = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
	ostart = dst
	oend = ostart + uintptr(dstCapacity)
	op = ostart
	longOffsets = 0
	entropyWorkspace = count + uintptr(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*4
	entropyWkspSize = entropyWkspSize - libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)
	_ = libc.Uint64FromInt64(1)
	/* Compress literals */
	numSequences = libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	/* Base suspicion of uncompressibility on ratio of literals to sequences */
	suspectUncompressible = libc.BoolInt32(numSequences == uint64(0) || litSize/numSequences >= uint64(SUSPECT_UNCOMPRESSIBLE_LITERAL_RATIO))
	cSize = ZSTD_compressLiterals(tls, op, dstCapacity, literals, litSize, entropyWorkspace, entropyWkspSize, prevEntropy, nextEntropy, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy, ZSTD_literalsCompressionIsDisabled(tls, cctxParams), suspectUncompressible, bmi2)
	err_code = cSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2775, 0)
		}
		return err_code
	}
	op = op + uintptr(cSize)
	/* Sequences Header */
	if int64(oend)-int64(op) < int64(libc.Int32FromInt32(3)+libc.Int32FromInt32(1)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2804, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbSeq < uint64(128) {
		v1 = op
		op = op + 1
		*(*BYTE)(unsafe.Pointer(v1)) = uint8(nbSeq)
	} else {
		if nbSeq < uint64(LONGNBSEQ) {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(nbSeq>>libc.Int32FromInt32(8) + libc.Uint64FromInt32(0x80))
			*(*BYTE)(unsafe.Pointer(op + 1)) = uint8(nbSeq)
			op = op + uintptr(2)
		} else {
			*(*BYTE)(unsafe.Pointer(op)) = uint8(0xFF)
			MEM_writeLE16(tls, op+uintptr(1), uint16(nbSeq-libc.Uint64FromInt32(LONGNBSEQ)))
			op = op + uintptr(3)
		}
	}
	if nbSeq == uint64(0) {
		/* Copy the old tables over as if we repeated them */
		libc.Xmemcpy(tls, nextEntropy+2064, prevEntropy+2064, libc.Uint64FromInt64(3552))
		return libc.Uint64FromInt64(int64(op) - int64(ostart))
	}
	v1 = op
	op = op + 1
	seqHead = v1
	/* build stats for sequences */
	stats = ZSTD_buildSequencesStatistics(tls, seqStorePtr, nbSeq, prevEntropy+2064, nextEntropy+2064, op, oend, strategy, count, entropyWorkspace, entropyWkspSize)
	err_code1 = stats.Fsize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2837, 0)
		}
		return err_code1
	}
	*(*BYTE)(unsafe.Pointer(seqHead)) = uint8(stats.FLLtype<<libc.Int32FromInt32(6) + stats.FOfftype<<libc.Int32FromInt32(4) + stats.FMLtype<<libc.Int32FromInt32(2))
	lastCountSize = stats.FlastCountSize
	op = op + uintptr(stats.Fsize)
	longOffsets = stats.FlongOffsets
	bitstreamSize = ZSTD_encodeSequences(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), CTable_MatchLength, mlCodeTable, CTable_OffsetBits, ofCodeTable, CTable_LitLength, llCodeTable, sequences, nbSeq, longOffsets, bmi2)
	err_code2 = bitstreamSize
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1608, 0)
		}
		return err_code2
	}
	op = op + uintptr(bitstreamSize)
	/* zstd versions <= 1.3.4 mistakenly report corruption when
	 * FSE_readNCount() receives a buffer < 4 bytes.
	 * Fixed by https://github.com/facebook/zstd/pull/1146.
	 * This can happen when the last set_compressed table present is 2
	 * bytes and the bitstream is only one byte.
	 * In this exceedingly rare case, we will simply emit an uncompressed
	 * block, since it isn't worth optimizing.
	 */
	if lastCountSize != 0 && lastCountSize+bitstreamSize < uint64(4) {
		/* lastCountSize >= 2 && bitstreamSize > 0 ==> lastCountSize == 3 */
		return uint64(0)
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_entropyCompressSeqStore_wExtLitBuffer(tls *libc.TLS, dst uintptr, dstCapacity size_t, literals uintptr, litSize size_t, blockSize size_t, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, entropyWorkspace uintptr, entropyWkspSize size_t, bmi2 int32) (r size_t) {
	var cSize, err_code, maxCSize size_t
	_, _, _ = cSize, err_code, maxCSize
	cSize = ZSTD_entropyCompressSeqStore_internal(tls, dst, dstCapacity, literals, litSize, seqStorePtr, prevEntropy, nextEntropy, cctxParams, entropyWorkspace, entropyWkspSize, bmi2)
	if cSize == uint64(0) {
		return uint64(0)
	}
	/* When srcSize <= dstCapacity, there is enough space to write a raw uncompressed block.
	 * Since we ran out of space, block must be not compressible, so fall back to raw uncompressed block.
	 */
	if libc.BoolInt32(cSize == libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall)))&libc.BoolInt32(blockSize <= dstCapacity) != 0 {
		return uint64(0) /* block not compressed */
	}
	err_code = cSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2875, 0)
		}
		return err_code
	}
	/* Check compressibility */
	maxCSize = blockSize - ZSTD_minGain(tls, blockSize, (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy)
	if cSize >= maxCSize {
		return uint64(0)
	} /* block not compressed */
	/* libzstd decoder before  > v1.5.4 is not compatible with compressed blocks of size ZSTD_BLOCKSIZE_MAX exactly.
	 * This restriction is indirectly already fulfilled by respecting ZSTD_minGain() condition above.
	 */
	return cSize
}

func ZSTD_entropyCompressSeqStore(tls *libc.TLS, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, dst uintptr, dstCapacity size_t, srcSize size_t, entropyWorkspace uintptr, entropyWkspSize size_t, bmi2 int32) (r size_t) {
	return ZSTD_entropyCompressSeqStore_wExtLitBuffer(tls, dst, dstCapacity, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart, libc.Uint64FromInt64(int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit)-int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart)), srcSize, seqStorePtr, prevEntropy, nextEntropy, cctxParams, entropyWorkspace, entropyWkspSize, bmi2)
}

// C documentation
//
//	/* ZSTD_selectBlockCompressor() :
//	 * Not static, but internal use only (used by long distance matcher)
//	 * assumption : strat is a valid strategy */
func ZSTD_selectBlockCompressor(tls *libc.TLS, strat ZSTD_strategy, useRowMatchFinder ZSTD_ParamSwitch_e, dictMode ZSTD_dictMode_e) (r ZSTD_BlockCompressor_f) {
	var selectedCompressor ZSTD_BlockCompressor_f
	_ = selectedCompressor
	_ = libc.Uint64FromInt64(1)
	if ZSTD_rowMatchFinderUsed(tls, strat, useRowMatchFinder) != 0 {
		selectedCompressor = *(*ZSTD_BlockCompressor_f)(unsafe.Pointer(uintptr(unsafe.Pointer(&rowBasedBlockCompressors)) + uintptr(dictMode)*24 + uintptr(strat-int32(ZSTD_greedy))*8))
	} else {
		selectedCompressor = *(*ZSTD_BlockCompressor_f)(unsafe.Pointer(uintptr(unsafe.Pointer(&blockCompressor)) + uintptr(dictMode)*80 + uintptr(strat)*8))
	}
	return selectedCompressor
}

var blockCompressor = [4][10]ZSTD_BlockCompressor_f{
	0: {},
	1: {},
	2: {},
	3: {},
}

func init() {
	p := unsafe.Pointer(&blockCompressor)
	*(*uintptr)(unsafe.Add(p, 0)) = __ccgo_fp(ZSTD_compressBlock_fast)
	*(*uintptr)(unsafe.Add(p, 8)) = __ccgo_fp(ZSTD_compressBlock_fast)
	*(*uintptr)(unsafe.Add(p, 16)) = __ccgo_fp(ZSTD_compressBlock_doubleFast)
	*(*uintptr)(unsafe.Add(p, 24)) = __ccgo_fp(ZSTD_compressBlock_greedy)
	*(*uintptr)(unsafe.Add(p, 32)) = __ccgo_fp(ZSTD_compressBlock_lazy)
	*(*uintptr)(unsafe.Add(p, 40)) = __ccgo_fp(ZSTD_compressBlock_lazy2)
	*(*uintptr)(unsafe.Add(p, 48)) = __ccgo_fp(ZSTD_compressBlock_btlazy2)
	*(*uintptr)(unsafe.Add(p, 56)) = __ccgo_fp(ZSTD_compressBlock_btopt)
	*(*uintptr)(unsafe.Add(p, 64)) = __ccgo_fp(ZSTD_compressBlock_btultra)
	*(*uintptr)(unsafe.Add(p, 72)) = __ccgo_fp(ZSTD_compressBlock_btultra2)
	*(*uintptr)(unsafe.Add(p, 80)) = __ccgo_fp(ZSTD_compressBlock_fast_extDict)
	*(*uintptr)(unsafe.Add(p, 88)) = __ccgo_fp(ZSTD_compressBlock_fast_extDict)
	*(*uintptr)(unsafe.Add(p, 96)) = __ccgo_fp(ZSTD_compressBlock_doubleFast_extDict)
	*(*uintptr)(unsafe.Add(p, 104)) = __ccgo_fp(ZSTD_compressBlock_greedy_extDict)
	*(*uintptr)(unsafe.Add(p, 112)) = __ccgo_fp(ZSTD_compressBlock_lazy_extDict)
	*(*uintptr)(unsafe.Add(p, 120)) = __ccgo_fp(ZSTD_compressBlock_lazy2_extDict)
	*(*uintptr)(unsafe.Add(p, 128)) = __ccgo_fp(ZSTD_compressBlock_btlazy2_extDict)
	*(*uintptr)(unsafe.Add(p, 136)) = __ccgo_fp(ZSTD_compressBlock_btopt_extDict)
	*(*uintptr)(unsafe.Add(p, 144)) = __ccgo_fp(ZSTD_compressBlock_btultra_extDict)
	*(*uintptr)(unsafe.Add(p, 152)) = __ccgo_fp(ZSTD_compressBlock_btultra_extDict)
	*(*uintptr)(unsafe.Add(p, 160)) = __ccgo_fp(ZSTD_compressBlock_fast_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 168)) = __ccgo_fp(ZSTD_compressBlock_fast_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 176)) = __ccgo_fp(ZSTD_compressBlock_doubleFast_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 184)) = __ccgo_fp(ZSTD_compressBlock_greedy_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 192)) = __ccgo_fp(ZSTD_compressBlock_lazy_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 200)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 208)) = __ccgo_fp(ZSTD_compressBlock_btlazy2_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 216)) = __ccgo_fp(ZSTD_compressBlock_btopt_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 224)) = __ccgo_fp(ZSTD_compressBlock_btultra_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 232)) = __ccgo_fp(ZSTD_compressBlock_btultra_dictMatchState)
	*(*uintptr)(unsafe.Add(p, 240)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 248)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 256)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 264)) = __ccgo_fp(ZSTD_compressBlock_greedy_dedicatedDictSearch)
	*(*uintptr)(unsafe.Add(p, 272)) = __ccgo_fp(ZSTD_compressBlock_lazy_dedicatedDictSearch)
	*(*uintptr)(unsafe.Add(p, 280)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dedicatedDictSearch)
	*(*uintptr)(unsafe.Add(p, 288)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 296)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 304)) = libc.UintptrFromInt32(0)
	*(*uintptr)(unsafe.Add(p, 312)) = libc.UintptrFromInt32(0)
}

var rowBasedBlockCompressors = [4][3]ZSTD_BlockCompressor_f{
	0: {},
	1: {},
	2: {},
	3: {},
}

func init() {
	p := unsafe.Pointer(&rowBasedBlockCompressors)
	*(*uintptr)(unsafe.Add(p, 0)) = __ccgo_fp(ZSTD_compressBlock_greedy_row)
	*(*uintptr)(unsafe.Add(p, 8)) = __ccgo_fp(ZSTD_compressBlock_lazy_row)
	*(*uintptr)(unsafe.Add(p, 16)) = __ccgo_fp(ZSTD_compressBlock_lazy2_row)
	*(*uintptr)(unsafe.Add(p, 24)) = __ccgo_fp(ZSTD_compressBlock_greedy_extDict_row)
	*(*uintptr)(unsafe.Add(p, 32)) = __ccgo_fp(ZSTD_compressBlock_lazy_extDict_row)
	*(*uintptr)(unsafe.Add(p, 40)) = __ccgo_fp(ZSTD_compressBlock_lazy2_extDict_row)
	*(*uintptr)(unsafe.Add(p, 48)) = __ccgo_fp(ZSTD_compressBlock_greedy_dictMatchState_row)
	*(*uintptr)(unsafe.Add(p, 56)) = __ccgo_fp(ZSTD_compressBlock_lazy_dictMatchState_row)
	*(*uintptr)(unsafe.Add(p, 64)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dictMatchState_row)
	*(*uintptr)(unsafe.Add(p, 72)) = __ccgo_fp(ZSTD_compressBlock_greedy_dedicatedDictSearch_row)
	*(*uintptr)(unsafe.Add(p, 80)) = __ccgo_fp(ZSTD_compressBlock_lazy_dedicatedDictSearch_row)
	*(*uintptr)(unsafe.Add(p, 88)) = __ccgo_fp(ZSTD_compressBlock_lazy2_dedicatedDictSearch_row)
}

func ZSTD_storeLastLiterals(tls *libc.TLS, seqStorePtr uintptr, anchor uintptr, lastLLSize size_t) {
	libc.Xmemcpy(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit, anchor, lastLLSize)
	*(*uintptr)(unsafe.Pointer(seqStorePtr + 24)) += uintptr(lastLLSize)
}

func ZSTD_resetSeqStore(tls *libc.TLS, ssPtr uintptr) {
	(*SeqStore_t)(unsafe.Pointer(ssPtr)).Flit = (*SeqStore_t)(unsafe.Pointer(ssPtr)).FlitStart
	(*SeqStore_t)(unsafe.Pointer(ssPtr)).Fsequences = (*SeqStore_t)(unsafe.Pointer(ssPtr)).FsequencesStart
	(*SeqStore_t)(unsafe.Pointer(ssPtr)).FlongLengthType = int32(ZSTD_llt_none)
}

// C documentation
//
//	/* ZSTD_postProcessSequenceProducerResult() :
//	 * Validates and post-processes sequences obtained through the external matchfinder API:
//	 *   - Checks whether nbExternalSeqs represents an error condition.
//	 *   - Appends a block delimiter to outSeqs if one is not already present.
//	 *     See zstd.h for context regarding block delimiters.
//	 * Returns the number of sequences after post-processing, or an error code. */
func ZSTD_postProcessSequenceProducerResult(tls *libc.TLS, outSeqs uintptr, nbExternalSeqs size_t, outSeqsCapacity size_t, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var lastSeq ZSTD_Sequence
	_ = lastSeq
	if nbExternalSeqs > outSeqsCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2920, libc.VaList(bp+8, nbExternalSeqs))
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_sequenceProducer_failed))
	}
	if nbExternalSeqs == uint64(0) && srcSize > uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2971, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_sequenceProducer_failed))
	}
	if srcSize == uint64(0) {
		libc.Xmemset(tls, outSeqs, 0, libc.Uint64FromInt64(16))
		return uint64(1)
	}
	lastSeq = *(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbExternalSeqs-uint64(1))*16))
	/* We can return early if lastSeq is already a block delimiter. */
	if lastSeq.Foffset == uint32(0) && lastSeq.FmatchLength == uint32(0) {
		return nbExternalSeqs
	}
	/* This error condition is only possible if the external matchfinder
	 * produced an invalid parse, by definition of ZSTD_sequenceBound(). */
	if nbExternalSeqs == outSeqsCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3050, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_sequenceProducer_failed))
	}
	/* lastSeq is not a block delimiter, so we need to append one. */
	libc.Xmemset(tls, outSeqs+uintptr(nbExternalSeqs)*16, 0, libc.Uint64FromInt64(16))
	return nbExternalSeqs + uint64(1)
	return r
}

// C documentation
//
//	/* ZSTD_fastSequenceLengthSum() :
//	 * Returns sum(litLen) + sum(matchLen) + lastLits for *seqBuf*.
//	 * Similar to another function in zstd_compress.c (determine_blockSize),
//	 * except it doesn't check for a block delimiter to end summation.
//	 * Removing the early exit allows the compiler to auto-vectorize (https://godbolt.org/z/cY1cajz9P).
//	 * This function can be deleted and replaced by determine_blockSize after we resolve issue #3456. */
func ZSTD_fastSequenceLengthSum(tls *libc.TLS, seqBuf uintptr, seqBufSize size_t) (r size_t) {
	var i, litLenSum, matchLenSum size_t
	_, _, _ = i, litLenSum, matchLenSum
	matchLenSum = uint64(0)
	litLenSum = uint64(0)
	i = uint64(0)
	for {
		if !(i < seqBufSize) {
			break
		}
		litLenSum = litLenSum + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqBuf + uintptr(i)*16))).FlitLength)
		matchLenSum = matchLenSum + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqBuf + uintptr(i)*16))).FmatchLength)
		goto _1
	_1:
		;
		i = i + 1
	}
	return litLenSum + matchLenSum
}

// C documentation
//
//	/**
//	 * Function to validate sequences produced by a block compressor.
//	 */
func ZSTD_validateSeqStore(tls *libc.TLS, seqStore uintptr, cParams uintptr) {
	_ = seqStore
	_ = cParams
}

type ZSTD_BuildSeqStore_e = int32

const ZSTDbss_compress = 0
const ZSTDbss_noCompress = 1

func ZSTD_buildSeqStore(tls *libc.TLS, zc uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var base, istart, lastLiterals, ms uintptr
	var blockCompressor, blockCompressor1 ZSTD_BlockCompressor_f
	var curr, windowSize U32
	var dictMode ZSTD_dictMode_e
	var err_code, err_code1, lastLLSize, nbExternalSeqs, nbPostProcessedSeqs, seqLenSum size_t
	var i int32
	var v1 uint32
	var _ /* ldmSeqStore at bp+0 */ RawSeqStore_t
	var _ /* seqPos at bp+40 */ ZSTD_SequencePosition
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, blockCompressor, blockCompressor1, curr, dictMode, err_code, err_code1, i, istart, lastLLSize, lastLiterals, ms, nbExternalSeqs, nbPostProcessedSeqs, seqLenSum, windowSize, v1
	ms = zc + 3224 + 16
	/* Assert that we have correctly flushed the ctx params into the ms's copy */
	ZSTD_assertEqualCParams(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams)
	/* TODO: See 3090. We reduced MIN_CBLOCK_SIZE from 3 to 2 so to compensate we are adding
	 * additional 1. We need to revisit and change this logic to be more consistent */
	if srcSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)+libc.Int32FromInt32(1))+ZSTD_blockHeaderSize+uint64(1)+uint64(1) {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy >= int32(ZSTD_btopt) {
			ZSTD_ldm_skipRawSeqStoreBytes(tls, zc+3184, srcSize)
		} else {
			ZSTD_ldm_skipSequences(tls, zc+3184, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.FminMatch)
		}
		return uint64(ZSTDbss_noCompress) /* don't even attempt compression below a certain srcSize */
	}
	ZSTD_resetSeqStore(tls, zc+976)
	/* required for optimal parser to read stats from dictionary */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FsymbolCosts = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock
	/* tell the optimal parser how we expect to compress literals */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FliteralCompressionMode = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FliteralCompressionMode
	/* a gap between an attached dict and the current window is not safe,
	 * they must remain adjacent,
	 * and when that stops being the case, the dict must be unset */
	/* limited update after a very long match */
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	curr = libc.Uint32FromInt64(int64(istart) - int64(base))
	if uint64(8) == uint64(8) {
	} /* ensure no overflow */
	if curr > (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate+uint32(384) {
		if libc.Uint32FromInt32(libc.Int32FromInt32(192)) < curr-(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate-libc.Uint32FromInt32(384) {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(192))
		} else {
			v1 = curr - (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate - libc.Uint32FromInt32(384)
		}
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr - v1
	}
	/* select and store sequences */
	dictMode = ZSTD_matchState_dictMode(tls, ms)
	i = 0
	for {
		if !(i < int32(ZSTD_REP_NUM)) {
			break
		}
		*(*U32)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock + 5616 + uintptr(i)*4)) = *(*U32)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock + 5616 + uintptr(i)*4))
		goto _2
	_2:
		;
		i = i + 1
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FexternSeqStore.Fpos < (*ZSTD_CCtx)(unsafe.Pointer(zc)).FexternSeqStore.Fsize {
		/* External matchfinder + LDM is technically possible, just not implemented yet.
		 * We need to revisit soon and implement it. */
		if ZSTD_hasExtSeqProd(tls, zc+240) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3122, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_combination_unsupported))
		}
		/* Updates ldmSeqStore.pos */
		lastLLSize = ZSTD_ldm_blockCompress(tls, zc+3184, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, src, srcSize)
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
			*(*RawSeqStore_t)(unsafe.Pointer(bp)) = kNullRawSeqStore
			/* External matchfinder + LDM is technically possible, just not implemented yet.
			 * We need to revisit soon and implement it. */
			if ZSTD_hasExtSeqProd(tls, zc+240) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3122, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_combination_unsupported))
			}
			(*(*RawSeqStore_t)(unsafe.Pointer(bp))).Fseq = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FldmSequences
			(*(*RawSeqStore_t)(unsafe.Pointer(bp))).Fcapacity = (*ZSTD_CCtx)(unsafe.Pointer(zc)).FmaxNbLdmSequences
			/* Updates ldmSeqStore.size */
			err_code = ZSTD_ldm_generateSequences(tls, zc+1056, bp, zc+240+96, src, srcSize)
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code
			}
			/* Updates ldmSeqStore.pos */
			lastLLSize = ZSTD_ldm_blockCompress(tls, bp, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, src, srcSize)
		} else {
			if ZSTD_hasExtSeqProd(tls, zc+240) != 0 {
				windowSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.FwindowLog
				nbExternalSeqs = (*(*func(*libc.TLS, uintptr, uintptr, size_t, uintptr, size_t, uintptr, size_t, int32, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{(*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FextSeqProdFunc})))(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FextSeqProdState, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBufCapacity, src, srcSize, libc.UintptrFromInt32(0), uint64(0), (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcompressionLevel, uint64(windowSize))
				nbPostProcessedSeqs = ZSTD_postProcessSequenceProducerResult(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, nbExternalSeqs, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBufCapacity, srcSize)
				/* Return early if there is no error, since we don't need to worry about last literals */
				if !(ZSTD_isError(tls, nbPostProcessedSeqs) != 0) {
					*(*ZSTD_SequencePosition)(unsafe.Pointer(bp + 40)) = ZSTD_SequencePosition{}
					seqLenSum = ZSTD_fastSequenceLengthSum(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, nbPostProcessedSeqs)
					if seqLenSum > srcSize {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+3213, 0)
						}
						return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
					}
					err_code1 = ZSTD_transferSequences_wBlockDelim(tls, zc, bp+40, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FextSeqBuf, nbPostProcessedSeqs, src, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FsearchForExternalRepcodes)
					if ERR_isError(tls, err_code1) != 0 {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+3257, 0)
						}
						return err_code1
					}
					(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = libc.UintptrFromInt32(0)
					return uint64(ZSTDbss_compress)
				}
				/* Propagate the error if fallback is disabled */
				if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FenableMatchFinderFallback != 0) {
					return nbPostProcessedSeqs
				}
				/* Fallback to software matchfinder */
				blockCompressor = ZSTD_selectBlockCompressor(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, dictMode)
				(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = libc.UintptrFromInt32(0)
				lastLLSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, src, srcSize)
			} else { /* not long range mode and no external matchfinder */
				blockCompressor1 = ZSTD_selectBlockCompressor(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FuseRowMatchFinder, dictMode)
				(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = libc.UintptrFromInt32(0)
				lastLLSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor1})))(tls, ms, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock+5616, src, srcSize)
			}
		}
	}
	lastLiterals = src + uintptr(srcSize) - uintptr(lastLLSize)
	ZSTD_storeLastLiterals(tls, zc+976, lastLiterals, lastLLSize)
	ZSTD_validateSeqStore(tls, zc+976, zc+240+4)
	return uint64(ZSTDbss_compress)
}

func ZSTD_copyBlockSequences(tls *libc.TLS, seqCollector uintptr, seqStore uintptr, prevRepcodes uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var i, lastLLSize, nbInLiterals, nbInSequences, nbOutLiterals, nbOutSequences size_t
	var inSeqs, outSeqs, v1 uintptr
	var rawOffset, repcode U32
	var _ /* repcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _ = i, inSeqs, lastLLSize, nbInLiterals, nbInSequences, nbOutLiterals, nbOutSequences, outSeqs, rawOffset, repcode, v1
	inSeqs = (*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart
	nbInSequences = libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64(inSeqs)) / 8)
	nbInLiterals = libc.Uint64FromInt64(int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Flit) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart))
	if (*SeqCollector)(unsafe.Pointer(seqCollector)).FseqIndex == uint64(0) {
		v1 = (*SeqCollector)(unsafe.Pointer(seqCollector)).FseqStart
	} else {
		v1 = (*SeqCollector)(unsafe.Pointer(seqCollector)).FseqStart + uintptr((*SeqCollector)(unsafe.Pointer(seqCollector)).FseqIndex)*16
	}
	outSeqs = v1
	nbOutSequences = nbInSequences + uint64(1)
	nbOutLiterals = uint64(0)
	/* Bounds check that we have enough space for every input sequence
	 * and the block delimiter
	 */
	if nbOutSequences > (*SeqCollector)(unsafe.Pointer(seqCollector)).FmaxSequences-(*SeqCollector)(unsafe.Pointer(seqCollector)).FseqIndex {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3304, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	libc.Xmemcpy(tls, bp, prevRepcodes, libc.Uint64FromInt64(12))
	i = uint64(0)
	for {
		if !(i < nbInSequences) {
			break
		}
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength = uint32((*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FlitLength)
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FmatchLength = libc.Uint32FromInt32(libc.Int32FromUint16((*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FmlBase) + int32(MINMATCH))
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).Frep = uint32(0)
		/* Handle the possible single length >= 64K
		 * There can only be one because we add MINMATCH to every match length,
		 * and blocks are at most 128K.
		 */
		if i == uint64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos) {
			if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
				(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength += uint32(0x10000)
			} else {
				if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_matchLength) {
					(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FmatchLength += uint32(0x10000)
				}
			}
		}
		/* Determine the raw offset given the offBase, which may be a repcode. */
		if uint32(1) <= (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase && (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase <= uint32(ZSTD_REP_NUM) {
			repcode = (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase
			(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).Frep = repcode
			if (*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength != uint32(0) {
				rawOffset = *(*U32)(unsafe.Pointer(bp + uintptr(repcode-uint32(1))*4))
			} else {
				if repcode == uint32(3) {
					rawOffset = *(*U32)(unsafe.Pointer(bp)) - uint32(1)
				} else {
					rawOffset = *(*U32)(unsafe.Pointer(bp + uintptr(repcode)*4))
				}
			}
		} else {
			rawOffset = (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase - libc.Uint32FromInt32(ZSTD_REP_NUM)
		}
		(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).Foffset = rawOffset
		/* Update repcode history for the sequence */
		ZSTD_updateRep(tls, bp, (*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FoffBase, libc.BoolUint32(libc.Int32FromUint16((*(*SeqDef)(unsafe.Pointer(inSeqs + uintptr(i)*8))).FlitLength) == 0))
		nbOutLiterals = nbOutLiterals + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(i)*16))).FlitLength)
		goto _2
	_2:
		;
		i = i + 1
	}
	/* Insert last literals (if any exist) in the block as a sequence with ml == off == 0.
	 * If there are no last literals, then we'll emit (of: 0, ml: 0, ll: 0), which is a marker
	 * for the block boundary, according to the API.
	 */
	lastLLSize = nbInLiterals - nbOutLiterals
	(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbInSequences)*16))).FlitLength = uint32(lastLLSize)
	(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbInSequences)*16))).FmatchLength = uint32(0)
	(*(*ZSTD_Sequence)(unsafe.Pointer(outSeqs + uintptr(nbInSequences)*16))).Foffset = uint32(0)
	*(*size_t)(unsafe.Pointer(seqCollector + 16)) += nbOutSequences
	return uint64(0)
}

func ZSTD_sequenceBound(tls *libc.TLS, srcSize size_t) (r size_t) {
	var maxNbDelims, maxNbSeq size_t
	_, _ = maxNbDelims, maxNbSeq
	maxNbSeq = srcSize/uint64(ZSTD_MINMATCH_MIN) + uint64(1)
	maxNbDelims = srcSize/libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)) + uint64(1)
	return maxNbSeq + maxNbDelims
}

func ZSTD_generateSequences(tls *libc.TLS, zc uintptr, outSeqs uintptr, outSeqsSize size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var dst uintptr
	var dstCapacity, err_code, err_code1, err_code2, ret size_t
	var seqCollector SeqCollector
	var _ /* nbWorkers at bp+4 */ int32
	var _ /* targetCBlockSize at bp+0 */ int32
	_, _, _, _, _, _, _ = dst, dstCapacity, err_code, err_code1, err_code2, ret, seqCollector
	dstCapacity = ZSTD_compressBound(tls, srcSize)
	err_code = ZSTD_CCtx_getParameter(tls, zc, int32(ZSTD_c_targetCBlockSize), bp)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if *(*int32)(unsafe.Pointer(bp)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3339, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	}
	err_code1 = ZSTD_CCtx_getParameter(tls, zc, int32(ZSTD_c_nbWorkers), bp+4)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	if *(*int32)(unsafe.Pointer(bp + 4)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3361, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	}
	dst = ZSTD_customMalloc(tls, dstCapacity, ZSTD_defaultCMem)
	if dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	seqCollector.FcollectSequences = int32(1)
	seqCollector.FseqStart = outSeqs
	seqCollector.FseqIndex = uint64(0)
	seqCollector.FmaxSequences = outSeqsSize
	(*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector = seqCollector
	ret = ZSTD_compress2(tls, zc, dst, dstCapacity, src, srcSize)
	ZSTD_customFree(tls, dst, ZSTD_defaultCMem)
	err_code2 = ret
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3376, 0)
		}
		return err_code2
	}
	return (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FseqIndex
}

func ZSTD_mergeBlockDelimiters(tls *libc.TLS, sequences uintptr, seqsSize size_t) (r size_t) {
	var in, out size_t
	_, _ = in, out
	in = uint64(0)
	out = uint64(0)
	for {
		if !(in < seqsSize) {
			break
		}
		if (*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))).Foffset == uint32(0) && (*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))).FmatchLength == uint32(0) {
			if in != seqsSize-uint64(1) {
				(*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in+uint64(1))*16))).FlitLength += (*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))).FlitLength
			}
		} else {
			*(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(out)*16)) = *(*ZSTD_Sequence)(unsafe.Pointer(sequences + uintptr(in)*16))
			out = out + 1
		}
		goto _1
	_1:
		;
		in = in + 1
	}
	return out
}

// C documentation
//
//	/* Unrolled loop to read four size_ts of input at a time. Returns 1 if is RLE, 0 if not. */
func ZSTD_isRLE(tls *libc.TLS, src uintptr, length size_t) (r int32) {
	var i, prefixLength, u, unrollMask, unrollSize, valueST size_t
	var ip uintptr
	var value BYTE
	_, _, _, _, _, _, _, _ = i, ip, prefixLength, u, unrollMask, unrollSize, value, valueST
	ip = src
	value = *(*BYTE)(unsafe.Pointer(ip))
	valueST = uint64(uint64(value) * libc.Uint64FromUint64(0x0101010101010101))
	unrollSize = libc.Uint64FromInt64(8) * libc.Uint64FromInt32(4)
	unrollMask = unrollSize - uint64(1)
	prefixLength = length & unrollMask
	if length == uint64(1) {
		return int32(1)
	}
	/* Check if prefix is RLE first before using unrolled loop */
	if prefixLength != 0 && ZSTD_count(tls, ip+uintptr(1), ip, ip+uintptr(prefixLength)) != prefixLength-uint64(1) {
		return 0
	}
	i = prefixLength
	for {
		if !(i != length) {
			break
		}
		u = uint64(0)
		for {
			if !(u < unrollSize) {
				break
			}
			if MEM_readST(tls, ip+uintptr(i)+uintptr(u)) != valueST {
				return 0
			}
			goto _2
		_2:
			;
			u = u + uint64(8)
		}
		goto _1
	_1:
		;
		i = i + unrollSize
	}
	return int32(1)
}

// C documentation
//
//	/* Returns true if the given block may be RLE.
//	 * This is just a heuristic based on the compressibility.
//	 * It may return both false positives and false negatives.
//	 */
func ZSTD_maybeRLE(tls *libc.TLS, seqStore uintptr) (r int32) {
	var nbLits, nbSeqs size_t
	_, _ = nbLits, nbSeqs
	nbSeqs = libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart)) / 8)
	nbLits = libc.Uint64FromInt64(int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Flit) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart))
	return libc.BoolInt32(nbSeqs < uint64(4) && nbLits < uint64(10))
}

func ZSTD_blockState_confirmRepcodesAndEntropyTables(tls *libc.TLS, bs uintptr) {
	var tmp uintptr
	_ = tmp
	tmp = (*ZSTD_blockState_t)(unsafe.Pointer(bs)).FprevCBlock
	(*ZSTD_blockState_t)(unsafe.Pointer(bs)).FprevCBlock = (*ZSTD_blockState_t)(unsafe.Pointer(bs)).FnextCBlock
	(*ZSTD_blockState_t)(unsafe.Pointer(bs)).FnextCBlock = tmp
}

// C documentation
//
//	/* Writes the block header */
func writeBlockHeader(tls *libc.TLS, op uintptr, cSize size_t, blockSize size_t, lastBlock U32) {
	var cBlockHeader U32
	var v1 uint32
	_, _ = cBlockHeader, v1
	if cSize == uint64(1) {
		v1 = lastBlock + uint32(bt_rle)<<libc.Int32FromInt32(1) + uint32(blockSize<<libc.Int32FromInt32(3))
	} else {
		v1 = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(cSize<<libc.Int32FromInt32(3))
	}
	cBlockHeader = v1
	MEM_writeLE24(tls, op, cBlockHeader)
}

// C documentation
//
//	/** ZSTD_buildBlockEntropyStats_literals() :
//	 *  Builds entropy for the literals.
//	 *  Stores literals block type (raw, rle, compressed, repeat) and
//	 *  huffman description table to hufMetadata.
//	 *  Requires ENTROPY_WORKSPACE_SIZE workspace
//	 * @return : size of huffman description table, or an error code
//	 */
func ZSTD_buildBlockEntropyStats_literals(tls *libc.TLS, src uintptr, srcSize size_t, prevHuf uintptr, nextHuf uintptr, hufMetadata uintptr, literalsCompressionIsDisabled int32, workspace uintptr, wkspSize size_t, hufFlags int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var countWksp, countWkspStart, nodeWksp, wkspEnd, wkspStart uintptr
	var countWkspSize, err_code, err_code1, hSize, largest, maxBits, minLitSize, newCSize, nodeWkspSize, oldCSize size_t
	var huffLog uint32
	var repeat HUF_repeat
	var v1 int32
	var _ /* maxSymbolValue at bp+0 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = countWksp, countWkspSize, countWkspStart, err_code, err_code1, hSize, huffLog, largest, maxBits, minLitSize, newCSize, nodeWksp, nodeWkspSize, oldCSize, repeat, wkspEnd, wkspStart, v1
	wkspStart = workspace
	wkspEnd = wkspStart + uintptr(wkspSize)
	countWkspStart = wkspStart
	countWksp = workspace
	countWkspSize = libc.Uint64FromInt32(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)) * libc.Uint64FromInt64(4)
	nodeWksp = countWkspStart + uintptr(countWkspSize)
	nodeWkspSize = libc.Uint64FromInt64(int64(wkspEnd) - int64(nodeWksp))
	*(*uint32)(unsafe.Pointer(bp)) = uint32(HUF_SYMBOLVALUE_MAX)
	huffLog = uint32(LitHufLog)
	repeat = (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode
	/* Prepare nextEntropy assuming reusing the existing table */
	libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
	if literalsCompressionIsDisabled != 0 {
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	/* small ? don't even attempt compression (speed opt) */
	if (*ZSTD_hufCTables_t)(unsafe.Pointer(prevHuf)).FrepeatMode == int32(HUF_repeat_valid) {
		v1 = int32(6)
	} else {
		v1 = int32(COMPRESS_LITERALS_SIZE_MIN)
	}
	minLitSize = libc.Uint64FromInt32(v1)
	if srcSize <= minLitSize {
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	/* Scan input and build symbol stats */
	largest = HIST_count_wksp(tls, countWksp, bp, src, srcSize, workspace, wkspSize)
	err_code = largest
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3398, 0)
		}
		return err_code
	}
	if largest == srcSize {
		/* only one literal symbol */
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_rle)
		return uint64(0)
	}
	if largest <= srcSize>>libc.Int32FromInt32(7)+uint64(4) {
		/* heuristic: likely not compressible */
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	/* Validate the previous Huffman table */
	if repeat == int32(HUF_repeat_check) && !(HUF_validateCTable(tls, prevHuf, countWksp, *(*uint32)(unsafe.Pointer(bp))) != 0) {
		repeat = int32(HUF_repeat_none)
	}
	/* Build Huffman Tree */
	libc.Xmemset(tls, nextHuf, 0, libc.Uint64FromInt64(2056))
	huffLog = HUF_optimalTableLog(tls, huffLog, srcSize, *(*uint32)(unsafe.Pointer(bp)), nodeWksp, nodeWkspSize, nextHuf, countWksp, hufFlags)
	maxBits = HUF_buildCTable_wksp(tls, nextHuf, countWksp, *(*uint32)(unsafe.Pointer(bp)), huffLog, nodeWksp, nodeWkspSize)
	err_code1 = maxBits
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3421, 0)
		}
		return err_code1
	}
	huffLog = uint32(maxBits)
	/* Build and write the CTable */
	newCSize = HUF_estimateCompressedSize(tls, nextHuf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
	hSize = HUF_writeCTable_wksp(tls, hufMetadata+4, uint64(128), nextHuf, *(*uint32)(unsafe.Pointer(bp)), huffLog, nodeWksp, nodeWkspSize)
	/* Check against repeating the previous CTable */
	if repeat != int32(HUF_repeat_none) {
		oldCSize = HUF_estimateCompressedSize(tls, prevHuf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
		if oldCSize < srcSize && (oldCSize <= hSize+newCSize || hSize+uint64(12) >= srcSize) {
			libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
			(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_repeat)
			return uint64(0)
		}
	}
	if newCSize+hSize >= srcSize {
		libc.Xmemcpy(tls, nextHuf, prevHuf, libc.Uint64FromInt64(2064))
		(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_basic)
		return uint64(0)
	}
	(*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType = int32(set_compressed)
	(*ZSTD_hufCTables_t)(unsafe.Pointer(nextHuf)).FrepeatMode = int32(HUF_repeat_check)
	return hSize
	return r
}

// C documentation
//
//	/* ZSTD_buildDummySequencesStatistics():
//	 * Returns a ZSTD_symbolEncodingTypeStats_t with all encoding types as set_basic,
//	 * and updates nextEntropy to the appropriate repeatMode.
//	 */
func ZSTD_buildDummySequencesStatistics(tls *libc.TLS, nextEntropy uintptr) (r ZSTD_symbolEncodingTypeStats_t) {
	var stats ZSTD_symbolEncodingTypeStats_t
	_ = stats
	stats = ZSTD_symbolEncodingTypeStats_t{}
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Flitlength_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Foffcode_repeatMode = int32(FSE_repeat_none)
	(*ZSTD_fseCTables_t)(unsafe.Pointer(nextEntropy)).Fmatchlength_repeatMode = int32(FSE_repeat_none)
	return stats
}

// C documentation
//
//	/** ZSTD_buildBlockEntropyStats_sequences() :
//	 *  Builds entropy for the sequences.
//	 *  Stores symbol compression modes and fse table to fseMetadata.
//	 *  Requires ENTROPY_WORKSPACE_SIZE wksp.
//	 * @return : size of fse tables or error code */
func ZSTD_buildBlockEntropyStats_sequences(tls *libc.TLS, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, fseMetadata uintptr, workspace uintptr, wkspSize size_t) (r size_t) {
	var countWorkspace, entropyWorkspace, oend, op, ostart uintptr
	var entropyWorkspaceSize, err_code, nbSeq size_t
	var stats, v1 ZSTD_symbolEncodingTypeStats_t
	var strategy ZSTD_strategy
	_, _, _, _, _, _, _, _, _, _, _ = countWorkspace, entropyWorkspace, entropyWorkspaceSize, err_code, nbSeq, oend, op, ostart, stats, strategy, v1
	strategy = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy
	nbSeq = libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
	ostart = fseMetadata + 12
	oend = ostart + uintptr(133)
	op = ostart
	countWorkspace = workspace
	entropyWorkspace = countWorkspace + uintptr(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*4
	entropyWorkspaceSize = wkspSize - libc.Uint64FromInt32(libc.Int32FromInt32(MaxML)+libc.Int32FromInt32(1))*libc.Uint64FromInt64(4)
	if nbSeq != uint64(0) {
		v1 = ZSTD_buildSequencesStatistics(tls, seqStorePtr, nbSeq, prevEntropy, nextEntropy, op, oend, strategy, countWorkspace, entropyWorkspace, entropyWorkspaceSize)
	} else {
		v1 = ZSTD_buildDummySequencesStatistics(tls, nextEntropy)
	}
	stats = v1
	err_code = stats.Fsize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+2837, 0)
		}
		return err_code
	}
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType = libc.Int32FromUint32(stats.FLLtype)
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType = libc.Int32FromUint32(stats.FOfftype)
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType = libc.Int32FromUint32(stats.FMLtype)
	(*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FlastCountSize = stats.FlastCountSize
	return stats.Fsize
}

// C documentation
//
//	/** ZSTD_buildBlockEntropyStats() :
//	 *  Builds entropy for the block.
//	 *  Requires workspace size ENTROPY_WORKSPACE_SIZE
//	 * @return : 0 on success, or an error code
//	 *  Note : also employed in superblock
//	 */
func ZSTD_buildBlockEntropyStats(tls *libc.TLS, seqStorePtr uintptr, prevEntropy uintptr, nextEntropy uintptr, cctxParams uintptr, entropyMetadata uintptr, workspace uintptr, wkspSize size_t) (r size_t) {
	var err_code, err_code1, litSize size_t
	var hufFlags, huf_useOptDepth, v1 int32
	_, _, _, _, _, _ = err_code, err_code1, hufFlags, huf_useOptDepth, litSize, v1
	litSize = libc.Uint64FromInt64(int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart))
	huf_useOptDepth = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcParams.Fstrategy >= int32(ZSTD_btultra))
	if huf_useOptDepth != 0 {
		v1 = int32(HUF_flags_optimalDepth)
	} else {
		v1 = 0
	}
	hufFlags = v1
	(*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhufDesSize = ZSTD_buildBlockEntropyStats_literals(tls, (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart, litSize, prevEntropy, nextEntropy, entropyMetadata, ZSTD_literalsCompressionIsDisabled(tls, cctxParams), workspace, wkspSize, hufFlags)
	err_code = (*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhufDesSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3442, 0)
		}
		return err_code
	}
	(*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FfseMetadata.FfseTablesSize = ZSTD_buildBlockEntropyStats_sequences(tls, seqStorePtr, prevEntropy+2064, nextEntropy+2064, cctxParams, entropyMetadata+144, workspace, wkspSize)
	err_code1 = (*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FfseMetadata.FfseTablesSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3486, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/* Returns the size estimate for the literals section (header + content) of a block */
func ZSTD_estimateBlockSize_literal(tls *libc.TLS, literals uintptr, litSize size_t, huf uintptr, hufMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cLitSizeEstimate, largest, literalSectionHeaderSize size_t
	var countWksp uintptr
	var singleStream U32
	var _ /* maxSymbolValue at bp+0 */ uint32
	_, _, _, _, _ = cLitSizeEstimate, countWksp, largest, literalSectionHeaderSize, singleStream
	countWksp = workspace
	*(*uint32)(unsafe.Pointer(bp)) = uint32(HUF_SYMBOLVALUE_MAX)
	literalSectionHeaderSize = libc.Uint64FromInt32(int32(3) + libc.BoolInt32(litSize >= libc.Uint64FromInt32(libc.Int32FromInt32(1)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(litSize >= libc.Uint64FromInt32(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	singleStream = libc.BoolUint32(litSize < uint64(256))
	if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_basic) {
		return litSize
	} else {
		if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_rle) {
			return uint64(1)
		} else {
			if (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_compressed) || (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhType == int32(set_repeat) {
				largest = HIST_count_wksp(tls, countWksp, bp, literals, litSize, workspace, wkspSize)
				if ZSTD_isError(tls, largest) != 0 {
					return litSize
				}
				cLitSizeEstimate = HUF_estimateCompressedSize(tls, huf, countWksp, *(*uint32)(unsafe.Pointer(bp)))
				if writeEntropy != 0 {
					cLitSizeEstimate = cLitSizeEstimate + (*ZSTD_hufCTablesMetadata_t)(unsafe.Pointer(hufMetadata)).FhufDesSize
				}
				if !(singleStream != 0) {
					cLitSizeEstimate = cLitSizeEstimate + uint64(6)
				} /* multi-stream huffman uses 6-byte jump table */
				return cLitSizeEstimate + literalSectionHeaderSize
			}
		}
	}
	/* impossible */
	return uint64(0)
}

// C documentation
//
//	/* Returns the size estimate for the FSE-compressed symbols (of, ml, ll) of a block */
func ZSTD_estimateBlockSize_symbolType(tls *libc.TLS, type1 SymbolEncodingType_e, codeTable uintptr, nbSeq size_t, maxCode uint32, fseCTable uintptr, additionalBits uintptr, defaultNorm uintptr, defaultNormLog U32, defaultMax U32, workspace uintptr, wkspSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cSymbolTypeSizeEstimateInBits size_t
	var countWksp, ctEnd, ctStart, ctp uintptr
	var _ /* max at bp+0 */ uint32
	_, _, _, _, _ = cSymbolTypeSizeEstimateInBits, countWksp, ctEnd, ctStart, ctp
	countWksp = workspace
	ctp = codeTable
	ctStart = ctp
	ctEnd = ctStart + uintptr(nbSeq)
	cSymbolTypeSizeEstimateInBits = uint64(0)
	*(*uint32)(unsafe.Pointer(bp)) = maxCode
	HIST_countFast_wksp(tls, countWksp, bp, codeTable, nbSeq, workspace, wkspSize) /* can't fail */
	if type1 == int32(set_basic) {
		/* We selected this encoding type, so it must be valid. */
		_ = defaultMax
		cSymbolTypeSizeEstimateInBits = ZSTD_crossEntropyCost(tls, defaultNorm, defaultNormLog, countWksp, *(*uint32)(unsafe.Pointer(bp)))
	} else {
		if type1 == int32(set_rle) {
			cSymbolTypeSizeEstimateInBits = uint64(0)
		} else {
			if type1 == int32(set_compressed) || type1 == int32(set_repeat) {
				cSymbolTypeSizeEstimateInBits = ZSTD_fseBitCost(tls, fseCTable, countWksp, *(*uint32)(unsafe.Pointer(bp)))
			}
		}
	}
	if ZSTD_isError(tls, cSymbolTypeSizeEstimateInBits) != 0 {
		return nbSeq * uint64(10)
	}
	for ctp < ctEnd {
		if additionalBits != 0 {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*U8)(unsafe.Pointer(additionalBits + uintptr(*(*BYTE)(unsafe.Pointer(ctp))))))
		} else {
			cSymbolTypeSizeEstimateInBits = cSymbolTypeSizeEstimateInBits + uint64(*(*BYTE)(unsafe.Pointer(ctp)))
		} /* for offset, offset code is also the number of additional bits */
		ctp = ctp + 1
	}
	return cSymbolTypeSizeEstimateInBits >> int32(3)
}

// C documentation
//
//	/* Returns the size estimate for the sequences section (header + content) of a block */
func ZSTD_estimateBlockSize_sequences(tls *libc.TLS, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, fseTables uintptr, fseMetadata uintptr, workspace uintptr, wkspSize size_t, writeEntropy int32) (r size_t) {
	var cSeqSizeEstimate, sequencesSectionHeaderSize size_t
	_, _ = cSeqSizeEstimate, sequencesSectionHeaderSize
	sequencesSectionHeaderSize = libc.Uint64FromInt32(libc.Int32FromInt32(1) + libc.Int32FromInt32(1) + libc.BoolInt32(nbSeq >= uint64(128)) + libc.BoolInt32(nbSeq >= uint64(LONGNBSEQ)))
	cSeqSizeEstimate = uint64(0)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FofType, ofCodeTable, nbSeq, uint32(MaxOff), fseTables, libc.UintptrFromInt32(0), uintptr(unsafe.Pointer(&OF_defaultNorm)), OF_defaultNormLog, uint32(DefaultMaxOff), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FllType, llCodeTable, nbSeq, uint32(MaxLL), fseTables+2224, uintptr(unsafe.Pointer(&LL_bits)), uintptr(unsafe.Pointer(&LL_defaultNorm)), LL_defaultNormLog, uint32(MaxLL), workspace, wkspSize)
	cSeqSizeEstimate = cSeqSizeEstimate + ZSTD_estimateBlockSize_symbolType(tls, (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FmlType, mlCodeTable, nbSeq, uint32(MaxML), fseTables+772, uintptr(unsafe.Pointer(&ML_bits)), uintptr(unsafe.Pointer(&ML_defaultNorm)), ML_defaultNormLog, uint32(MaxML), workspace, wkspSize)
	if writeEntropy != 0 {
		cSeqSizeEstimate = cSeqSizeEstimate + (*ZSTD_fseCTablesMetadata_t)(unsafe.Pointer(fseMetadata)).FfseTablesSize
	}
	return cSeqSizeEstimate + sequencesSectionHeaderSize
}

// C documentation
//
//	/* Returns the size estimate for a given stream of literals, of, ll, ml */
func ZSTD_estimateBlockSize(tls *libc.TLS, literals uintptr, litSize size_t, ofCodeTable uintptr, llCodeTable uintptr, mlCodeTable uintptr, nbSeq size_t, entropy uintptr, entropyMetadata uintptr, workspace uintptr, wkspSize size_t, writeLitEntropy int32, writeSeqEntropy int32) (r size_t) {
	var literalsSize, seqSize size_t
	_, _ = literalsSize, seqSize
	literalsSize = ZSTD_estimateBlockSize_literal(tls, literals, litSize, entropy, entropyMetadata, workspace, wkspSize, writeLitEntropy)
	seqSize = ZSTD_estimateBlockSize_sequences(tls, ofCodeTable, llCodeTable, mlCodeTable, nbSeq, entropy+2064, entropyMetadata+144, workspace, wkspSize, writeSeqEntropy)
	return seqSize + literalsSize + ZSTD_blockHeaderSize
}

// C documentation
//
//	/* Builds entropy statistics and uses them for blocksize estimation.
//	 *
//	 * @return: estimated compressed size of the seqStore, or a zstd error.
//	 */
func ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls *libc.TLS, seqStore uintptr, zc uintptr) (r size_t) {
	var entropyMetadata uintptr
	var err_code size_t
	_, _ = entropyMetadata, err_code
	entropyMetadata = zc + 3768 + 1184
	err_code = ZSTD_buildBlockEntropyStats(tls, seqStore, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, entropyMetadata, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_estimateBlockSize(tls, (*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart, libc.Uint64FromInt64(int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Flit)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlitStart)), (*SeqStore_t)(unsafe.Pointer(seqStore)).FofCode, (*SeqStore_t)(unsafe.Pointer(seqStore)).FllCode, (*SeqStore_t)(unsafe.Pointer(seqStore)).FmlCode, libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart))/8), (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, entropyMetadata, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize, libc.BoolInt32((*ZSTD_entropyCTablesMetadata_t)(unsafe.Pointer(entropyMetadata)).FhufMetadata.FhType == int32(set_compressed)), int32(1))
}

// C documentation
//
//	/* Returns literals bytes represented in a seqStore */
func ZSTD_countSeqStoreLiteralsBytes(tls *libc.TLS, seqStore uintptr) (r size_t) {
	var i, literalsBytes, nbSeqs size_t
	var seq SeqDef
	_, _, _, _ = i, literalsBytes, nbSeqs, seq
	literalsBytes = uint64(0)
	nbSeqs = libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart)) / 8)
	i = uint64(0)
	for {
		if !(i < nbSeqs) {
			break
		}
		seq = *(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart + uintptr(i)*8))
		literalsBytes = literalsBytes + uint64(seq.FlitLength)
		if i == uint64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos) && (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
			literalsBytes = literalsBytes + uint64(0x10000)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return literalsBytes
}

// C documentation
//
//	/* Returns match bytes represented in a seqStore */
func ZSTD_countSeqStoreMatchBytes(tls *libc.TLS, seqStore uintptr) (r size_t) {
	var i, matchBytes, nbSeqs size_t
	var seq SeqDef
	_, _, _, _ = i, matchBytes, nbSeqs, seq
	matchBytes = uint64(0)
	nbSeqs = libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart)) / 8)
	i = uint64(0)
	for {
		if !(i < nbSeqs) {
			break
		}
		seq = *(*SeqDef)(unsafe.Pointer((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart + uintptr(i)*8))
		matchBytes = matchBytes + libc.Uint64FromInt32(libc.Int32FromUint16(seq.FmlBase)+int32(MINMATCH))
		if i == uint64((*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos) && (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_matchLength) {
			matchBytes = matchBytes + uint64(0x10000)
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	return matchBytes
}

// C documentation
//
//	/* Derives the seqStore that is a chunk of the originalSeqStore from [startIdx, endIdx).
//	 * Stores the result in resultSeqStore.
//	 */
func ZSTD_deriveSeqStoreChunk(tls *libc.TLS, resultSeqStore uintptr, originalSeqStore uintptr, startIdx size_t, endIdx size_t) {
	var literalsBytes size_t
	_ = literalsBytes
	*(*SeqStore_t)(unsafe.Pointer(resultSeqStore)) = *(*SeqStore_t)(unsafe.Pointer(originalSeqStore))
	if startIdx > uint64(0) {
		(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).Fsequences = (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart + uintptr(startIdx)*8
		*(*uintptr)(unsafe.Pointer(resultSeqStore + 16)) += uintptr(ZSTD_countSeqStoreLiteralsBytes(tls, resultSeqStore))
	}
	/* Move longLengthPos into the correct position if necessary */
	if (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FlongLengthType != int32(ZSTD_llt_none) {
		if uint64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FlongLengthPos) < startIdx || uint64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FlongLengthPos) > endIdx {
			(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).FlongLengthType = int32(ZSTD_llt_none)
		} else {
			*(*U32)(unsafe.Pointer(resultSeqStore + 76)) -= uint32(startIdx)
		}
	}
	(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).FsequencesStart = (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart + uintptr(startIdx)*8
	(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).Fsequences = (*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart + uintptr(endIdx)*8
	if endIdx == libc.Uint64FromInt64((int64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).Fsequences)-int64((*SeqStore_t)(unsafe.Pointer(originalSeqStore)).FsequencesStart))/8) {
		/* This accounts for possible last literals if the derived chunk reaches the end of the block */
	} else {
		literalsBytes = ZSTD_countSeqStoreLiteralsBytes(tls, resultSeqStore)
		(*SeqStore_t)(unsafe.Pointer(resultSeqStore)).Flit = (*SeqStore_t)(unsafe.Pointer(resultSeqStore)).FlitStart + uintptr(literalsBytes)
	}
	*(*uintptr)(unsafe.Pointer(resultSeqStore + 32)) += uintptr(startIdx)
	*(*uintptr)(unsafe.Pointer(resultSeqStore + 40)) += uintptr(startIdx)
	*(*uintptr)(unsafe.Pointer(resultSeqStore + 48)) += uintptr(startIdx)
}

// C documentation
//
//	/**
//	 * Returns the raw offset represented by the combination of offBase, ll0, and repcode history.
//	 * offBase must represent a repcode in the numeric representation of ZSTD_storeSeq().
//	 */
func ZSTD_resolveRepcodeToRawOffset(tls *libc.TLS, rep uintptr, offBase U32, ll0 U32) (r U32) {
	var adjustedRepCode U32
	_ = adjustedRepCode
	adjustedRepCode = offBase - uint32(1) + ll0 /* [ 0 - 3 ] */
	if adjustedRepCode == uint32(ZSTD_REP_NUM) {
		/* litlength == 0 and offCode == 2 implies selection of first repcode - 1
		 * This is only valid if it results in a valid offset value, aka > 0.
		 * Note : it may happen that `rep[0]==1` in exceptional circumstances.
		 * In which case this function will return 0, which is an invalid offset.
		 * It's not an issue though, since this value will be
		 * compared and discarded within ZSTD_seqStore_resolveOffCodes().
		 */
		return *(*U32)(unsafe.Pointer(rep)) - uint32(1)
	}
	return *(*U32)(unsafe.Pointer(rep + uintptr(adjustedRepCode)*4))
}

// C documentation
//
//	/**
//	 * ZSTD_seqStore_resolveOffCodes() reconciles any possible divergences in offset history that may arise
//	 * due to emission of RLE/raw blocks that disturb the offset history,
//	 * and replaces any repcodes within the seqStore that may be invalid.
//	 *
//	 * dRepcodes are updated as would be on the decompression side.
//	 * cRepcodes are updated exactly in accordance with the seqStore.
//	 *
//	 * Note : this function assumes seq->offBase respects the following numbering scheme :
//	 *        0 : invalid
//	 *        1-3 : repcode 1-3
//	 *        4+ : real_offset+3
//	 */
func ZSTD_seqStore_resolveOffCodes(tls *libc.TLS, dRepcodes uintptr, cRepcodes uintptr, seqStore uintptr, nbSeq U32) {
	var cRawOffset, dRawOffset, idx, ll0, longLitLenIdx, offBase U32
	var seq uintptr
	var v1 uint32
	_, _, _, _, _, _, _, _ = cRawOffset, dRawOffset, idx, ll0, longLitLenIdx, offBase, seq, v1
	idx = uint32(0)
	if (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthType == int32(ZSTD_llt_literalLength) {
		v1 = (*SeqStore_t)(unsafe.Pointer(seqStore)).FlongLengthPos
	} else {
		v1 = nbSeq
	}
	longLitLenIdx = v1
	for {
		if !(idx < nbSeq) {
			break
		}
		seq = (*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart + uintptr(idx)*8
		ll0 = libc.BoolUint32(libc.Int32FromUint16((*SeqDef)(unsafe.Pointer(seq)).FlitLength) == 0 && idx != longLitLenIdx)
		offBase = (*SeqDef)(unsafe.Pointer(seq)).FoffBase
		if uint32(1) <= offBase && offBase <= uint32(ZSTD_REP_NUM) {
			dRawOffset = ZSTD_resolveRepcodeToRawOffset(tls, dRepcodes, offBase, ll0)
			cRawOffset = ZSTD_resolveRepcodeToRawOffset(tls, cRepcodes, offBase, ll0)
			/* Adjust simulated decompression repcode history if we come across a mismatch. Replace
			 * the repcode with the offset it actually references, determined by the compression
			 * repcode history.
			 */
			if dRawOffset != cRawOffset {
				(*SeqDef)(unsafe.Pointer(seq)).FoffBase = cRawOffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
			}
		}
		/* Compression repcode history is always updated with values directly from the unmodified seqStore.
		 * Decompression repcode history may use modified seq->offset value taken from compression repcode history.
		 */
		ZSTD_updateRep(tls, dRepcodes, (*SeqDef)(unsafe.Pointer(seq)).FoffBase, ll0)
		ZSTD_updateRep(tls, cRepcodes, offBase, ll0)
		goto _2
	_2:
		;
		idx = idx + 1
	}
}

// C documentation
//
//	/* ZSTD_compressSeqStore_singleBlock():
//	 * Compresses a seqStore into a block with a block header, into the buffer dst.
//	 *
//	 * Returns the total size of that block (including header) or a ZSTD error code.
//	 */
func ZSTD_compressSeqStore_singleBlock(tls *libc.TLS, zc uintptr, seqStore uintptr, dRep uintptr, cRep uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32, isPartition U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cSeqsSize, cSize, err_code, err_code1, err_code2, err_code3 size_t
	var ip, op uintptr
	var rleMaxLength U32
	var _ /* dRepOriginal at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _ = cSeqsSize, cSize, err_code, err_code1, err_code2, err_code3, ip, op, rleMaxLength
	rleMaxLength = uint32(25)
	op = dst
	ip = src
	/* In case of an RLE or raw block, the simulated decompression repcode history must be reset */
	*(*Repcodes_t)(unsafe.Pointer(bp)) = *(*Repcodes_t)(unsafe.Pointer(dRep))
	if isPartition != 0 {
		ZSTD_seqStore_resolveOffCodes(tls, dRep, cRep, seqStore, libc.Uint32FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences)-int64((*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart))/8))
	}
	if dstCapacity < ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3531, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	cSeqsSize = ZSTD_entropyCompressSeqStore(tls, seqStore, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).Fbmi2)
	err_code = cSeqsSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3556, 0)
		}
		return err_code
	}
	if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock != 0) && cSeqsSize < uint64(rleMaxLength) && ZSTD_isRLE(tls, src, srcSize) != 0 {
		/* We don't want to emit our first block as a RLE even if it qualifies because
		 * doing so will cause the decoder (cli only) to throw a "should consume all input error."
		 * This is only an issue for zstd <= v1.4.3
		 */
		cSeqsSize = uint64(1)
	}
	/* Sequence collection not supported when block splitting */
	if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
		err_code1 = ZSTD_copyBlockSequences(tls, zc+936, seqStore, bp)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3593, 0)
			}
			return err_code1
		}
		ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
		return uint64(0)
	}
	if cSeqsSize == uint64(0) {
		cSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, srcSize, lastBlock)
		err_code2 = cSize
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3619, 0)
			}
			return err_code2
		}
		*(*Repcodes_t)(unsafe.Pointer(dRep)) = *(*Repcodes_t)(unsafe.Pointer(bp)) /* reset simulated decompression repcode history */
	} else {
		if cSeqsSize == uint64(1) {
			cSize = ZSTD_rleCompressBlock(tls, op, dstCapacity, *(*BYTE)(unsafe.Pointer(ip)), srcSize, lastBlock)
			err_code3 = cSize
			if ERR_isError(tls, err_code3) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3643, 0)
				}
				return err_code3
			}
			*(*Repcodes_t)(unsafe.Pointer(dRep)) = *(*Repcodes_t)(unsafe.Pointer(bp)) /* reset simulated decompression repcode history */
		} else {
			ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
			writeBlockHeader(tls, op, cSeqsSize, srcSize, lastBlock)
			cSize = ZSTD_blockHeaderSize + cSeqsSize
		}
	}
	if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
	}
	return cSize
}

// C documentation
//
//	/* Struct to keep track of where we are in our recursive calls. */
type seqStoreSplits = struct {
	FsplitLocations uintptr
	Fidx            size_t
}

// C documentation
//
//	/* Helper function to perform the recursive search for block splits.
//	 * Estimates the cost of seqStore prior to split, and estimates the cost of splitting the sequences in half.
//	 * If advantageous to split, then we recurse down the two sub-blocks.
//	 * If not, or if an error occurred in estimation, then we do not recurse.
//	 *
//	 * Note: The recursion depth is capped by a heuristic minimum number of sequences,
//	 * defined by MIN_SEQUENCES_BLOCK_SPLITTING.
//	 * In theory, this means the absolute largest recursion depth is 10 == log2(maxNbSeqInBlock/MIN_SEQUENCES_BLOCK_SPLITTING).
//	 * In practice, recursion depth usually doesn't go beyond 4.
//	 *
//	 * Furthermore, the number of splits is capped by ZSTD_MAX_NB_BLOCK_SPLITS.
//	 * At ZSTD_MAX_NB_BLOCK_SPLITS == 196 with the current existing blockSize
//	 * maximum of 128 KB, this value is actually impossible to reach.
//	 */
func ZSTD_deriveBlockSplitsHelper(tls *libc.TLS, splits uintptr, startIdx size_t, endIdx size_t, zc uintptr, origSeqStore uintptr) {
	var estimatedFirstHalfSize, estimatedOriginalSize, estimatedSecondHalfSize, midIdx size_t
	var firstHalfSeqStore, fullSeqStoreChunk, secondHalfSeqStore uintptr
	_, _, _, _, _, _, _ = estimatedFirstHalfSize, estimatedOriginalSize, estimatedSecondHalfSize, firstHalfSeqStore, fullSeqStoreChunk, midIdx, secondHalfSeqStore
	fullSeqStoreChunk = zc + 3768
	firstHalfSeqStore = zc + 3768 + 80
	secondHalfSeqStore = zc + 3768 + 160
	midIdx = (startIdx + endIdx) / uint64(2)
	if endIdx-startIdx < uint64(MIN_SEQUENCES_BLOCK_SPLITTING) || (*seqStoreSplits)(unsafe.Pointer(splits)).Fidx >= uint64(ZSTD_MAX_NB_BLOCK_SPLITS) {
		return
	}
	ZSTD_deriveSeqStoreChunk(tls, fullSeqStoreChunk, origSeqStore, startIdx, endIdx)
	ZSTD_deriveSeqStoreChunk(tls, firstHalfSeqStore, origSeqStore, startIdx, midIdx)
	ZSTD_deriveSeqStoreChunk(tls, secondHalfSeqStore, origSeqStore, midIdx, endIdx)
	estimatedOriginalSize = ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls, fullSeqStoreChunk, zc)
	estimatedFirstHalfSize = ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls, firstHalfSeqStore, zc)
	estimatedSecondHalfSize = ZSTD_buildEntropyStatisticsAndEstimateSubBlockSize(tls, secondHalfSeqStore, zc)
	if ZSTD_isError(tls, estimatedOriginalSize) != 0 || ZSTD_isError(tls, estimatedFirstHalfSize) != 0 || ZSTD_isError(tls, estimatedSecondHalfSize) != 0 {
		return
	}
	if estimatedFirstHalfSize+estimatedSecondHalfSize < estimatedOriginalSize {
		ZSTD_deriveBlockSplitsHelper(tls, splits, startIdx, midIdx, zc, origSeqStore)
		*(*U32)(unsafe.Pointer((*seqStoreSplits)(unsafe.Pointer(splits)).FsplitLocations + uintptr((*seqStoreSplits)(unsafe.Pointer(splits)).Fidx)*4)) = uint32(midIdx)
		(*seqStoreSplits)(unsafe.Pointer(splits)).Fidx = (*seqStoreSplits)(unsafe.Pointer(splits)).Fidx + 1
		ZSTD_deriveBlockSplitsHelper(tls, splits, midIdx, endIdx, zc, origSeqStore)
	}
}

// C documentation
//
//	/* Base recursive function.
//	 * Populates a table with intra-block partition indices that can improve compression ratio.
//	 *
//	 * @return: number of splits made (which equals the size of the partition table - 1).
//	 */
func ZSTD_deriveBlockSplits(tls *libc.TLS, zc uintptr, partitions uintptr, nbSeq U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* splits at bp+0 */ seqStoreSplits
	(*(*seqStoreSplits)(unsafe.Pointer(bp))).FsplitLocations = partitions
	(*(*seqStoreSplits)(unsafe.Pointer(bp))).Fidx = uint64(0)
	if nbSeq <= uint32(4) {
		/* Refuse to try and split anything with less than 4 sequences */
		return uint64(0)
	}
	ZSTD_deriveBlockSplitsHelper(tls, bp, uint64(0), uint64(nbSeq), zc, zc+976)
	*(*U32)(unsafe.Pointer((*(*seqStoreSplits)(unsafe.Pointer(bp))).FsplitLocations + uintptr((*(*seqStoreSplits)(unsafe.Pointer(bp))).Fidx)*4)) = nbSeq
	return (*(*seqStoreSplits)(unsafe.Pointer(bp))).Fidx
}

// C documentation
//
//	/* ZSTD_compressBlock_splitBlock():
//	 * Attempts to split a given block into multiple blocks to improve compression ratio.
//	 *
//	 * Returns combined size of all blocks (which includes headers), or a ZSTD error code.
//	 */
func ZSTD_compressBlock_splitBlock_internal(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, blockSize size_t, lastBlock U32, nbSeq U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cSize, cSizeChunk, cSizeSingleBlock, err_code, err_code1, i, numSplits, srcBytes, srcBytesTotal size_t
	var currSeqStore, ip, nextSeqStore, op, partitions uintptr
	var lastBlockEntireSrc, lastPartition U32
	var _ /* cRep at bp+12 */ Repcodes_t
	var _ /* dRep at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cSize, cSizeChunk, cSizeSingleBlock, currSeqStore, err_code, err_code1, i, ip, lastBlockEntireSrc, lastPartition, nextSeqStore, numSplits, op, partitions, srcBytes, srcBytesTotal
	cSize = uint64(0)
	ip = src
	op = dst
	i = uint64(0)
	srcBytesTotal = uint64(0)
	partitions = zc + 3768 + 400 /* size == ZSTD_MAX_NB_BLOCK_SPLITS */
	nextSeqStore = zc + 3768 + 320
	currSeqStore = zc + 3768 + 240
	numSplits = ZSTD_deriveBlockSplits(tls, zc, partitions, nbSeq)
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	libc.Xmemcpy(tls, bp+12, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	libc.Xmemset(tls, nextSeqStore, 0, libc.Uint64FromInt64(80))
	if numSplits == uint64(0) {
		cSizeSingleBlock = ZSTD_compressSeqStore_singleBlock(tls, zc, zc+976, bp, bp+12, op, dstCapacity, ip, blockSize, lastBlock, uint32(0))
		err_code = cSizeSingleBlock
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3669, 0)
			}
			return err_code
		}
		return cSizeSingleBlock
	}
	ZSTD_deriveSeqStoreChunk(tls, currSeqStore, zc+976, uint64(0), uint64(*(*U32)(unsafe.Pointer(partitions))))
	i = uint64(0)
	for {
		if !(i <= numSplits) {
			break
		}
		lastPartition = libc.BoolUint32(i == numSplits)
		lastBlockEntireSrc = uint32(0)
		srcBytes = ZSTD_countSeqStoreLiteralsBytes(tls, currSeqStore) + ZSTD_countSeqStoreMatchBytes(tls, currSeqStore)
		srcBytesTotal = srcBytesTotal + srcBytes
		if lastPartition != 0 {
			/* This is the final partition, need to account for possible last literals */
			srcBytes = srcBytes + (blockSize - srcBytesTotal)
			lastBlockEntireSrc = lastBlock
		} else {
			ZSTD_deriveSeqStoreChunk(tls, nextSeqStore, zc+976, uint64(*(*U32)(unsafe.Pointer(partitions + uintptr(i)*4))), uint64(*(*U32)(unsafe.Pointer(partitions + uintptr(i+uint64(1))*4))))
		}
		cSizeChunk = ZSTD_compressSeqStore_singleBlock(tls, zc, currSeqStore, bp, bp+12, op, dstCapacity, ip, srcBytes, lastBlockEntireSrc, uint32(1))
		err_code1 = cSizeChunk
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3729, 0)
			}
			return err_code1
		}
		ip = ip + uintptr(srcBytes)
		op = op + uintptr(cSizeChunk)
		dstCapacity = dstCapacity - cSizeChunk
		cSize = cSize + cSizeChunk
		*(*SeqStore_t)(unsafe.Pointer(currSeqStore)) = *(*SeqStore_t)(unsafe.Pointer(nextSeqStore))
		goto _1
	_1:
		;
		i = i + 1
	}
	/* cRep and dRep may have diverged during the compression.
	 * If so, we use the dRep repcodes for the next block.
	 */
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616, bp, libc.Uint64FromInt64(12))
	return cSize
}

func ZSTD_compressBlock_splitBlock(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32) (r size_t) {
	var bss, cSize, err_code, err_code1, err_code2 size_t
	var nbSeq U32
	_, _, _, _, _, _ = bss, cSize, err_code, err_code1, err_code2, nbSeq
	bss = ZSTD_buildSeqStore(tls, zc, src, srcSize)
	err_code = bss
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3755, 0)
		}
		return err_code
	}
	if bss == uint64(ZSTDbss_noCompress) {
		if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
			(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3781, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_sequenceProducer_failed))
		}
		cSize = ZSTD_noCompressBlock(tls, dst, dstCapacity, src, srcSize, lastBlock)
		err_code1 = cSize
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1741, 0)
			}
			return err_code1
		}
		return cSize
	}
	nbSeq = libc.Uint32FromInt64((int64((*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.Fsequences) - int64((*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqStore.FsequencesStart)) / 8)
	cSize = ZSTD_compressBlock_splitBlock_internal(tls, zc, dst, dstCapacity, src, srcSize, lastBlock, nbSeq)
	err_code2 = cSize
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3802, 0)
		}
		return err_code2
	}
	return cSize
}

func ZSTD_compressBlock_internal(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, frame U32) (r size_t) {
	var bss, cSize, err_code, err_code1 size_t
	var ip, op uintptr
	var rleMaxLength U32
	_, _, _, _, _, _, _ = bss, cSize, err_code, err_code1, ip, op, rleMaxLength
	/* This is an estimated upper bound for the length of an rle block.
	 * This isn't the actual upper bound.
	 * Finding the real threshold needs further investigation.
	 */
	rleMaxLength = uint32(25)
	ip = src
	op = dst
	bss = ZSTD_buildSeqStore(tls, zc, src, srcSize)
	err_code = bss
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3755, 0)
		}
		return err_code
	}
	if bss == uint64(ZSTDbss_noCompress) {
		if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3781, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_sequenceProducer_failed))
		}
		cSize = uint64(0)
		goto out
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(zc)).FseqCollector.FcollectSequences != 0 {
		err_code1 = ZSTD_copyBlockSequences(tls, zc+936, ZSTD_getSeqStore(tls, zc), (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock+5616)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3593, 0)
			}
			return err_code1
		}
		ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
		return uint64(0)
	}
	/* encode sequences and literals */
	cSize = ZSTD_entropyCompressSeqStore(tls, zc+976, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FnextCBlock, zc+240, dst, dstCapacity, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).Fbmi2)
	if frame != 0 && !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock != 0) && cSize < uint64(rleMaxLength) && ZSTD_isRLE(tls, ip, srcSize) != 0 {
		cSize = uint64(1)
		*(*BYTE)(unsafe.Pointer(op)) = *(*BYTE)(unsafe.Pointer(ip))
	}
	goto out
out:
	;
	if !(ZSTD_isError(tls, cSize) != 0) && cSize > uint64(1) {
		ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
	}
	/* We check that dictionaries have offset codes available for the first
	 * block. After the first block, the offcode table might not have large
	 * enough codes to represent the offsets in the data.
	 */
	if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
	}
	return cSize
}

func ZSTD_compressBlock_targetCBlockSize_body(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, bss size_t, lastBlock U32) (r size_t) {
	var cSize, err_code, maxCSize size_t
	_, _, _ = cSize, err_code, maxCSize
	if bss == uint64(ZSTDbss_compress) {
		if !((*ZSTD_CCtx)(unsafe.Pointer(zc)).FisFirstBlock != 0) && ZSTD_maybeRLE(tls, zc+976) != 0 && ZSTD_isRLE(tls, src, srcSize) != 0 {
			return ZSTD_rleCompressBlock(tls, dst, dstCapacity, *(*BYTE)(unsafe.Pointer(src)), srcSize, lastBlock)
		}
		/* Attempt superblock compression.
		 *
		 * Note that compressed size of ZSTD_compressSuperBlock() is not bound by the
		 * standard ZSTD_compressBound(). This is a problem, because even if we have
		 * space now, taking an extra byte now could cause us to run out of space later
		 * and violate ZSTD_compressBound().
		 *
		 * Define blockBound(blockSize) = blockSize + ZSTD_blockHeaderSize.
		 *
		 * In order to respect ZSTD_compressBound() we must attempt to emit a raw
		 * uncompressed block in these cases:
		 *   * cSize == 0: Return code for an uncompressed block.
		 *   * cSize == dstSize_tooSmall: We may have expanded beyond blockBound(srcSize).
		 *     ZSTD_noCompressBlock() will return dstSize_tooSmall if we are really out of
		 *     output space.
		 *   * cSize >= blockBound(srcSize): We have expanded the block too much so
		 *     emit an uncompressed block.
		 */
		cSize = ZSTD_compressSuperBlock(tls, zc, dst, dstCapacity, src, srcSize, lastBlock)
		if cSize != libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall)) {
			maxCSize = srcSize - ZSTD_minGain(tls, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(zc)).FappliedParams.FcParams.Fstrategy)
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3827, 0)
				}
				return err_code
			}
			if cSize != uint64(0) && cSize < maxCSize+ZSTD_blockHeaderSize {
				ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, zc+3224)
				return cSize
			}
		}
	} /* if (bss == ZSTDbss_compress)*/
	/* Superblock compression failed, attempt to emit a single no compress block.
	 * The decoder will be able to stream this block since it is uncompressed.
	 */
	return ZSTD_noCompressBlock(tls, dst, dstCapacity, src, srcSize, lastBlock)
}

func ZSTD_compressBlock_targetCBlockSize(tls *libc.TLS, zc uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastBlock U32) (r size_t) {
	var bss, cSize, err_code, err_code1 size_t
	_, _, _, _ = bss, cSize, err_code, err_code1
	cSize = uint64(0)
	bss = ZSTD_buildSeqStore(tls, zc, src, srcSize)
	err_code = bss
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3755, 0)
		}
		return err_code
	}
	cSize = ZSTD_compressBlock_targetCBlockSize_body(tls, zc, dst, dstCapacity, src, srcSize, bss, lastBlock)
	err_code1 = cSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+3858, 0)
		}
		return err_code1
	}
	if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(zc)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
	}
	return cSize
}

func ZSTD_overflowCorrectIfNeeded(tls *libc.TLS, ms uintptr, ws uintptr, params uintptr, ip uintptr, iend uintptr) {
	var correction, cycleLog, maxDist U32
	_, _, _ = correction, cycleLog, maxDist
	cycleLog = ZSTD_cycleLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy)
	maxDist = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
	if ZSTD_window_needOverflowCorrection(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow, cycleLog, maxDist, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd, ip, iend) != 0 {
		correction = ZSTD_window_correctOverflow(tls, ms, cycleLog, maxDist, ip)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		ZSTD_cwksp_mark_tables_dirty(tls, ws)
		ZSTD_reduceIndex(tls, ms, params, correction)
		ZSTD_cwksp_mark_tables_clean(tls, ws)
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate < correction {
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = uint32(0)
		} else {
			*(*U32)(unsafe.Pointer(ms + 44)) -= correction
		}
		/* invalidate dictionaries on overflow correction */
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd = uint32(0)
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState = libc.UintptrFromInt32(0)
	}
}

/**** skipping file: zstd_preSplit.h ****/

func ZSTD_optimalBlockSize(tls *libc.TLS, cctx uintptr, src uintptr, srcSize size_t, blockSizeMax size_t, splitLevel int32, strat ZSTD_strategy, savings S64) (r size_t) {
	var v1 uint64
	_ = v1
	/* note: conservatively only split full blocks (128 KB) currently.
	 * While it's possible to go lower, let's keep it simple for a first implementation.
	 * Besides, benefits of splitting are reduced when blocks are already small.
	 */
	if srcSize < libc.Uint64FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) || blockSizeMax < libc.Uint64FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) {
		if srcSize < blockSizeMax {
			v1 = srcSize
		} else {
			v1 = blockSizeMax
		}
		return v1
	}
	/* do not split incompressible data though:
	 * require verified savings to allow pre-splitting.
	 * Note: as a consequence, the first full block is not split.
	 */
	if savings < int64(3) {
		return libc.Uint64FromInt32(libc.Int32FromInt32(128) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	/* apply @splitLevel, or use default value (which depends on @strat).
	 * note that splitting heuristic is still conditioned by @savings >= 3,
	 * so the first block will not reach this code path */
	if splitLevel == int32(1) {
		return libc.Uint64FromInt32(libc.Int32FromInt32(128) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	if splitLevel == 0 {
		splitLevel = splitLevels[strat]
	} else {
		splitLevel = splitLevel - int32(2)
	}
	return ZSTD_splitBlock(tls, src, blockSizeMax, splitLevel, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize)
}

/* split level based on compression strategy, from `fast` to `btultra2` */
var splitLevels = [10]int32{
	2: int32(1),
	3: int32(2),
	4: int32(2),
	5: int32(3),
	6: int32(3),
	7: int32(4),
	8: int32(4),
	9: int32(4),
}

// C documentation
//
//	/*! ZSTD_compress_frameChunk() :
//	*   Compress a chunk of data into one or multiple blocks.
//	*   All blocks will be terminated, all input will be consumed.
//	*   Function will issue an error if there is not enough `dstCapacity` to hold the compressed content.
//	*   Frame is supposed already started (header already produced)
//	*  @return : compressed size, or an error code
//	*/
func ZSTD_compress_frameChunk(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, lastFrameChunk U32) (r size_t) {
	var blockSize, blockSizeMax, cSize, err_code, err_code1, err_code2, err_code3, remaining size_t
	var cBlockHeader, lastBlock, maxDist U32
	var ip, ms, op, ostart uintptr
	var savings S64
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, blockSizeMax, cBlockHeader, cSize, err_code, err_code1, err_code2, err_code3, ip, lastBlock, maxDist, ms, op, ostart, remaining, savings, v1
	blockSizeMax = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax
	remaining = srcSize
	ip = src
	ostart = dst
	op = ostart
	maxDist = libc.Uint32FromInt32(1) << (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FwindowLog
	savings = libc.Int64FromUint64((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize) - libc.Int64FromUint64((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize)
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 && srcSize != 0 {
		XXH_INLINE_XXH64_update(tls, cctx+808, src, srcSize)
	}
	for remaining != 0 {
		ms = cctx + 3224 + 16
		blockSize = ZSTD_optimalBlockSize(tls, cctx, ip, remaining, blockSizeMax, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FpreBlockSplitter_level, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.Fstrategy, savings)
		lastBlock = lastFrameChunk & libc.BoolUint32(blockSize == remaining)
		/* TODO: See 3090. We reduced MIN_CBLOCK_SIZE from 3 to 2 so to compensate we are adding
		 * additional 1. We need to revisit and change this logic to be more consistent */
		if dstCapacity < ZSTD_blockHeaderSize+libc.Uint64FromInt32(libc.Int32FromInt32(1)+libc.Int32FromInt32(1))+uint64(1) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+3906, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		ZSTD_overflowCorrectIfNeeded(tls, ms, cctx+704, cctx+240, ip, ip+uintptr(blockSize))
		ZSTD_checkDictValidity(tls, ms, ip+uintptr(blockSize), maxDist, ms+40, ms+248)
		ZSTD_window_enforceMaxDist(tls, ms, ip, maxDist, ms+40, ms+248)
		/* Ensure hash/chain table insertion resumes no sooner than lowlimit */
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate < (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit {
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
		}
		if ZSTD_useTargetCBlockSize(tls, cctx+240) != 0 {
			cSize = ZSTD_compressBlock_targetCBlockSize(tls, cctx, op, dstCapacity, ip, blockSize, lastBlock)
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3949, 0)
				}
				return err_code
			}
		} else {
			if ZSTD_blockSplitterEnabled(tls, cctx+240) != 0 {
				cSize = ZSTD_compressBlock_splitBlock(tls, cctx, op, dstCapacity, ip, blockSize, lastBlock)
				err_code1 = cSize
				if ERR_isError(tls, err_code1) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+3992, 0)
					}
					return err_code1
				}
			} else {
				cSize = ZSTD_compressBlock_internal(tls, cctx, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, ip, blockSize, uint32(1))
				err_code2 = cSize
				if ERR_isError(tls, err_code2) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+4029, 0)
					}
					return err_code2
				}
				if cSize == uint64(0) { /* block is not compressible */
					cSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, blockSize, lastBlock)
					err_code3 = cSize
					if ERR_isError(tls, err_code3) != 0 {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+1741, 0)
						}
						return err_code3
					}
				} else {
					if cSize == uint64(1) {
						v1 = lastBlock + uint32(bt_rle)<<libc.Int32FromInt32(1) + uint32(blockSize<<libc.Int32FromInt32(3))
					} else {
						v1 = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(cSize<<libc.Int32FromInt32(3))
					}
					cBlockHeader = v1
					MEM_writeLE24(tls, op, cBlockHeader)
					cSize = cSize + ZSTD_blockHeaderSize
				}
			}
		} /* if (ZSTD_useTargetCBlockSize(&cctx->appliedParams))*/
		/* @savings is employed to ensure that splitting doesn't worsen expansion of incompressible data.
		 * Without splitting, the maximum expansion is 3 bytes per full block.
		 * An adversarial input could attempt to fudge the split detector,
		 * and make it split incompressible data, resulting in more block headers.
		 * Note that, since ZSTD_COMPRESSBOUND() assumes a worst case scenario of 1KB per block,
		 * and the splitter never creates blocks that small (current lower limit is 8 KB),
		 * there is already no risk to expand beyond ZSTD_COMPRESSBOUND() limit.
		 * But if the goal is to not expand by more than 3-bytes per 128 KB full block,
		 * then yes, it becomes possible to make the block splitter oversplit incompressible data.
		 * Using @savings, we enforce an even more conservative condition,
		 * requiring the presence of enough savings (at least 3 bytes) to authorize splitting,
		 * otherwise only full blocks are used.
		 * But being conservative is fine,
		 * since splitting barely compressible blocks is not fruitful anyway */
		savings = savings + (libc.Int64FromUint64(blockSize) - libc.Int64FromUint64(cSize))
		ip = ip + uintptr(blockSize)
		remaining = remaining - blockSize
		op = op + uintptr(cSize)
		dstCapacity = dstCapacity - cSize
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock = 0
	}
	if lastFrameChunk != 0 && op > ostart {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_ending)
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_writeFrameHeader(tls *libc.TLS, dst uintptr, dstCapacity size_t, params uintptr, pledgedSrcSize U64, dictID U32) (r size_t) {
	var checksumFlag, dictIDSizeCode, dictIDSizeCodeLength, fcsCode, singleSegment, windowSize U32
	var frameHeaderDescriptionByte, windowLogByte BYTE
	var op uintptr
	var pos, v3 size_t
	var v1 uint32
	var v2 int32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = checksumFlag, dictIDSizeCode, dictIDSizeCodeLength, fcsCode, frameHeaderDescriptionByte, op, pos, singleSegment, windowLogByte, windowSize, v1, v2, v3
	op = dst
	dictIDSizeCodeLength = libc.Uint32FromInt32(libc.BoolInt32(dictID > uint32(0)) + libc.BoolInt32(dictID >= uint32(256)) + libc.BoolInt32(dictID >= uint32(65536)))
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FnoDictIDFlag != 0 {
		v1 = uint32(0)
	} else {
		v1 = dictIDSizeCodeLength
	} /* 0-3 */
	dictIDSizeCode = v1 /* 0-3 */
	checksumFlag = libc.BoolUint32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FchecksumFlag > 0)
	windowSize = libc.Uint32FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
	singleSegment = libc.BoolUint32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FcontentSizeFlag != 0 && uint64(windowSize) >= pledgedSrcSize)
	windowLogByte = uint8(((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog - libc.Uint32FromInt32(ZSTD_WINDOWLOG_ABSOLUTEMIN)) << libc.Int32FromInt32(3))
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FcontentSizeFlag != 0 {
		v2 = libc.BoolInt32(pledgedSrcSize >= uint64(256)) + libc.BoolInt32(pledgedSrcSize >= libc.Uint64FromInt32(libc.Int32FromInt32(65536)+libc.Int32FromInt32(256))) + libc.BoolInt32(pledgedSrcSize >= uint64(0xFFFFFFFF))
	} else {
		v2 = 0
	}
	fcsCode = libc.Uint32FromInt32(v2) /* 0-3 */
	frameHeaderDescriptionByte = uint8(dictIDSizeCode + checksumFlag<<libc.Int32FromInt32(2) + singleSegment<<libc.Int32FromInt32(5) + fcsCode<<libc.Int32FromInt32(6))
	pos = uint64(0)
	if dstCapacity < uint64(ZSTD_FRAMEHEADERSIZE_MAX) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4064, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).Fformat == int32(ZSTD_f_zstd1) {
		MEM_writeLE32(tls, dst, uint32(ZSTD_MAGICNUMBER))
		pos = uint64(4)
	}
	v3 = pos
	pos = pos + 1
	*(*BYTE)(unsafe.Pointer(op + uintptr(v3))) = frameHeaderDescriptionByte
	if !(singleSegment != 0) {
		v3 = pos
		pos = pos + 1
		*(*BYTE)(unsafe.Pointer(op + uintptr(v3))) = windowLogByte
	}
	switch dictIDSizeCode {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
	case uint32(1):
		*(*BYTE)(unsafe.Pointer(op + uintptr(pos))) = uint8(dictID)
		pos = pos + 1
	case uint32(2):
		MEM_writeLE16(tls, op+uintptr(pos), uint16(dictID))
		pos = pos + uint64(2)
	case uint32(3):
		MEM_writeLE32(tls, op+uintptr(pos), dictID)
		pos = pos + uint64(4)
		break
	}
	switch fcsCode {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
		if singleSegment != 0 {
			v3 = pos
			pos = pos + 1
			*(*BYTE)(unsafe.Pointer(op + uintptr(v3))) = uint8(pledgedSrcSize)
		}
	case uint32(1):
		MEM_writeLE16(tls, op+uintptr(pos), uint16(pledgedSrcSize-libc.Uint64FromInt32(256)))
		pos = pos + uint64(2)
	case uint32(2):
		MEM_writeLE32(tls, op+uintptr(pos), uint32(pledgedSrcSize))
		pos = pos + uint64(4)
	case uint32(3):
		MEM_writeLE64(tls, op+uintptr(pos), pledgedSrcSize)
		pos = pos + uint64(8)
		break
	}
	return pos
}

// C documentation
//
//	/* ZSTD_writeSkippableFrame_advanced() :
//	 * Writes out a skippable frame with the specified magic number variant (16 are supported),
//	 * from ZSTD_MAGIC_SKIPPABLE_START to ZSTD_MAGIC_SKIPPABLE_START+15, and the desired source data.
//	 *
//	 * Returns the total number of bytes written, or a ZSTD error code.
//	 */
func ZSTD_writeSkippableFrame(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, magicVariant uint32) (r size_t) {
	var op uintptr
	_ = op
	op = dst
	if dstCapacity < srcSize+uint64(ZSTD_SKIPPABLEHEADERSIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4122, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if srcSize > uint64(libc.Uint32FromUint32(0xFFFFFFFF)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4158, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if magicVariant > uint32(15) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4197, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	MEM_writeLE32(tls, op, libc.Uint32FromInt32(ZSTD_MAGIC_SKIPPABLE_START)+magicVariant)
	MEM_writeLE32(tls, op+uintptr(4), uint32(srcSize))
	libc.Xmemcpy(tls, op+libc.UintptrFromInt32(8), src, srcSize)
	return srcSize + uint64(ZSTD_SKIPPABLEHEADERSIZE)
}

// C documentation
//
//	/* ZSTD_writeLastEmptyBlock() :
//	 * output an empty Block with end-of-frame mark to complete a frame
//	 * @return : size of data written into `dst` (== ZSTD_blockHeaderSize (defined in zstd_internal.h))
//	 *           or an error code if `dstCapacity` is too small (<ZSTD_blockHeaderSize)
//	 */
func ZSTD_writeLastEmptyBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t) (r size_t) {
	var cBlockHeader24 U32
	_ = cBlockHeader24
	if dstCapacity < ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4248, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1) /* 0 size */
	MEM_writeLE24(tls, dst, cBlockHeader24)
	return ZSTD_blockHeaderSize
	return r
}

func ZSTD_referenceExternalSequences(tls *libc.TLS, cctx uintptr, seq uintptr, nbSeq size_t) {
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fseq = seq
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fsize = nbSeq
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fcapacity = nbSeq
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.Fpos = uint64(0)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexternSeqStore.FposInSequence = uint64(0)
}

func ZSTD_compressContinue_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, frame U32, lastFrameChunk U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cSize, err_code, err_code1, fhSize size_t
	var ms, v2 uintptr
	var v1 uint64
	_, _, _, _, _, _, _ = cSize, err_code, err_code1, fhSize, ms, v1, v2
	ms = cctx + 3224 + 16
	fhSize = uint64(0)
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_created) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4305, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	if frame != 0 && (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_init) {
		fhSize = ZSTD_writeFrameHeader(tls, dst, dstCapacity, cctx+240, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID)
		err_code = fhSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4339, 0)
			}
			return err_code
		}
		dstCapacity = dstCapacity - fhSize
		dst = dst + uintptr(fhSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_ongoing)
	}
	if !(srcSize != 0) {
		return fhSize
	} /* do not generate an empty block if no input */
	if !(ZSTD_window_update(tls, ms, src, srcSize, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FforceNonContiguous) != 0) {
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FforceNonContiguous = 0
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		ZSTD_window_update(tls, cctx+1056, src, srcSize, 0)
	}
	if !(frame != 0) {
		/* overflow check and correction for block mode */
		ZSTD_overflowCorrectIfNeeded(tls, ms, cctx+704, cctx+240, src, src+uintptr(srcSize))
	}
	if frame != 0 {
		v1 = ZSTD_compress_frameChunk(tls, cctx, dst, dstCapacity, src, srcSize, lastFrameChunk)
	} else {
		v1 = ZSTD_compressBlock_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(0))
	}
	cSize = v1
	err_code1 = cSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			if frame != 0 {
				v2 = __ccgo_ts + 4368
			} else {
				v2 = __ccgo_ts + 4029
			}
			_force_has_format_string(tls, __ccgo_ts+4400, libc.VaList(bp+8, v2))
		}
		return err_code1
	}
	*(*uint64)(unsafe.Pointer(cctx + 792)) += srcSize
	*(*uint64)(unsafe.Pointer(cctx + 800)) += cSize + fhSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne != uint64(0) { /* control src size */
		_ = libc.Uint64FromInt64(1)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize+uint64(1) > (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4403, libc.VaList(bp+8, uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne)-uint32(1), uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize)))
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
		}
	}
	return cSize + fhSize
	return r
}

func ZSTD_compressContinue_public(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressContinue_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(1), uint32(0))
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressContinue_public() */
func ZSTD_compressContinue(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressContinue_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

func ZSTD_getBlockSize_deprecated(tls *libc.TLS, cctx uintptr) (r size_t) {
	var cParams ZSTD_compressionParameters
	var v1 uint64
	_, _ = cParams, v1
	cParams = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FmaxBlockSize < libc.Uint64FromInt32(1)<<cParams.FwindowLog {
		v1 = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FmaxBlockSize
	} else {
		v1 = libc.Uint64FromInt32(1) << cParams.FwindowLog
	}
	return v1
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_getBlockSize_deprecated() */
func ZSTD_getBlockSize(tls *libc.TLS, cctx uintptr) (r size_t) {
	return ZSTD_getBlockSize_deprecated(tls, cctx)
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressBlock_deprecated() */
func ZSTD_compressBlock_deprecated(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var blockSizeMax size_t
	_ = blockSizeMax
	blockSizeMax = ZSTD_getBlockSize_deprecated(tls, cctx)
	if srcSize > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4456, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	return ZSTD_compressContinue_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(0), uint32(0))
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressBlock_deprecated() */
func ZSTD_compressBlock(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_deprecated(tls, cctx, dst, dstCapacity, src, srcSize)
}

// C documentation
//
//	/*! ZSTD_loadDictionaryContent() :
//	 *  @return : 0, or an error code
//	 */
func ZSTD_loadDictionaryContent(tls *libc.TLS, ms uintptr, ls uintptr, ws uintptr, params uintptr, src uintptr, srcSize size_t, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e) (r size_t) {
	var CDictTaggedIndices, loadLdmDict int32
	var iend, ip uintptr
	var maxDictSize, maxDictSize1, shortCacheMaxDictSize U32
	var tagTableSize size_t
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _ = CDictTaggedIndices, iend, ip, loadLdmDict, maxDictSize, maxDictSize1, shortCacheMaxDictSize, tagTableSize, v1, v2, v3
	ip = src
	iend = ip + uintptr(srcSize)
	loadLdmDict = libc.BoolInt32((*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) && ls != libc.UintptrFromInt32(0))
	/* Assert that the ms params match the params we're being given */
	ZSTD_assertEqualCParams(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams)
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} /* Ensure large dictionaries can't cause index overflow */
	/* Allow the dictionary to set indices up to exactly ZSTD_CURRENT_MAX.
	 * Dictionaries right at the edge will immediately trigger overflow
	 * correction, but I don't want to insert extra constraints here.
	 */
	maxDictSize = v1 - uint32(ZSTD_WINDOW_START_INDEX)
	CDictTaggedIndices = ZSTD_CDictIndicesAreTagged(tls, params+4)
	if CDictTaggedIndices != 0 && tfp == int32(ZSTD_tfp_forCDict) {
		/* Some dictionary matchfinders in zstd use "short cache",
		 * which treats the lower ZSTD_SHORT_CACHE_TAG_BITS of each
		 * CDict hashtable entry as a tag rather than as part of an index.
		 * When short cache is used, we need to truncate the dictionary
		 * so that its indices don't overlap with the tag. */
		shortCacheMaxDictSize = libc.Uint32FromUint32(1)<<(libc.Int32FromInt32(32)-libc.Int32FromInt32(ZSTD_SHORT_CACHE_TAG_BITS)) - libc.Uint32FromInt32(ZSTD_WINDOW_START_INDEX)
		if maxDictSize < shortCacheMaxDictSize {
			v2 = maxDictSize
		} else {
			v2 = shortCacheMaxDictSize
		}
		maxDictSize = v2
	}
	/* If the dictionary is too large, only load the suffix of the dictionary. */
	if srcSize > uint64(maxDictSize) {
		ip = iend - uintptr(maxDictSize)
		src = ip
		srcSize = uint64(maxDictSize)
	}
	if MEM_64bits(tls) != 0 {
		v1 = libc.Uint32FromUint32(3500) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	} else {
		v1 = libc.Uint32FromUint32(2000) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20))
	}
	if srcSize > uint64(libc.Uint32FromInt32(-libc.Int32FromInt32(1))-v1) {
		/* We must have cleared our windows when our source is this large. */
		if loadLdmDict != 0 {
		}
	}
	ZSTD_window_update(tls, ms, src, srcSize, 0)
	if loadLdmDict != 0 { /* Load the entire dict into LDM matchfinders. */
		ZSTD_window_update(tls, ls, src, srcSize, 0)
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FforceWindow != 0 {
			v1 = uint32(0)
		} else {
			v1 = libc.Uint32FromInt64(int64(iend) - int64((*ldmState_t)(unsafe.Pointer(ls)).Fwindow.Fbase))
		}
		(*ldmState_t)(unsafe.Pointer(ls)).FloadedDictEnd = v1
		ZSTD_ldm_fillHashTable(tls, ls, ip, iend, params+96)
	}
	/* If the dict is larger than we can reasonably index in our tables, only load the suffix. */
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog+uint32(3) > (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog+uint32(1) {
		v2 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog + uint32(3)
	} else {
		v2 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog + uint32(1)
	}
	if v2 < libc.Uint32FromInt32(libc.Int32FromInt32(31)) {
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog+uint32(3) > (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog+uint32(1) {
			v3 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog + uint32(3)
		} else {
			v3 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog + uint32(1)
		}
		v1 = v3
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(31))
	}
	maxDictSize1 = uint32(1) << v1
	if srcSize > uint64(maxDictSize1) {
		ip = iend - uintptr(maxDictSize1)
		src = ip
		srcSize = uint64(maxDictSize1)
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = libc.Uint32FromInt64(int64(ip) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FforceWindow != 0 {
		v1 = uint32(0)
	} else {
		v1 = libc.Uint32FromInt64(int64(iend) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd = v1
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FforceNonContiguous = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FdeterministicRefPrefix
	if srcSize <= uint64(HASH_READ_SIZE) {
		return uint64(0)
	}
	ZSTD_overflowCorrectIfNeeded(tls, ms, ws, params, ip, iend)
	switch (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy {
	case int32(ZSTD_fast):
		ZSTD_fillHashTable(tls, ms, iend, dtlm, tfp)
	case int32(ZSTD_dfast):
		ZSTD_fillDoubleHashTable(tls, ms, iend, dtlm, tfp)
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdedicatedDictSearch != 0 {
			ZSTD_dedicatedDictSearch_lazy_loadDictionary(tls, ms, iend-uintptr(HASH_READ_SIZE))
		} else {
			if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FuseRowMatchFinder == int32(ZSTD_ps_enable) {
				tagTableSize = libc.Uint64FromInt32(1) << (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FhashLog
				libc.Xmemset(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable, 0, tagTableSize)
				ZSTD_row_update(tls, ms, iend-uintptr(HASH_READ_SIZE))
			} else {
				ZSTD_insertAndFindFirstIndex(tls, ms, iend-uintptr(HASH_READ_SIZE))
			}
		}
	case int32(ZSTD_btlazy2): /* we want the dictionary table fully sorted */
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
		ZSTD_updateTree(tls, ms, iend-uintptr(HASH_READ_SIZE), iend)
	default:
		/* not possible : not a valid strategy id */
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = libc.Uint32FromInt64(int64(iend) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	return uint64(0)
}

// C documentation
//
//	/* Dictionaries that assign zero probability to symbols that show up causes problems
//	 * when FSE encoding. Mark dictionaries with zero probability symbols as FSE_repeat_check
//	 * and only dictionaries with 100% valid symbols can be assumed valid.
//	 */
func ZSTD_dictNCountRepeat(tls *libc.TLS, normalizedCounter uintptr, dictMaxSymbolValue uint32, maxSymbolValue uint32) (r FSE_repeat) {
	var s U32
	_ = s
	if dictMaxSymbolValue < maxSymbolValue {
		return int32(FSE_repeat_check)
	}
	s = uint32(0)
	for {
		if !(s <= maxSymbolValue) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) == 0 {
			return int32(FSE_repeat_check)
		}
		goto _1
	_1:
		;
		s = s + 1
	}
	return int32(FSE_repeat_valid)
}

func ZSTD_loadCEntropy(tls *libc.TLS, bs uintptr, workspace uintptr, dict uintptr, dictSize size_t) (r size_t) {
	bp := tls.Alloc(288)
	defer tls.Free(288)
	var dictContentSize, hufHeaderSize, litlengthHeaderSize, matchlengthHeaderSize, offcodeHeaderSize size_t
	var dictEnd, dictPtr uintptr
	var maxOffset, offcodeMax, u U32
	var v1 uint32
	var _ /* hasZeroWeights at bp+72 */ uint32
	var _ /* litlengthLog at bp+272 */ uint32
	var _ /* litlengthMaxValue at bp+268 */ uint32
	var _ /* litlengthNCount at bp+196 */ [36]int16
	var _ /* matchlengthLog at bp+192 */ uint32
	var _ /* matchlengthMaxValue at bp+188 */ uint32
	var _ /* matchlengthNCount at bp+80 */ [53]int16
	var _ /* maxSymbolValue at bp+68 */ uint32
	var _ /* offcodeLog at bp+76 */ uint32
	var _ /* offcodeMaxValue at bp+64 */ uint32
	var _ /* offcodeNCount at bp+0 */ [32]int16
	_, _, _, _, _, _, _, _, _, _, _ = dictContentSize, dictEnd, dictPtr, hufHeaderSize, litlengthHeaderSize, matchlengthHeaderSize, maxOffset, offcodeHeaderSize, offcodeMax, u, v1
	*(*uint32)(unsafe.Pointer(bp + 64)) = uint32(MaxOff)
	dictPtr = dict /* skip magic num and dict ID */
	dictEnd = dictPtr + uintptr(dictSize)
	dictPtr = dictPtr + uintptr(8)
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Fhuf.FrepeatMode = int32(HUF_repeat_check)
	*(*uint32)(unsafe.Pointer(bp + 68)) = uint32(255)
	*(*uint32)(unsafe.Pointer(bp + 72)) = uint32(1)
	hufHeaderSize = HUF_readCTable(tls, bs, bp+68, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)), bp+72)
	/* We only set the loaded table as valid if it contains all non-zero
	 * weights. Otherwise, we set it to check */
	if !(*(*uint32)(unsafe.Pointer(bp + 72)) != 0) && *(*uint32)(unsafe.Pointer(bp + 68)) == uint32(255) {
		(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Fhuf.FrepeatMode = int32(HUF_repeat_valid)
	}
	if ERR_isError(tls, hufHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	dictPtr = dictPtr + uintptr(hufHeaderSize)
	offcodeHeaderSize = FSE_readNCount(tls, bp, bp+64, bp+76, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, offcodeHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 76)) > uint32(OffFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	/* fill all offset symbols to avoid garbage at end of table */
	if ERR_isError(tls, FSE_buildCTable_wksp(tls, bs+2064, bp, uint32(MaxOff), *(*uint32)(unsafe.Pointer(bp + 76)), workspace, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	/* Defer checking offcodeMaxValue because we need to know the size of the dictionary content */
	dictPtr = dictPtr + uintptr(offcodeHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 188)) = uint32(MaxML)
	matchlengthHeaderSize = FSE_readNCount(tls, bp+80, bp+188, bp+192, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, matchlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 192)) > uint32(MLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if ERR_isError(tls, FSE_buildCTable_wksp(tls, bs+2064+772, bp+80, *(*uint32)(unsafe.Pointer(bp + 188)), *(*uint32)(unsafe.Pointer(bp + 192)), workspace, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Fmatchlength_repeatMode = ZSTD_dictNCountRepeat(tls, bp+80, *(*uint32)(unsafe.Pointer(bp + 188)), uint32(MaxML))
	dictPtr = dictPtr + uintptr(matchlengthHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 268)) = uint32(MaxLL)
	litlengthHeaderSize = FSE_readNCount(tls, bp+196, bp+268, bp+272, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, litlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 272)) > uint32(LLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if ERR_isError(tls, FSE_buildCTable_wksp(tls, bs+2064+2224, bp+196, *(*uint32)(unsafe.Pointer(bp + 268)), *(*uint32)(unsafe.Pointer(bp + 272)), workspace, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Flitlength_repeatMode = ZSTD_dictNCountRepeat(tls, bp+196, *(*uint32)(unsafe.Pointer(bp + 268)), uint32(MaxLL))
	dictPtr = dictPtr + uintptr(litlengthHeaderSize)
	if dictPtr+uintptr(12) > dictEnd {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	*(*U32)(unsafe.Pointer(bs + 5616)) = MEM_readLE32(tls, dictPtr+uintptr(0))
	*(*U32)(unsafe.Pointer(bs + 5616 + 1*4)) = MEM_readLE32(tls, dictPtr+uintptr(4))
	*(*U32)(unsafe.Pointer(bs + 5616 + 2*4)) = MEM_readLE32(tls, dictPtr+uintptr(8))
	dictPtr = dictPtr + uintptr(12)
	dictContentSize = libc.Uint64FromInt64(int64(dictEnd) - int64(dictPtr))
	offcodeMax = uint32(MaxOff)
	if dictContentSize <= uint64(libc.Uint32FromInt32(-libc.Int32FromInt32(1))-libc.Uint32FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) {
		maxOffset = uint32(dictContentSize) + libc.Uint32FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) /* The maximum offset that must be supported */
		offcodeMax = ZSTD_highbit32(tls, maxOffset)                                                                                            /* Calculate minimum offset code required to represent maxOffset */
	}
	/* All offset values <= dictContentSize + 128 KB must be representable for a valid table */
	if offcodeMax < libc.Uint32FromInt32(libc.Int32FromInt32(MaxOff)) {
		v1 = offcodeMax
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(MaxOff))
	}
	(*ZSTD_compressedBlockState_t)(unsafe.Pointer(bs)).Fentropy.Ffse.Foffcode_repeatMode = ZSTD_dictNCountRepeat(tls, bp, *(*uint32)(unsafe.Pointer(bp + 64)), v1)
	/* All repCodes must be <= dictContentSize and != 0 */
	u = uint32(0)
	for {
		if !(u < uint32(3)) {
			break
		}
		if *(*U32)(unsafe.Pointer(bs + 5616 + uintptr(u)*4)) == uint32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
		}
		if uint64(*(*U32)(unsafe.Pointer(bs + 5616 + uintptr(u)*4))) > dictContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
		}
		goto _2
	_2:
		;
		u = u + 1
	}
	return libc.Uint64FromInt64(int64(dictPtr) - int64(dict))
}

// C documentation
//
//	/* Dictionary format :
//	 * See :
//	 * https://github.com/facebook/zstd/blob/release/doc/zstd_compression_format.md#dictionary-format
//	 */
//	/*! ZSTD_loadZstdDictionary() :
//	 * @return : dictID, or an error code
//	 *  assumptions : magic number supposed already checked
//	 *                dictSize supposed >= 8
//	 */
func ZSTD_loadZstdDictionary(tls *libc.TLS, bs uintptr, ms uintptr, ws uintptr, params uintptr, dict uintptr, dictSize size_t, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e, workspace uintptr) (r size_t) {
	var dictContentSize, dictID, eSize, err_code, err_code1 size_t
	var dictEnd, dictPtr uintptr
	var v1 uint32
	_, _, _, _, _, _, _, _ = dictContentSize, dictEnd, dictID, dictPtr, eSize, err_code, err_code1, v1
	dictPtr = dict
	dictEnd = dictPtr + uintptr(dictSize)
	_ = libc.Uint64FromInt64(1)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FfParams.FnoDictIDFlag != 0 {
		v1 = uint32(0)
	} else {
		v1 = MEM_readLE32(tls, dictPtr+uintptr(4))
	}
	dictID = uint64(v1)
	eSize = ZSTD_loadCEntropy(tls, bs, workspace, dict, dictSize)
	err_code = eSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4485, 0)
		}
		return err_code
	}
	dictPtr = dictPtr + uintptr(eSize)
	dictContentSize = libc.Uint64FromInt64(int64(dictEnd) - int64(dictPtr))
	err_code1 = ZSTD_loadDictionaryContent(tls, ms, libc.UintptrFromInt32(0), ws, params, dictPtr, dictContentSize, dtlm, tfp)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return dictID
}

// C documentation
//
//	/** ZSTD_compress_insertDictionary() :
//	*   @return : dictID, or an error code */
func ZSTD_compress_insertDictionary(tls *libc.TLS, bs uintptr, ms uintptr, ls uintptr, ws uintptr, params uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e, workspace uintptr) (r size_t) {
	if dict == libc.UintptrFromInt32(0) || dictSize < uint64(8) {
		if dictContentType == int32(ZSTD_dct_fullDict) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_wrong))
		}
		return uint64(0)
	}
	ZSTD_reset_compressedBlockState(tls, bs)
	/* dict restricted modes */
	if dictContentType == int32(ZSTD_dct_rawContent) {
		return ZSTD_loadDictionaryContent(tls, ms, ls, ws, params, dict, dictSize, dtlm, tfp)
	}
	if MEM_readLE32(tls, dict) != uint32(ZSTD_MAGIC_DICTIONARY) {
		if dictContentType == int32(ZSTD_dct_auto) {
			return ZSTD_loadDictionaryContent(tls, ms, ls, ws, params, dict, dictSize, dtlm, tfp)
		}
		if dictContentType == int32(ZSTD_dct_fullDict) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_wrong))
		}
		/* impossible */
	}
	/* dict as full zstd dictionary */
	return ZSTD_loadZstdDictionary(tls, bs, ms, ws, params, dict, dictSize, dtlm, tfp, workspace)
}

// C documentation
//
//	/*! ZSTD_compressBegin_internal() :
//	 * Assumption : either @dict OR @cdict (or none) is non-NULL, never both
//	 * @return : 0, or an error code */
func ZSTD_compressBegin_internal(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, dtlm ZSTD_dictTableLoadMethod_e, cdict uintptr, params uintptr, pledgedSrcSize U64, zbuff ZSTD_buffered_policy_e) (r size_t) {
	var dictContentSize, dictID, err_code, err_code1 size_t
	var v1 uint64
	_, _, _, _, _ = dictContentSize, dictID, err_code, err_code1, v1
	if cdict != 0 {
		v1 = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize
	} else {
		v1 = dictSize
	}
	dictContentSize = v1
	/* params are supposed to be fully validated at this point */
	/* either dict or cdict, not both */
	if cdict != 0 && (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize > uint64(0) && (pledgedSrcSize < libc.Uint64FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) || pledgedSrcSize < (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize*uint64(6) || pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) || (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel == 0) && (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FattachDictPref != int32(ZSTD_dictForceLoad) {
		return ZSTD_resetCCtx_usingCDict(tls, cctx, cdict, params, pledgedSrcSize, zbuff)
	}
	err_code = ZSTD_resetCCtx_internal(tls, cctx, params, pledgedSrcSize, dictContentSize, int32(ZSTDcrp_makeClean), zbuff)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if cdict != 0 {
		v1 = ZSTD_compress_insertDictionary(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cctx+3224+16, cctx+1056, cctx+704, cctx+240, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentType, dtlm, int32(ZSTD_tfp_forCCtx), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace)
	} else {
		v1 = ZSTD_compress_insertDictionary(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, cctx+3224+16, cctx+1056, cctx+704, cctx+240, dict, dictSize, dictContentType, dtlm, int32(ZSTD_tfp_forCCtx), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace)
	}
	dictID = v1
	err_code1 = dictID
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4510, 0)
		}
		return err_code1
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = uint32(dictID)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = dictContentSize
	return uint64(0)
}

func ZSTD_compressBegin_advanced_internal(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, dtlm ZSTD_dictTableLoadMethod_e, cdict uintptr, params uintptr, pledgedSrcSize uint64) (r size_t) {
	var err_code size_t
	_ = err_code
	/* compression parameters verification and optimization */
	err_code = ZSTD_checkCParams(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_compressBegin_internal(tls, cctx, dict, dictSize, dictContentType, dtlm, cdict, params, pledgedSrcSize, int32(ZSTDb_not_buffered))
}

// C documentation
//
//	/*! ZSTD_compressBegin_advanced() :
//	*   @return : 0, or an error code */
func ZSTD_compressBegin_advanced(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, _params ZSTD_parameters, pledgedSrcSize uint64) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var _ /* cctxParams at bp+40 */ ZSTD_CCtx_params
	ZSTD_CCtxParams_init_internal(tls, bp+40, bp, ZSTD_NO_CLEVEL)
	return ZSTD_compressBegin_advanced_internal(tls, cctx, dict, dictSize, int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), bp+40, pledgedSrcSize)
}

func ZSTD_compressBegin_usingDict_deprecated(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var v1 int32
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	var _ /* params at bp+224 */ ZSTD_parameters
	_ = v1
	*(*ZSTD_parameters)(unsafe.Pointer(bp + 224)) = ZSTD_getParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_noAttachDict))
	if compressionLevel == 0 {
		v1 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v1 = compressionLevel
	}
	ZSTD_CCtxParams_init_internal(tls, bp, bp+224, v1)
	return ZSTD_compressBegin_internal(tls, cctx, dict, dictSize, int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), bp, uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), int32(ZSTDb_not_buffered))
}

func ZSTD_compressBegin_usingDict(tls *libc.TLS, cctx uintptr, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	return ZSTD_compressBegin_usingDict_deprecated(tls, cctx, dict, dictSize, compressionLevel)
}

func ZSTD_compressBegin(tls *libc.TLS, cctx uintptr, compressionLevel int32) (r size_t) {
	return ZSTD_compressBegin_usingDict_deprecated(tls, cctx, libc.UintptrFromInt32(0), uint64(0), compressionLevel)
}

// C documentation
//
//	/*! ZSTD_writeEpilogue() :
//	*   Ends a frame.
//	*   @return : nb of bytes written into dst (or an error code) */
func ZSTD_writeEpilogue(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t) (r size_t) {
	var cBlockHeader24, checksum U32
	var err_code, fhSize size_t
	var op, ostart uintptr
	_, _, _, _, _, _ = cBlockHeader24, checksum, err_code, fhSize, op, ostart
	ostart = dst
	op = ostart
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_created) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4548, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	/* special case : empty frame */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage == int32(ZSTDcs_init) {
		fhSize = ZSTD_writeFrameHeader(tls, dst, dstCapacity, cctx+240, uint64(0), uint32(0))
		err_code = fhSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4339, 0)
			}
			return err_code
		}
		dstCapacity = dstCapacity - fhSize
		op = op + uintptr(fhSize)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_ongoing)
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage != int32(ZSTDcs_ending) {
		/* write one last empty block, make it the "last" block */
		cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1) + libc.Uint32FromInt32(0)
		_ = libc.Uint64FromInt64(1)
		if dstCapacity < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4561, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE24(tls, op, cBlockHeader24)
		op = op + uintptr(ZSTD_blockHeaderSize)
		dstCapacity = dstCapacity - ZSTD_blockHeaderSize
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 {
		checksum = uint32(XXH_INLINE_XXH64_digest(tls, cctx+808))
		if dstCapacity < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4582, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE32(tls, op, checksum)
		op = op + uintptr(4)
	}
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fstage = int32(ZSTDcs_created) /* return to "created but no init" status */
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_CCtx_trace(tls *libc.TLS, cctx uintptr, extraCSize size_t) {
	_ = cctx
	_ = extraCSize
}

func ZSTD_compressEnd_public(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cSize, endResult, err_code, err_code1 size_t
	_, _, _, _ = cSize, endResult, err_code, err_code1
	cSize = ZSTD_compressContinue_internal(tls, cctx, dst, dstCapacity, src, srcSize, uint32(1), uint32(1))
	err_code = cSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4603, 0)
		}
		return err_code
	}
	endResult = ZSTD_writeEpilogue(tls, cctx, dst+uintptr(cSize), dstCapacity-cSize)
	err_code1 = endResult
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4641, 0)
		}
		return err_code1
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne != uint64(0) { /* control src size */
		_ = libc.Uint64FromInt64(1)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne != (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize+uint64(1) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4667, libc.VaList(bp+8, uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne)-uint32(1), uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize)))
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
		}
	}
	ZSTD_CCtx_trace(tls, cctx, endResult)
	return cSize + endResult
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_compressEnd_public() */
func ZSTD_compressEnd(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressEnd_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

func ZSTD_compress_advanced(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t, _params ZSTD_parameters) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var err_code size_t
	_ = err_code
	err_code = ZSTD_checkCParams(tls, (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ZSTD_CCtxParams_init_internal(tls, cctx+464, bp, ZSTD_NO_CLEVEL)
	return ZSTD_compress_advanced_internal(tls, cctx, dst, dstCapacity, src, srcSize, dict, dictSize, cctx+464)
}

// C documentation
//
//	/* Internal */
func ZSTD_compress_advanced_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t, params uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_compressBegin_internal(tls, cctx, dict, dictSize, int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), params, srcSize, int32(ZSTDb_not_buffered))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_compressEnd_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

func ZSTD_compress_usingDict(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var v1 uint64
	var v2 int32
	var _ /* params at bp+0 */ ZSTD_parameters
	_, _ = v1, v2
	if dict != 0 {
		v1 = dictSize
	} else {
		v1 = uint64(0)
	}
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = ZSTD_getParams_internal(tls, compressionLevel, srcSize, v1, int32(ZSTD_cpm_noAttachDict))
	if compressionLevel == 0 {
		v2 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v2 = compressionLevel
	}
	ZSTD_CCtxParams_init_internal(tls, cctx+464, bp, v2)
	return ZSTD_compress_advanced_internal(tls, cctx, dst, dstCapacity, src, srcSize, dict, dictSize, cctx+464)
}

func ZSTD_compressCCtx(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, compressionLevel int32) (r size_t) {
	return ZSTD_compress_usingDict(tls, cctx, dst, dstCapacity, src, srcSize, libc.UintptrFromInt32(0), uint64(0), compressionLevel)
}

func ZSTD_compress(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, compressionLevel int32) (r size_t) {
	bp := tls.Alloc(5280)
	defer tls.Free(5280)
	var result size_t
	var _ /* ctxBody at bp+0 */ ZSTD_CCtx
	_ = result
	ZSTD_initCCtx(tls, bp, ZSTD_defaultCMem)
	result = ZSTD_compressCCtx(tls, bp, dst, dstCapacity, src, srcSize, compressionLevel)
	ZSTD_freeCCtxContent(tls, bp) /* can't free ctxBody itself, as it's on stack; free only heap content */
	return result
}

/* =====  Dictionary API  ===== */

// C documentation
//
//	/*! ZSTD_estimateCDictSize_advanced() :
//	 *  Estimate amount of memory that will be needed to create a dictionary with following arguments */
func ZSTD_estimateCDictSize_advanced(tls *libc.TLS, dictSize size_t, _cParams ZSTD_compressionParameters, dictLoadMethod ZSTD_dictLoadMethod_e) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var v1 uint64
	_ = v1
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
	}
	return ZSTD_cwksp_alloc_size(tls, uint64(6080)) + ZSTD_cwksp_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))) + ZSTD_sizeof_matchState(tls, bp, ZSTD_resolveRowMatchFinderMode(tls, int32(ZSTD_ps_auto), bp), int32(1), uint32(0)) + v1
}

func ZSTD_estimateCDictSize(tls *libc.TLS, dictSize size_t, compressionLevel int32) (r size_t) {
	var cParams ZSTD_compressionParameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	return ZSTD_estimateCDictSize_advanced(tls, dictSize, cParams, int32(ZSTD_dlm_byCopy))
}

func ZSTD_sizeof_CDict(tls *libc.TLS, cdict uintptr) (r size_t) {
	var v1 uint64
	_ = v1
	if cdict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof on NULL */
	/* cdict may be in the workspace */
	if (*ZSTD_CDict)(unsafe.Pointer(cdict)).Fworkspace.Fworkspace == cdict {
		v1 = uint64(0)
	} else {
		v1 = uint64(6080)
	}
	return v1 + ZSTD_cwksp_sizeof(tls, cdict+32)
}

func ZSTD_initCDict_internal(tls *libc.TLS, cdict uintptr, dictBuffer uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, _params ZSTD_CCtx_params) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var dictID, err_code, err_code1 size_t
	var internalBuffer uintptr
	_, _, _, _ = dictID, err_code, err_code1, internalBuffer
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FdedicatedDictSearch = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch
	if dictLoadMethod == int32(ZSTD_dlm_byRef) || !(dictBuffer != 0) || !(dictSize != 0) {
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent = dictBuffer
	} else {
		internalBuffer = ZSTD_cwksp_reserve_object(tls, cdict+32, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
		if !(internalBuffer != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1377, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent = internalBuffer
		libc.Xmemcpy(tls, internalBuffer, dictBuffer, dictSize)
	}
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize = dictSize
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentType = dictContentType
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FentropyWorkspace = ZSTD_cwksp_reserve_object(tls, cdict+32, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))
	/* Reset the state to no dictionary */
	ZSTD_reset_compressedBlockState(tls, cdict+408)
	err_code = ZSTD_reset_matchState(tls, cdict+104, cdict+32, bp+4, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, int32(ZSTDcrp_makeClean), int32(ZSTDirp_reset), int32(ZSTD_resetTarget_CDict))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	/* (Maybe) load the dictionary
	 * Skips loading the dictionary if it is < 8 bytes.
	 */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams.FcontentSizeFlag = int32(1)
	dictID = ZSTD_compress_insertDictionary(tls, cdict+408, cdict+104, libc.UintptrFromInt32(0), cdict+32, bp, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContent, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize, dictContentType, int32(ZSTD_dtlm_full), int32(ZSTD_tfp_forCDict), (*ZSTD_CDict)(unsafe.Pointer(cdict)).FentropyWorkspace)
	err_code1 = dictID
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4510, 0)
		}
		return err_code1
	}
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID = uint32(dictID)
	return uint64(0)
}

func ZSTD_createCDict_advanced_internal(tls *libc.TLS, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, _cParams ZSTD_compressionParameters, useRowMatchFinder ZSTD_ParamSwitch_e, enableDedicatedDictSearch int32, customMem ZSTD_customMem) (r uintptr) {
	bp := tls.Alloc(112)
	defer tls.Free(112)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var cdict, workspace uintptr
	var workspaceSize size_t
	var v1 uint64
	var _ /* ws at bp+32 */ ZSTD_cwksp
	_, _, _, _ = cdict, workspace, workspaceSize, v1
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
	}
	workspaceSize = ZSTD_cwksp_alloc_size(tls, uint64(6080)) + ZSTD_cwksp_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))) + ZSTD_sizeof_matchState(tls, bp, useRowMatchFinder, enableDedicatedDictSearch, uint32(0)) + v1
	workspace = ZSTD_customMalloc(tls, workspaceSize, customMem)
	if !(workspace != 0) {
		ZSTD_customFree(tls, workspace, customMem)
		return libc.UintptrFromInt32(0)
	}
	ZSTD_cwksp_init(tls, bp+32, workspace, workspaceSize, int32(ZSTD_cwksp_dynamic_alloc))
	cdict = ZSTD_cwksp_reserve_object(tls, bp+32, uint64(6080))
	ZSTD_cwksp_move(tls, cdict+32, bp+32)
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcustomMem = customMem
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = ZSTD_NO_CLEVEL /* signals advanced API usage */
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder = useRowMatchFinder
	return cdict
	return r
}

func ZSTD_createCDict_advanced(tls *libc.TLS, dictBuffer uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, cParams ZSTD_compressionParameters, customMem ZSTD_customMem) (r uintptr) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(224))
	ZSTD_CCtxParams_init(tls, bp, 0)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = cParams
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcustomMem = customMem
	return ZSTD_createCDict_advanced2(tls, dictBuffer, dictSize, dictLoadMethod, dictContentType, bp, customMem)
}

func ZSTD_createCDict_advanced2(tls *libc.TLS, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, originalCctxParams uintptr, customMem ZSTD_customMem) (r uintptr) {
	bp := tls.Alloc(256)
	defer tls.Free(256)
	var cdict uintptr
	var _ /* cParams at bp+224 */ ZSTD_compressionParameters
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	_ = cdict
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = *(*ZSTD_CCtx_params)(unsafe.Pointer(originalCctxParams))
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch != 0 {
		*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = ZSTD_dedicatedDictSearch_getCParams(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel, dictSize)
		ZSTD_overrideCParams(tls, bp+224, bp+4)
	} else {
		*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = ZSTD_getCParamsFromCCtxParams(tls, bp, uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), dictSize, int32(ZSTD_cpm_createCDict))
	}
	if !(ZSTD_dedicatedDictSearch_isSupported(tls, bp+224) != 0) {
		/* Fall back to non-DDSS params */
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch = 0
		*(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224)) = ZSTD_getCParamsFromCCtxParams(tls, bp, uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), dictSize, int32(ZSTD_cpm_createCDict))
	}
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(bp + 224))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, bp+224)
	cdict = ZSTD_createCDict_advanced_internal(tls, dictSize, dictLoadMethod, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FenableDedicatedDictSearch, customMem)
	if !(cdict != 0) || ZSTD_isError(tls, ZSTD_initCDict_internal(tls, cdict, dict, dictSize, dictLoadMethod, dictContentType, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp)))) != 0 {
		ZSTD_freeCDict(tls, cdict)
		return libc.UintptrFromInt32(0)
	}
	return cdict
}

func ZSTD_createCDict(tls *libc.TLS, dict uintptr, dictSize size_t, compressionLevel int32) (r uintptr) {
	var cParams ZSTD_compressionParameters
	var cdict uintptr
	var v1 int32
	_, _, _ = cParams, cdict, v1
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	cdict = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto), cParams, ZSTD_defaultCMem)
	if cdict != 0 {
		if compressionLevel == 0 {
			v1 = int32(ZSTD_CLEVEL_DEFAULT)
		} else {
			v1 = compressionLevel
		}
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = v1
	}
	return cdict
}

func ZSTD_createCDict_byReference(tls *libc.TLS, dict uintptr, dictSize size_t, compressionLevel int32) (r uintptr) {
	var cParams ZSTD_compressionParameters
	var cdict uintptr
	var v1 int32
	_, _, _ = cParams, cdict, v1
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1), dictSize, int32(ZSTD_cpm_createCDict))
	cdict = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto), cParams, ZSTD_defaultCMem)
	if cdict != 0 {
		if compressionLevel == 0 {
			v1 = int32(ZSTD_CLEVEL_DEFAULT)
		} else {
			v1 = compressionLevel
		}
		(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = v1
	}
	return cdict
}

func ZSTD_freeCDict(tls *libc.TLS, cdict uintptr) (r size_t) {
	var cMem ZSTD_customMem
	var cdictInWorkspace int32
	_, _ = cMem, cdictInWorkspace
	if cdict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	cMem = (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcustomMem
	cdictInWorkspace = ZSTD_cwksp_owns_buffer(tls, cdict+32, cdict)
	ZSTD_cwksp_free(tls, cdict+32, cMem)
	if !(cdictInWorkspace != 0) {
		ZSTD_customFree(tls, cdict, cMem)
	}
	return uint64(0)
	return r
}

// C documentation
//
//	/*! ZSTD_initStaticCDict_advanced() :
//	 *  Generate a digested dictionary in provided memory area.
//	 *  workspace: The memory area to emplace the dictionary into.
//	 *             Provided pointer must 8-bytes aligned.
//	 *             It must outlive dictionary usage.
//	 *  workspaceSize: Use ZSTD_estimateCDictSize()
//	 *                 to determine how large workspace must be.
//	 *  cParams : use ZSTD_getCParams() to transform a compression level
//	 *            into its relevant cParams.
//	 * @return : pointer to ZSTD_CDict*, or NULL if error (size too small)
//	 *  Note : there is no corresponding "free" function.
//	 *         Since workspace was allocated externally, it must be freed externally.
//	 */
func ZSTD_initStaticCDict(tls *libc.TLS, workspace uintptr, workspaceSize size_t, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, _cParams ZSTD_compressionParameters) (r uintptr) {
	bp := tls.Alloc(336)
	defer tls.Free(336)
	*(*ZSTD_compressionParameters)(unsafe.Pointer(bp)) = _cParams
	var cdict uintptr
	var matchStateSize, neededSize size_t
	var useRowMatchFinder ZSTD_ParamSwitch_e
	var v1 uint64
	var _ /* params at bp+32 */ ZSTD_CCtx_params
	var _ /* ws at bp+256 */ ZSTD_cwksp
	_, _, _, _, _ = cdict, matchStateSize, neededSize, useRowMatchFinder, v1
	useRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, int32(ZSTD_ps_auto), bp)
	/* enableDedicatedDictSearch == 1 ensures matchstate is not too small in case this CDict will be used for DDS + row hash */
	matchStateSize = ZSTD_sizeof_matchState(tls, bp, useRowMatchFinder, int32(1), uint32(0))
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = ZSTD_cwksp_alloc_size(tls, ZSTD_cwksp_align(tls, dictSize, uint64(8)))
	}
	neededSize = ZSTD_cwksp_alloc_size(tls, uint64(6080)) + v1 + ZSTD_cwksp_alloc_size(tls, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512))) + matchStateSize
	if uint64(workspace)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* 8-aligned */
	ZSTD_cwksp_init(tls, bp+256, workspace, workspaceSize, int32(ZSTD_cwksp_static_alloc))
	cdict = ZSTD_cwksp_reserve_object(tls, bp+256, uint64(6080))
	if cdict == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_cwksp_move(tls, cdict+32, bp+256)
	if workspaceSize < neededSize {
		return libc.UintptrFromInt32(0)
	}
	ZSTD_CCtxParams_init(tls, bp+32, 0)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FcParams = *(*ZSTD_compressionParameters)(unsafe.Pointer(bp))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32))).FuseRowMatchFinder = useRowMatchFinder
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FuseRowMatchFinder = useRowMatchFinder
	(*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel = ZSTD_NO_CLEVEL
	if ZSTD_isError(tls, ZSTD_initCDict_internal(tls, cdict, dict, dictSize, dictLoadMethod, dictContentType, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp + 32)))) != 0 {
		return libc.UintptrFromInt32(0)
	}
	return cdict
}

func ZSTD_getCParamsFromCDict(tls *libc.TLS, cdict uintptr) (r ZSTD_compressionParameters) {
	return (*ZSTD_CDict)(unsafe.Pointer(cdict)).FmatchState.FcParams
}

// C documentation
//
//	/*! ZSTD_getDictID_fromCDict() :
//	 *  Provides the dictID of the dictionary loaded into `cdict`.
//	 *  If @return == 0, the dictionary is not conformant to Zstandard specification, or empty.
//	 *  Non-conformant dictionaries can still be loaded, but as content-only dictionaries. */
func ZSTD_getDictID_fromCDict(tls *libc.TLS, cdict uintptr) (r uint32) {
	if cdict == libc.UintptrFromInt32(0) {
		return uint32(0)
	}
	return (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictID
}

// C documentation
//
//	/* ZSTD_compressBegin_usingCDict_internal() :
//	 * Implementation of various ZSTD_compressBegin_usingCDict* functions.
//	 */
func ZSTD_compressBegin_usingCDict_internal(tls *libc.TLS, cctx uintptr, cdict uintptr, fParams ZSTD_frameParameters, pledgedSrcSize uint64) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var limitedSrcLog, limitedSrcSize U32
	var v1 ZSTD_compressionParameters
	var v2 uint64
	var v3, v4 uint32
	var _ /* cctxParams at bp+0 */ ZSTD_CCtx_params
	var _ /* params at bp+224 */ ZSTD_parameters
	_, _, _, _, _, _ = limitedSrcLog, limitedSrcSize, v1, v2, v3, v4
	if cdict == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_wrong))
	}
	/* Initialize the cctxParams from the cdict */
	(*(*ZSTD_parameters)(unsafe.Pointer(bp + 224))).FfParams = fParams
	if pledgedSrcSize < libc.Uint64FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) || pledgedSrcSize < (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize*uint64(6) || pledgedSrcSize == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) || (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel == 0 {
		v1 = ZSTD_getCParamsFromCDict(tls, cdict)
	} else {
		v1 = ZSTD_getCParams(tls, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel, pledgedSrcSize, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FdictContentSize)
	}
	(*(*ZSTD_parameters)(unsafe.Pointer(bp + 224))).FcParams = v1
	ZSTD_CCtxParams_init_internal(tls, bp, bp+224, (*ZSTD_CDict)(unsafe.Pointer(cdict)).FcompressionLevel)
	/* Increase window log to fit the entire dictionary and source if the
	 * source size is known. Limit the increase to 19, which is the
	 * window log for compression level 1 with the largest source size.
	 */
	if pledgedSrcSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		if pledgedSrcSize < uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(19)) {
			v2 = pledgedSrcSize
		} else {
			v2 = uint64(libc.Uint32FromUint32(1) << libc.Int32FromInt32(19))
		}
		limitedSrcSize = uint32(v2)
		if limitedSrcSize > uint32(1) {
			v3 = ZSTD_highbit32(tls, limitedSrcSize-uint32(1)) + uint32(1)
		} else {
			v3 = uint32(1)
		}
		limitedSrcLog = v3
		if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog > limitedSrcLog {
			v4 = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog
		} else {
			v4 = limitedSrcLog
		}
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams.FwindowLog = v4
	}
	return ZSTD_compressBegin_internal(tls, cctx, libc.UintptrFromInt32(0), uint64(0), int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), cdict, bp, pledgedSrcSize, int32(ZSTDb_not_buffered))
}

// C documentation
//
//	/* ZSTD_compressBegin_usingCDict_advanced() :
//	 * This function is DEPRECATED.
//	 * cdict must be != NULL */
func ZSTD_compressBegin_usingCDict_advanced(tls *libc.TLS, cctx uintptr, cdict uintptr, fParams ZSTD_frameParameters, pledgedSrcSize uint64) (r size_t) {
	return ZSTD_compressBegin_usingCDict_internal(tls, cctx, cdict, fParams, pledgedSrcSize)
}

// C documentation
//
//	/* ZSTD_compressBegin_usingCDict() :
//	 * cdict must be != NULL */
func ZSTD_compressBegin_usingCDict_deprecated(tls *libc.TLS, cctx uintptr, cdict uintptr) (r size_t) {
	var fParams ZSTD_frameParameters
	_ = fParams
	fParams = ZSTD_frameParameters{}
	return ZSTD_compressBegin_usingCDict_internal(tls, cctx, cdict, fParams, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
}

func ZSTD_compressBegin_usingCDict(tls *libc.TLS, cctx uintptr, cdict uintptr) (r size_t) {
	return ZSTD_compressBegin_usingCDict_deprecated(tls, cctx, cdict)
}

// C documentation
//
//	/*! ZSTD_compress_usingCDict_internal():
//	 * Implementation of various ZSTD_compress_usingCDict* functions.
//	 */
func ZSTD_compress_usingCDict_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, cdict uintptr, fParams ZSTD_frameParameters) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_compressBegin_usingCDict_internal(tls, cctx, cdict, fParams, srcSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	} /* will check if cdict != NULL */
	return ZSTD_compressEnd_public(tls, cctx, dst, dstCapacity, src, srcSize)
}

// C documentation
//
//	/*! ZSTD_compress_usingCDict_advanced():
//	 * This function is DEPRECATED.
//	 */
func ZSTD_compress_usingCDict_advanced(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, cdict uintptr, fParams ZSTD_frameParameters) (r size_t) {
	return ZSTD_compress_usingCDict_internal(tls, cctx, dst, dstCapacity, src, srcSize, cdict, fParams)
}

// C documentation
//
//	/*! ZSTD_compress_usingCDict() :
//	 *  Compression using a digested Dictionary.
//	 *  Faster startup than ZSTD_compress_usingDict(), recommended when same dictionary is used multiple times.
//	 *  Note that compression parameters are decided at CDict creation time
//	 *  while frame parameters are hardcoded */
func ZSTD_compress_usingCDict(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, cdict uintptr) (r size_t) {
	var fParams ZSTD_frameParameters
	_ = fParams
	fParams = ZSTD_frameParameters{
		FcontentSizeFlag: int32(1),
	}
	return ZSTD_compress_usingCDict_internal(tls, cctx, dst, dstCapacity, src, srcSize, cdict, fParams)
}

/* ******************************************************************
*  Streaming
********************************************************************/

func ZSTD_createCStream(tls *libc.TLS) (r uintptr) {
	return ZSTD_createCStream_advanced(tls, ZSTD_defaultCMem)
}

func ZSTD_initStaticCStream(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	return ZSTD_initStaticCCtx(tls, workspace, workspaceSize)
}

func ZSTD_createCStream_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	/* CStream and CCtx are now same object */
	return ZSTD_createCCtx_advanced(tls, customMem)
}

func ZSTD_freeCStream(tls *libc.TLS, zcs uintptr) (r size_t) {
	return ZSTD_freeCCtx(tls, zcs) /* same object */
}

/*======   Initialization   ======*/

func ZSTD_CStreamInSize(tls *libc.TLS) (r size_t) {
	return libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
}

func ZSTD_CStreamOutSize(tls *libc.TLS) (r size_t) {
	return ZSTD_compressBound(tls, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))) + ZSTD_blockHeaderSize + uint64(4)
}

func ZSTD_getCParamMode(tls *libc.TLS, cdict uintptr, params uintptr, pledgedSrcSize U64) (r ZSTD_CParamMode_e) {
	if cdict != libc.UintptrFromInt32(0) && ZSTD_shouldAttachDict(tls, cdict, params, pledgedSrcSize) != 0 {
		return int32(ZSTD_cpm_attachDict)
	} else {
		return int32(ZSTD_cpm_noAttachDict)
	}
	return r
}

// C documentation
//
//	/* ZSTD_resetCStream():
//	 * pledgedSrcSize == 0 means "unknown" */
func ZSTD_resetCStream(tls *libc.TLS, zcs uintptr, pss uint64) (r size_t) {
	var err_code, err_code1 size_t
	var pledgedSrcSize U64
	var v1 uint64
	_, _, _, _ = err_code, err_code1, pledgedSrcSize, v1
	if pss == uint64(0) {
		v1 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v1 = pss
	}
	/* temporary : 0 interpreted as "unknown" during transition period.
	 * Users willing to specify "unknown" **must** use ZSTD_CONTENTSIZE_UNKNOWN.
	 * 0 will be interpreted as "empty" in the future.
	 */
	pledgedSrcSize = v1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_initCStream_internal() :
//	 *  Note : for lib/compress only. Used by zstdmt_compress.c.
//	 *  Assumption 1 : params are valid
//	 *  Assumption 2 : either dict, or cdict, is defined, not both */
func ZSTD_initCStream_internal(tls *libc.TLS, zcs uintptr, dict uintptr, dictSize size_t, cdict uintptr, params uintptr, pledgedSrcSize uint64) (r size_t) {
	var err_code, err_code1, err_code2, err_code3 size_t
	_, _, _, _ = err_code, err_code1, err_code2, err_code3
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	(*ZSTD_CStream)(unsafe.Pointer(zcs)).FrequestedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(params))
	/* either dict or cdict, not both */
	if dict != 0 {
		err_code2 = ZSTD_CCtx_loadDictionary(tls, zcs, dict, dictSize)
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code2
		}
	} else {
		/* Dictionary is cleared if !cdict */
		err_code3 = ZSTD_CCtx_refCDict(tls, zcs, cdict)
		if ERR_isError(tls, err_code3) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code3
		}
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_initCStream_usingCDict_advanced() :
//	 * same as ZSTD_initCStream_usingCDict(), with control over frame parameters */
func ZSTD_initCStream_usingCDict_advanced(tls *libc.TLS, zcs uintptr, cdict uintptr, fParams ZSTD_frameParameters, pledgedSrcSize uint64) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	(*ZSTD_CStream)(unsafe.Pointer(zcs)).FrequestedParams.FfParams = fParams
	err_code2 = ZSTD_CCtx_refCDict(tls, zcs, cdict)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

// C documentation
//
//	/* note : cdict must outlive compression session */
func ZSTD_initCStream_usingCDict(tls *libc.TLS, zcs uintptr, cdict uintptr) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_refCDict(tls, zcs, cdict)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_initCStream_advanced() :
//	 * pledgedSrcSize must be exact.
//	 * if srcSize is not known at init time, use value ZSTD_CONTENTSIZE_UNKNOWN.
//	 * dict is loaded with default parameters ZSTD_dct_auto and ZSTD_dlm_byCopy. */
func ZSTD_initCStream_advanced(tls *libc.TLS, zcs uintptr, dict uintptr, dictSize size_t, _params ZSTD_parameters, pss uint64) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	*(*ZSTD_parameters)(unsafe.Pointer(bp)) = _params
	var err_code, err_code1, err_code2, err_code3 size_t
	var pledgedSrcSize U64
	var v1 uint64
	_, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, pledgedSrcSize, v1
	if pss == uint64(0) && (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FfParams.FcontentSizeFlag == 0 {
		v1 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v1 = pss
	}
	/* for compatibility with older programs relying on this behavior.
	 * Users should now specify ZSTD_CONTENTSIZE_UNKNOWN.
	 * This line will be removed in the future.
	 */
	pledgedSrcSize = v1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_checkCParams(tls, (*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	ZSTD_CCtxParams_setZstdParams(tls, zcs+16, bp)
	err_code3 = ZSTD_CCtx_loadDictionary(tls, zcs, dict, dictSize)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	return uint64(0)
}

func ZSTD_initCStream_usingDict(tls *libc.TLS, zcs uintptr, dict uintptr, dictSize size_t, compressionLevel int32) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_setParameter(tls, zcs, int32(ZSTD_c_compressionLevel), compressionLevel)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_loadDictionary(tls, zcs, dict, dictSize)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

func ZSTD_initCStream_srcSize(tls *libc.TLS, zcs uintptr, compressionLevel int32, pss uint64) (r size_t) {
	var err_code, err_code1, err_code2, err_code3 size_t
	var pledgedSrcSize U64
	var v1 uint64
	_, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, pledgedSrcSize, v1
	if pss == uint64(0) {
		v1 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v1 = pss
	}
	/* temporary : 0 interpreted as "unknown" during transition period.
	 * Users willing to specify "unknown" **must** use ZSTD_CONTENTSIZE_UNKNOWN.
	 * 0 will be interpreted as "empty" in the future.
	 */
	pledgedSrcSize = v1
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_refCDict(tls, zcs, libc.UintptrFromInt32(0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, zcs, int32(ZSTD_c_compressionLevel), compressionLevel)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	err_code3 = ZSTD_CCtx_setPledgedSrcSize(tls, zcs, pledgedSrcSize)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	return uint64(0)
}

func ZSTD_initCStream(tls *libc.TLS, zcs uintptr, compressionLevel int32) (r size_t) {
	var err_code, err_code1, err_code2 size_t
	_, _, _ = err_code, err_code1, err_code2
	err_code = ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_CCtx_refCDict(tls, zcs, libc.UintptrFromInt32(0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	err_code2 = ZSTD_CCtx_setParameter(tls, zcs, int32(ZSTD_c_compressionLevel), compressionLevel)
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	return uint64(0)
}

/*======   Compression   ======*/

func ZSTD_nextInputSizeHint(tls *libc.TLS, cctx uintptr) (r size_t) {
	var hintInSize size_t
	_ = hintInSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		return (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed
	}
	hintInSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffTarget - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffPos
	if hintInSize == uint64(0) {
		hintInSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax
	}
	return hintInSize
	return r
}

// C documentation
//
//	/** ZSTD_compressStream_generic():
//	 *  internal function for all *compressStream*() variants
//	 * @return : hint size for next input to complete ongoing block */
func ZSTD_compressStream_generic(tls *libc.TLS, zcs uintptr, output uintptr, input uintptr, flushMode ZSTD_EndDirective) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cDst, iend, ip, istart, oend, op, ostart, v1, v2, v3, v4 uintptr
	var cSize, cSize1, err_code, err_code1, err_code2, flushed, iSize, loaded, oSize, toFlush, toLoad, v19 size_t
	var inputBuffered int32
	var lastBlock, lastBlock1 uint32
	var someMoreWork U32
	var v13, v14 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cDst, cSize, cSize1, err_code, err_code1, err_code2, flushed, iSize, iend, inputBuffered, ip, istart, lastBlock, lastBlock1, loaded, oSize, oend, op, ostart, someMoreWork, toFlush, toLoad, v1, v13, v14, v19, v2, v3, v4
	istart = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc
	if istart != libc.UintptrFromInt32(0) {
		v1 = istart + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize)
	} else {
		v1 = istart
	}
	iend = v1
	if istart != libc.UintptrFromInt32(0) {
		v2 = istart + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos)
	} else {
		v2 = istart
	}
	ip = v2
	ostart = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst
	if ostart != libc.UintptrFromInt32(0) {
		v3 = ostart + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize)
	} else {
		v3 = ostart
	}
	oend = v3
	if ostart != libc.UintptrFromInt32(0) {
		v4 = ostart + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos)
	} else {
		v4 = ostart
	}
	op = v4
	someMoreWork = uint32(1)
	/* check expectations */
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		*(*size_t)(unsafe.Pointer(input + 16)) -= (*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed
		if ip != 0 {
			ip = ip - uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed)
		}
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed = uint64(0)
	}
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered) {
	}
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_buffered) {
	}
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc == libc.UintptrFromInt32(0) {
	}
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst == libc.UintptrFromInt32(0) {
	}
	for someMoreWork != 0 {
		switch (*ZSTD_CStream)(unsafe.Pointer(zcs)).FstreamStage {
		case int32(zcss_init):
			goto _5
		case int32(zcss_load):
			goto _6
		case int32(zcss_flush):
			goto _7
		default:
			goto _8
		}
		goto _9
	_5:
		;
	_12:
		;
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4719, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_init_missing))
		goto _11
	_11:
		;
		if 0 != 0 {
			goto _12
		}
		goto _10
	_10:
		;
	_6:
		;
		if flushMode == int32(ZSTD_e_end) && (libc.Uint64FromInt64(int64(oend)-int64(op)) >= ZSTD_compressBound(tls, libc.Uint64FromInt64(int64(iend)-int64(ip))) || (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable)) && (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos == uint64(0) {
			/* shortcut to compression pass directly into output buffer */
			cSize = ZSTD_compressEnd_public(tls, zcs, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, libc.Uint64FromInt64(int64(iend)-int64(ip)))
			err_code = cSize
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+4750, 0)
				}
				return err_code
			}
			ip = iend
			op = op + uintptr(cSize)
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded = uint32(1)
			ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
			someMoreWork = uint32(0)
			goto _9
		}
		/* complete loading into inBuffer in buffered mode */
		if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered) {
			toLoad = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget - (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos
			loaded = ZSTD_limitCopy(tls, (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos), toLoad, ip, libc.Uint64FromInt64(int64(iend)-int64(ip)))
			*(*size_t)(unsafe.Pointer(zcs + 3592)) += loaded
			if ip != 0 {
				ip = ip + uintptr(loaded)
			}
			if flushMode == int32(ZSTD_e_continue) && (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos < (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget {
				/* not enough input to fill full block : stop here */
				someMoreWork = uint32(0)
				goto _9
			}
			if flushMode == int32(ZSTD_e_flush) && (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos == (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress {
				/* empty */
				someMoreWork = uint32(0)
				goto _9
			}
		} else {
			if flushMode == int32(ZSTD_e_continue) && libc.Uint64FromInt64(int64(iend)-int64(ip)) < (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax {
				/* can't compress a full block : stop here */
				(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstableIn_notConsumed = libc.Uint64FromInt64(int64(iend) - int64(ip))
				ip = iend /* pretend to have consumed input */
				someMoreWork = uint32(0)
				goto _9
			}
			if flushMode == int32(ZSTD_e_flush) && ip == iend {
				/* empty */
				someMoreWork = uint32(0)
				goto _9
			}
		}
		/* compress current block (note : this stage cannot be stopped in the middle) */
		inputBuffered = libc.BoolInt32((*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered))
		oSize = libc.Uint64FromInt64(int64(oend) - int64(op))
		if inputBuffered != 0 {
			v13 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos - (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress
		} else {
			if libc.Uint64FromInt64(int64(iend)-int64(ip)) < (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax {
				v14 = libc.Uint64FromInt64(int64(iend) - int64(ip))
			} else {
				v14 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax
			}
			v13 = v14
		}
		iSize = v13
		if oSize >= ZSTD_compressBound(tls, iSize) || (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable) {
			cDst = op
		} else {
			cDst = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuff
			oSize = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffSize
		}
		if inputBuffered != 0 {
			lastBlock = libc.BoolUint32(flushMode == int32(ZSTD_e_end) && ip == iend)
			if lastBlock != 0 {
				v13 = ZSTD_compressEnd_public(tls, zcs, cDst, oSize, (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress), iSize)
			} else {
				v13 = ZSTD_compressContinue_public(tls, zcs, cDst, oSize, (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress), iSize)
			}
			cSize1 = v13
			err_code1 = cSize1
			if ERR_isError(tls, err_code1) != 0 {
				if 0 != 0 {
					if lastBlock != 0 {
						v1 = __ccgo_ts + 4750
					} else {
						v1 = __ccgo_ts + 4774
					}
					_force_has_format_string(tls, __ccgo_ts+4400, libc.VaList(bp+8, v1))
				}
				return err_code1
			}
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded = lastBlock
			/* prepare next block */
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos + (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax
			if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget > (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffSize {
				(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos = uint64(0)
				(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffTarget = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FblockSizeMax
			}
			if !(lastBlock != 0) {
			}
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FinToCompress = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FinBuffPos
		} else { /* !inputBuffered, hence ZSTD_bm_stable */
			lastBlock1 = libc.BoolUint32(flushMode == int32(ZSTD_e_end) && ip+uintptr(iSize) == iend)
			if lastBlock1 != 0 {
				v13 = ZSTD_compressEnd_public(tls, zcs, cDst, oSize, ip, iSize)
			} else {
				v13 = ZSTD_compressContinue_public(tls, zcs, cDst, oSize, ip, iSize)
			}
			cSize1 = v13
			/* Consume the input prior to error checking to mirror buffered mode. */
			if ip != 0 {
				ip = ip + uintptr(iSize)
			}
			err_code2 = cSize1
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					if lastBlock1 != 0 {
						v1 = __ccgo_ts + 4750
					} else {
						v1 = __ccgo_ts + 4774
					}
					_force_has_format_string(tls, __ccgo_ts+4400, libc.VaList(bp+8, v1))
				}
				return err_code2
			}
			(*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded = lastBlock1
			if lastBlock1 != 0 {
			}
		}
		if cDst == op { /* no need to flush */
			op = op + uintptr(cSize1)
			if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
				someMoreWork = uint32(0)
				ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
			}
			goto _9
		}
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffContentSize = cSize1
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize = uint64(0)
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstreamStage = int32(zcss_flush) /* pass-through to flush stage */
	_7:
		;
		toFlush = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffContentSize - (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize
		flushed = ZSTD_limitCopy(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), (*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuff+uintptr((*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize), toFlush)
		if flushed != 0 {
			op = op + uintptr(flushed)
		}
		*(*size_t)(unsafe.Pointer(zcs + 3632)) += flushed
		if toFlush != flushed {
			/* flush not fully completed, presumably because dst is too small */
			someMoreWork = uint32(0)
			goto _9
		}
		v19 = libc.Uint64FromInt32(0)
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffFlushedSize = v19
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FoutBuffContentSize = v19
		if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
			someMoreWork = uint32(0)
			ZSTD_CCtx_reset(tls, zcs, int32(ZSTD_reset_session_only))
			goto _9
		}
		(*ZSTD_CStream)(unsafe.Pointer(zcs)).FstreamStage = int32(zcss_load)
		goto _9
	_8:
		; /* impossible */
	_9:
	}
	(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = libc.Uint64FromInt64(int64(ip) - int64(istart))
	(*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos = libc.Uint64FromInt64(int64(op) - int64(ostart))
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
		return uint64(0)
	}
	return ZSTD_nextInputSizeHint(tls, zcs)
}

func ZSTD_nextInputSizeHint_MTorST(tls *libc.TLS, cctx uintptr) (r size_t) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers >= int32(1) {
		return ZSTDMT_nextInputSizeHint(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx)
	}
	return ZSTD_nextInputSizeHint(tls, cctx)
}

func ZSTD_compressStream(tls *libc.TLS, zcs uintptr, output uintptr, input uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_compressStream2(tls, zcs, output, input, int32(ZSTD_e_continue))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_nextInputSizeHint_MTorST(tls, zcs)
}

// C documentation
//
//	/* After a compression call set the expected input/output buffer.
//	 * This is validated at the start of the next compression call.
//	 */
func ZSTD_setBufferExpectations(tls *libc.TLS, cctx uintptr, output uintptr, input uintptr) {
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer = *(*ZSTD_inBuffer)(unsafe.Pointer(input))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedOutBufferSize = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize - (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
	}
}

// C documentation
//
//	/* Validate that the input/output buffers match the expectations set by
//	 * ZSTD_setBufferExpectations.
//	 */
func ZSTD_checkBufferStability(tls *libc.TLS, cctx uintptr, output uintptr, input uintptr, endOp ZSTD_EndDirective) (r size_t) {
	var expect ZSTD_inBuffer
	var outBufferSize size_t
	_, _ = expect, outBufferSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable) {
		expect = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer
		if expect.Fsrc != (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc || expect.Fpos != (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4803, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_stabilityCondition_notRespected))
		}
	}
	_ = endOp
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FoutBufferMode == int32(ZSTD_bm_stable) {
		outBufferSize = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize - (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedOutBufferSize != outBufferSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4852, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_stabilityCondition_notRespected))
		}
	}
	return uint64(0)
}

// C documentation
//
//	/*
//	 * If @endOp == ZSTD_e_end, @inSize becomes pledgedSrcSize.
//	 * Otherwise, it's ignored.
//	 * @return: 0 on success, or a ZSTD_error code otherwise.
//	 */
func ZSTD_CCtx_init_compressStream2(tls *libc.TLS, cctx uintptr, endOp ZSTD_EndDirective, inSize size_t) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	var dictSize, err_code, err_code1, err_code2, v5 size_t
	var mode ZSTD_CParamMode_e
	var pledgedSrcSize U64
	var prefixDict ZSTD_prefixDict
	var v1, v2 uint64
	var v3 uint32
	var _ /* params at bp+0 */ ZSTD_CCtx_params
	_, _, _, _, _, _, _, _, _, _, _ = dictSize, err_code, err_code1, err_code2, mode, pledgedSrcSize, prefixDict, v1, v2, v3, v5
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams
	prefixDict = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict
	err_code = ZSTD_initLocalDict(tls, cctx)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	} /* Init the local dict if present. */
	libc.Xmemset(tls, cctx+3736, 0, libc.Uint64FromInt64(24)) /* single usage */
	/* only one can be set */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 && !((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FlocalDict.Fcdict != 0) {
		/* Let the cdict's compression level take priority over the requested params.
		 * But do not take the cdict's compression level if the "cdict" is actually a localDict
		 * generated from ZSTD_initLocalDict().
		 */
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FcompressionLevel
	}
	if endOp == int32(ZSTD_e_end) {
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne = inSize + uint64(1)
	} /* auto-determine pledgedSrcSize */
	if prefixDict.Fdict != 0 {
		v1 = prefixDict.FdictSize
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
			v2 = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize
		} else {
			v2 = uint64(0)
		}
		v1 = v2
	}
	dictSize = v1
	mode = ZSTD_getCParamMode(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1))
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams = ZSTD_getCParamsFromCCtxParams(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1), dictSize, mode)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FpostBlockSplitter = ZSTD_resolveBlockSplitterMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FpostBlockSplitter, bp+4)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm = ZSTD_resolveEnableLdm(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm, bp+4)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder = ZSTD_resolveRowMatchFinderMode(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FuseRowMatchFinder, bp+4)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FvalidateSequences = ZSTD_resolveExternalSequenceValidation(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FvalidateSequences)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FmaxBlockSize = ZSTD_resolveMaxBlockSize(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FmaxBlockSize)
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FsearchForExternalRepcodes = ZSTD_resolveExternalRepcodeSearch(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FsearchForExternalRepcodes, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcompressionLevel)
	/* If external matchfinder is enabled, make sure to fail before checking job size (for consistency) */
	if ZSTD_hasExtSeqProd(tls, bp) != 0 && (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers >= int32(1) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4908, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_combination_unsupported))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1) <= libc.Uint64FromInt32(libc.Int32FromInt32(512)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers = 0 /* do not invoke multi-threading when src size is too small */
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers > 0 {
		/* mt context creation */
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx == libc.UintptrFromInt32(0) {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx = ZSTDMT_createCCtx_advanced(tls, libc.Uint32FromInt32((*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcustomMem, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fpool)
			if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx == libc.UintptrFromInt32(0) {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1377, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
			}
		}
		/* mt compression */
		err_code1 = ZSTDMT_initCStream_internal(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx, prefixDict.Fdict, prefixDict.FdictSize, prefixDict.FdictContentType, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp)), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne-uint64(1))
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code1
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
			v3 = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictID
		} else {
			v3 = uint32(0)
		}
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID = v3
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
			v1 = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize
		} else {
			v1 = prefixDict.FdictSize
		}
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictContentSize = v1
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FconsumedSrcSize = uint64(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FproducedCSize = uint64(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage = int32(zcss_load)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams = *(*ZSTD_CCtx_params)(unsafe.Pointer(bp))
	} else {
		pledgedSrcSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FpledgedSrcSizePlusOne - uint64(1)
		err_code2 = ZSTD_compressBegin_internal(tls, cctx, prefixDict.Fdict, prefixDict.FdictSize, prefixDict.FdictContentType, int32(ZSTD_dtlm_fast), (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict, bp, pledgedSrcSize, int32(ZSTDb_buffered))
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code2
		}
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinToCompress = uint64(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffPos = uint64(0)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FinBufferMode == int32(ZSTD_bm_buffered) {
			/* for small input: avoid automatic flush on reaching end of block, since
			 * it would require to add a 3-bytes null block to end frame
			 */
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffTarget = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax + libc.BoolUint64((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax == pledgedSrcSize)
		} else {
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FinBuffTarget = uint64(0)
		}
		v5 = libc.Uint64FromInt32(0)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffFlushedSize = v5
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffContentSize = v5
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage = int32(zcss_load)
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FframeEnded = uint32(0)
	}
	return uint64(0)
}

// C documentation
//
//	/* @return provides a minimum amount of data remaining to be flushed from internal buffers
//	 */
func ZSTD_compressStream2(tls *libc.TLS, cctx uintptr, output uintptr, input uintptr, endOp ZSTD_EndDirective) (r size_t) {
	var err_code, err_code1, err_code2, err_code3, flushMin, inputSize, ipos, opos, totalInputSize size_t
	var v1 int32
	_, _, _, _, _, _, _, _, _, _ = err_code, err_code1, err_code2, err_code3, flushMin, inputSize, ipos, opos, totalInputSize, v1
	/* check conditions */
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos > (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4971, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos > (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+4993, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if libc.Uint32FromInt32(endOp) > uint32(ZSTD_e_end) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5014, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	/* transparent initialization stage */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstreamStage == int32(zcss_init) {
		inputSize = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize - (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos /* no obligation to start from pos==0 */
		totalInputSize = inputSize + (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode == int32(ZSTD_bm_stable) && endOp == int32(ZSTD_e_continue) && totalInputSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) { /* not even reached one block yet */
			if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed != 0 { /* not the first time */
				/* check stable source guarantees */
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc != (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer.Fsrc {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+5035, 0)
					}
					return libc.Uint64FromInt32(-int32(ZSTD_error_stabilityCondition_notRespected))
				}
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos != (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer.Fsize {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+5093, 0)
					}
					return libc.Uint64FromInt32(-int32(ZSTD_error_stabilityCondition_notRespected))
				}
			}
			/* pretend input was consumed, to give a sense forward progress */
			(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize
			/* save stable inBuffer, for later control, and flush/end */
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FexpectedInBuffer = *(*ZSTD_inBuffer)(unsafe.Pointer(input))
			/* but actually input wasn't consumed, so keep track of position from where compression shall resume */
			*(*size_t)(unsafe.Pointer(cctx + 3672)) += inputSize
			/* don't initialize yet, wait for the first block of flush() order, for better parameters adaptation */
			if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.Fformat == int32(ZSTD_f_zstd1) {
				v1 = int32(6)
			} else {
				v1 = int32(2)
			}
			return libc.Uint64FromInt32(v1) /* at least some header to produce */
		}
		err_code = ZSTD_CCtx_init_compressStream2(tls, cctx, endOp, totalInputSize)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5157, 0)
			}
			return err_code
		}
		ZSTD_setBufferExpectations(tls, cctx, output, input) /* Set initial buffer expectations now that we've initialized */
	}
	/* end of transparent initialization stage */
	err_code1 = ZSTD_checkBufferStability(tls, cctx, output, input, endOp)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5195, 0)
		}
		return err_code1
	}
	/* compression stage */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FnbWorkers > 0 {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcParamsChanged != 0 {
			ZSTDMT_updateCParams_whileCompressing(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx, cctx+16)
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FcParamsChanged = 0
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed != 0 {
			/* some early data was skipped - make it available for consumption */
			*(*size_t)(unsafe.Pointer(input + 16)) -= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FstableIn_notConsumed = uint64(0)
		}
		for {
			ipos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos
			opos = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
			flushMin = ZSTDMT_compressStream_generic(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fmtctx, output, input, endOp)
			*(*uint64)(unsafe.Pointer(cctx + 792)) += (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos - ipos
			*(*uint64)(unsafe.Pointer(cctx + 800)) += (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos - opos
			if ZSTD_isError(tls, flushMin) != 0 || endOp == int32(ZSTD_e_end) && flushMin == uint64(0) { /* compression completed */
				if flushMin == uint64(0) {
					ZSTD_CCtx_trace(tls, cctx, uint64(0))
				}
				ZSTD_CCtx_reset(tls, cctx, int32(ZSTD_reset_session_only))
			}
			err_code2 = flushMin
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5211, 0)
				}
				return err_code2
			}
			if endOp == int32(ZSTD_e_continue) {
				/* We only require some progress with ZSTD_e_continue, not maximal progress.
				 * We're done if we've consumed or produced any bytes, or either buffer is
				 * full.
				 */
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos != ipos || (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos != opos || (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos == (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize || (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
					break
				}
			} else {
				/* We require maximal progress. We're done when the flush is complete or the
				 * output buffer is full.
				 */
				if flushMin == uint64(0) || (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
					break
				}
			}
			goto _2
		_2:
		}
		/* Either we don't require maximum forward progress, we've finished the
		 * flush, or we are out of output space.
		 */
		ZSTD_setBufferExpectations(tls, cctx, output, input)
		return flushMin
	}
	err_code3 = ZSTD_compressStream_generic(tls, cctx, output, input, endOp)
	if ERR_isError(tls, err_code3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code3
	}
	ZSTD_setBufferExpectations(tls, cctx, output, input)
	return (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffContentSize - (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FoutBuffFlushedSize /* remaining to flush */
}

func ZSTD_compressStream2_simpleArgs(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, dstPos uintptr, src uintptr, srcSize size_t, srcPos uintptr, endOp ZSTD_EndDirective) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var cErr size_t
	var _ /* input at bp+24 */ ZSTD_inBuffer
	var _ /* output at bp+0 */ ZSTD_outBuffer
	_ = cErr
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fdst = dst
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fsize = dstCapacity
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos = *(*size_t)(unsafe.Pointer(dstPos))
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsrc = src
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsize = srcSize
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos = *(*size_t)(unsafe.Pointer(srcPos))
	/* ZSTD_compressStream2() will check validity of dstPos and srcPos */
	cErr = ZSTD_compressStream2(tls, cctx, bp, bp+24, endOp)
	*(*size_t)(unsafe.Pointer(dstPos)) = (*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos
	*(*size_t)(unsafe.Pointer(srcPos)) = (*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos
	return cErr
	return r
}

func ZSTD_compress2(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var err_code, result size_t
	var originalInBufferMode, originalOutBufferMode ZSTD_bufferMode_e
	var _ /* iPos at bp+8 */ size_t
	var _ /* oPos at bp+0 */ size_t
	_, _, _, _ = err_code, originalInBufferMode, originalOutBufferMode, result
	originalInBufferMode = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode
	originalOutBufferMode = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FoutBufferMode
	ZSTD_CCtx_reset(tls, cctx, int32(ZSTD_reset_session_only))
	/* Enable stable input/output buffers. */
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode = int32(ZSTD_bm_stable)
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FoutBufferMode = int32(ZSTD_bm_stable)
	*(*size_t)(unsafe.Pointer(bp)) = uint64(0)
	*(*size_t)(unsafe.Pointer(bp + 8)) = uint64(0)
	result = ZSTD_compressStream2_simpleArgs(tls, cctx, dst, dstCapacity, bp, src, srcSize, bp+8, int32(ZSTD_e_end))
	/* Reset to the original values. */
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FinBufferMode = originalInBufferMode
	(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FrequestedParams.FoutBufferMode = originalOutBufferMode
	err_code = result
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5248, 0)
		}
		return err_code
	}
	if result != uint64(0) { /* compression not completed, due to lack of output space */
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* all input is expected consumed */
	return *(*size_t)(unsafe.Pointer(bp))
	return r
}

// C documentation
//
//	/* ZSTD_validateSequence() :
//	 * @offBase : must use the format required by ZSTD_storeSeq()
//	 * @returns a ZSTD error code if sequence is not valid
//	 */
func ZSTD_validateSequence(tls *libc.TLS, offBase U32, matchLength U32, minMatch U32, posInSrc size_t, windowLog U32, dictSize size_t, useSequenceProducer int32) (r size_t) {
	var matchLenLowerBound, offsetBound size_t
	var windowSize U32
	var v1 uint64
	var v2 int32
	_, _, _, _, _ = matchLenLowerBound, offsetBound, windowSize, v1, v2
	windowSize = uint32(1) << windowLog
	if posInSrc > uint64(windowSize) {
		v1 = uint64(windowSize)
	} else {
		v1 = posInSrc + dictSize
	}
	/* posInSrc represents the amount of data the decoder would decode up to this point.
	 * As long as the amount of data decoded is less than or equal to window size, offsets may be
	 * larger than the total length of output decoded in order to reference the dict, even larger than
	 * window size. After output surpasses windowSize, we're limited to windowSize offsets again.
	 */
	offsetBound = v1
	if minMatch == uint32(3) || useSequenceProducer != 0 {
		v2 = int32(3)
	} else {
		v2 = int32(4)
	}
	matchLenLowerBound = libc.Uint64FromInt32(v2)
	if uint64(offBase) > offsetBound+libc.Uint64FromInt32(ZSTD_REP_NUM) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5287, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	/* Validate maxNbSeq is large enough for the given matchLength and minMatch */
	if uint64(matchLength) < matchLenLowerBound {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5305, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	return uint64(0)
}

// C documentation
//
//	/* Returns an offset code, given a sequence's raw offset, the ongoing repcode array, and whether litLength == 0 */
func ZSTD_finalizeOffBase(tls *libc.TLS, rawOffset U32, rep uintptr, ll0 U32) (r U32) {
	var offBase U32
	_ = offBase
	offBase = rawOffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
	if !(ll0 != 0) && rawOffset == *(*U32)(unsafe.Pointer(rep)) {
		offBase = libc.Uint32FromInt32(libc.Int32FromInt32(1))
	} else {
		if rawOffset == *(*U32)(unsafe.Pointer(rep + 1*4)) {
			offBase = libc.Uint32FromInt32(2) - ll0
		} else {
			if rawOffset == *(*U32)(unsafe.Pointer(rep + 2*4)) {
				offBase = libc.Uint32FromInt32(3) - ll0
			} else {
				if ll0 != 0 && rawOffset == *(*U32)(unsafe.Pointer(rep))-uint32(1) {
					offBase = libc.Uint32FromInt32(libc.Int32FromInt32(3))
				}
			}
		}
	}
	return offBase
}

// C documentation
//
//	/* This function scans through an array of ZSTD_Sequence,
//	 * storing the sequences it reads, until it reaches a block delimiter.
//	 * Note that the block delimiter includes the last literals of the block.
//	 * @blockSize must be == sum(sequence_lengths).
//	 * @returns @blockSize on success, and a ZSTD_error otherwise.
//	 */
func ZSTD_transferSequences_wBlockDelim(tls *libc.TLS, cctx uintptr, seqPos uintptr, inSeqs uintptr, inSeqsSize size_t, src uintptr, blockSize size_t, externalRepSearch ZSTD_ParamSwitch_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var dictSize, idx, lastSeqIdx, litLength, ll0, matchLength, offBase, startIdx U32
	var err_code size_t
	var iend, ip, rep uintptr
	var _ /* updatedRepcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _ = dictSize, err_code, idx, iend, ip, lastSeqIdx, litLength, ll0, matchLength, offBase, rep, startIdx
	idx = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx
	startIdx = idx
	ip = src
	iend = ip + uintptr(blockSize)
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
		dictSize = uint32((*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize)
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.Fdict != 0 {
			dictSize = uint32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictSize)
		} else {
			dictSize = uint32(0)
		}
	}
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	for {
		if !(uint64(idx) < inSeqsSize && ((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FmatchLength != uint32(0) || (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).Foffset != uint32(0))) {
			break
		}
		litLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength
		matchLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FmatchLength
		if externalRepSearch == int32(ZSTD_ps_disable) {
			offBase = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).Foffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
		} else {
			ll0 = libc.BoolUint32(litLength == libc.Uint32FromInt32(0))
			offBase = ZSTD_finalizeOffBase(tls, (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).Foffset, bp, ll0)
			ZSTD_updateRep(tls, bp, offBase, ll0)
		}
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FvalidateSequences != 0 {
			*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64(litLength + matchLength)
			err_code = ZSTD_validateSequence(tls, offBase, matchLength, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch, (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSrc, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FwindowLog, uint64(dictSize), ZSTD_hasExtSeqProd(tls, cctx+240))
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5344, 0)
				}
				return err_code
			}
		}
		if uint64(idx-(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx) >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FmaxNbSeq {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5371, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
		}
		ZSTD_storeSeq(tls, cctx+976, uint64(litLength), ip, iend, offBase, uint64(matchLength))
		ip = ip + uintptr(matchLength+litLength)
		goto _1
	_1:
		;
		idx = idx + 1
	}
	if uint64(idx) == inSeqsSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5431, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	/* If we skipped repcode search while parsing, we need to update repcodes now */
	if externalRepSearch == int32(ZSTD_ps_disable) && idx != startIdx {
		rep = bp
		lastSeqIdx = idx - uint32(1) /* index of last non-block-delimiter sequence */
		if lastSeqIdx >= startIdx+uint32(2) {
			*(*U32)(unsafe.Pointer(rep + 2*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(2))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(1))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
		} else {
			if lastSeqIdx == startIdx+uint32(1) {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(1))*16))).Foffset
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
			} else {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep + 1*4))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
			}
		}
	}
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock+5616, bp, libc.Uint64FromInt64(12))
	if (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength != 0 {
		ZSTD_storeLastLiterals(tls, cctx+976, ip, uint64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength))
		ip = ip + uintptr((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength)
		*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))).FlitLength)
	}
	if ip != iend {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5458, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx = idx + uint32(1)
	return blockSize
}

// C documentation
//
//	/*
//	 * This function attempts to scan through @blockSize bytes in @src
//	 * represented by the sequences in @inSeqs,
//	 * storing any (partial) sequences.
//	 *
//	 * Occasionally, we may want to reduce the actual number of bytes consumed from @src
//	 * to avoid splitting a match, notably if it would produce a match smaller than MINMATCH.
//	 *
//	 * @returns the number of bytes consumed from @src, necessarily <= @blockSize.
//	 * Otherwise, it may return a ZSTD error if something went wrong.
//	 */
func ZSTD_transferSequences_noDelim(tls *libc.TLS, cctx uintptr, seqPos uintptr, inSeqs uintptr, inSeqsSize size_t, src uintptr, blockSize size_t, externalRepSearch ZSTD_ParamSwitch_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var bytesAdjustment, endPosInSequence, finalMatchSplit, firstHalfMatchLength, idx, lastLLSize, litLength, ll0, matchLength, offBase, rawOffset, secondHalfMatchLength, startPosInSequence U32
	var currSeq ZSTD_Sequence
	var dictSize, err_code size_t
	var iend, ip, istart uintptr
	var v1 uint32
	var _ /* updatedRepcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bytesAdjustment, currSeq, dictSize, endPosInSequence, err_code, finalMatchSplit, firstHalfMatchLength, idx, iend, ip, istart, lastLLSize, litLength, ll0, matchLength, offBase, rawOffset, secondHalfMatchLength, startPosInSequence, v1
	idx = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx
	startPosInSequence = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSequence
	endPosInSequence = (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSequence + uint32(blockSize)
	istart = src
	ip = istart
	iend = istart + uintptr(blockSize)
	bytesAdjustment = uint32(0)
	finalMatchSplit = uint32(0)
	/* TODO(embg) support fast parsing mode in noBlockDelim mode */
	_ = externalRepSearch
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict != 0 {
		dictSize = (*ZSTD_CDict)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fcdict)).FdictContentSize
	} else {
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.Fdict != 0 {
			dictSize = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FprefixDict.FdictSize
		} else {
			dictSize = uint64(0)
		}
	}
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	for endPosInSequence != 0 && uint64(idx) < inSeqsSize && !(finalMatchSplit != 0) {
		currSeq = *(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(idx)*16))
		litLength = currSeq.FlitLength
		matchLength = currSeq.FmatchLength
		rawOffset = currSeq.Foffset
		/* Modify the sequence depending on where endPosInSequence lies */
		if endPosInSequence >= currSeq.FlitLength+currSeq.FmatchLength {
			if startPosInSequence >= litLength {
				startPosInSequence = startPosInSequence - litLength
				litLength = uint32(0)
				matchLength = matchLength - startPosInSequence
			} else {
				litLength = litLength - startPosInSequence
			}
			/* Move to the next sequence */
			endPosInSequence = endPosInSequence - (currSeq.FlitLength + currSeq.FmatchLength)
			startPosInSequence = uint32(0)
		} else {
			/* This is the final (partial) sequence we're adding from inSeqs, and endPosInSequence
			   does not reach the end of the match. So, we have to split the sequence */
			if endPosInSequence > litLength {
				if startPosInSequence >= litLength {
					v1 = uint32(0)
				} else {
					v1 = litLength - startPosInSequence
				}
				litLength = v1
				firstHalfMatchLength = endPosInSequence - startPosInSequence - litLength
				if uint64(matchLength) > blockSize && firstHalfMatchLength >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch {
					/* Only ever split the match if it is larger than the block size */
					secondHalfMatchLength = currSeq.FmatchLength + currSeq.FlitLength - endPosInSequence
					if secondHalfMatchLength < (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch {
						/* Move the endPosInSequence backward so that it creates match of minMatch length */
						endPosInSequence = endPosInSequence - ((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch - secondHalfMatchLength)
						bytesAdjustment = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch - secondHalfMatchLength
						firstHalfMatchLength = firstHalfMatchLength - bytesAdjustment
					}
					matchLength = firstHalfMatchLength
					/* Flag that we split the last match - after storing the sequence, exit the loop,
					   but keep the value of endPosInSequence */
					finalMatchSplit = uint32(1)
				} else {
					/* Move the position in sequence backwards so that we don't split match, and break to store
					 * the last literals. We use the original currSeq.litLength as a marker for where endPosInSequence
					 * should go. We prefer to do this whenever it is not necessary to split the match, or if doing so
					 * would cause the first half of the match to be too small
					 */
					bytesAdjustment = endPosInSequence - currSeq.FlitLength
					endPosInSequence = currSeq.FlitLength
					break
				}
			} else {
				/* This sequence ends inside the literals, break to store the last literals */
				break
			}
		}
		/* Check if this offset can be represented with a repcode */
		ll0 = libc.BoolUint32(litLength == libc.Uint32FromInt32(0))
		offBase = ZSTD_finalizeOffBase(tls, rawOffset, bp, ll0)
		ZSTD_updateRep(tls, bp, offBase, ll0)
		if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FvalidateSequences != 0 {
			*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64(litLength + matchLength)
			err_code = ZSTD_validateSequence(tls, offBase, matchLength, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FminMatch, (*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSrc, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FcParams.FwindowLog, dictSize, ZSTD_hasExtSeqProd(tls, cctx+240))
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5344, 0)
				}
				return err_code
			}
		}
		if uint64(idx-(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx) >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FmaxNbSeq {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5371, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
		}
		ZSTD_storeSeq(tls, cctx+976, uint64(litLength), ip, iend, offBase, uint64(matchLength))
		ip = ip + uintptr(matchLength+litLength)
		if !(finalMatchSplit != 0) {
			idx = idx + 1
		} /* Next Sequence */
	}
	(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).Fidx = idx
	(*ZSTD_SequencePosition)(unsafe.Pointer(seqPos)).FposInSequence = endPosInSequence
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock+5616, bp, libc.Uint64FromInt64(12))
	iend = iend - uintptr(bytesAdjustment)
	if ip != iend {
		/* Store any last literals */
		lastLLSize = libc.Uint32FromInt64(int64(iend) - int64(ip))
		ZSTD_storeLastLiterals(tls, cctx+976, ip, uint64(lastLLSize))
		*(*size_t)(unsafe.Pointer(seqPos + 8)) += uint64(lastLLSize)
	}
	return libc.Uint64FromInt64(int64(iend) - int64(istart))
}

// C documentation
//
//	/* @seqPos represents a position within @inSeqs,
//	 * it is read and updated by this function,
//	 * once the goal to produce a block of size @blockSize is reached.
//	 * @return: nb of bytes consumed from @src, necessarily <= @blockSize.
//	 */
type ZSTD_SequenceCopier_f = uintptr

func ZSTD_selectSequenceCopier(tls *libc.TLS, mode ZSTD_SequenceFormat_e) (r ZSTD_SequenceCopier_f) {
	if mode == int32(ZSTD_sf_explicitBlockDelimiters) {
		return __ccgo_fp(ZSTD_transferSequences_wBlockDelim)
	}
	return __ccgo_fp(ZSTD_transferSequences_noDelim)
}

// C documentation
//
//	/* Discover the size of next block by searching for the delimiter.
//	 * Note that a block delimiter **must** exist in this mode,
//	 * otherwise it's an input error.
//	 * The block size retrieved will be later compared to ensure it remains within bounds */
func blockSize_explicitDelimiter(tls *libc.TLS, inSeqs uintptr, inSeqsSize size_t, seqPos ZSTD_SequencePosition) (r size_t) {
	var blockSize, spos size_t
	var end int32
	_, _, _ = blockSize, end, spos
	end = 0
	blockSize = uint64(0)
	spos = uint64(seqPos.Fidx)
	for spos < inSeqsSize {
		end = libc.BoolInt32((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).Foffset == uint32(0))
		blockSize = blockSize + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).FlitLength+(*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).FmatchLength)
		if end != 0 {
			if (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(spos)*16))).FmatchLength != uint32(0) {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+5504, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
			}
			break
		}
		spos = spos + 1
	}
	if !(end != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5570, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	return blockSize
}

func determine_blockSize(tls *libc.TLS, mode ZSTD_SequenceFormat_e, blockSize size_t, remaining size_t, inSeqs uintptr, inSeqsSize size_t, seqPos ZSTD_SequencePosition) (r size_t) {
	var err_code, explicitBlockSize size_t
	var v1 uint64
	_, _, _ = err_code, explicitBlockSize, v1
	if mode == int32(ZSTD_sf_noBlockDelimiters) {
		/* Note: more a "target" block size */
		if remaining < blockSize {
			v1 = remaining
		} else {
			v1 = blockSize
		}
		return v1
	}
	explicitBlockSize = blockSize_explicitDelimiter(tls, inSeqs, inSeqsSize, seqPos)
	err_code = explicitBlockSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5629, 0)
		}
		return err_code
	}
	if explicitBlockSize > blockSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5689, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	if explicitBlockSize > remaining {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5736, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	return explicitBlockSize
	return r
}

// C documentation
//
//	/* Compress all provided sequences, block-by-block.
//	 *
//	 * Returns the cumulative size of all compressed blocks (including their headers),
//	 * otherwise a ZSTD error.
//	 */
func ZSTD_compressSequences_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, inSeqsSize size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var blockSize, cBlockSize, cSize, compressedSeqsSize, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, remaining size_t
	var cBlockHeader, cBlockHeader24, lastBlock U32
	var ip, op uintptr
	var sequenceCopier ZSTD_SequenceCopier_f
	var _ /* seqPos at bp+0 */ ZSTD_SequencePosition
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSize, cBlockHeader, cBlockHeader24, cBlockSize, cSize, compressedSeqsSize, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, ip, lastBlock, op, remaining, sequenceCopier
	cSize = uint64(0)
	remaining = srcSize
	*(*ZSTD_SequencePosition)(unsafe.Pointer(bp)) = ZSTD_SequencePosition{}
	ip = src
	op = dst
	sequenceCopier = ZSTD_selectSequenceCopier(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FblockDelimiters)
	/* Special case: empty frame */
	if remaining == uint64(0) {
		cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1)
		if dstCapacity < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5780, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE32(tls, op, cBlockHeader24)
		op = op + uintptr(ZSTD_blockHeaderSize)
		dstCapacity = dstCapacity - ZSTD_blockHeaderSize
		cSize = cSize + ZSTD_blockHeaderSize
	}
	for remaining != 0 {
		blockSize = determine_blockSize(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FblockDelimiters, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax, remaining, inSeqs, inSeqsSize, *(*ZSTD_SequencePosition)(unsafe.Pointer(bp)))
		lastBlock = libc.BoolUint32(blockSize == remaining)
		err_code = blockSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5817, 0)
			}
			return err_code
		}
		ZSTD_resetSeqStore(tls, cctx+976)
		blockSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, size_t, uintptr, size_t, ZSTD_ParamSwitch_e) size_t)(unsafe.Pointer(&struct{ uintptr }{sequenceCopier})))(tls, cctx, bp, inSeqs, inSeqsSize, ip, blockSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FsearchForExternalRepcodes)
		err_code1 = blockSize
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5860, 0)
			}
			return err_code1
		}
		/* If blocks are too small, emit as a nocompress block */
		/* TODO: See 3090. We reduced MIN_CBLOCK_SIZE from 3 to 2 so to compensate we are adding
		 * additional 1. We need to revisit and change this logic to be more consistent */
		if blockSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)+libc.Int32FromInt32(1))+ZSTD_blockHeaderSize+uint64(1)+uint64(1) {
			cBlockSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, blockSize, lastBlock)
			err_code2 = cBlockSize
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+3619, 0)
				}
				return err_code2
			}
			cSize = cSize + cBlockSize
			ip = ip + uintptr(blockSize)
			op = op + uintptr(cBlockSize)
			remaining = remaining - blockSize
			dstCapacity = dstCapacity - cBlockSize
			continue
		}
		if dstCapacity < ZSTD_blockHeaderSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5878, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		compressedSeqsSize = ZSTD_entropyCompressSeqStore(tls, cctx+976, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock, cctx+240, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, blockSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2)
		err_code3 = compressedSeqsSize
		if ERR_isError(tls, err_code3) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5933, 0)
			}
			return err_code3
		}
		if !((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock != 0) && ZSTD_maybeRLE(tls, cctx+976) != 0 && ZSTD_isRLE(tls, ip, blockSize) != 0 {
			/* Note: don't emit the first block as RLE even if it qualifies because
			 * doing so will cause the decoder (cli <= v1.4.3 only) to throw an (invalid) error
			 * "should consume all input error."
			 */
			compressedSeqsSize = uint64(1)
		}
		if compressedSeqsSize == uint64(0) {
			/* ZSTD_noCompressBlock writes the block header as well */
			cBlockSize = ZSTD_noCompressBlock(tls, op, dstCapacity, ip, blockSize, lastBlock)
			err_code4 = cBlockSize
			if ERR_isError(tls, err_code4) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1741, 0)
				}
				return err_code4
			}
		} else {
			if compressedSeqsSize == uint64(1) {
				cBlockSize = ZSTD_rleCompressBlock(tls, op, dstCapacity, *(*BYTE)(unsafe.Pointer(ip)), blockSize, lastBlock)
				err_code5 = cBlockSize
				if ERR_isError(tls, err_code5) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+5971, 0)
					}
					return err_code5
				}
			} else {
				/* Error checking and repcodes update */
				ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, cctx+3224)
				if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
					(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
				}
				/* Write block header into beginning of block*/
				cBlockHeader = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(compressedSeqsSize<<libc.Int32FromInt32(3))
				MEM_writeLE24(tls, op, cBlockHeader)
				cBlockSize = ZSTD_blockHeaderSize + compressedSeqsSize
			}
		}
		cSize = cSize + cBlockSize
		if lastBlock != 0 {
			break
		} else {
			ip = ip + uintptr(blockSize)
			op = op + uintptr(cBlockSize)
			remaining = remaining - blockSize
			dstCapacity = dstCapacity - cBlockSize
			(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock = 0
		}
	}
	return cSize
}

func ZSTD_compressSequences(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, inSeqsSize size_t, src uintptr, srcSize size_t) (r size_t) {
	var cBlocksSize, cSize, err_code, err_code1, frameHeaderSize size_t
	var checksum U32
	var op uintptr
	_, _, _, _, _, _, _ = cBlocksSize, cSize, checksum, err_code, err_code1, frameHeaderSize, op
	op = dst
	cSize = uint64(0)
	/* Transparent initialization stage, same as compressStream2() */
	err_code = ZSTD_CCtx_init_compressStream2(tls, cctx, int32(ZSTD_e_end), srcSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6000, 0)
		}
		return err_code
	}
	/* Begin writing output, starting with frame header */
	frameHeaderSize = ZSTD_writeFrameHeader(tls, op, dstCapacity, cctx+240, srcSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID)
	op = op + uintptr(frameHeaderSize)
	dstCapacity = dstCapacity - frameHeaderSize
	cSize = cSize + frameHeaderSize
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 && srcSize != 0 {
		XXH_INLINE_XXH64_update(tls, cctx+808, src, srcSize)
	}
	/* Now generate compressed blocks */
	cBlocksSize = ZSTD_compressSequences_internal(tls, cctx, op, dstCapacity, inSeqs, inSeqsSize, src, srcSize)
	err_code1 = cBlocksSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6027, 0)
		}
		return err_code1
	}
	cSize = cSize + cBlocksSize
	dstCapacity = dstCapacity - cBlocksSize
	/* Complete with frame checksum, if needed */
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 {
		checksum = uint32(XXH_INLINE_XXH64_digest(tls, cctx+808))
		if dstCapacity < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+4582, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE32(tls, dst+uintptr(cSize), checksum)
		cSize = cSize + uint64(4)
	}
	return cSize
}

func convertSequences_noRepcodes(tls *libc.TLS, dstSeqs uintptr, inSeqs uintptr, nbSequences size_t) (r size_t) {
	var longLen, n size_t
	_, _ = longLen, n
	longLen = uint64(0)
	n = uint64(0)
	for {
		if !(n < nbSequences) {
			break
		}
		(*(*SeqDef)(unsafe.Pointer(dstSeqs + uintptr(n)*8))).FoffBase = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).Foffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
		(*(*SeqDef)(unsafe.Pointer(dstSeqs + uintptr(n)*8))).FlitLength = uint16((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FlitLength)
		(*(*SeqDef)(unsafe.Pointer(dstSeqs + uintptr(n)*8))).FmlBase = uint16((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FmatchLength - libc.Uint32FromInt32(MINMATCH))
		/* check for long length > 65535 */
		if libc.BoolInt64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FmatchLength > libc.Uint32FromInt32(libc.Int32FromInt32(65535)+libc.Int32FromInt32(MINMATCH))) != 0 {
			longLen = n + uint64(1)
		}
		if libc.BoolInt64((*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(n)*16))).FlitLength > libc.Uint32FromInt32(65535)) != 0 {
			longLen = n + nbSequences + uint64(1)
		}
		goto _1
	_1:
		;
		n = n + 1
	}
	return longLen
}

// C documentation
//
//	/*
//	 * Precondition: Sequences must end on an explicit Block Delimiter
//	 * @return: 0 on success, or an error code.
//	 * Note: Sequence validation functionality has been disabled (removed).
//	 * This is helpful to generate a lean main pipeline, improving performance.
//	 * It may be re-inserted later.
//	 */
func ZSTD_convertBlockSequences(tls *libc.TLS, cctx uintptr, inSeqs uintptr, nbSequences size_t, repcodeResolution int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var lastSeqIdx, litLength, ll0, matchLength, offBase U32
	var longl, seqNb size_t
	var rep uintptr
	var _ /* updatedRepcodes at bp+0 */ Repcodes_t
	_, _, _, _, _, _, _, _ = lastSeqIdx, litLength, ll0, longl, matchLength, offBase, rep, seqNb
	seqNb = uint64(0)
	if nbSequences >= (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FmaxNbSeq {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+5371, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	libc.Xmemcpy(tls, bp, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock+5616, libc.Uint64FromInt64(12))
	/* check end condition */
	/* Convert Sequences from public format to internal format */
	if !(repcodeResolution != 0) {
		longl = convertSequences_noRepcodes(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FsequencesStart, inSeqs, nbSequences-uint64(1))
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.Fsequences = (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FsequencesStart + uintptr(nbSequences)*8 - uintptr(1)*8
		if longl != 0 {
			if longl <= nbSequences-uint64(1) {
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthType = int32(ZSTD_llt_matchLength)
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthPos = uint32(longl - libc.Uint64FromInt32(1))
			} else {
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthType = int32(ZSTD_llt_literalLength)
				(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FseqStore.FlongLengthPos = uint32(longl - (nbSequences - libc.Uint64FromInt32(1)) - libc.Uint64FromInt32(1))
			}
		}
	} else {
		seqNb = uint64(0)
		for {
			if !(seqNb < nbSequences-uint64(1)) {
				break
			}
			litLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(seqNb)*16))).FlitLength
			matchLength = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(seqNb)*16))).FmatchLength
			ll0 = libc.BoolUint32(litLength == libc.Uint32FromInt32(0))
			offBase = ZSTD_finalizeOffBase(tls, (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(seqNb)*16))).Foffset, bp, ll0)
			ZSTD_storeSeqOnly(tls, cctx+976, uint64(litLength), offBase, uint64(matchLength))
			ZSTD_updateRep(tls, bp, offBase, ll0)
			goto _1
		_1:
			;
			seqNb = seqNb + 1
		}
	}
	/* If we skipped repcode search while parsing, we need to update repcodes now */
	if !(repcodeResolution != 0) && nbSequences > uint64(1) {
		rep = bp
		if nbSequences >= uint64(4) {
			lastSeqIdx = uint32(nbSequences) - uint32(2) /* index of last full sequence */
			*(*U32)(unsafe.Pointer(rep + 2*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(2))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx-uint32(1))*16))).Foffset
			*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + uintptr(lastSeqIdx)*16))).Foffset
		} else {
			if nbSequences == uint64(3) {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs))).Foffset
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs + 1*16))).Foffset
			} else {
				*(*U32)(unsafe.Pointer(rep + 2*4)) = *(*U32)(unsafe.Pointer(rep + 1*4))
				*(*U32)(unsafe.Pointer(rep + 1*4)) = *(*U32)(unsafe.Pointer(rep))
				*(*U32)(unsafe.Pointer(rep)) = (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs))).Foffset
			}
		}
	}
	libc.Xmemcpy(tls, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock+5616, bp, libc.Uint64FromInt64(12))
	return uint64(0)
}

func ZSTD_get1BlockSummary(tls *libc.TLS, seqs uintptr, nbSeqs size_t) (r BlockSummary) {
	var bs, bs1 BlockSummary
	var litSize, n, totalMatchSize size_t
	_, _, _, _, _ = bs, bs1, litSize, n, totalMatchSize
	totalMatchSize = uint64(0)
	litSize = uint64(0)
	n = uint64(0)
	for {
		if !(n < nbSeqs) {
			break
		}
		totalMatchSize = totalMatchSize + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqs + uintptr(n)*16))).FmatchLength)
		litSize = litSize + uint64((*(*ZSTD_Sequence)(unsafe.Pointer(seqs + uintptr(n)*16))).FlitLength)
		if (*(*ZSTD_Sequence)(unsafe.Pointer(seqs + uintptr(n)*16))).FmatchLength == uint32(0) {
			break
		}
		goto _1
	_1:
		;
		n = n + 1
	}
	if n == nbSeqs {
		bs.FnbSequences = libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
		return bs
	}
	bs1.FnbSequences = n + uint64(1)
	bs1.FblockSize = litSize + totalMatchSize
	bs1.FlitSize = litSize
	return bs1
	return r
}

func ZSTD_compressSequencesAndLiterals_internal(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, nbSequences size_t, literals uintptr, litSize size_t, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var block BlockSummary
	var cBlockHeader, cBlockHeader24, lastBlock U32
	var cBlockSize, cSize, compressedSeqsSize, conversionStatus, err_code, err_code1, err_code2, remaining size_t
	var op uintptr
	var repcodeResolution int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = block, cBlockHeader, cBlockHeader24, cBlockSize, cSize, compressedSeqsSize, conversionStatus, err_code, err_code1, err_code2, lastBlock, op, remaining, repcodeResolution
	remaining = srcSize
	cSize = uint64(0)
	op = dst
	repcodeResolution = libc.BoolInt32((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FsearchForExternalRepcodes == int32(ZSTD_ps_enable))
	if nbSequences == uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6054, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	/* Special case: empty frame */
	if nbSequences == uint64(1) && (*(*ZSTD_Sequence)(unsafe.Pointer(inSeqs))).FlitLength == uint32(0) {
		cBlockHeader24 = libc.Uint32FromInt32(1) + uint32(bt_raw)<<libc.Int32FromInt32(1)
		if dstCapacity < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5780, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		MEM_writeLE24(tls, op, cBlockHeader24)
		op = op + uintptr(ZSTD_blockHeaderSize)
		dstCapacity = dstCapacity - ZSTD_blockHeaderSize
		cSize = cSize + ZSTD_blockHeaderSize
	}
	for nbSequences != 0 {
		block = ZSTD_get1BlockSummary(tls, inSeqs, nbSequences)
		lastBlock = libc.BoolUint32(block.FnbSequences == nbSequences)
		err_code = block.FnbSequences
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6087, 0)
			}
			return err_code
		}
		if block.FlitSize > litSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6147, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
		}
		ZSTD_resetSeqStore(tls, cctx+976)
		conversionStatus = ZSTD_convertBlockSequences(tls, cctx, inSeqs, block.FnbSequences, repcodeResolution)
		err_code1 = conversionStatus
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6215, 0)
			}
			return err_code1
		}
		inSeqs = inSeqs + uintptr(block.FnbSequences)*16
		nbSequences = nbSequences - block.FnbSequences
		remaining = remaining - block.FblockSize
		/* Note: when blockSize is very small, other variant send it uncompressed.
		 * Here, we still send the sequences, because we don't have the original source to send it uncompressed.
		 * One could imagine in theory reproducing the source from the sequences,
		 * but that's complex and costly memory intensive, and goes against the objectives of this variant. */
		if dstCapacity < ZSTD_blockHeaderSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5878, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		compressedSeqsSize = ZSTD_entropyCompressSeqStore_internal(tls, op+uintptr(ZSTD_blockHeaderSize), dstCapacity-ZSTD_blockHeaderSize, literals, block.FlitSize, cctx+976, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FnextCBlock, cctx+240, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWorkspace, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FtmpWkspSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).Fbmi2)
		err_code2 = compressedSeqsSize
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+5933, 0)
			}
			return err_code2
		}
		/* note: the spec forbids for any compressed block to be larger than maximum block size */
		if compressedSeqsSize > (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockSizeMax {
			compressedSeqsSize = uint64(0)
		}
		litSize = litSize - block.FlitSize
		literals = literals + uintptr(block.FlitSize)
		/* Note: difficult to check source for RLE block when only Literals are provided,
		 * but it could be considered from analyzing the sequence directly */
		if compressedSeqsSize == uint64(0) {
			/* Sending uncompressed blocks is out of reach, because the source is not provided.
			 * In theory, one could use the sequences to regenerate the source, like a decompressor,
			 * but it's complex, and memory hungry, killing the purpose of this variant.
			 * Current outcome: generate an error code.
			 */
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6239, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_cannotProduce_uncompressedBlock))
		} else {
			/* no RLE */
			/* Error checking and repcodes update */
			ZSTD_blockState_confirmRepcodesAndEntropyTables(tls, cctx+3224)
			if (*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode == int32(FSE_repeat_valid) {
				(*ZSTD_compressedBlockState_t)(unsafe.Pointer((*ZSTD_CCtx)(unsafe.Pointer(cctx)).FblockState.FprevCBlock)).Fentropy.Ffse.Foffcode_repeatMode = int32(FSE_repeat_check)
			}
			/* Write block header into beginning of block*/
			cBlockHeader = lastBlock + uint32(bt_compressed)<<libc.Int32FromInt32(1) + uint32(compressedSeqsSize<<libc.Int32FromInt32(3))
			MEM_writeLE24(tls, op, cBlockHeader)
			cBlockSize = ZSTD_blockHeaderSize + compressedSeqsSize
		}
		cSize = cSize + cBlockSize
		op = op + uintptr(cBlockSize)
		dstCapacity = dstCapacity - cBlockSize
		(*ZSTD_CCtx)(unsafe.Pointer(cctx)).FisFirstBlock = 0
		if lastBlock != 0 {
			break
		}
	}
	if litSize != uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6311, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	if remaining != uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6358, libc.VaList(bp+8, srcSize))
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_externalSequences_invalid))
	}
	return cSize
}

func ZSTD_compressSequencesAndLiterals(tls *libc.TLS, cctx uintptr, dst uintptr, dstCapacity size_t, inSeqs uintptr, inSeqsSize size_t, literals uintptr, litSize size_t, litCapacity size_t, decompressedSize size_t) (r size_t) {
	var cBlocksSize, cSize, err_code, err_code1, frameHeaderSize size_t
	var op uintptr
	_, _, _, _, _, _ = cBlocksSize, cSize, err_code, err_code1, frameHeaderSize, op
	op = dst
	cSize = uint64(0)
	/* Transparent initialization stage, same as compressStream2() */
	if litCapacity < litSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6414, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_workSpace_tooSmall))
	}
	err_code = ZSTD_CCtx_init_compressStream2(tls, cctx, int32(ZSTD_e_end), decompressedSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6000, 0)
		}
		return err_code
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FblockDelimiters == int32(ZSTD_sf_noBlockDelimiters) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6524, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_unsupported))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FvalidateSequences != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6578, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	}
	if (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FappliedParams.FfParams.FchecksumFlag != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6631, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_unsupported))
	}
	/* Begin writing output, starting with frame header */
	frameHeaderSize = ZSTD_writeFrameHeader(tls, op, dstCapacity, cctx+240, decompressedSize, (*ZSTD_CCtx)(unsafe.Pointer(cctx)).FdictID)
	op = op + uintptr(frameHeaderSize)
	dstCapacity = dstCapacity - frameHeaderSize
	cSize = cSize + frameHeaderSize
	/* Now generate compressed blocks */
	cBlocksSize = ZSTD_compressSequencesAndLiterals_internal(tls, cctx, op, dstCapacity, inSeqs, inSeqsSize, literals, litSize, decompressedSize)
	err_code1 = cBlocksSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6027, 0)
		}
		return err_code1
	}
	cSize = cSize + cBlocksSize
	dstCapacity = dstCapacity - cBlocksSize
	return cSize
}

/*======   Finalize   ======*/

func inBuffer_forEndFlush(tls *libc.TLS, zcs uintptr) (r ZSTD_inBuffer) {
	var nullInput, v1 ZSTD_inBuffer
	var stableInput int32
	_, _, _ = nullInput, stableInput, v1
	nullInput = ZSTD_inBuffer{}
	stableInput = libc.BoolInt32((*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FinBufferMode == int32(ZSTD_bm_stable))
	if stableInput != 0 {
		v1 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FexpectedInBuffer
	} else {
		v1 = nullInput
	}
	return v1
}

// C documentation
//
//	/*! ZSTD_flushStream() :
//	 * @return : amount of data remaining to flush */
func ZSTD_flushStream(tls *libc.TLS, zcs uintptr, output uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _ /* input at bp+0 */ ZSTD_inBuffer
	*(*ZSTD_inBuffer)(unsafe.Pointer(bp)) = inBuffer_forEndFlush(tls, zcs)
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp))).Fsize = (*(*ZSTD_inBuffer)(unsafe.Pointer(bp))).Fpos /* do not ingest more input during flush */
	return ZSTD_compressStream2(tls, zcs, output, bp, int32(ZSTD_e_flush))
}

func ZSTD_endStream(tls *libc.TLS, zcs uintptr, output uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var checksumSize, err_code, lastBlockSize, remainingToFlush, toFlush size_t
	var v1, v2 int32
	var _ /* input at bp+0 */ ZSTD_inBuffer
	_, _, _, _, _, _, _ = checksumSize, err_code, lastBlockSize, remainingToFlush, toFlush, v1, v2
	*(*ZSTD_inBuffer)(unsafe.Pointer(bp)) = inBuffer_forEndFlush(tls, zcs)
	remainingToFlush = ZSTD_compressStream2(tls, zcs, output, bp, int32(ZSTD_e_end))
	err_code = remainingToFlush
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6679, 0)
		}
		return err_code
	}
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FnbWorkers > 0 {
		return remainingToFlush
	} /* minimal estimation */
	/* single thread mode : attempt to calculate remaining to flush more precisely */
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
		v1 = 0
	} else {
		v1 = int32(ZSTD_BLOCKHEADERSIZE)
	}
	lastBlockSize = libc.Uint64FromInt32(v1)
	if (*ZSTD_CStream)(unsafe.Pointer(zcs)).FframeEnded != 0 {
		v2 = 0
	} else {
		v2 = (*ZSTD_CStream)(unsafe.Pointer(zcs)).FappliedParams.FfParams.FchecksumFlag * int32(4)
	}
	checksumSize = libc.Uint64FromInt32(v2)
	toFlush = remainingToFlush + lastBlockSize + checksumSize
	return toFlush
	return r
}

/*-=====  Pre-defined compression levels  =====-*/
/**** start inlining clevels.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../zstd.h ****/

/*-=====  Pre-defined compression levels  =====-*/

var ZSTD_defaultCParameters = [4][23]ZSTD_compressionParameters{
	0: {
		0: {
			FwindowLog:    uint32(19),
			FchainLog:     uint32(12),
			FhashLog:      uint32(13),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(6),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(19),
			FchainLog:  uint32(13),
			FhashLog:   uint32(14),
			FsearchLog: uint32(1),
			FminMatch:  uint32(7),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(20),
			FchainLog:  uint32(15),
			FhashLog:   uint32(16),
			FsearchLog: uint32(1),
			FminMatch:  uint32(6),
			Fstrategy:  int32(ZSTD_fast),
		},
		3: {
			FwindowLog: uint32(21),
			FchainLog:  uint32(16),
			FhashLog:   uint32(17),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog: uint32(21),
			FchainLog:  uint32(18),
			FhashLog:   uint32(18),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		5: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		6: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		7: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(19),
			FhashLog:      uint32(20),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy),
		},
		8: {
			FwindowLog:    uint32(21),
			FchainLog:     uint32(19),
			FhashLog:      uint32(20),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(20),
			FhashLog:      uint32(21),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		10: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(21),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		11: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(21),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(5),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		12: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		13: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		14: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		15: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(23),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(5),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		16: {
			FwindowLog:    uint32(22),
			FchainLog:     uint32(22),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(48),
			Fstrategy:     int32(ZSTD_btopt),
		},
		17: {
			FwindowLog:    uint32(23),
			FchainLog:     uint32(23),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(64),
			Fstrategy:     int32(ZSTD_btopt),
		},
		18: {
			FwindowLog:    uint32(23),
			FchainLog:     uint32(23),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(64),
			Fstrategy:     int32(ZSTD_btultra),
		},
		19: {
			FwindowLog:    uint32(23),
			FchainLog:     uint32(24),
			FhashLog:      uint32(22),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(25),
			FchainLog:     uint32(25),
			FhashLog:      uint32(23),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(26),
			FchainLog:     uint32(26),
			FhashLog:      uint32(24),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(27),
			FchainLog:     uint32(27),
			FhashLog:      uint32(25),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
	1: {
		0: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(12),
			FhashLog:      uint32(13),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(5),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(18),
			FchainLog:  uint32(13),
			FhashLog:   uint32(14),
			FsearchLog: uint32(1),
			FminMatch:  uint32(6),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(18),
			FchainLog:  uint32(14),
			FhashLog:   uint32(14),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		3: {
			FwindowLog: uint32(18),
			FchainLog:  uint32(16),
			FhashLog:   uint32(16),
			FsearchLog: uint32(1),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		5: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(17),
			FhashLog:      uint32(18),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(5),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		6: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(5),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		7: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		8: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		10: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		11: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		12: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		13: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(16),
			Fstrategy:     int32(ZSTD_btopt),
		},
		14: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(3),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btopt),
		},
		15: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(18),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btopt),
		},
		16: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra),
		},
		17: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra),
		},
		18: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		19: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(10),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(12),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(18),
			FchainLog:     uint32(19),
			FhashLog:      uint32(19),
			FsearchLog:    uint32(13),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
	2: {
		0: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(12),
			FhashLog:      uint32(12),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(5),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(12),
			FhashLog:   uint32(13),
			FsearchLog: uint32(1),
			FminMatch:  uint32(6),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(13),
			FhashLog:   uint32(15),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_fast),
		},
		3: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(15),
			FhashLog:   uint32(16),
			FsearchLog: uint32(2),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog: uint32(17),
			FchainLog:  uint32(17),
			FhashLog:   uint32(17),
			FsearchLog: uint32(2),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_dfast),
		},
		5: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		6: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		7: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		8: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		10: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(16),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		11: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(17),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		12: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		13: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btopt),
		},
		14: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(3),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btopt),
		},
		15: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btopt),
		},
		16: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra),
		},
		17: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra),
		},
		18: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(10),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra),
		},
		19: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(17),
			FchainLog:     uint32(18),
			FhashLog:      uint32(17),
			FsearchLog:    uint32(11),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
	3: {
		0: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(12),
			FhashLog:      uint32(13),
			FsearchLog:    uint32(1),
			FminMatch:     uint32(5),
			FtargetLength: uint32(1),
			Fstrategy:     int32(ZSTD_fast),
		},
		1: {
			FwindowLog: uint32(14),
			FchainLog:  uint32(14),
			FhashLog:   uint32(15),
			FsearchLog: uint32(1),
			FminMatch:  uint32(5),
			Fstrategy:  int32(ZSTD_fast),
		},
		2: {
			FwindowLog: uint32(14),
			FchainLog:  uint32(14),
			FhashLog:   uint32(15),
			FsearchLog: uint32(1),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_fast),
		},
		3: {
			FwindowLog: uint32(14),
			FchainLog:  uint32(14),
			FhashLog:   uint32(15),
			FsearchLog: uint32(2),
			FminMatch:  uint32(4),
			Fstrategy:  int32(ZSTD_dfast),
		},
		4: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(2),
			Fstrategy:     int32(ZSTD_greedy),
		},
		5: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(4),
			Fstrategy:     int32(ZSTD_lazy),
		},
		6: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		7: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		8: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(14),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_lazy2),
		},
		9: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		10: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(4),
			FtargetLength: uint32(8),
			Fstrategy:     int32(ZSTD_btlazy2),
		},
		11: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(3),
			FminMatch:     uint32(4),
			FtargetLength: uint32(12),
			Fstrategy:     int32(ZSTD_btopt),
		},
		12: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(4),
			FminMatch:     uint32(3),
			FtargetLength: uint32(24),
			Fstrategy:     int32(ZSTD_btopt),
		},
		13: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(14),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(3),
			FtargetLength: uint32(32),
			Fstrategy:     int32(ZSTD_btultra),
		},
		14: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(64),
			Fstrategy:     int32(ZSTD_btultra),
		},
		15: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra),
		},
		16: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(5),
			FminMatch:     uint32(3),
			FtargetLength: uint32(48),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		17: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(6),
			FminMatch:     uint32(3),
			FtargetLength: uint32(128),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		18: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(7),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		19: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(256),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		20: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(8),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		21: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(9),
			FminMatch:     uint32(3),
			FtargetLength: uint32(512),
			Fstrategy:     int32(ZSTD_btultra2),
		},
		22: {
			FwindowLog:    uint32(14),
			FchainLog:     uint32(15),
			FhashLog:      uint32(15),
			FsearchLog:    uint32(10),
			FminMatch:     uint32(3),
			FtargetLength: uint32(999),
			Fstrategy:     int32(ZSTD_btultra2),
		},
	},
}

/**** ended inlining clevels.h ****/

func ZSTD_maxCLevel(tls *libc.TLS) (r int32) {
	return int32(ZSTD_MAX_CLEVEL)
}

func ZSTD_minCLevel(tls *libc.TLS) (r int32) {
	return -(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
}

func ZSTD_defaultCLevel(tls *libc.TLS) (r int32) {
	return int32(ZSTD_CLEVEL_DEFAULT)
}

func ZSTD_dedicatedDictSearch_getCParams(tls *libc.TLS, compressionLevel int32, dictSize size_t) (r ZSTD_compressionParameters) {
	var cParams ZSTD_compressionParameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, uint64(0), dictSize, int32(ZSTD_cpm_createCDict))
	switch cParams.Fstrategy {
	case int32(ZSTD_fast):
		fallthrough
	case int32(ZSTD_dfast):
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		cParams.FhashLog += uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
		break
	}
	return cParams
}

func ZSTD_dedicatedDictSearch_isSupported(tls *libc.TLS, cParams uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_greedy) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy <= int32(ZSTD_lazy2) && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog > (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog && (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog <= uint32(24))
}

// C documentation
//
//	/**
//	 * Reverses the adjustment applied to cparams when enabling dedicated dict
//	 * search. This is used to recover the params set to be used in the working
//	 * context. (Otherwise, those tables would also grow.)
//	 */
func ZSTD_dedicatedDictSearch_revertCParams(tls *libc.TLS, cParams uintptr) {
	switch (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy {
	case int32(ZSTD_fast):
		fallthrough
	case int32(ZSTD_dfast):
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		*(*uint32)(unsafe.Pointer(cParams + 8)) -= uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog < uint32(ZSTD_HASHLOG_MIN) {
			(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog = uint32(ZSTD_HASHLOG_MIN)
		}
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
		break
	}
}

func ZSTD_getCParamRowSize(tls *libc.TLS, srcSizeHint U64, dictSize size_t, mode ZSTD_CParamMode_e) (r U64) {
	var addedSize size_t
	var unknown, v1 int32
	var v2 uint64
	_, _, _, _ = addedSize, unknown, v1, v2
	switch mode {
	case int32(ZSTD_cpm_unknown):
		fallthrough
	case int32(ZSTD_cpm_noAttachDict):
		fallthrough
	case int32(ZSTD_cpm_createCDict):
	case int32(ZSTD_cpm_attachDict):
		dictSize = uint64(0)
	default:
		break
	}
	unknown = libc.BoolInt32(srcSizeHint == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
	if unknown != 0 && dictSize > uint64(0) {
		v1 = int32(500)
	} else {
		v1 = 0
	}
	addedSize = libc.Uint64FromInt32(v1)
	if unknown != 0 && dictSize == uint64(0) {
		v2 = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	} else {
		v2 = srcSizeHint + dictSize + addedSize
	}
	return v2
	return r
}

// C documentation
//
//	/*! ZSTD_getCParams_internal() :
//	 * @return ZSTD_compressionParameters structure for a selected compression level, srcSize and dictSize.
//	 *  Note: srcSizeHint 0 means 0, use ZSTD_CONTENTSIZE_UNKNOWN for unknown.
//	 *        Use dictSize == 0 for unknown or unused.
//	 *  Note: `mode` controls how we treat the `dictSize`. See docs for `ZSTD_CParamMode_e`. */
func ZSTD_getCParams_internal(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t, mode ZSTD_CParamMode_e) (r ZSTD_compressionParameters) {
	var clampedCompressionLevel, row, v1 int32
	var cp ZSTD_compressionParameters
	var rSize U64
	var tableID U32
	_, _, _, _, _, _ = clampedCompressionLevel, cp, rSize, row, tableID, v1
	rSize = ZSTD_getCParamRowSize(tls, srcSizeHint, dictSize, mode)
	tableID = libc.Uint32FromInt32(libc.BoolInt32(rSize <= libc.Uint64FromInt32(libc.Int32FromInt32(256)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(rSize <= libc.Uint64FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))) + libc.BoolInt32(rSize <= libc.Uint64FromInt32(libc.Int32FromInt32(16)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	/* row */
	if compressionLevel == 0 {
		row = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		if compressionLevel < 0 {
			row = 0
		} else {
			if compressionLevel > int32(ZSTD_MAX_CLEVEL) {
				row = int32(ZSTD_MAX_CLEVEL)
			} else {
				row = compressionLevel
			}
		}
	}
	cp = *(*ZSTD_compressionParameters)(unsafe.Pointer(uintptr(unsafe.Pointer(&ZSTD_defaultCParameters)) + uintptr(tableID)*644 + uintptr(row)*28))
	/* acceleration factor */
	if compressionLevel < 0 {
		if ZSTD_minCLevel(tls) > compressionLevel {
			v1 = ZSTD_minCLevel(tls)
		} else {
			v1 = compressionLevel
		}
		clampedCompressionLevel = v1
		cp.FtargetLength = libc.Uint32FromInt32(-clampedCompressionLevel)
	}
	/* refine parameters based on srcSize & dictSize */
	return ZSTD_adjustCParams_internal(tls, cp, srcSizeHint, dictSize, mode, int32(ZSTD_ps_auto))
	return r
}

// C documentation
//
//	/*! ZSTD_getCParams() :
//	 * @return ZSTD_compressionParameters structure for a selected compression level, srcSize and dictSize.
//	 *  Size values are optional, provide 0 if not known or unused */
func ZSTD_getCParams(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t) (r ZSTD_compressionParameters) {
	if srcSizeHint == uint64(0) {
		srcSizeHint = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	return ZSTD_getCParams_internal(tls, compressionLevel, srcSizeHint, dictSize, int32(ZSTD_cpm_unknown))
}

// C documentation
//
//	/*! ZSTD_getParams() :
//	 *  same idea as ZSTD_getCParams()
//	 * @return a `ZSTD_parameters` structure (instead of `ZSTD_compressionParameters`).
//	 *  Fields of `ZSTD_frameParameters` are set to default values */
func ZSTD_getParams_internal(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t, mode ZSTD_CParamMode_e) (r ZSTD_parameters) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var cParams ZSTD_compressionParameters
	var _ /* params at bp+0 */ ZSTD_parameters
	_ = cParams
	cParams = ZSTD_getCParams_internal(tls, compressionLevel, srcSizeHint, dictSize, mode)
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(40))
	(*(*ZSTD_parameters)(unsafe.Pointer(bp))).FcParams = cParams
	(*(*ZSTD_parameters)(unsafe.Pointer(bp))).FfParams.FcontentSizeFlag = int32(1)
	return *(*ZSTD_parameters)(unsafe.Pointer(bp))
}

// C documentation
//
//	/*! ZSTD_getParams() :
//	 *  same idea as ZSTD_getCParams()
//	 * @return a `ZSTD_parameters` structure (instead of `ZSTD_compressionParameters`).
//	 *  Fields of `ZSTD_frameParameters` are set to default values */
func ZSTD_getParams(tls *libc.TLS, compressionLevel int32, srcSizeHint uint64, dictSize size_t) (r ZSTD_parameters) {
	if srcSizeHint == uint64(0) {
		srcSizeHint = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1)
	}
	return ZSTD_getParams_internal(tls, compressionLevel, srcSizeHint, dictSize, int32(ZSTD_cpm_unknown))
}

type __ccgo_fp__XZSTD_registerSequenceProducer_2 = func(*libc.TLS, uintptr, uintptr, uint64, uintptr, uint64, uintptr, uint64, int32, uint64) uint64

func ZSTD_registerSequenceProducer(tls *libc.TLS, zc uintptr, extSeqProdState uintptr, __ccgo_fp_extSeqProdFunc ZSTD_sequenceProducer_F) {
	ZSTD_CCtxParams_registerSequenceProducer(tls, zc+16, extSeqProdState, __ccgo_fp_extSeqProdFunc)
}

type __ccgo_fp__XZSTD_CCtxParams_registerSequenceProducer_2 = func(*libc.TLS, uintptr, uintptr, uint64, uintptr, uint64, uintptr, uint64, int32, uint64) uint64

func ZSTD_CCtxParams_registerSequenceProducer(tls *libc.TLS, params uintptr, extSeqProdState uintptr, __ccgo_fp_extSeqProdFunc ZSTD_sequenceProducer_F) {
	if __ccgo_fp_extSeqProdFunc != libc.UintptrFromInt32(0) {
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdFunc = __ccgo_fp_extSeqProdFunc
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdState = extSeqProdState
	} else {
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdFunc = libc.UintptrFromInt32(0)
		(*ZSTD_CCtx_params)(unsafe.Pointer(params)).FextSeqProdState = libc.UintptrFromInt32(0)
	}
}

/**** ended inlining compress/zstd_compress.c ****/
/**** start inlining compress/zstd_double_fast.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_double_fast.h ****/

func ZSTD_fillDoubleHashTableForCDict(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashLarge, hashSmall, iend, ip uintptr
	var curr, fastHashFillStep, hBitsL, hBitsS, i, mls U32
	var lgHashAndTag, smHashAndTag size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBitsL, hBitsS, hashLarge, hashSmall, i, iend, ip, lgHashAndTag, mls, smHashAndTag
	cParams = ms + 256
	hashLarge = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Always insert every fastHashFillStep position into the hash tables.
	 * Insert the other positions into the large hash table if their entry
	 * is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep)-uintptr(1) <= iend) {
			break
		}
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		i = uint32(0)
		for {
			if !(i < fastHashFillStep) {
				break
			}
			smHashAndTag = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsS, mls)
			lgHashAndTag = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsL, uint32(8))
			if i == uint32(0) {
				ZSTD_writeTaggedIndex(tls, hashSmall, smHashAndTag, curr+i)
			}
			if i == uint32(0) || *(*U32)(unsafe.Pointer(hashLarge + uintptr(lgHashAndTag>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4)) == uint32(0) {
				ZSTD_writeTaggedIndex(tls, hashLarge, lgHashAndTag, curr+i)
			}
			/* Only load extra positions for ZSTD_dtlm_full */
			if dtlm == int32(ZSTD_dtlm_fast) {
				break
			}
			goto _2
		_2:
			;
			i = i + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillDoubleHashTableForCCtx(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashLarge, hashSmall, iend, ip uintptr
	var curr, fastHashFillStep, hBitsL, hBitsS, i, mls U32
	var lgHash, smHash size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBitsL, hBitsS, hashLarge, hashSmall, i, iend, ip, lgHash, mls, smHash
	cParams = ms + 256
	hashLarge = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Always insert every fastHashFillStep position into the hash tables.
	 * Insert the other positions into the large hash table if their entry
	 * is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep)-uintptr(1) <= iend) {
			break
		}
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		i = uint32(0)
		for {
			if !(i < fastHashFillStep) {
				break
			}
			smHash = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsS, mls)
			lgHash = ZSTD_hashPtr(tls, ip+uintptr(i), hBitsL, uint32(8))
			if i == uint32(0) {
				*(*U32)(unsafe.Pointer(hashSmall + uintptr(smHash)*4)) = curr + i
			}
			if i == uint32(0) || *(*U32)(unsafe.Pointer(hashLarge + uintptr(lgHash)*4)) == uint32(0) {
				*(*U32)(unsafe.Pointer(hashLarge + uintptr(lgHash)*4)) = curr + i
			}
			/* Only load extra positions for ZSTD_dtlm_full */
			if dtlm == int32(ZSTD_dtlm_fast) {
				break
			}
			goto _2
		_2:
			;
			i = i + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillDoubleHashTable(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e) {
	if tfp == int32(ZSTD_tfp_forCDict) {
		ZSTD_fillDoubleHashTableForCDict(tls, ms, end, dtlm)
	} else {
		ZSTD_fillDoubleHashTableForCCtx(tls, ms, end, dtlm)
	}
}

func ZSTD_compressBlock_doubleFast_noDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var anchor, base, cParams, hashLong, hashSmall, iend, ilimit, ip, ip1, istart, matchl0, matchl0_safe, matchl1, matchs0, matchs0_safe, nextStep, prefixLowest uintptr
	var curr, current, endIndex, hBitsL, hBitsS, idxl0, idxl1, idxs0, indexToInsert, maxRep, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowestIndex, tmpOff, windowLow, v1 U32
	var hl0, hl1, hs0, kStepIncr, l1len, mLength, rLength, step size_t
	var v2 uint32
	var _ /* dummy at bp+0 */ [10]BYTE
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current, endIndex, hBitsL, hBitsS, hashLong, hashSmall, hl0, hl1, hs0, idxl0, idxl1, idxs0, iend, ilimit, indexToInsert, ip, ip1, istart, kStepIncr, l1len, mLength, matchl0, matchl0_safe, matchl1, matchs0, matchs0_safe, maxRep, nextStep, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowest, prefixLowestIndex, rLength, step, tmpOff, windowLow, v1, v2
	cParams = ms + 256
	hashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	anchor = istart
	endIndex = uint32(libc.Uint64FromInt64(int64(istart)-int64(base)) + srcSize)
	/* presumes that, if there is a dictionary, it must be using Attach mode */
	prefixLowestIndex = ZSTD_getLowestPrefixIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	prefixLowest = base + uintptr(prefixLowestIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	/* how many positions to search before increasing step size */
	kStepIncr = libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(kSearchStrength)) /* matchs0 or safe address */
	ip = istart                                                                                      /* the next position */
	/* Array of ~random data, should have low probability of matching data
	 * we load from here instead of from tables, if matchl0/matchl1 are
	 * invalid indices. Used to avoid unpredictable branches. */
	*(*[10]BYTE)(unsafe.Pointer(bp)) = [10]BYTE{
		0: uint8(0x12),
		1: uint8(0x34),
		2: uint8(0x56),
		3: uint8(0x78),
		4: uint8(0x9a),
		5: uint8(0xbc),
		6: uint8(0xde),
		7: uint8(0xf0),
		8: uint8(0xe2),
		9: uint8(0xb4),
	}
	/* init */
	ip = ip + libc.BoolUintptr(int64(ip)-int64(prefixLowest) == libc.Int64FromInt32(0))
	current = libc.Uint32FromInt64(int64(ip) - int64(base))
	windowLow = ZSTD_getLowestPrefixIndex(tls, ms, current, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	maxRep = current - windowLow
	if offset_2 > maxRep {
		offsetSaved2 = offset_2
		offset_2 = libc.Uint32FromInt32(0)
	}
	if offset_1 > maxRep {
		offsetSaved1 = offset_1
		offset_1 = libc.Uint32FromInt32(0)
	}
	/* Outer Loop: one iteration per match found and stored */
	for int32(1) != 0 {
		step = uint64(1)
		nextStep = ip + uintptr(kStepIncr)
		ip1 = ip + uintptr(step)
		if ip1 > ilimit {
			goto _cleanup
		}
		hl0 = ZSTD_hashPtr(tls, ip, hBitsL, uint32(8))
		idxl0 = *(*U32)(unsafe.Pointer(hashLong + uintptr(hl0)*4))
		matchl0 = base + uintptr(idxl0)
		/* Inner Loop: one iteration per search / position */
		for cond := true; cond; cond = ip1 <= ilimit {
			hs0 = ZSTD_hashPtr(tls, ip, hBitsS, mls)
			idxs0 = *(*U32)(unsafe.Pointer(hashSmall + uintptr(hs0)*4))
			curr = libc.Uint32FromInt64(int64(ip) - int64(base))
			matchs0 = base + uintptr(idxs0)
			v1 = curr
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(hs0)*4)) = v1
			*(*U32)(unsafe.Pointer(hashLong + uintptr(hl0)*4)) = v1 /* update hash tables */
			/* check noDict repcode */
			if libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip+uintptr(1)-uintptr(offset_1)) == MEM_read32(tls, ip+uintptr(1))) != 0 {
				mLength = ZSTD_count(tls, ip+uintptr(1)+uintptr(4), ip+uintptr(1)+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
				ip = ip + 1
				ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip)-int64(anchor)), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), mLength)
				goto _match_stored
			}
			hl1 = ZSTD_hashPtr(tls, ip1, hBitsL, uint32(8))
			/* idxl0 > prefixLowestIndex is a (somewhat) unpredictable branch.
			 * However expression below complies into conditional move. Since
			 * match is unlikely and we only *branch* on idxl0 > prefixLowestIndex
			 * if there is a match, all branches become predictable. */
			matchl0_safe = ZSTD_selectAddr(tls, idxl0, prefixLowestIndex, matchl0, bp)
			/* check prefix long match */
			if MEM_read64(tls, matchl0_safe) == MEM_read64(tls, ip) && matchl0_safe == matchl0 {
				mLength = ZSTD_count(tls, ip+uintptr(8), matchl0+uintptr(8), iend) + uint64(8)
				offset = libc.Uint32FromInt64(int64(ip) - int64(matchl0))
				for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchl0 > prefixLowest) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(matchl0 + uintptr(-libc.Int32FromInt32(1))))) {
					ip = ip - 1
					matchl0 = matchl0 - 1
					mLength = mLength + 1
				} /* catch up */
				goto _match_found
			}
			idxl1 = *(*U32)(unsafe.Pointer(hashLong + uintptr(hl1)*4))
			matchl1 = base + uintptr(idxl1)
			/* Same optimization as matchl0 above */
			matchs0_safe = ZSTD_selectAddr(tls, idxs0, prefixLowestIndex, matchs0, bp)
			/* check prefix short match */
			if MEM_read32(tls, matchs0_safe) == MEM_read32(tls, ip) && matchs0_safe == matchs0 {
				goto _search_next_long
			}
			if ip1 >= nextStep {
				libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(64), libc.VaList(bp+24, 0, int32(3)))
				libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(128), libc.VaList(bp+24, 0, int32(3)))
				step = step + 1
				nextStep = nextStep + uintptr(kStepIncr)
			}
			ip = ip1
			ip1 = ip1 + uintptr(step)
			hl0 = hl1
			idxl0 = idxl1
			matchl0 = matchl1
		}
		goto _cleanup
	_cleanup:
		;
		/* If offset_1 started invalid (offsetSaved1 != 0) and became valid (offset_1 != 0),
		 * rotate saved offsets. See comment in ZSTD_compressBlock_fast_noDict for more context. */
		if offsetSaved1 != uint32(0) && offset_1 != uint32(0) {
			v2 = offsetSaved1
		} else {
			v2 = offsetSaved2
		}
		offsetSaved2 = v2
		/* save reps for next block */
		if offset_1 != 0 {
			v2 = offset_1
		} else {
			v2 = offsetSaved1
		}
		*(*U32)(unsafe.Pointer(rep)) = v2
		if offset_2 != 0 {
			v2 = offset_2
		} else {
			v2 = offsetSaved2
		}
		*(*U32)(unsafe.Pointer(rep + 1*4)) = v2
		/* Return the last literals size */
		return libc.Uint64FromInt64(int64(iend) - int64(anchor))
		goto _search_next_long
	_search_next_long:
		;
		/* short match found: let's check for a longer one */
		mLength = ZSTD_count(tls, ip+uintptr(4), matchs0+uintptr(4), iend) + uint64(4)
		offset = libc.Uint32FromInt64(int64(ip) - int64(matchs0))
		/* check long match at +1 position */
		if idxl1 > prefixLowestIndex && MEM_read64(tls, matchl1) == MEM_read64(tls, ip1) {
			l1len = ZSTD_count(tls, ip1+uintptr(8), matchl1+uintptr(8), iend) + uint64(8)
			if l1len > mLength {
				/* use the long match instead */
				ip = ip1
				mLength = l1len
				offset = libc.Uint32FromInt64(int64(ip) - int64(matchl1))
				matchs0 = matchl1
			}
		}
		for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchs0 > prefixLowest) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(matchs0 + uintptr(-libc.Int32FromInt32(1))))) {
			ip = ip - 1
			matchs0 = matchs0 - 1
			mLength = mLength + 1
		} /* complete backward */
		/* fall-through */
		goto _match_found
	_match_found:
		; /* requires ip, offset, mLength */
		offset_2 = offset_1
		offset_1 = offset
		if step < uint64(4) {
			/* It is unsafe to write this value back to the hashtable when ip1 is
			 * greater than or equal to the new ip we will have after we're done
			 * processing this match. Rather than perform that test directly
			 * (ip1 >= ip + mLength), which costs speed in practice, we do a simpler
			 * more predictable test. The minmatch even if we take a short match is
			 * 4 bytes, so as long as step, the distance between ip and ip1
			 * (initially) is less than 4, we know ip1 < new ip. */
			*(*U32)(unsafe.Pointer(hashLong + uintptr(hl1)*4)) = libc.Uint32FromInt64(int64(ip1) - int64(base))
		}
		ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
		goto _match_stored
	_match_stored:
		;
		/* match found */
		ip = ip + uintptr(mLength)
		anchor = ip
		if ip <= ilimit {
			/* Complementary insertion */
			/* done after iLimit test, as candidates could be > iend-8 */
			indexToInsert = curr + uint32(2)
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsL, uint32(8)))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip-uintptr(2), hBitsL, uint32(8)))*4)) = libc.Uint32FromInt64(int64(ip-libc.UintptrFromInt32(2)) - int64(base))
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsS, mls))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip-uintptr(1), hBitsS, mls))*4)) = libc.Uint32FromInt64(int64(ip-libc.UintptrFromInt32(1)) - int64(base))
			/* check immediate repcode */
			for ip <= ilimit && libc.BoolInt32(offset_2 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_2))) != 0 {
				/* store sequence */
				rLength = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_2), iend) + uint64(4)
				tmpOff = offset_2
				offset_2 = offset_1
				offset_1 = tmpOff /* swap offset_2 <=> offset_1 */
				*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip, hBitsS, mls))*4)) = libc.Uint32FromInt64(int64(ip) - int64(base))
				*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip, hBitsL, uint32(8)))*4)) = libc.Uint32FromInt64(int64(ip) - int64(base))
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), rLength)
				ip = ip + uintptr(rLength)
				anchor = ip
				continue /* faster when present ... (?) */
			}
		}
	}
	return r
}

func ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _pos, _pos1, _size, _size1, chainTableBytes, dictHashAndTagL, dictHashAndTagL3, dictHashAndTagS, h, h2, hashTableBytes, hl3, mLength, repLength2 size_t
	var _ptr, _ptr1, anchor, base, cParams, dictBase, dictCParams, dictEnd, dictHashLong, dictHashSmall, dictMatchL, dictMatchL3, dictStart, dms, hashLong, hashSmall, iend, ilimit, ip, istart, match, matchL3, matchLong, prefixLowest, repEnd2, repMatch, repMatch2, repMatchEnd, v3, v5 uintptr
	var curr, current2, dictAndPrefixLength, dictHBitsL, dictHBitsS, dictIndexDelta, dictMatchIndexAndTagL, dictMatchIndexAndTagL3, dictMatchIndexAndTagS, dictMatchIndexL, dictMatchIndexL3, dictMatchIndexS, dictStartIndex, endIndex, hBitsL, hBitsS, indexToInsert, matchIndexL, matchIndexL3, matchIndexS, offset, offset_1, offset_2, prefixLowestIndex, repIndex, repIndex2, tmpOffset, v4 U32
	var dictTagsMatchL, dictTagsMatchL3, dictTagsMatchS int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _pos, _pos1, _ptr, _ptr1, _size, _size1, anchor, base, cParams, chainTableBytes, curr, current2, dictAndPrefixLength, dictBase, dictCParams, dictEnd, dictHBitsL, dictHBitsS, dictHashAndTagL, dictHashAndTagL3, dictHashAndTagS, dictHashLong, dictHashSmall, dictIndexDelta, dictMatchIndexAndTagL, dictMatchIndexAndTagL3, dictMatchIndexAndTagS, dictMatchIndexL, dictMatchIndexL3, dictMatchIndexS, dictMatchL, dictMatchL3, dictStart, dictStartIndex, dictTagsMatchL, dictTagsMatchL3, dictTagsMatchS, dms, endIndex, h, h2, hBitsL, hBitsS, hashLong, hashSmall, hashTableBytes, hl3, iend, ilimit, indexToInsert, ip, istart, mLength, match, matchIndexL, matchIndexL3, matchIndexS, matchL3, matchLong, offset, offset_1, offset_2, prefixLowest, prefixLowestIndex, repEnd2, repIndex, repIndex2, repLength2, repMatch, repMatch2, repMatchEnd, tmpOffset, v3, v4, v5
	cParams = ms + 256
	hashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	ip = istart
	anchor = istart
	endIndex = uint32(libc.Uint64FromInt64(int64(istart)-int64(base)) + srcSize)
	/* presumes that, if there is a dictionary, it must be using Attach mode */
	prefixLowestIndex = ZSTD_getLowestPrefixIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	prefixLowest = base + uintptr(prefixLowestIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	dictCParams = dms + 256
	dictHashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
	dictHashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
	dictStartIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	dictStart = dictBase + uintptr(dictStartIndex)
	dictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	dictIndexDelta = prefixLowestIndex - libc.Uint32FromInt64(int64(dictEnd)-int64(dictBase))
	dictHBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	dictHBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FchainLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	dictAndPrefixLength = libc.Uint32FromInt64(int64(ip) - int64(prefixLowest) + (int64(dictEnd) - int64(dictStart)))
	/* if a dictionary is attached, it must be within window range */
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FprefetchCDictTables != 0 {
		hashTableBytes = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog * uint64(4)
		chainTableBytes = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FchainLog * uint64(4)
		_ptr = dictHashLong
		_size = hashTableBytes
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+8, 0, int32(2)))
			goto _1
		_1:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
		_ptr1 = dictHashSmall
		_size1 = chainTableBytes
		_pos1 = uint64(0)
		for {
			if !(_pos1 < _size1) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr1+uintptr(_pos1), libc.VaList(bp+8, 0, int32(2)))
			goto _2
		_2:
			;
			_pos1 = _pos1 + uint64(CACHELINE_SIZE)
		}
	}
	/* init */
	ip = ip + libc.BoolUintptr(dictAndPrefixLength == libc.Uint32FromInt32(0))
	/* dictMatchState repCode checks don't currently handle repCode == 0
	 * disabling. */
	/* Main Search Loop */
	for ip < ilimit {
		h2 = ZSTD_hashPtr(tls, ip, hBitsL, uint32(8))
		h = ZSTD_hashPtr(tls, ip, hBitsS, mls)
		dictHashAndTagL = ZSTD_hashPtr(tls, ip, dictHBitsL, uint32(8))
		dictHashAndTagS = ZSTD_hashPtr(tls, ip, dictHBitsS, mls)
		dictMatchIndexAndTagL = *(*U32)(unsafe.Pointer(dictHashLong + uintptr(dictHashAndTagL>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictMatchIndexAndTagS = *(*U32)(unsafe.Pointer(dictHashSmall + uintptr(dictHashAndTagS>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictTagsMatchL = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTagL), dictHashAndTagL)
		dictTagsMatchS = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTagS), dictHashAndTagS)
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		matchIndexL = *(*U32)(unsafe.Pointer(hashLong + uintptr(h2)*4))
		matchIndexS = *(*U32)(unsafe.Pointer(hashSmall + uintptr(h)*4))
		matchLong = base + uintptr(matchIndexL)
		match = base + uintptr(matchIndexS)
		repIndex = curr + uint32(1) - offset_1
		if repIndex < prefixLowestIndex {
			v3 = dictBase + uintptr(repIndex-dictIndexDelta)
		} else {
			v3 = base + uintptr(repIndex)
		}
		repMatch = v3
		v4 = curr
		*(*U32)(unsafe.Pointer(hashSmall + uintptr(h)*4)) = v4
		*(*U32)(unsafe.Pointer(hashLong + uintptr(h2)*4)) = v4 /* update hash tables */
		/* check repcode */
		if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip+uintptr(1)) {
			if repIndex < prefixLowestIndex {
				v3 = dictEnd
			} else {
				v3 = iend
			}
			repMatchEnd = v3
			mLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixLowest) + uint64(4)
			ip = ip + 1
			ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip)-int64(anchor)), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), mLength)
			goto _match_stored
		}
		if matchIndexL >= prefixLowestIndex && MEM_read64(tls, matchLong) == MEM_read64(tls, ip) {
			/* check prefix long match */
			mLength = ZSTD_count(tls, ip+uintptr(8), matchLong+uintptr(8), iend) + uint64(8)
			offset = libc.Uint32FromInt64(int64(ip) - int64(matchLong))
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchLong > prefixLowest) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(matchLong + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				matchLong = matchLong - 1
				mLength = mLength + 1
			} /* catch up */
			goto _match_found
		} else {
			if dictTagsMatchL != 0 {
				/* check dictMatchState long match */
				dictMatchIndexL = dictMatchIndexAndTagL >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				dictMatchL = dictBase + uintptr(dictMatchIndexL)
				if dictMatchL > dictStart && MEM_read64(tls, dictMatchL) == MEM_read64(tls, ip) {
					mLength = ZSTD_count_2segments(tls, ip+uintptr(8), dictMatchL+uintptr(8), iend, dictEnd, prefixLowest) + uint64(8)
					offset = curr - dictMatchIndexL - dictIndexDelta
					for libc.BoolInt32(ip > anchor)&libc.BoolInt32(dictMatchL > dictStart) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(dictMatchL + uintptr(-libc.Int32FromInt32(1))))) {
						ip = ip - 1
						dictMatchL = dictMatchL - 1
						mLength = mLength + 1
					} /* catch up */
					goto _match_found
				}
			}
		}
		if matchIndexS > prefixLowestIndex {
			/* short match  candidate */
			if MEM_read32(tls, match) == MEM_read32(tls, ip) {
				goto _search_next_long
			}
		} else {
			if dictTagsMatchS != 0 {
				/* check dictMatchState short match */
				dictMatchIndexS = dictMatchIndexAndTagS >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				match = dictBase + uintptr(dictMatchIndexS)
				matchIndexS = dictMatchIndexS + dictIndexDelta
				if match > dictStart && MEM_read32(tls, match) == MEM_read32(tls, ip) {
					goto _search_next_long
				}
			}
		}
		ip = ip + uintptr((int64(ip)-int64(anchor))>>libc.Int32FromInt32(kSearchStrength)+int64(1))
		continue
		goto _search_next_long
	_search_next_long:
		;
		hl3 = ZSTD_hashPtr(tls, ip+uintptr(1), hBitsL, uint32(8))
		dictHashAndTagL3 = ZSTD_hashPtr(tls, ip+uintptr(1), dictHBitsL, uint32(8))
		matchIndexL3 = *(*U32)(unsafe.Pointer(hashLong + uintptr(hl3)*4))
		dictMatchIndexAndTagL3 = *(*U32)(unsafe.Pointer(dictHashLong + uintptr(dictHashAndTagL3>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictTagsMatchL3 = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTagL3), dictHashAndTagL3)
		matchL3 = base + uintptr(matchIndexL3)
		*(*U32)(unsafe.Pointer(hashLong + uintptr(hl3)*4)) = curr + uint32(1)
		/* check prefix long +1 match */
		if matchIndexL3 >= prefixLowestIndex && MEM_read64(tls, matchL3) == MEM_read64(tls, ip+uintptr(1)) {
			mLength = ZSTD_count(tls, ip+uintptr(9), matchL3+uintptr(8), iend) + uint64(8)
			ip = ip + 1
			offset = libc.Uint32FromInt64(int64(ip) - int64(matchL3))
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchL3 > prefixLowest) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(matchL3 + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				matchL3 = matchL3 - 1
				mLength = mLength + 1
			} /* catch up */
			goto _match_found
		} else {
			if dictTagsMatchL3 != 0 {
				/* check dict long +1 match */
				dictMatchIndexL3 = dictMatchIndexAndTagL3 >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				dictMatchL3 = dictBase + uintptr(dictMatchIndexL3)
				if dictMatchL3 > dictStart && MEM_read64(tls, dictMatchL3) == MEM_read64(tls, ip+uintptr(1)) {
					mLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(8), dictMatchL3+uintptr(8), iend, dictEnd, prefixLowest) + uint64(8)
					ip = ip + 1
					offset = curr + libc.Uint32FromInt32(1) - dictMatchIndexL3 - dictIndexDelta
					for libc.BoolInt32(ip > anchor)&libc.BoolInt32(dictMatchL3 > dictStart) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(dictMatchL3 + uintptr(-libc.Int32FromInt32(1))))) {
						ip = ip - 1
						dictMatchL3 = dictMatchL3 - 1
						mLength = mLength + 1
					} /* catch up */
					goto _match_found
				}
			}
		}
		/* if no long +1 match, explore the short match we found */
		if matchIndexS < prefixLowestIndex {
			mLength = ZSTD_count_2segments(tls, ip+uintptr(4), match+uintptr(4), iend, dictEnd, prefixLowest) + uint64(4)
			offset = curr - matchIndexS
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match > dictStart) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				match = match - 1
				mLength = mLength + 1
			} /* catch up */
		} else {
			mLength = ZSTD_count(tls, ip+uintptr(4), match+uintptr(4), iend) + uint64(4)
			offset = libc.Uint32FromInt64(int64(ip) - int64(match))
			for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match > prefixLowest) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
				ip = ip - 1
				match = match - 1
				mLength = mLength + 1
			} /* catch up */
		}
		goto _match_found
	_match_found:
		;
		offset_2 = offset_1
		offset_1 = offset
		ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
		goto _match_stored
	_match_stored:
		;
		/* match found */
		ip = ip + uintptr(mLength)
		anchor = ip
		if ip <= ilimit {
			/* Complementary insertion */
			/* done after iLimit test, as candidates could be > iend-8 */
			indexToInsert = curr + uint32(2)
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsL, uint32(8)))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip-uintptr(2), hBitsL, uint32(8)))*4)) = libc.Uint32FromInt64(int64(ip-libc.UintptrFromInt32(2)) - int64(base))
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsS, mls))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip-uintptr(1), hBitsS, mls))*4)) = libc.Uint32FromInt64(int64(ip-libc.UintptrFromInt32(1)) - int64(base))
			/* check immediate repcode */
			for ip <= ilimit {
				current2 = libc.Uint32FromInt64(int64(ip) - int64(base))
				repIndex2 = current2 - offset_2
				if repIndex2 < prefixLowestIndex {
					v3 = dictBase + uintptr(repIndex2) - uintptr(dictIndexDelta)
				} else {
					v3 = base + uintptr(repIndex2)
				}
				repMatch2 = v3
				if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex2) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip) {
					if repIndex2 < prefixLowestIndex {
						v5 = dictEnd
					} else {
						v5 = iend
					}
					repEnd2 = v5
					repLength2 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixLowest) + uint64(4)
					tmpOffset = offset_2
					offset_2 = offset_1
					offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), repLength2)
					*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip, hBitsS, mls))*4)) = current2
					*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip, hBitsL, uint32(8)))*4)) = current2
					ip = ip + uintptr(repLength2)
					anchor = ip
					continue
				}
				break
			}
		}
	} /* while (ip < ilimit) */
	/* save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_doubleFast_noDict_4(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4))
}

func ZSTD_compressBlock_doubleFast_noDict_5(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5))
}

func ZSTD_compressBlock_doubleFast_noDict_6(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6))
}

func ZSTD_compressBlock_doubleFast_noDict_7(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_4(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_5(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_6(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6))
}

func ZSTD_compressBlock_doubleFast_dictMatchState_7(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7))
}

func ZSTD_compressBlock_doubleFast(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_doubleFast_noDict_4(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_doubleFast_noDict_5(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_doubleFast_noDict_6(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_doubleFast_noDict_7(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

func ZSTD_compressBlock_doubleFast_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_doubleFast_dictMatchState_4(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_doubleFast_dictMatchState_5(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_doubleFast_dictMatchState_6(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_doubleFast_dictMatchState_7(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

func ZSTD_compressBlock_doubleFast_extDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32) (r size_t) {
	var anchor, base, cParams, dictBase, dictEnd, dictStart, hashLong, hashSmall, iend, ilimit, ip, istart, lowMatchPtr, lowMatchPtr1, lowMatchPtr2, match, match3, match3Base, matchBase, matchEnd, matchEnd1, matchEnd2, matchLong, matchLongBase, prefixStart, repBase, repEnd2, repMatch, repMatch2, repMatchEnd, v2, v3, v4 uintptr
	var curr, current2, dictLimit, dictStartIndex, endIndex, hBitsL, hBitsS, indexToInsert, lowLimit, matchIndex, matchIndex3, matchLongIndex, offset, offset1, offset_1, offset_2, prefixStartIndex, repIndex, repIndex2, tmpOffset, v5 U32
	var h3, hLong, hSmall, mLength, repLength2 size_t
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current2, dictBase, dictEnd, dictLimit, dictStart, dictStartIndex, endIndex, h3, hBitsL, hBitsS, hLong, hSmall, hashLong, hashSmall, iend, ilimit, indexToInsert, ip, istart, lowLimit, lowMatchPtr, lowMatchPtr1, lowMatchPtr2, mLength, match, match3, match3Base, matchBase, matchEnd, matchEnd1, matchEnd2, matchIndex, matchIndex3, matchLong, matchLongBase, matchLongIndex, offset, offset1, offset_1, offset_2, prefixStart, prefixStartIndex, repBase, repEnd2, repIndex, repIndex2, repLength2, repMatch, repMatch2, repMatchEnd, tmpOffset, v1, v2, v3, v4, v5
	cParams = ms + 256
	hashLong = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBitsL = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	hashSmall = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	hBitsS = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(8)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	endIndex = uint32(libc.Uint64FromInt64(int64(istart)-int64(base)) + srcSize)
	lowLimit = ZSTD_getLowestMatchIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	dictStartIndex = lowLimit
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if dictLimit > lowLimit {
		v1 = dictLimit
	} else {
		v1 = lowLimit
	}
	prefixStartIndex = v1
	prefixStart = base + uintptr(prefixStartIndex)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictStart = dictBase + uintptr(dictStartIndex)
	dictEnd = dictBase + uintptr(prefixStartIndex)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	/* if extDict is invalidated due to maxDistance, switch to "regular" variant */
	if prefixStartIndex == dictStartIndex {
		return ZSTD_compressBlock_doubleFast(tls, ms, seqStore, rep, src, srcSize)
	}
	/* Search Loop */
	for ip < ilimit { /* < instead of <=, because (ip+1) */
		hSmall = ZSTD_hashPtr(tls, ip, hBitsS, mls)
		matchIndex = *(*U32)(unsafe.Pointer(hashSmall + uintptr(hSmall)*4))
		if matchIndex < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		matchBase = v2
		match = matchBase + uintptr(matchIndex)
		hLong = ZSTD_hashPtr(tls, ip, hBitsL, uint32(8))
		matchLongIndex = *(*U32)(unsafe.Pointer(hashLong + uintptr(hLong)*4))
		if matchLongIndex < prefixStartIndex {
			v3 = dictBase
		} else {
			v3 = base
		}
		matchLongBase = v3
		matchLong = matchLongBase + uintptr(matchLongIndex)
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		repIndex = curr + uint32(1) - offset_1
		if repIndex < prefixStartIndex {
			v4 = dictBase
		} else {
			v4 = base
		} /* offset_1 expected <= curr +1 */
		repBase = v4
		repMatch = repBase + uintptr(repIndex)
		v5 = curr
		*(*U32)(unsafe.Pointer(hashLong + uintptr(hLong)*4)) = v5
		*(*U32)(unsafe.Pointer(hashSmall + uintptr(hSmall)*4)) = v5 /* update hash table */
		if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex)&libc.BoolInt32(offset_1 <= curr+uint32(1)-dictStartIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip+uintptr(1)) {
			if repIndex < prefixStartIndex {
				v2 = dictEnd
			} else {
				v2 = iend
			}
			repMatchEnd = v2
			mLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixStart) + uint64(4)
			ip = ip + 1
			ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip)-int64(anchor)), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), mLength)
		} else {
			if matchLongIndex > dictStartIndex && MEM_read64(tls, matchLong) == MEM_read64(tls, ip) {
				if matchLongIndex < prefixStartIndex {
					v2 = dictEnd
				} else {
					v2 = iend
				}
				matchEnd = v2
				if matchLongIndex < prefixStartIndex {
					v3 = dictStart
				} else {
					v3 = prefixStart
				}
				lowMatchPtr = v3
				mLength = ZSTD_count_2segments(tls, ip+uintptr(8), matchLong+uintptr(8), iend, matchEnd, prefixStart) + uint64(8)
				offset = curr - matchLongIndex
				for libc.BoolInt32(ip > anchor)&libc.BoolInt32(matchLong > lowMatchPtr) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(matchLong + uintptr(-libc.Int32FromInt32(1))))) {
					ip = ip - 1
					matchLong = matchLong - 1
					mLength = mLength + 1
				} /* catch up */
				offset_2 = offset_1
				offset_1 = offset
				ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
			} else {
				if matchIndex > dictStartIndex && MEM_read32(tls, match) == MEM_read32(tls, ip) {
					h3 = ZSTD_hashPtr(tls, ip+uintptr(1), hBitsL, uint32(8))
					matchIndex3 = *(*U32)(unsafe.Pointer(hashLong + uintptr(h3)*4))
					if matchIndex3 < prefixStartIndex {
						v2 = dictBase
					} else {
						v2 = base
					}
					match3Base = v2
					match3 = match3Base + uintptr(matchIndex3)
					*(*U32)(unsafe.Pointer(hashLong + uintptr(h3)*4)) = curr + uint32(1)
					if matchIndex3 > dictStartIndex && MEM_read64(tls, match3) == MEM_read64(tls, ip+uintptr(1)) {
						if matchIndex3 < prefixStartIndex {
							v2 = dictEnd
						} else {
							v2 = iend
						}
						matchEnd1 = v2
						if matchIndex3 < prefixStartIndex {
							v3 = dictStart
						} else {
							v3 = prefixStart
						}
						lowMatchPtr1 = v3
						mLength = ZSTD_count_2segments(tls, ip+uintptr(9), match3+uintptr(8), iend, matchEnd1, prefixStart) + uint64(8)
						ip = ip + 1
						offset1 = curr + uint32(1) - matchIndex3
						for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match3 > lowMatchPtr1) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match3 + uintptr(-libc.Int32FromInt32(1))))) {
							ip = ip - 1
							match3 = match3 - 1
							mLength = mLength + 1
						} /* catch up */
					} else {
						if matchIndex < prefixStartIndex {
							v2 = dictEnd
						} else {
							v2 = iend
						}
						matchEnd2 = v2
						if matchIndex < prefixStartIndex {
							v3 = dictStart
						} else {
							v3 = prefixStart
						}
						lowMatchPtr2 = v3
						mLength = ZSTD_count_2segments(tls, ip+uintptr(4), match+uintptr(4), iend, matchEnd2, prefixStart) + uint64(4)
						offset1 = curr - matchIndex
						for libc.BoolInt32(ip > anchor)&libc.BoolInt32(match > lowMatchPtr2) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
							ip = ip - 1
							match = match - 1
							mLength = mLength + 1
						} /* catch up */
					}
					offset_2 = offset_1
					offset_1 = offset1
					ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip)-int64(anchor)), anchor, iend, offset1+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
				} else {
					ip = ip + uintptr((int64(ip)-int64(anchor))>>libc.Int32FromInt32(kSearchStrength)+int64(1))
					continue
				}
			}
		}
		/* move to next sequence start */
		ip = ip + uintptr(mLength)
		anchor = ip
		if ip <= ilimit {
			/* Complementary insertion */
			/* done after iLimit test, as candidates could be > iend-8 */
			indexToInsert = curr + uint32(2)
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsL, uint32(8)))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip-uintptr(2), hBitsL, uint32(8)))*4)) = libc.Uint32FromInt64(int64(ip-libc.UintptrFromInt32(2)) - int64(base))
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, base+uintptr(indexToInsert), hBitsS, mls))*4)) = indexToInsert
			*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip-uintptr(1), hBitsS, mls))*4)) = libc.Uint32FromInt64(int64(ip-libc.UintptrFromInt32(1)) - int64(base))
			/* check immediate repcode */
			for ip <= ilimit {
				current2 = libc.Uint32FromInt64(int64(ip) - int64(base))
				repIndex2 = current2 - offset_2
				if repIndex2 < prefixStartIndex {
					v2 = dictBase + uintptr(repIndex2)
				} else {
					v2 = base + uintptr(repIndex2)
				}
				repMatch2 = v2
				if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex2)&libc.BoolInt32(offset_2 <= current2-dictStartIndex) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip) {
					if repIndex2 < prefixStartIndex {
						v3 = dictEnd
					} else {
						v3 = iend
					}
					repEnd2 = v3
					repLength2 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
					tmpOffset = offset_2
					offset_2 = offset_1
					offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), repLength2)
					*(*U32)(unsafe.Pointer(hashSmall + uintptr(ZSTD_hashPtr(tls, ip, hBitsS, mls))*4)) = current2
					*(*U32)(unsafe.Pointer(hashLong + uintptr(ZSTD_hashPtr(tls, ip, hBitsL, uint32(8)))*4)) = current2
					ip = ip + uintptr(repLength2)
					anchor = ip
					continue
				}
				break
			}
		}
	}
	/* save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_doubleFast_extDict_4(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4))
}

func ZSTD_compressBlock_doubleFast_extDict_5(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5))
}

func ZSTD_compressBlock_doubleFast_extDict_6(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6))
}

func ZSTD_compressBlock_doubleFast_extDict_7(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_doubleFast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7))
}

func ZSTD_compressBlock_doubleFast_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_doubleFast_extDict_4(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_doubleFast_extDict_5(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_doubleFast_extDict_6(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_doubleFast_extDict_7(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

/**** ended inlining compress/zstd_double_fast.c ****/
/**** start inlining compress/zstd_fast.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_fast.h ****/

func ZSTD_fillHashTableForCDict(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashTable, iend, ip uintptr
	var curr, fastHashFillStep, hBits, mls, p U32
	var hashAndTag, hashAndTag1 size_t
	_, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBits, hashAndTag, hashAndTag1, hashTable, iend, ip, mls, p
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBits = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Currently, we always use ZSTD_dtlm_full for filling CDict tables.
	 * Feel free to remove this assert if there's a good reason! */
	/* Always insert every fastHashFillStep position into the hash table.
	 * Insert the other positions if their hash entry is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep) < iend+uintptr(2)) {
			break
		}
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		hashAndTag = ZSTD_hashPtr(tls, ip, hBits, mls)
		ZSTD_writeTaggedIndex(tls, hashTable, hashAndTag, curr)
		if dtlm == int32(ZSTD_dtlm_fast) {
			goto _1
		}
		/* Only load extra positions for ZSTD_dtlm_full */
		p = uint32(1)
		for {
			if !(p < fastHashFillStep) {
				break
			}
			hashAndTag1 = ZSTD_hashPtr(tls, ip+uintptr(p), hBits, mls)
			if *(*U32)(unsafe.Pointer(hashTable + uintptr(hashAndTag1>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4)) == uint32(0) { /* not yet filled */
				ZSTD_writeTaggedIndex(tls, hashTable, hashAndTag1, curr+p)
			}
			goto _2
		_2:
			;
			p = p + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillHashTableForCCtx(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e) {
	var base, cParams, hashTable, iend, ip uintptr
	var curr, fastHashFillStep, hBits, mls, p U32
	var hash, hash0 size_t
	_, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, curr, fastHashFillStep, hBits, hash, hash0, hashTable, iend, ip, mls, p
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hBits = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	mls = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	ip = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate)
	iend = end - uintptr(HASH_READ_SIZE)
	fastHashFillStep = uint32(3)
	/* Currently, we always use ZSTD_dtlm_fast for filling CCtx tables.
	 * Feel free to remove this assert if there's a good reason! */
	/* Always insert every fastHashFillStep position into the hash table.
	 * Insert the other positions if their hash entry is empty.
	 */
	for {
		if !(ip+uintptr(fastHashFillStep) < iend+uintptr(2)) {
			break
		}
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		hash0 = ZSTD_hashPtr(tls, ip, hBits, mls)
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = curr
		if dtlm == int32(ZSTD_dtlm_fast) {
			goto _1
		}
		/* Only load extra positions for ZSTD_dtlm_full */
		p = uint32(1)
		for {
			if !(p < fastHashFillStep) {
				break
			}
			hash = ZSTD_hashPtr(tls, ip+uintptr(p), hBits, mls)
			if *(*U32)(unsafe.Pointer(hashTable + uintptr(hash)*4)) == uint32(0) { /* not yet filled */
				*(*U32)(unsafe.Pointer(hashTable + uintptr(hash)*4)) = curr + p
			}
			goto _2
		_2:
			;
			p = p + 1
		}
		goto _1
	_1:
		;
		ip = ip + uintptr(fastHashFillStep)
	}
}

func ZSTD_fillHashTable(tls *libc.TLS, ms uintptr, end uintptr, dtlm ZSTD_dictTableLoadMethod_e, tfp ZSTD_tableFillPurpose_e) {
	if tfp == int32(ZSTD_tfp_forCDict) {
		ZSTD_fillHashTableForCDict(tls, ms, end, dtlm)
	} else {
		ZSTD_fillHashTableForCCtx(tls, ms, end, dtlm)
	}
}

type ZSTD_match4Found = uintptr

func ZSTD_match4Found_cmov(tls *libc.TLS, currentPtr uintptr, matchAddress uintptr, matchIdx U32, idxLowLimit U32) (r int32) {
	var mvalAddr uintptr
	_ = mvalAddr
	/* currentIdx >= lowLimit is a (somewhat) unpredictable branch.
	 * However expression below compiles into conditional move.
	 */
	mvalAddr = ZSTD_selectAddr(tls, matchIdx, idxLowLimit, matchAddress, uintptr(unsafe.Pointer(&dummy)))
	/* Note: this used to be written as : return test1 && test2;
	 * Unfortunately, once inlined, these tests become branches,
	 * in which case it becomes critical that they are executed in the right order (test1 then test2).
	 * So we have to write these tests in a specific manner to ensure their ordering.
	 */
	if MEM_read32(tls, currentPtr) != MEM_read32(tls, mvalAddr) {
		return 0
	}
	/* force ordering of these tests, which matters once the function is inlined, as they become branches */
	return libc.BoolInt32(matchIdx >= idxLowLimit)
}

/* Array of ~random data, should have low probability of matching data.
 * Load from here if the index is invalid.
 * Used to avoid unpredictable branches. */
var dummy = [4]BYTE{
	0: uint8(0x12),
	1: uint8(0x34),
	2: uint8(0x56),
	3: uint8(0x78),
}

func ZSTD_match4Found_branch(tls *libc.TLS, currentPtr uintptr, matchAddress uintptr, matchIdx U32, idxLowLimit U32) (r int32) {
	var mval U32
	_ = mval
	if matchIdx >= idxLowLimit {
		mval = MEM_read32(tls, matchAddress)
	} else {
		mval = MEM_read32(tls, currentPtr) ^ uint32(1) /* guaranteed to not match. */
	}
	return libc.BoolInt32(MEM_read32(tls, currentPtr) == mval)
}

// C documentation
//
//	/**
//	 * If you squint hard enough (and ignore repcodes), the search operation at any
//	 * given position is broken into 4 stages:
//	 *
//	 * 1. Hash   (map position to hash value via input read)
//	 * 2. Lookup (map hash val to index via hashtable read)
//	 * 3. Load   (map index to value at that position via input read)
//	 * 4. Compare
//	 *
//	 * Each of these steps involves a memory read at an address which is computed
//	 * from the previous step. This means these steps must be sequenced and their
//	 * latencies are cumulative.
//	 *
//	 * Rather than do 1->2->3->4 sequentially for a single position before moving
//	 * onto the next, this implementation interleaves these operations across the
//	 * next few positions:
//	 *
//	 * R = Repcode Read & Compare
//	 * H = Hash
//	 * T = Table Lookup
//	 * M = Match Read & Compare
//	 *
//	 * Pos | Time -->
//	 * ----+-------------------
//	 * N   | ... M
//	 * N+1 | ...   TM
//	 * N+2 |    R H   T M
//	 * N+3 |         H    TM
//	 * N+4 |           R H   T M
//	 * N+5 |                H   ...
//	 * N+6 |                  R ...
//	 *
//	 * This is very much analogous to the pipelining of execution in a CPU. And just
//	 * like a CPU, we have to dump the pipeline when we find a match (i.e., take a
//	 * branch).
//	 *
//	 * When this happens, we throw away our current state, and do the following prep
//	 * to re-enter the loop:
//	 *
//	 * Pos | Time -->
//	 * ----+-------------------
//	 * N   | H T
//	 * N+1 |  H
//	 *
//	 * This is also the work we do at the beginning to enter the loop initially.
//	 */
func ZSTD_compressBlock_fast_noDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32, useCmov int32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, cParams, hashTable, iend, ilimit, ip0, ip1, ip2, ip3, istart, match0, nextStep, prefixStart, v1 uintptr
	var curr, current0, endIndex, hlog, matchIdx, maxRep, offcode, offsetSaved1, offsetSaved2, prefixStartIndex, rep_offset1, rep_offset2, rval, tmpOff, windowLow U32
	var hash0, hash1, kStepIncr, mLength, rLength, step, stepSize size_t
	var matchFound ZSTD_match4Found
	var v2 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current0, endIndex, hash0, hash1, hashTable, hlog, iend, ilimit, ip0, ip1, ip2, ip3, istart, kStepIncr, mLength, match0, matchFound, matchIdx, maxRep, nextStep, offcode, offsetSaved1, offsetSaved2, prefixStart, prefixStartIndex, rLength, rep_offset1, rep_offset2, rval, step, stepSize, tmpOff, windowLow, v1, v2
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hlog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	stepSize = uint64((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength + libc.BoolUint32(!((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength != 0)) + uint32(1)) /* min 2 */
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	endIndex = uint32(libc.Uint64FromInt64(int64(istart)-int64(base)) + srcSize)
	prefixStartIndex = ZSTD_getLowestPrefixIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	prefixStart = base + uintptr(prefixStartIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	anchor = istart
	ip0 = istart
	rep_offset1 = *(*U32)(unsafe.Pointer(rep))
	rep_offset2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	kStepIncr = libc.Uint64FromInt32(libc.Int32FromInt32(1) << (libc.Int32FromInt32(kSearchStrength) - libc.Int32FromInt32(1)))
	if useCmov != 0 {
		v1 = __ccgo_fp(ZSTD_match4Found_cmov)
	} else {
		v1 = __ccgo_fp(ZSTD_match4Found_branch)
	}
	matchFound = v1
	ip0 = ip0 + libc.BoolUintptr(ip0 == prefixStart)
	curr = libc.Uint32FromInt64(int64(ip0) - int64(base))
	windowLow = ZSTD_getLowestPrefixIndex(tls, ms, curr, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	maxRep = curr - windowLow
	if rep_offset2 > maxRep {
		offsetSaved2 = rep_offset2
		rep_offset2 = libc.Uint32FromInt32(0)
	}
	if rep_offset1 > maxRep {
		offsetSaved1 = rep_offset1
		rep_offset1 = libc.Uint32FromInt32(0)
	}
	/* start each op */
	goto _start
_start:
	; /* Requires: ip0 */
	step = stepSize
	nextStep = ip0 + uintptr(kStepIncr)
	/* calculate positions, ip0 - anchor == 0, so we skip step calc */
	ip1 = ip0 + uintptr(1)
	ip2 = ip0 + uintptr(step)
	ip3 = ip2 + uintptr(1)
	if ip3 >= ilimit {
		goto _cleanup
	}
	hash0 = ZSTD_hashPtr(tls, ip0, hlog, mls)
	hash1 = ZSTD_hashPtr(tls, ip1, hlog, mls)
	matchIdx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4))
	for cond := true; cond; cond = ip3 < ilimit {
		/* load repcode match for ip[2]*/
		rval = MEM_read32(tls, ip2-uintptr(rep_offset1))
		/* write back hash table entry */
		current0 = libc.Uint32FromInt64(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		/* check repcode at ip[2] */
		if libc.BoolInt32(MEM_read32(tls, ip2) == rval)&libc.BoolInt32(rep_offset1 > uint32(0)) != 0 {
			ip0 = ip2
			match0 = ip0 - uintptr(rep_offset1)
			mLength = libc.BoolUint64(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))))
			ip0 = ip0 - uintptr(mLength)
			match0 = match0 - uintptr(mLength)
			offcode = libc.Uint32FromInt32(libc.Int32FromInt32(1))
			mLength = mLength + uint64(4)
			/* Write next hash table entry: it's already calculated.
			 * This write is known to be safe because ip1 is before the
			 * repcode (ip2). */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = libc.Uint32FromInt64(int64(ip1) - int64(base))
			goto _match
		}
		if (*(*func(*libc.TLS, uintptr, uintptr, U32, U32) int32)(unsafe.Pointer(&struct{ uintptr }{matchFound})))(tls, ip0, base+uintptr(matchIdx), matchIdx, prefixStartIndex) != 0 {
			/* Write next hash table entry (it's already calculated).
			 * This write is known to be safe because the ip1 == ip0 + 1,
			 * so searching will resume after ip1 */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = libc.Uint32FromInt64(int64(ip1) - int64(base))
			goto _offset
		}
		/* lookup ip[1] */
		matchIdx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip3
		/* write back hash table entry */
		current0 = libc.Uint32FromInt64(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		if (*(*func(*libc.TLS, uintptr, uintptr, U32, U32) int32)(unsafe.Pointer(&struct{ uintptr }{matchFound})))(tls, ip0, base+uintptr(matchIdx), matchIdx, prefixStartIndex) != 0 {
			/* Write next hash table entry, since it's already calculated */
			if step <= uint64(4) {
				/* Avoid writing an index if it's >= position where search will resume.
				 * The minimum possible match has length 4, so search can resume at ip0 + 4.
				 */
				*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = libc.Uint32FromInt64(int64(ip1) - int64(base))
			}
			goto _offset
		}
		/* lookup ip[1] */
		matchIdx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip0 + uintptr(step)
		ip3 = ip1 + uintptr(step)
		/* calculate step */
		if ip2 >= nextStep {
			step = step + 1
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(64), libc.VaList(bp+8, 0, int32(3)))
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(128), libc.VaList(bp+8, 0, int32(3)))
			nextStep = nextStep + uintptr(kStepIncr)
		}
	}
	goto _cleanup
_cleanup:
	;
	/* Note that there are probably still a couple positions one could search.
	 * However, it seems to be a meaningful performance hit to try to search
	 * them. So let's not. */
	/* When the repcodes are outside of the prefix, we set them to zero before the loop.
	 * When the offsets are still zero, we need to restore them after the block to have a correct
	 * repcode history. If only one offset was invalid, it is easy. The tricky case is when both
	 * offsets were invalid. We need to figure out which offset to refill with.
	 *     - If both offsets are zero they are in the same order.
	 *     - If both offsets are non-zero, we won't restore the offsets from `offsetSaved[12]`.
	 *     - If only one is zero, we need to decide which offset to restore.
	 *         - If rep_offset1 is non-zero, then rep_offset2 must be offsetSaved1.
	 *         - It is impossible for rep_offset2 to be non-zero.
	 *
	 * So if rep_offset1 started invalid (offsetSaved1 != 0) and became valid (rep_offset1 != 0), then
	 * set rep[0] = rep_offset1 and rep[1] = offsetSaved1.
	 */
	if offsetSaved1 != uint32(0) && rep_offset1 != uint32(0) {
		v2 = offsetSaved1
	} else {
		v2 = offsetSaved2
	}
	offsetSaved2 = v2
	/* save reps for next block */
	if rep_offset1 != 0 {
		v2 = rep_offset1
	} else {
		v2 = offsetSaved1
	}
	*(*U32)(unsafe.Pointer(rep)) = v2
	if rep_offset2 != 0 {
		v2 = rep_offset2
	} else {
		v2 = offsetSaved2
	}
	*(*U32)(unsafe.Pointer(rep + 1*4)) = v2
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
	goto _offset
_offset:
	; /* Requires: ip0, idx */
	/* Compute the offset code. */
	match0 = base + uintptr(matchIdx)
	rep_offset2 = rep_offset1
	rep_offset1 = libc.Uint32FromInt64(int64(ip0) - int64(match0))
	offcode = rep_offset1 + libc.Uint32FromInt32(ZSTD_REP_NUM)
	mLength = uint64(4)
	/* Count the backwards match length. */
	for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(match0 > prefixStart) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))) {
		ip0 = ip0 - 1
		match0 = match0 - 1
		mLength = mLength + 1
	}
	goto _match
_match:
	; /* Requires: ip0, match0, offcode */
	/* Count the forward length. */
	mLength = mLength + ZSTD_count(tls, ip0+uintptr(mLength), match0+uintptr(mLength), iend)
	ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip0)-int64(anchor)), anchor, iend, offcode, mLength)
	ip0 = ip0 + uintptr(mLength)
	anchor = ip0
	/* Fill table and check for immediate repcode. */
	if ip0 <= ilimit {
		/* Fill Table */
		/* check base overflow */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, base+uintptr(current0)+uintptr(2), hlog, mls))*4)) = current0 + uint32(2) /* here because current+2 could be > iend-8 */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0-uintptr(2), hlog, mls))*4)) = libc.Uint32FromInt64(int64(ip0-libc.UintptrFromInt32(2)) - int64(base))
		if rep_offset2 > uint32(0) { /* rep_offset2==0 means rep_offset2 is invalidated */
			for ip0 <= ilimit && MEM_read32(tls, ip0) == MEM_read32(tls, ip0-uintptr(rep_offset2)) {
				/* store sequence */
				rLength = ZSTD_count(tls, ip0+uintptr(4), ip0+uintptr(4)-uintptr(rep_offset2), iend) + uint64(4)
				tmpOff = rep_offset2
				rep_offset2 = rep_offset1
				rep_offset1 = tmpOff /* swap rep_offset2 <=> rep_offset1 */
				*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0, hlog, mls))*4)) = libc.Uint32FromInt64(int64(ip0) - int64(base))
				ip0 = ip0 + uintptr(rLength)
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), rLength)
				anchor = ip0
				continue /* faster when present (confirmed on gcc-8) ... (?) */
			}
		}
	}
	goto _start
	return r
}

func ZSTD_compressBlock_fast_noDict_4_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), int32(1))
}

func ZSTD_compressBlock_fast_noDict_5_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), int32(1))
}

func ZSTD_compressBlock_fast_noDict_6_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), int32(1))
}

func ZSTD_compressBlock_fast_noDict_7_1(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), int32(1))
}

func ZSTD_compressBlock_fast_noDict_4_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), 0)
}

func ZSTD_compressBlock_fast_noDict_5_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), 0)
}

func ZSTD_compressBlock_fast_noDict_6_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), 0)
}

func ZSTD_compressBlock_fast_noDict_7_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_noDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), 0)
}

func ZSTD_compressBlock_fast(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mml U32
	var useCmov int32
	_, _ = mml, useCmov
	mml = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	/* use cmov when "candidate in range" branch is likely unpredictable */
	useCmov = libc.BoolInt32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FwindowLog < uint32(19))
	if useCmov != 0 {
		switch mml {
		default: /* includes case 3 */
			fallthrough
		case uint32(4):
			return ZSTD_compressBlock_fast_noDict_4_1(tls, ms, seqStore, rep, src, srcSize)
		case uint32(5):
			return ZSTD_compressBlock_fast_noDict_5_1(tls, ms, seqStore, rep, src, srcSize)
		case uint32(6):
			return ZSTD_compressBlock_fast_noDict_6_1(tls, ms, seqStore, rep, src, srcSize)
		case uint32(7):
			return ZSTD_compressBlock_fast_noDict_7_1(tls, ms, seqStore, rep, src, srcSize)
		}
	} else {
		/* use a branch instead */
		switch mml {
		default: /* includes case 3 */
			fallthrough
		case uint32(4):
			return ZSTD_compressBlock_fast_noDict_4_0(tls, ms, seqStore, rep, src, srcSize)
		case uint32(5):
			return ZSTD_compressBlock_fast_noDict_5_0(tls, ms, seqStore, rep, src, srcSize)
		case uint32(6):
			return ZSTD_compressBlock_fast_noDict_6_0(tls, ms, seqStore, rep, src, srcSize)
		case uint32(7):
			return ZSTD_compressBlock_fast_noDict_7_0(tls, ms, seqStore, rep, src, srcSize)
		}
	}
	return r
}

func ZSTD_compressBlock_fast_dictMatchState_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32, hasStep U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _pos, _size, dictHashAndTag0, dictHashAndTag1, hash0, hash1, hashTableBytes, kStepIncr, mLength, repLength2, step size_t
	var _ptr, anchor, base, cParams, dictBase, dictCParams, dictEnd, dictHashTable, dictMatch, dictStart, dms, hashTable, iend, ilimit, ip0, ip1, istart, match, nextStep, prefixStart, repEnd2, repMatch, repMatch2, repMatchEnd, v2, v3 uintptr
	var curr, current2, dictAndPrefixLength, dictHBits, dictIndexDelta, dictMatchIndex, dictMatchIndexAndTag, dictStartIndex, endIndex, hlog, matchIndex, maxDistance, offset, offset1, offset_1, offset_2, prefixStartIndex, repIndex, repIndex2, stepSize, tmpOffset U32
	var dictTagsMatch int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _pos, _ptr, _size, anchor, base, cParams, curr, current2, dictAndPrefixLength, dictBase, dictCParams, dictEnd, dictHBits, dictHashAndTag0, dictHashAndTag1, dictHashTable, dictIndexDelta, dictMatch, dictMatchIndex, dictMatchIndexAndTag, dictStart, dictStartIndex, dictTagsMatch, dms, endIndex, hash0, hash1, hashTable, hashTableBytes, hlog, iend, ilimit, ip0, ip1, istart, kStepIncr, mLength, match, matchIndex, maxDistance, nextStep, offset, offset1, offset_1, offset_2, prefixStart, prefixStartIndex, repEnd2, repIndex, repIndex2, repLength2, repMatch, repMatch2, repMatchEnd, step, stepSize, tmpOffset, v2, v3
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hlog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	/* support stepSize of 0 */
	stepSize = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength + libc.BoolUint32(!((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength != 0))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	istart = src
	ip0 = istart
	ip1 = ip0 + uintptr(stepSize) /* we assert below that stepSize >= 1 */
	anchor = istart
	prefixStartIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(prefixStartIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	dictCParams = dms + 256
	dictHashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
	dictStartIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	dictStart = dictBase + uintptr(dictStartIndex)
	dictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	dictIndexDelta = prefixStartIndex - libc.Uint32FromInt64(int64(dictEnd)-int64(dictBase))
	dictAndPrefixLength = libc.Uint32FromInt64(int64(uintptr(int64(istart)-int64(prefixStart))+dictEnd) - int64(dictStart))
	dictHBits = (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog + uint32(ZSTD_SHORT_CACHE_TAG_BITS)
	/* if a dictionary is still attached, it necessarily means that
	 * it is within window size. So we just check it. */
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	endIndex = uint32(libc.Uint64FromInt64(int64(istart)-int64(base)) + srcSize)
	_ = maxDistance
	_ = endIndex /* these variables are not used when assert() is disabled */
	_ = hasStep  /* not currently specialized on whether it's accelerated */
	/* ensure there will be no underflow
	 * when translating a dict index into a local index */
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FprefetchCDictTables != 0 {
		hashTableBytes = libc.Uint64FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(dictCParams)).FhashLog * uint64(4)
		_ptr = dictHashTable
		_size = hashTableBytes
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+8, 0, int32(2)))
			goto _1
		_1:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
	}
	/* init */
	ip0 = ip0 + libc.BoolUintptr(dictAndPrefixLength == libc.Uint32FromInt32(0))
	/* dictMatchState repCode checks don't currently handle repCode == 0
	 * disabling. */
	/* Outer search loop */
	for ip1 <= ilimit {
		hash0 = ZSTD_hashPtr(tls, ip0, hlog, mls)
		dictHashAndTag0 = ZSTD_hashPtr(tls, ip0, dictHBits, mls)
		dictMatchIndexAndTag = *(*U32)(unsafe.Pointer(dictHashTable + uintptr(dictHashAndTag0>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
		dictTagsMatch = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTag), dictHashAndTag0)
		matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4))
		curr = libc.Uint32FromInt64(int64(ip0) - int64(base))
		step = uint64(stepSize)
		kStepIncr = libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(kSearchStrength))
		nextStep = ip0 + uintptr(kStepIncr)
		/* Inner search loop */
		for int32(1) != 0 {
			match = base + uintptr(matchIndex)
			repIndex = curr + uint32(1) - offset_1
			if repIndex < prefixStartIndex {
				v2 = dictBase + uintptr(repIndex-dictIndexDelta)
			} else {
				v2 = base + uintptr(repIndex)
			}
			repMatch = v2
			hash1 = ZSTD_hashPtr(tls, ip1, hlog, mls)
			dictHashAndTag1 = ZSTD_hashPtr(tls, ip1, dictHBits, mls)
			*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = curr /* update hash table */
			if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip0+uintptr(1)) {
				if repIndex < prefixStartIndex {
					v2 = dictEnd
				} else {
					v2 = iend
				}
				repMatchEnd = v2
				mLength = ZSTD_count_2segments(tls, ip0+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixStart) + uint64(4)
				ip0 = ip0 + 1
				ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip0)-int64(anchor)), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), mLength)
				break
			}
			if dictTagsMatch != 0 {
				/* Found a possible dict match */
				dictMatchIndex = dictMatchIndexAndTag >> int32(ZSTD_SHORT_CACHE_TAG_BITS)
				dictMatch = dictBase + uintptr(dictMatchIndex)
				if dictMatchIndex > dictStartIndex && MEM_read32(tls, dictMatch) == MEM_read32(tls, ip0) {
					/* To replicate extDict parse behavior, we only use dict matches when the normal matchIndex is invalid */
					if matchIndex <= prefixStartIndex {
						offset = curr - dictMatchIndex - dictIndexDelta
						mLength = ZSTD_count_2segments(tls, ip0+uintptr(4), dictMatch+uintptr(4), iend, dictEnd, prefixStart) + uint64(4)
						for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(dictMatch > dictStart) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(dictMatch + uintptr(-libc.Int32FromInt32(1))))) {
							ip0 = ip0 - 1
							dictMatch = dictMatch - 1
							mLength = mLength + 1
						} /* catch up */
						offset_2 = offset_1
						offset_1 = offset
						ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip0)-int64(anchor)), anchor, iend, offset+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
						break
					}
				}
			}
			if ZSTD_match4Found_cmov(tls, ip0, match, matchIndex, prefixStartIndex) != 0 {
				/* found a regular match of size >= 4 */
				offset1 = libc.Uint32FromInt64(int64(ip0) - int64(match))
				mLength = ZSTD_count(tls, ip0+uintptr(4), match+uintptr(4), iend) + uint64(4)
				for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(match > prefixStart) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
					ip0 = ip0 - 1
					match = match - 1
					mLength = mLength + 1
				} /* catch up */
				offset_2 = offset_1
				offset_1 = offset1
				ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip0)-int64(anchor)), anchor, iend, offset1+libc.Uint32FromInt32(ZSTD_REP_NUM), mLength)
				break
			}
			/* Prepare for next iteration */
			dictMatchIndexAndTag = *(*U32)(unsafe.Pointer(dictHashTable + uintptr(dictHashAndTag1>>int32(ZSTD_SHORT_CACHE_TAG_BITS))*4))
			dictTagsMatch = ZSTD_comparePackedTags(tls, uint64(dictMatchIndexAndTag), dictHashAndTag1)
			matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
			if ip1 >= nextStep {
				step = step + 1
				nextStep = nextStep + uintptr(kStepIncr)
			}
			ip0 = ip1
			ip1 = ip1 + uintptr(step)
			if ip1 > ilimit {
				goto _cleanup
			}
			curr = libc.Uint32FromInt64(int64(ip0) - int64(base))
			hash0 = hash1
		} /* end inner search loop */
		/* match found */
		ip0 = ip0 + uintptr(mLength)
		anchor = ip0
		if ip0 <= ilimit {
			/* Fill Table */
			/* check base overflow */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, base+uintptr(curr)+uintptr(2), hlog, mls))*4)) = curr + uint32(2) /* here because curr+2 could be > iend-8 */
			*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0-uintptr(2), hlog, mls))*4)) = libc.Uint32FromInt64(int64(ip0-libc.UintptrFromInt32(2)) - int64(base))
			/* check immediate repcode */
			for ip0 <= ilimit {
				current2 = libc.Uint32FromInt64(int64(ip0) - int64(base))
				repIndex2 = current2 - offset_2
				if repIndex2 < prefixStartIndex {
					v2 = dictBase - uintptr(dictIndexDelta) + uintptr(repIndex2)
				} else {
					v2 = base + uintptr(repIndex2)
				}
				repMatch2 = v2
				if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex2) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip0) {
					if repIndex2 < prefixStartIndex {
						v3 = dictEnd
					} else {
						v3 = iend
					}
					repEnd2 = v3
					repLength2 = ZSTD_count_2segments(tls, ip0+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
					tmpOffset = offset_2
					offset_2 = offset_1
					offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), repLength2)
					*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0, hlog, mls))*4)) = current2
					ip0 = ip0 + uintptr(repLength2)
					anchor = ip0
					continue
				}
				break
			}
		}
		/* Prepare for next iteration */
		ip1 = ip0 + uintptr(stepSize)
	}
	goto _cleanup
_cleanup:
	;
	/* save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_fast_dictMatchState_4_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState_5_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState_6_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState_7_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_dictMatchState_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), uint32(0))
}

func ZSTD_compressBlock_fast_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_fast_dictMatchState_4_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_fast_dictMatchState_5_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_fast_dictMatchState_6_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_fast_dictMatchState_7_0(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

func ZSTD_compressBlock_fast_extDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, mls U32, hasStep U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, cParams, dictBase, dictEnd, dictStart, hashTable, idxBase, iend, ilimit, ip0, ip1, ip2, ip3, istart, lowMatchPtr, match0, matchEnd, nextStep, prefixStart, repBase, repEnd2, repMatch2, v2, v3 uintptr
	var curr, current0, current2, dictLimit, dictStartIndex, endIndex, hlog, idx, lowLimit, maxRep, mval, mval1, offcode, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixStartIndex, repIndex, repIndex2, rval, tmpOffset U32
	var hash0, hash1, kStepIncr, mLength, repLength2, step, stepSize size_t
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, cParams, curr, current0, current2, dictBase, dictEnd, dictLimit, dictStart, dictStartIndex, endIndex, hash0, hash1, hashTable, hlog, idx, idxBase, iend, ilimit, ip0, ip1, ip2, ip3, istart, kStepIncr, lowLimit, lowMatchPtr, mLength, match0, matchEnd, maxRep, mval, mval1, nextStep, offcode, offset, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixStart, prefixStartIndex, repBase, repEnd2, repIndex, repIndex2, repLength2, repMatch2, rval, step, stepSize, tmpOffset, v1, v2, v3
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hlog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	/* support stepSize of 0 */
	stepSize = uint64((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength + libc.BoolUint32(!((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength != 0)) + uint32(1))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	istart = src
	anchor = istart
	endIndex = uint32(libc.Uint64FromInt64(int64(istart)-int64(base)) + srcSize)
	lowLimit = ZSTD_getLowestMatchIndex(tls, ms, endIndex, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	dictStartIndex = lowLimit
	dictStart = dictBase + uintptr(dictStartIndex)
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if dictLimit < lowLimit {
		v1 = lowLimit
	} else {
		v1 = dictLimit
	}
	prefixStartIndex = v1
	prefixStart = base + uintptr(prefixStartIndex)
	dictEnd = dictBase + uintptr(prefixStartIndex)
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(8)
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	ip0 = istart
	matchEnd = uintptr(0)
	kStepIncr = libc.Uint64FromInt32(libc.Int32FromInt32(1) << (libc.Int32FromInt32(kSearchStrength) - libc.Int32FromInt32(1)))
	_ = hasStep /* not currently specialized on whether it's accelerated */
	/* switch to "regular" variant if extDict is invalidated due to maxDistance */
	if prefixStartIndex == dictStartIndex {
		return ZSTD_compressBlock_fast(tls, ms, seqStore, rep, src, srcSize)
	}
	curr = libc.Uint32FromInt64(int64(ip0) - int64(base))
	maxRep = curr - dictStartIndex
	if offset_2 >= maxRep {
		offsetSaved2 = offset_2
		offset_2 = libc.Uint32FromInt32(0)
	}
	if offset_1 >= maxRep {
		offsetSaved1 = offset_1
		offset_1 = libc.Uint32FromInt32(0)
	}
	/* start each op */
	goto _start
_start:
	; /* Requires: ip0 */
	step = stepSize
	nextStep = ip0 + uintptr(kStepIncr)
	/* calculate positions, ip0 - anchor == 0, so we skip step calc */
	ip1 = ip0 + uintptr(1)
	ip2 = ip0 + uintptr(step)
	ip3 = ip2 + uintptr(1)
	if ip3 >= ilimit {
		goto _cleanup
	}
	hash0 = ZSTD_hashPtr(tls, ip0, hlog, mls)
	hash1 = ZSTD_hashPtr(tls, ip1, hlog, mls)
	idx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4))
	if idx < prefixStartIndex {
		v2 = dictBase
	} else {
		v2 = base
	}
	idxBase = v2
	for cond := true; cond; cond = ip3 < ilimit {
		/* load repcode match for ip[2] */
		current2 = libc.Uint32FromInt64(int64(ip2) - int64(base))
		repIndex = current2 - offset_1
		if repIndex < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		repBase = v2
		if libc.BoolInt32(prefixStartIndex-repIndex >= uint32(4))&libc.BoolInt32(offset_1 > uint32(0)) != 0 {
			rval = MEM_read32(tls, repBase+uintptr(repIndex))
		} else {
			rval = MEM_read32(tls, ip2) ^ uint32(1) /* guaranteed to not match. */
		}
		/* write back hash table entry */
		current0 = libc.Uint32FromInt64(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		/* check repcode at ip[2] */
		if MEM_read32(tls, ip2) == rval {
			ip0 = ip2
			match0 = repBase + uintptr(repIndex)
			if repIndex < prefixStartIndex {
				v2 = dictEnd
			} else {
				v2 = iend
			}
			matchEnd = v2
			mLength = libc.BoolUint64(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))))
			ip0 = ip0 - uintptr(mLength)
			match0 = match0 - uintptr(mLength)
			offcode = libc.Uint32FromInt32(libc.Int32FromInt32(1))
			mLength = mLength + uint64(4)
			goto _match
		}
		if idx >= dictStartIndex {
			v1 = MEM_read32(tls, idxBase+uintptr(idx))
		} else {
			v1 = MEM_read32(tls, ip0) ^ uint32(1)
		} /* load match for ip[0] */
		mval = v1 /* guaranteed not to match */
		/* check match at ip[0] */
		if MEM_read32(tls, ip0) == mval {
			/* found a match! */
			goto _offset
		}
		/* lookup ip[1] */
		idx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		if idx < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		idxBase = v2
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip3
		/* write back hash table entry */
		current0 = libc.Uint32FromInt64(int64(ip0) - int64(base))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash0)*4)) = current0
		if idx >= dictStartIndex {
			v1 = MEM_read32(tls, idxBase+uintptr(idx))
		} else {
			v1 = MEM_read32(tls, ip0) ^ uint32(1)
		} /* load match for ip[0] */
		mval1 = v1 /* guaranteed not to match */
		/* check match at ip[0] */
		if MEM_read32(tls, ip0) == mval1 {
			/* found a match! */
			goto _offset
		}
		/* lookup ip[1] */
		idx = *(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4))
		if idx < prefixStartIndex {
			v2 = dictBase
		} else {
			v2 = base
		}
		idxBase = v2
		/* hash ip[2] */
		hash0 = hash1
		hash1 = ZSTD_hashPtr(tls, ip2, hlog, mls)
		/* advance to next positions */
		ip0 = ip1
		ip1 = ip2
		ip2 = ip0 + uintptr(step)
		ip3 = ip1 + uintptr(step)
		/* calculate step */
		if ip2 >= nextStep {
			step = step + 1
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(64), libc.VaList(bp+8, 0, int32(3)))
			libc.X__builtin_prefetch(tls, ip1+libc.UintptrFromInt32(128), libc.VaList(bp+8, 0, int32(3)))
			nextStep = nextStep + uintptr(kStepIncr)
		}
	}
	goto _cleanup
_cleanup:
	;
	/* Note that there are probably still a couple positions we could search.
	 * However, it seems to be a meaningful performance hit to try to search
	 * them. So let's not. */
	/* If offset_1 started invalid (offsetSaved1 != 0) and became valid (offset_1 != 0),
	 * rotate saved offsets. See comment in ZSTD_compressBlock_fast_noDict for more context. */
	if offsetSaved1 != uint32(0) && offset_1 != uint32(0) {
		v1 = offsetSaved1
	} else {
		v1 = offsetSaved2
	}
	offsetSaved2 = v1
	/* save reps for next block */
	if offset_1 != 0 {
		v1 = offset_1
	} else {
		v1 = offsetSaved1
	}
	*(*U32)(unsafe.Pointer(rep)) = v1
	if offset_2 != 0 {
		v1 = offset_2
	} else {
		v1 = offsetSaved2
	}
	*(*U32)(unsafe.Pointer(rep + 1*4)) = v1
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
	goto _offset
_offset:
	; /* Requires: ip0, idx, idxBase */
	/* Compute the offset code. */
	offset = current0 - idx
	if idx < prefixStartIndex {
		v2 = dictStart
	} else {
		v2 = prefixStart
	}
	lowMatchPtr = v2
	if idx < prefixStartIndex {
		v3 = dictEnd
	} else {
		v3 = iend
	}
	matchEnd = v3
	match0 = idxBase + uintptr(idx)
	offset_2 = offset_1
	offset_1 = offset
	offcode = offset + libc.Uint32FromInt32(ZSTD_REP_NUM)
	mLength = uint64(4)
	/* Count the backwards match length. */
	for libc.BoolInt32(ip0 > anchor)&libc.BoolInt32(match0 > lowMatchPtr) != 0 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip0 + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match0 + uintptr(-libc.Int32FromInt32(1))))) {
		ip0 = ip0 - 1
		match0 = match0 - 1
		mLength = mLength + 1
	}
	goto _match
_match:
	; /* Requires: ip0, match0, offcode, matchEnd */
	/* Count the forward length. */
	mLength = mLength + ZSTD_count_2segments(tls, ip0+uintptr(mLength), match0+uintptr(mLength), iend, matchEnd, prefixStart)
	ZSTD_storeSeq(tls, seqStore, libc.Uint64FromInt64(int64(ip0)-int64(anchor)), anchor, iend, offcode, mLength)
	ip0 = ip0 + uintptr(mLength)
	anchor = ip0
	/* write next hash table entry */
	if ip1 < ip0 {
		*(*U32)(unsafe.Pointer(hashTable + uintptr(hash1)*4)) = libc.Uint32FromInt64(int64(ip1) - int64(base))
	}
	/* Fill table and check for immediate repcode. */
	if ip0 <= ilimit {
		/* Fill Table */
		/* check base overflow */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, base+uintptr(current0)+uintptr(2), hlog, mls))*4)) = current0 + uint32(2) /* here because current+2 could be > iend-8 */
		*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0-uintptr(2), hlog, mls))*4)) = libc.Uint32FromInt64(int64(ip0-libc.UintptrFromInt32(2)) - int64(base))
		for ip0 <= ilimit {
			repIndex2 = libc.Uint32FromInt64(int64(ip0)-int64(base)) - offset_2
			if repIndex2 < prefixStartIndex {
				v2 = dictBase + uintptr(repIndex2)
			} else {
				v2 = base + uintptr(repIndex2)
			}
			repMatch2 = v2
			if ZSTD_index_overlap_check(tls, prefixStartIndex, repIndex2)&libc.BoolInt32(offset_2 > uint32(0)) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip0) {
				if repIndex2 < prefixStartIndex {
					v3 = dictEnd
				} else {
					v3 = iend
				}
				repEnd2 = v3
				repLength2 = ZSTD_count_2segments(tls, ip0+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
				tmpOffset = offset_2
				offset_2 = offset_1
				offset_1 = tmpOffset /* swap offset_2 <=> offset_1 */
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), repLength2)
				*(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip0, hlog, mls))*4)) = libc.Uint32FromInt64(int64(ip0) - int64(base))
				ip0 = ip0 + uintptr(repLength2)
				anchor = ip0
				continue
			}
			break
		}
	}
	goto _start
	return r
}

func ZSTD_compressBlock_fast_extDict_4_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(4), uint32(0))
}

func ZSTD_compressBlock_fast_extDict_5_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(5), uint32(0))
}

func ZSTD_compressBlock_fast_extDict_6_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(6), uint32(0))
}

func ZSTD_compressBlock_fast_extDict_7_0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_fast_extDict_generic(tls, ms, seqStore, rep, src, srcSize, uint32(7), uint32(0))
}

func ZSTD_compressBlock_fast_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var mls U32
	_ = mls
	mls = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	switch mls {
	default: /* includes case 3 */
		fallthrough
	case uint32(4):
		return ZSTD_compressBlock_fast_extDict_4_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(5):
		return ZSTD_compressBlock_fast_extDict_5_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(6):
		return ZSTD_compressBlock_fast_extDict_6_0(tls, ms, seqStore, rep, src, srcSize)
	case uint32(7):
		return ZSTD_compressBlock_fast_extDict_7_0(tls, ms, seqStore, rep, src, srcSize)
	}
	return r
}

/**** ended inlining compress/zstd_fast.c ****/
/**** start inlining compress/zstd_lazy.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_lazy.h ****/
/**** skipping file: ../common/bits.h ****/

/*-*************************************
*  Binary Tree search
***************************************/

func ZSTD_updateDUBT(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, mls U32) {
	var base, bt, cParams, hashTable, nextCandidatePtr, sortMarkPtr uintptr
	var btLog, btMask, hashLog, idx, matchIndex, target U32
	var h size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _ = base, bt, btLog, btMask, cParams, h, hashLog, hashTable, idx, matchIndex, nextCandidatePtr, sortMarkPtr, target
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = libc.Uint32FromInt32(int32(1)<<btLog - int32(1))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = libc.Uint32FromInt64(int64(ip) - int64(base))
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	if idx != target {
	}
	/* condition for ZSTD_hashPtr */
	_ = iend
	/* condition for valid base+idx */
	for {
		if !(idx < target) {
			break
		}
		h = ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, mls) /* assumption : ip + 8 <= iend */
		matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
		nextCandidatePtr = bt + uintptr(uint32(2)*(idx&btMask))*4
		sortMarkPtr = nextCandidatePtr + uintptr(1)*4
		*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = idx /* Update Hash Table */
		*(*U32)(unsafe.Pointer(nextCandidatePtr)) = matchIndex  /* update BT like a chain */
		*(*U32)(unsafe.Pointer(sortMarkPtr)) = uint32(ZSTD_DUBT_UNSORTED_MARK)
		goto _1
	_1:
		;
		idx = idx + 1
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

// C documentation
//
//	/** ZSTD_insertDUBT1() :
//	 *  sort one already inserted but unsorted position
//	 *  assumption : curr >= btlow == (curr - btmask)
//	 *  doesn't fail */
func ZSTD_insertDUBT1(tls *libc.TLS, ms uintptr, curr U32, inputEnd uintptr, nbCompares U32, btLow U32, dictMode ZSTD_dictMode_e) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, iend, ip, largerPtr, mBase, match, nextPtr, prefixStart, smallerPtr, v1, v2 uintptr
	var btLog, btMask, dictLimit, matchIndex, maxDistance, windowLow, windowValid, v7 U32
	var commonLengthLarger, commonLengthSmaller, matchLength size_t
	var v3 uint32
	var v5 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bt, btLog, btMask, cParams, commonLengthLarger, commonLengthSmaller, dictBase, dictEnd, dictLimit, iend, ip, largerPtr, mBase, match, matchIndex, matchLength, maxDistance, nextPtr, prefixStart, smallerPtr, windowLow, windowValid, v1, v2, v3, v5, v7
	cParams = ms + 256
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = libc.Uint32FromInt32(int32(1)<<btLog - int32(1))
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	if curr >= dictLimit {
		v1 = base + uintptr(curr)
	} else {
		v1 = dictBase + uintptr(curr)
	}
	ip = v1
	if curr >= dictLimit {
		v2 = inputEnd
	} else {
		v2 = dictBase + uintptr(dictLimit)
	}
	iend = v2
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = smallerPtr + uintptr(1)*4
	matchIndex = *(*U32)(unsafe.Pointer(smallerPtr)) /* to be nullified at the end */
	windowValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	if curr-windowValid > maxDistance {
		v3 = curr - maxDistance
	} else {
		v3 = windowValid
	}
	windowLow = v3
	/* condition for ZSTD_count */
	for {
		if !(nbCompares != 0 && matchIndex > windowLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v5 = commonLengthSmaller
		} else {
			v5 = commonLengthLarger
		}
		matchLength = v5 /* guaranteed minimum nb of common bytes */
		/* note : all candidates are now supposed sorted,
		 * but it's still possible to have nextPtr[1] == ZSTD_DUBT_UNSORTED_MARK
		 * when a real index has the same value as ZSTD_DUBT_UNSORTED_MARK */
		if dictMode != int32(ZSTD_extDict) || uint64(matchIndex)+matchLength >= uint64(dictLimit) || curr < dictLimit {
			if dictMode != int32(ZSTD_extDict) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				v1 = base
			} else {
				v1 = dictBase
			}
			mBase = v1
			match = mBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend)
		} else {
			match = dictBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match = base + uintptr(matchIndex)
			} /* preparation for next read of match[matchLength] */
		}
		if ip+uintptr(matchLength) == iend { /* equal : no way to know if inf or sup */
			break /* drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt tree */
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) { /* necessarily within buffer */
			/* match is smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop searching */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new "candidate" => larger than match, which was smaller than target */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex, larger than previous and closer to current */
		} else {
			/* match is larger than current */
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop searching */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _4
	_4:
		;
		nbCompares = nbCompares - 1
	}
	v7 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v7
	*(*U32)(unsafe.Pointer(smallerPtr)) = v7
}

func ZSTD_DUBT_findBetterDictMatch(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, offsetPtr uintptr, bestLength size_t, nbCompares U32, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	var base, dictBase, dictBt, dictEnd, dictHashTable, dms, dmsCParams, match, nextPtr, prefixStart uintptr
	var btLog, btLow, btMask, curr, dictHighLimit, dictIndexDelta, dictLowLimit, dictMatchIndex, hashLog, mIndex, matchIndex U32
	var commonLengthLarger, commonLengthSmaller, h, matchLength size_t
	var v1 uint32
	var v3 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, btLog, btLow, btMask, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictBt, dictEnd, dictHashTable, dictHighLimit, dictIndexDelta, dictLowLimit, dictMatchIndex, dms, dmsCParams, h, hashLog, mIndex, match, matchIndex, matchLength, nextPtr, prefixStart, v1, v3
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	dmsCParams = dms + 256
	dictHashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FhashLog
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	dictMatchIndex = *(*U32)(unsafe.Pointer(dictHashTable + uintptr(h)*4))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	prefixStart = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit)
	curr = libc.Uint32FromInt64(int64(ip) - int64(base))
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	dictEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	dictHighLimit = libc.Uint32FromInt64(int64((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase))
	dictLowLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FlowLimit
	dictIndexDelta = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit - dictHighLimit
	dictBt = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FchainLog - uint32(1)
	btMask = libc.Uint32FromInt32(int32(1)<<btLog - int32(1))
	if btMask >= dictHighLimit-dictLowLimit {
		v1 = dictLowLimit
	} else {
		v1 = dictHighLimit - btMask
	}
	btLow = v1
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	_ = dictMode
	for {
		if !(nbCompares != 0 && dictMatchIndex > dictLowLimit) {
			break
		}
		nextPtr = dictBt + uintptr(uint32(2)*(dictMatchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v3 = commonLengthSmaller
		} else {
			v3 = commonLengthLarger
		}
		matchLength = v3 /* guaranteed minimum nb of common bytes */
		match = dictBase + uintptr(dictMatchIndex)
		matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
		if uint64(dictMatchIndex)+matchLength >= uint64(dictHighLimit) {
			match = base + uintptr(dictMatchIndex) + uintptr(dictIndexDelta)
		} /* to prepare for next usage of match[matchLength] */
		if matchLength > bestLength {
			matchIndex = dictMatchIndex + dictIndexDelta
			if int32(4)*libc.Int32FromUint64(matchLength-bestLength) > libc.Int32FromUint32(ZSTD_highbit32(tls, curr-matchIndex+uint32(1))-ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(offsetPtr)))+uint32(1))) {
				bestLength = matchLength
				*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM))
			}
			if ip+uintptr(matchLength) == iend { /* reached end of input : ip[matchLength] is not valid, no way to know if it's larger or smaller than match */
				break /* drop, to guarantee consistency (miss a little bit of compression) */
			}
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) {
			if dictMatchIndex <= btLow {
				break
			} /* beyond tree size, stop the search */
			commonLengthSmaller = matchLength                       /* all smaller will now have at least this guaranteed common length */
			dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex larger than previous (closer to current) */
		} else {
			/* match is larger than current */
			if dictMatchIndex <= btLow {
				break
			} /* beyond tree size, stop the search */
			commonLengthLarger = matchLength
			dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _2
	_2:
		;
		nbCompares = nbCompares - 1
	}
	if bestLength >= uint64(MINMATCH) {
		mIndex = curr - uint32(*(*size_t)(unsafe.Pointer(offsetPtr))-libc.Uint64FromInt32(ZSTD_REP_NUM))
		_ = mIndex
	}
	return bestLength
}

func ZSTD_DUBT_findBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, offBasePtr uintptr, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, hashTable, largerPtr, match, nextCandidate, nextCandidateIdxPtr, nextPtr, prefixStart, smallerPtr, unsortedMark uintptr
	var bestLength, commonLengthLarger, commonLengthSmaller, h, matchLength size_t
	var btLog, btLow, btMask, curr, dictLimit, hashLog, mIndex, matchEndIdx, matchIndex, nbCandidates, nbCompares, nextCandidateIdx, previousCandidate, unsortLimit, windowLow, v3 U32
	var v1, v2 uint32
	var v5 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bestLength, bt, btLog, btLow, btMask, cParams, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictEnd, dictLimit, h, hashLog, hashTable, largerPtr, mIndex, match, matchEndIdx, matchIndex, matchLength, nbCandidates, nbCompares, nextCandidate, nextCandidateIdx, nextCandidateIdxPtr, nextPtr, prefixStart, previousCandidate, smallerPtr, unsortLimit, unsortedMark, windowLow, v1, v2, v3, v5
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	curr = libc.Uint32FromInt64(int64(ip) - int64(base))
	windowLow = ZSTD_getLowestMatchIndex(tls, ms, curr, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = libc.Uint32FromInt32(int32(1)<<btLog - int32(1))
	if btMask >= curr {
		v1 = uint32(0)
	} else {
		v1 = curr - btMask
	}
	btLow = v1
	if btLow > windowLow {
		v2 = btLow
	} else {
		v2 = windowLow
	}
	unsortLimit = v2
	nextCandidate = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
	unsortedMark = bt + uintptr(uint32(2)*(matchIndex&btMask))*4 + uintptr(1)*4
	nbCompares = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	nbCandidates = nbCompares
	previousCandidate = uint32(0)
	/* required for h calculation */
	/* reach end of unsorted candidates list */
	for matchIndex > unsortLimit && *(*U32)(unsafe.Pointer(unsortedMark)) == uint32(ZSTD_DUBT_UNSORTED_MARK) && nbCandidates > uint32(1) {
		*(*U32)(unsafe.Pointer(unsortedMark)) = previousCandidate /* the unsortedMark becomes a reversed chain, to move up back to original position */
		previousCandidate = matchIndex
		matchIndex = *(*U32)(unsafe.Pointer(nextCandidate))
		nextCandidate = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		unsortedMark = bt + uintptr(uint32(2)*(matchIndex&btMask))*4 + uintptr(1)*4
		nbCandidates = nbCandidates - 1
	}
	/* nullify last candidate if it's still unsorted
	 * simplification, detrimental to compression ratio, beneficial for speed */
	if matchIndex > unsortLimit && *(*U32)(unsafe.Pointer(unsortedMark)) == uint32(ZSTD_DUBT_UNSORTED_MARK) {
		v3 = libc.Uint32FromInt32(0)
		*(*U32)(unsafe.Pointer(unsortedMark)) = v3
		*(*U32)(unsafe.Pointer(nextCandidate)) = v3
	}
	/* batch sort stacked candidates */
	matchIndex = previousCandidate
	for matchIndex != 0 { /* will end on matchIndex == 0 */
		nextCandidateIdxPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4 + uintptr(1)*4
		nextCandidateIdx = *(*U32)(unsafe.Pointer(nextCandidateIdxPtr))
		ZSTD_insertDUBT1(tls, ms, matchIndex, iend, nbCandidates, unsortLimit, dictMode)
		matchIndex = nextCandidateIdx
		nbCandidates = nbCandidates + 1
	}
	/* find longest match */
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4 + uintptr(1)*4
	matchEndIdx = curr + uint32(8) + uint32(1) /* to be nullified at the end */
	bestLength = uint64(0)
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = curr /* Update Hash Table */
	for {
		if !(nbCompares != 0 && matchIndex > windowLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v5 = commonLengthSmaller
		} else {
			v5 = commonLengthLarger
		}
		matchLength = v5
		if dictMode != int32(ZSTD_extDict) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
			match = base + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend)
		} else {
			match = dictBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match = base + uintptr(matchIndex)
			} /* to prepare for next usage of match[matchLength] */
		}
		if matchLength > bestLength {
			if matchLength > uint64(matchEndIdx-matchIndex) {
				matchEndIdx = matchIndex + uint32(matchLength)
			}
			if int32(4)*libc.Int32FromUint64(matchLength-bestLength) > libc.Int32FromUint32(ZSTD_highbit32(tls, curr-matchIndex+uint32(1))-ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(offBasePtr))))) {
				bestLength = matchLength
				*(*size_t)(unsafe.Pointer(offBasePtr)) = uint64(curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM))
			}
			if ip+uintptr(matchLength) == iend { /* equal : no way to know if inf or sup */
				if dictMode == int32(ZSTD_dictMatchState) {
					nbCompares = uint32(0) /* in addition to avoiding checking any
					 * further in this loop, make sure we
					 * skip checking in the dictionary. */
				}
				break /* drop, to guarantee consistency (miss a little bit of compression) */
			}
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) {
			/* match is smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop the search */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new "smaller" => larger of match */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex larger than previous (closer to current) */
		} else {
			/* match is larger than current */
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop the search */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _4
	_4:
		;
		nbCompares = nbCompares - 1
	}
	v3 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v3
	*(*U32)(unsafe.Pointer(smallerPtr)) = v3
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dictMatchState) && nbCompares != 0 {
		bestLength = ZSTD_DUBT_findBetterDictMatch(tls, ms, ip, iend, offBasePtr, bestLength, nbCompares, mls, dictMode)
	}
	/* ensure nextToUpdate is increased */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = matchEndIdx - uint32(8) /* skip repetitive patterns */
	if bestLength >= uint64(MINMATCH) {
		mIndex = curr - uint32(*(*size_t)(unsafe.Pointer(offBasePtr))-libc.Uint64FromInt32(ZSTD_REP_NUM))
		_ = mIndex
	}
	return bestLength
	return r
}

// C documentation
//
//	/** ZSTD_BtFindBestMatch() : Tree updater, providing best match */
func ZSTD_BtFindBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	if ip < (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase+uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate) {
		return uint64(0)
	} /* skipped area */
	ZSTD_updateDUBT(tls, ms, ip, iLimit, mls)
	return ZSTD_DUBT_findBestMatch(tls, ms, ip, iLimit, offBasePtr, mls, dictMode)
}

/***********************************
* Dedicated dict search
***********************************/

func ZSTD_dedicatedDictSearch_lazy_loadDictionary(tls *libc.TLS, ms uintptr, ip uintptr) {
	var base, chainTable, hashTable, tmpChainTable, tmpHashTable uintptr
	var bucketIdx, bucketSize, cacheSize, chainAttempts, chainLimit, chainPackedPointer, chainPos, chainSize, count, countBeyondMinChain, h, h1, hashIdx, hashLog, i, i1, i2, idx, minChain, target, tmpChainSize, tmpMinChain, v8 U32
	var v1, v2, v3 uint32
	var v9 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bucketIdx, bucketSize, cacheSize, chainAttempts, chainLimit, chainPackedPointer, chainPos, chainSize, chainTable, count, countBeyondMinChain, h, h1, hashIdx, hashLog, hashTable, i, i1, i2, idx, minChain, target, tmpChainSize, tmpChainTable, tmpHashTable, tmpMinChain, v1, v2, v3, v8, v9
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = libc.Uint32FromInt64(int64(ip) - int64(base))
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	chainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	chainSize = libc.Uint32FromInt32(int32(1) << (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FchainLog)
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	if chainSize < target-idx {
		v1 = target - chainSize
	} else {
		v1 = idx
	}
	minChain = v1
	bucketSize = libc.Uint32FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_LAZY_DDSS_BUCKET_LOG))
	cacheSize = bucketSize - uint32(1)
	chainAttempts = libc.Uint32FromInt32(libc.Int32FromInt32(1)<<(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog) - cacheSize
	if chainAttempts > uint32(255) {
		v2 = uint32(255)
	} else {
		v2 = chainAttempts
	}
	chainLimit = v2
	/* We know the hashtable is oversized by a factor of `bucketSize`.
	 * We are going to temporarily pretend `bucketSize == 1`, keeping only a
	 * single entry. We will use the rest of the space to construct a temporary
	 * chaintable.
	 */
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FhashLog - uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	tmpHashTable = hashTable
	tmpChainTable = hashTable + uintptr(libc.Uint64FromInt32(1)<<hashLog)*4
	tmpChainSize = libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_LAZY_DDSS_BUCKET_LOG)-libc.Int32FromInt32(1)) << hashLog
	if tmpChainSize < target {
		v3 = target - tmpChainSize
	} else {
		v3 = idx
	}
	tmpMinChain = v3
	/* fill conventional hash table and conventional chain table */
	for {
		if !(idx < target) {
			break
		}
		h = uint32(ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch))
		if idx >= tmpMinChain {
			*(*U32)(unsafe.Pointer(tmpChainTable + uintptr(idx-tmpMinChain)*4)) = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
		}
		*(*U32)(unsafe.Pointer(tmpHashTable + uintptr(h)*4)) = idx
		goto _4
	_4:
		;
		idx = idx + 1
	}
	/* sort chains into ddss chain table */
	chainPos = uint32(0)
	hashIdx = uint32(0)
	for {
		if !(hashIdx < uint32(1)<<hashLog) {
			break
		}
		countBeyondMinChain = uint32(0)
		i = *(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4))
		count = uint32(0)
		for {
			if !(i >= tmpMinChain && count < cacheSize) {
				break
			}
			/* skip through the chain to the first position that won't be
			 * in the hash cache bucket */
			if i < minChain {
				countBeyondMinChain = countBeyondMinChain + 1
			}
			i = *(*U32)(unsafe.Pointer(tmpChainTable + uintptr(i-tmpMinChain)*4))
			goto _6
		_6:
			;
			count = count + 1
		}
		if count == cacheSize {
			count = uint32(0)
			for {
				if !(count < chainLimit) {
					break
				}
				if i < minChain {
					if v9 = !(i != 0); !v9 {
						countBeyondMinChain = countBeyondMinChain + 1
						v8 = countBeyondMinChain
					}
					if v9 || v8 > cacheSize {
						/* only allow pulling `cacheSize` number of entries
						 * into the cache or chainTable beyond `minChain`,
						 * to replace the entries pulled out of the
						 * chainTable into the cache. This lets us reach
						 * back further without increasing the total number
						 * of entries in the chainTable, guaranteeing the
						 * DDSS chain table will fit into the space
						 * allocated for the regular one. */
						break
					}
				}
				v8 = chainPos
				chainPos = chainPos + 1
				*(*U32)(unsafe.Pointer(chainTable + uintptr(v8)*4)) = i
				count = count + 1
				if i < tmpMinChain {
					break
				}
				i = *(*U32)(unsafe.Pointer(tmpChainTable + uintptr(i-tmpMinChain)*4))
				goto _7
			_7:
			}
		} else {
			count = uint32(0)
		}
		if count != 0 {
			*(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4)) = (chainPos-count)<<int32(8) + count
		} else {
			*(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4)) = uint32(0)
		}
		goto _5
	_5:
		;
		hashIdx = hashIdx + 1
	}
	/* I believe this is guaranteed... */
	/* move chain pointers into the last entry of each hash bucket */
	hashIdx = libc.Uint32FromInt32(libc.Int32FromInt32(1) << hashLog)
	for {
		if !(hashIdx != 0) {
			break
		}
		hashIdx = hashIdx - 1
		v8 = hashIdx
		bucketIdx = v8 << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		chainPackedPointer = *(*U32)(unsafe.Pointer(tmpHashTable + uintptr(hashIdx)*4))
		i1 = uint32(0)
		for {
			if !(i1 < cacheSize) {
				break
			}
			*(*U32)(unsafe.Pointer(hashTable + uintptr(bucketIdx+i1)*4)) = uint32(0)
			goto _13
		_13:
			;
			i1 = i1 + 1
		}
		*(*U32)(unsafe.Pointer(hashTable + uintptr(bucketIdx+bucketSize-uint32(1))*4)) = chainPackedPointer
		goto _11
	_11:
	}
	/* fill the buckets of the hash table */
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	for {
		if !(idx < target) {
			break
		}
		h1 = uint32(ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch)) << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		/* Shift hash cache down 1. */
		i2 = cacheSize - uint32(1)
		for {
			if !(i2 != 0) {
				break
			}
			*(*U32)(unsafe.Pointer(hashTable + uintptr(h1+i2)*4)) = *(*U32)(unsafe.Pointer(hashTable + uintptr(h1+i2-uint32(1))*4))
			goto _15
		_15:
			;
			i2 = i2 - 1
		}
		*(*U32)(unsafe.Pointer(hashTable + uintptr(h1)*4)) = idx
		goto _14
	_14:
		;
		idx = idx + 1
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

// C documentation
//
//	/* Returns the longest match length found in the dedicated dict search structure.
//	 * If none are longer than the argument ml, then ml will be returned.
//	 */
func ZSTD_dedicatedDictSearch_lazy_search(tls *libc.TLS, offsetPtr uintptr, ml size_t, nbAttempts U32, dms uintptr, ip uintptr, iLimit uintptr, prefixStart uintptr, curr U32, dictLimit U32, ddsIdx size_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var bucketLimit, bucketSize, chainAttempt, chainAttempts, chainIndex, chainIndex1, chainLength, chainLimit, chainPackedPointer, chainPackedPointer1, ddsAttempt, ddsIndexDelta, ddsLowestIndex, ddsSize, matchIndex U32
	var currentMl, currentMl1 size_t
	var ddsBase, ddsEnd, match, match1 uintptr
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bucketLimit, bucketSize, chainAttempt, chainAttempts, chainIndex, chainIndex1, chainLength, chainLimit, chainPackedPointer, chainPackedPointer1, currentMl, currentMl1, ddsAttempt, ddsBase, ddsEnd, ddsIndexDelta, ddsLowestIndex, ddsSize, match, match1, matchIndex, v1
	ddsLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	ddsBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	ddsEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	ddsSize = libc.Uint32FromInt64(int64(ddsEnd) - int64(ddsBase))
	ddsIndexDelta = dictLimit - ddsSize
	bucketSize = libc.Uint32FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_LAZY_DDSS_BUCKET_LOG))
	if nbAttempts < bucketSize-uint32(1) {
		v1 = nbAttempts
	} else {
		v1 = bucketSize - uint32(1)
	}
	bucketLimit = v1
	ddsAttempt = uint32(0)
	for {
		if !(ddsAttempt < bucketSize-uint32(1)) {
			break
		}
		libc.X__builtin_prefetch(tls, ddsBase+uintptr(*(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(ddsAttempt))*4))), libc.VaList(bp+8, 0, int32(3)))
		goto _2
	_2:
		;
		ddsAttempt = ddsAttempt + 1
	}
	chainPackedPointer = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(bucketSize)-uint64(1))*4))
	chainIndex = chainPackedPointer >> int32(8)
	libc.X__builtin_prefetch(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable+uintptr(chainIndex)*4, libc.VaList(bp+8, 0, int32(3)))
	ddsAttempt = uint32(0)
	for {
		if !(ddsAttempt < bucketLimit) {
			break
		}
		currentMl = uint64(0)
		matchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(ddsAttempt))*4))
		match = ddsBase + uintptr(matchIndex)
		if !(matchIndex != 0) {
			return ml
		}
		/* guaranteed by table construction */
		_ = ddsLowestIndex
		if MEM_read32(tls, match) == MEM_read32(tls, ip) {
			/* assumption : matchIndex <= dictLimit-4 (by table construction) */
			currentMl = ZSTD_count_2segments(tls, ip+uintptr(4), match+uintptr(4), iLimit, ddsEnd, prefixStart) + uint64(4)
		}
		/* save best solution */
		if currentMl > ml {
			ml = currentMl
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex + ddsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl) == iLimit {
				/* best possible, avoids read overflow on next attempt */
				return ml
			}
		}
		goto _3
	_3:
		;
		ddsAttempt = ddsAttempt + 1
	}
	chainPackedPointer1 = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx+uint64(bucketSize)-uint64(1))*4))
	chainIndex1 = chainPackedPointer1 >> int32(8)
	chainLength = chainPackedPointer1 & uint32(0xFF)
	chainAttempts = nbAttempts - ddsAttempt
	if chainAttempts > chainLength {
		v1 = chainLength
	} else {
		v1 = chainAttempts
	}
	chainLimit = v1
	chainAttempt = uint32(0)
	for {
		if !(chainAttempt < chainLimit) {
			break
		}
		libc.X__builtin_prefetch(tls, ddsBase+uintptr(*(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable + uintptr(chainIndex1+chainAttempt)*4))), libc.VaList(bp+8, 0, int32(3)))
		goto _5
	_5:
		;
		chainAttempt = chainAttempt + 1
	}
	chainAttempt = uint32(0)
	for {
		if !(chainAttempt < chainLimit) {
			break
		}
		currentMl1 = uint64(0)
		matchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable + uintptr(chainIndex1)*4))
		match1 = ddsBase + uintptr(matchIndex)
		/* guaranteed by table construction */
		if MEM_read32(tls, match1) == MEM_read32(tls, ip) {
			/* assumption : matchIndex <= dictLimit-4 (by table construction) */
			currentMl1 = ZSTD_count_2segments(tls, ip+uintptr(4), match1+uintptr(4), iLimit, ddsEnd, prefixStart) + uint64(4)
		}
		/* save best solution */
		if currentMl1 > ml {
			ml = currentMl1
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex + ddsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl1) == iLimit {
				break
			} /* best possible, avoids read overflow on next attempt */
		}
		goto _6
	_6:
		;
		chainAttempt = chainAttempt + 1
		chainIndex1 = chainIndex1 + 1
	}
	return ml
}

/* *********************************
*  Hash Chain
***********************************/

// C documentation
//
//	/* Update chains up to ip (excluded)
//	   Assumption : always within prefix (i.e. not within extDict) */
func ZSTD_insertAndFindFirstIndex_internal(tls *libc.TLS, ms uintptr, cParams uintptr, ip uintptr, mls U32, lazySkipping U32) (r U32) {
	var base, chainTable, hashTable uintptr
	var chainMask, hashLog, idx, target U32
	var h size_t
	_, _, _, _, _, _, _, _ = base, chainMask, chainTable, h, hashLog, hashTable, idx, target
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	chainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	chainMask = libc.Uint32FromInt32(int32(1)<<(*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - int32(1))
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = libc.Uint32FromInt64(int64(ip) - int64(base))
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	for idx < target { /* catch up */
		h = ZSTD_hashPtr(tls, base+uintptr(idx), hashLog, mls)
		*(*U32)(unsafe.Pointer(chainTable + uintptr(idx&chainMask)*4)) = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
		*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = idx
		idx = idx + 1
		/* Stop inserting every position when in the lazy skipping mode. */
		if lazySkipping != 0 {
			break
		}
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
	return *(*U32)(unsafe.Pointer(hashTable + uintptr(ZSTD_hashPtr(tls, ip, hashLog, mls))*4))
}

func ZSTD_insertAndFindFirstIndex(tls *libc.TLS, ms uintptr, ip uintptr) (r U32) {
	var cParams uintptr
	_ = cParams
	cParams = ms + 256
	return ZSTD_insertAndFindFirstIndex_internal(tls, ms, cParams, ip, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch, uint32(0))
}

// C documentation
//
//	/* inlining is important to hardwire a hot branch (template emulation) */
func ZSTD_HcFindBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr, mls U32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var base, cParams, chainTable, dictBase, dictEnd, dms, dmsBase, dmsChainTable, dmsEnd, entry, match, match1, match2, prefixStart uintptr
	var chainMask, chainSize, curr, ddsHashLog, dictLimit, dmsChainMask, dmsChainSize, dmsIndexDelta, dmsLowestIndex, dmsMinChain, dmsSize, isDictionary, lowLimit, lowestValid, matchIndex, maxDistance, minChain, nbAttempts, withinMaxDistance U32
	var currentMl, currentMl1, ddsIdx, ml size_t
	var v1, v2, v3, v4 uint32
	var v5 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, chainMask, chainSize, chainTable, curr, currentMl, currentMl1, ddsHashLog, ddsIdx, dictBase, dictEnd, dictLimit, dms, dmsBase, dmsChainMask, dmsChainSize, dmsChainTable, dmsEnd, dmsIndexDelta, dmsLowestIndex, dmsMinChain, dmsSize, entry, isDictionary, lowLimit, lowestValid, match, match1, match2, matchIndex, maxDistance, minChain, ml, nbAttempts, prefixStart, withinMaxDistance, v1, v2, v3, v4, v5
	cParams = ms + 256
	chainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	chainSize = libc.Uint32FromInt32(libc.Int32FromInt32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog)
	chainMask = chainSize - uint32(1)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(dictLimit)
	dictEnd = dictBase + uintptr(dictLimit)
	curr = libc.Uint32FromInt64(int64(ip) - int64(base))
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinMaxDistance = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinMaxDistance
	}
	lowLimit = v2
	if curr > chainSize {
		v3 = curr - chainSize
	} else {
		v3 = uint32(0)
	}
	minChain = v3
	nbAttempts = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	ml = libc.Uint64FromInt32(libc.Int32FromInt32(4) - libc.Int32FromInt32(1))
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FhashLog - uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	} else {
		v4 = uint32(0)
	}
	ddsHashLog = v4
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		v5 = ZSTD_hashPtr(tls, ip, ddsHashLog, mls) << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
	} else {
		v5 = uint64(0)
	}
	ddsIdx = v5
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		entry = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ddsIdx)*4
		libc.X__builtin_prefetch(tls, entry, libc.VaList(bp+8, 0, int32(3)))
	}
	/* HC4 match finder */
	matchIndex = ZSTD_insertAndFindFirstIndex_internal(tls, ms, cParams, ip, mls, libc.Uint32FromInt32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping))
	for {
		if !(libc.BoolInt32(matchIndex >= lowLimit)&libc.BoolInt32(nbAttempts > uint32(0)) != 0) {
			break
		}
		currentMl = uint64(0)
		if dictMode != int32(ZSTD_extDict) || matchIndex >= dictLimit {
			match = base + uintptr(matchIndex)
			/* ensures this is true if dictMode != ZSTD_extDict */
			/* read 4B starting from (match + ml + 1 - sizeof(U32)) */
			if MEM_read32(tls, match+uintptr(ml)-uintptr(3)) == MEM_read32(tls, ip+uintptr(ml)-uintptr(3)) { /* potentially better */
				currentMl = ZSTD_count(tls, ip, match, iLimit)
			}
		} else {
			match1 = dictBase + uintptr(matchIndex)
			if MEM_read32(tls, match1) == MEM_read32(tls, ip) { /* assumption : matchIndex <= dictLimit-4 (by table construction) */
				currentMl = ZSTD_count_2segments(tls, ip+uintptr(4), match1+uintptr(4), iLimit, dictEnd, prefixStart) + uint64(4)
			}
		}
		/* save best solution */
		if currentMl > ml {
			ml = currentMl
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl) == iLimit {
				break
			} /* best possible, avoids read overflow on next attempt */
		}
		if matchIndex <= minChain {
			break
		}
		matchIndex = *(*U32)(unsafe.Pointer(chainTable + uintptr(matchIndex&chainMask)*4))
		goto _6
	_6:
		;
		nbAttempts = nbAttempts - 1
	}
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		ml = ZSTD_dedicatedDictSearch_lazy_search(tls, offsetPtr, ml, nbAttempts, dms, ip, iLimit, prefixStart, curr, dictLimit, ddsIdx)
	} else {
		if dictMode == int32(ZSTD_dictMatchState) {
			dmsChainTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
			dmsChainSize = libc.Uint32FromInt32(libc.Int32FromInt32(1) << (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FchainLog)
			dmsChainMask = dmsChainSize - uint32(1)
			dmsLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
			dmsBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
			dmsEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
			dmsSize = libc.Uint32FromInt64(int64(dmsEnd) - int64(dmsBase))
			dmsIndexDelta = dictLimit - dmsSize
			if dmsSize > dmsChainSize {
				v1 = dmsSize - dmsChainSize
			} else {
				v1 = uint32(0)
			}
			dmsMinChain = v1
			matchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(ZSTD_hashPtr(tls, ip, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FhashLog, mls))*4))
			for {
				if !(libc.BoolInt32(matchIndex >= dmsLowestIndex)&libc.BoolInt32(nbAttempts > uint32(0)) != 0) {
					break
				}
				currentMl1 = uint64(0)
				match2 = dmsBase + uintptr(matchIndex)
				if MEM_read32(tls, match2) == MEM_read32(tls, ip) { /* assumption : matchIndex <= dictLimit-4 (by table construction) */
					currentMl1 = ZSTD_count_2segments(tls, ip+uintptr(4), match2+uintptr(4), iLimit, dmsEnd, prefixStart) + uint64(4)
				}
				/* save best solution */
				if currentMl1 > ml {
					ml = currentMl1
					*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex + dmsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
					if ip+uintptr(currentMl1) == iLimit {
						break
					} /* best possible, avoids read overflow on next attempt */
				}
				if matchIndex <= dmsMinChain {
					break
				}
				matchIndex = *(*U32)(unsafe.Pointer(dmsChainTable + uintptr(matchIndex&dmsChainMask)*4))
				goto _8
			_8:
				;
				nbAttempts = nbAttempts - 1
			}
		}
	}
	return ml
}

/* *********************************
* (SIMD) Row-based matchfinder
***********************************/
/* Constants for row-based hash */

type ZSTD_VecMask = uint64 /* Clarifies when we are interacting with a U64 representing a mask of matches */

// C documentation
//
//	/* ZSTD_VecMask_next():
//	 * Starting from the LSB, returns the idx of the next non-zero bit.
//	 * Basically counting the nb of trailing zeroes.
//	 */
func ZSTD_VecMask_next(tls *libc.TLS, val ZSTD_VecMask) (r U32) {
	return ZSTD_countTrailingZeros64(tls, val)
}

// C documentation
//
//	/* ZSTD_row_nextIndex():
//	 * Returns the next index to insert at within a tagTable row, and updates the "head"
//	 * value to reflect the update. Essentially cycles backwards from [1, {entries per row})
//	 */
func ZSTD_row_nextIndex(tls *libc.TLS, tagRow uintptr, rowMask U32) (r U32) {
	var next U32
	var v1 uint32
	_, _ = next, v1
	next = libc.Uint32FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(tagRow)))-libc.Int32FromInt32(1)) & rowMask
	if next == uint32(0) {
		v1 = rowMask
	} else {
		v1 = uint32(0)
	}
	next = next + v1 /* skip first position */
	*(*BYTE)(unsafe.Pointer(tagRow)) = uint8(next)
	return next
}

// C documentation
//
//	/* ZSTD_isAligned():
//	 * Checks that a pointer is aligned to "align" bytes which must be a power of 2.
//	 */
func ZSTD_isAligned(tls *libc.TLS, ptr uintptr, align size_t) (r int32) {
	return libc.BoolInt32(uint64(ptr)&(align-uint64(1)) == uint64(0))
}

// C documentation
//
//	/* ZSTD_row_prefetch():
//	 * Performs prefetching for the hashTable and tagTable at a given row.
//	 */
func ZSTD_row_prefetch(tls *libc.TLS, hashTable uintptr, tagTable uintptr, relRow U32, rowLog U32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	libc.X__builtin_prefetch(tls, hashTable+uintptr(relRow)*4, libc.VaList(bp+8, 0, int32(3)))
	if rowLog >= uint32(5) {
		libc.X__builtin_prefetch(tls, hashTable+uintptr(relRow)*4+libc.UintptrFromInt32(16)*4, libc.VaList(bp+8, 0, int32(3)))
		/* Note: prefetching more of the hash table does not appear to be beneficial for 128-entry rows */
	}
	libc.X__builtin_prefetch(tls, tagTable+uintptr(relRow), libc.VaList(bp+8, 0, int32(3)))
	if rowLog == uint32(6) {
		libc.X__builtin_prefetch(tls, tagTable+uintptr(relRow)+libc.UintptrFromInt32(32), libc.VaList(bp+8, 0, int32(3)))
	}
	/* prefetched hash row always 64-byte aligned */
	/* prefetched tagRow sits on correct multiple of bytes (32,64,128) */
}

// C documentation
//
//	/* ZSTD_row_fillHashCache():
//	 * Fill up the hash cache starting at idx, prefetching up to ZSTD_ROW_HASH_CACHE_SIZE entries,
//	 * but not beyond iLimit.
//	 */
func ZSTD_row_fillHashCache(tls *libc.TLS, ms uintptr, base uintptr, rowLog U32, mls U32, idx U32, iLimit uintptr) {
	var hash, hashLog, lim, maxElemsToPrefetch, row U32
	var hashTable, tagTable uintptr
	var v1, v2 uint32
	_, _, _, _, _, _, _, _, _ = hash, hashLog, hashTable, lim, maxElemsToPrefetch, row, tagTable, v1, v2
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	tagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog
	if base+uintptr(idx) > iLimit {
		v1 = uint32(0)
	} else {
		v1 = libc.Uint32FromInt64(int64(iLimit) - int64(base+uintptr(idx)) + libc.Int64FromInt32(1))
	}
	maxElemsToPrefetch = v1
	if libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)) < maxElemsToPrefetch {
		v2 = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE))
	} else {
		v2 = maxElemsToPrefetch
	}
	lim = idx + v2
	for {
		if !(idx < lim) {
			break
		}
		hash = uint32(ZSTD_hashPtrSalted(tls, base+uintptr(idx), hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt))
		row = hash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
		ZSTD_row_prefetch(tls, hashTable, tagTable, row, rowLog)
		*(*U32)(unsafe.Pointer(ms + 64 + uintptr(idx&libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)-libc.Int32FromInt32(1)))*4)) = hash
		goto _3
	_3:
		;
		idx = idx + 1
	}
}

// C documentation
//
//	/* ZSTD_row_nextCachedHash():
//	 * Returns the hash of base + idx, and replaces the hash in the hash cache with the byte at
//	 * base + idx + ZSTD_ROW_HASH_CACHE_SIZE. Also prefetches the appropriate rows from hashTable and tagTable.
//	 */
func ZSTD_row_nextCachedHash(tls *libc.TLS, cache uintptr, hashTable uintptr, tagTable uintptr, base uintptr, idx U32, hashLog U32, rowLog U32, mls U32, hashSalt U64) (r U32) {
	var hash, newHash, row U32
	_, _, _ = hash, newHash, row
	newHash = uint32(ZSTD_hashPtrSalted(tls, base+uintptr(idx)+uintptr(ZSTD_ROW_HASH_CACHE_SIZE), hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, hashSalt))
	row = newHash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
	ZSTD_row_prefetch(tls, hashTable, tagTable, row, rowLog)
	hash = *(*U32)(unsafe.Pointer(cache + uintptr(idx&libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)-libc.Int32FromInt32(1)))*4))
	*(*U32)(unsafe.Pointer(cache + uintptr(idx&libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_ROW_HASH_CACHE_SIZE)-libc.Int32FromInt32(1)))*4)) = newHash
	return hash
	return r
}

// C documentation
//
//	/* ZSTD_row_update_internalImpl():
//	 * Updates the hash table with positions starting from updateStartIdx until updateEndIdx.
//	 */
func ZSTD_row_update_internalImpl(tls *libc.TLS, ms uintptr, updateStartIdx U32, updateEndIdx U32, mls U32, rowLog U32, rowMask U32, useCache U32) {
	var base, hashTable, row, tagRow, tagTable uintptr
	var hash, hashLog, pos, relRow U32
	var v2 uint32
	_, _, _, _, _, _, _, _, _, _ = base, hash, hashLog, hashTable, pos, relRow, row, tagRow, tagTable, v2
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	tagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	for {
		if !(updateStartIdx < updateEndIdx) {
			break
		}
		if useCache != 0 {
			v2 = ZSTD_row_nextCachedHash(tls, ms+64, hashTable, tagTable, base, updateStartIdx, hashLog, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt)
		} else {
			v2 = uint32(ZSTD_hashPtrSalted(tls, base+uintptr(updateStartIdx), hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt))
		}
		hash = v2
		relRow = hash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
		row = hashTable + uintptr(relRow)*4
		tagRow = tagTable + uintptr(relRow)
		pos = ZSTD_row_nextIndex(tls, tagRow, rowMask)
		*(*BYTE)(unsafe.Pointer(tagRow + uintptr(pos))) = uint8(hash & (libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) - libc.Uint32FromInt32(1)))
		*(*U32)(unsafe.Pointer(row + uintptr(pos)*4)) = updateStartIdx
		goto _1
	_1:
		;
		updateStartIdx = updateStartIdx + 1
	}
}

// C documentation
//
//	/* ZSTD_row_update_internal():
//	 * Inserts the byte at ip into the appropriate position in the hash table, and updates ms->nextToUpdate.
//	 * Skips sections of long matches as is necessary.
//	 */
func ZSTD_row_update_internal(tls *libc.TLS, ms uintptr, ip uintptr, mls U32, rowLog U32, rowMask U32, useCache U32) {
	var base uintptr
	var bound, idx, kMaxMatchEndPositionsToUpdate, kMaxMatchStartPositionsToUpdate, kSkipThreshold, target U32
	_, _, _, _, _, _, _ = base, bound, idx, kMaxMatchEndPositionsToUpdate, kMaxMatchStartPositionsToUpdate, kSkipThreshold, target
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = libc.Uint32FromInt64(int64(ip) - int64(base))
	kSkipThreshold = uint32(384)
	kMaxMatchStartPositionsToUpdate = uint32(96)
	kMaxMatchEndPositionsToUpdate = uint32(32)
	if useCache != 0 {
		/* Only skip positions when using hash cache, i.e.
		 * if we are loading a dict, don't skip anything.
		 * If we decide to skip, then we only update a set number
		 * of positions at the beginning and end of the match.
		 */
		if libc.BoolInt64(target-idx > kSkipThreshold) != 0 {
			bound = idx + kMaxMatchStartPositionsToUpdate
			ZSTD_row_update_internalImpl(tls, ms, idx, bound, mls, rowLog, rowMask, useCache)
			idx = target - kMaxMatchEndPositionsToUpdate
			ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, idx, ip+uintptr(1))
		}
	}
	ZSTD_row_update_internalImpl(tls, ms, idx, target, mls, rowLog, rowMask, useCache)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

// C documentation
//
//	/* ZSTD_row_update():
//	 * External wrapper for ZSTD_row_update_internal(). Used for filling the hashtable during dictionary
//	 * processing.
//	 */
func ZSTD_row_update(tls *libc.TLS, ms uintptr, ip uintptr) {
	var mls, rowLog, rowMask U32
	var v1, v2, v3, v4 uint32
	_, _, _, _, _, _, _ = mls, rowLog, rowMask, v1, v2, v3, v4
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
		v2 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
	} else {
		v2 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
	}
	if libc.Uint32FromInt32(libc.Int32FromInt32(4)) > v2 {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
		} else {
			v3 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		v1 = v3
	}
	rowLog = v1
	rowMask = uint32(1)<<rowLog - uint32(1)
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
		v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v4 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
	}
	mls = v4
	ZSTD_row_update_internal(tls, ms, ip, mls, rowLog, rowMask, uint32(0))
}

// C documentation
//
//	/* Returns the mask width of bits group of which will be set to 1. Given not all
//	 * architectures have easy movemask instruction, this helps to iterate over
//	 * groups of bits easier and faster.
//	 */
func ZSTD_row_matchMaskGroupWidth(tls *libc.TLS, rowEntries U32) (r U32) {
	_ = rowEntries
	return uint32(1)
}

// C documentation
//
//	/* Returns a ZSTD_VecMask (U64) that has the nth group (determined by
//	 * ZSTD_row_matchMaskGroupWidth) of bits set to 1 if the newly-computed "tag"
//	 * matches the hash at the nth position in a row of the tagTable.
//	 * Each row is a circular buffer beginning at the value of "headGrouped". So we
//	 * must rotate the "matches" bitfield to match up with the actual layout of the
//	 * entries within the hashTable */
func ZSTD_row_getMatchMask(tls *libc.TLS, tagRow uintptr, tag BYTE, headGrouped U32, rowEntries U32) (r ZSTD_VecMask) {
	var chunk, chunk1, extractMagic, extractMagic1, msb, shiftAmount, splatChar, x01, x80, xFF size_t
	var chunkSize, i int32
	var matches ZSTD_VecMask
	var src uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = chunk, chunk1, chunkSize, extractMagic, extractMagic1, i, matches, msb, shiftAmount, splatChar, src, x01, x80, xFF
	src = tagRow
	/* SWAR */
	chunkSize = int32(8)
	shiftAmount = libc.Uint64FromInt32(chunkSize*libc.Int32FromInt32(8) - chunkSize)
	xFF = ^libc.Uint64FromInt32(0)
	x01 = xFF / uint64(0xFF)
	x80 = x01 << int32(7)
	splatChar = uint64(tag) * x01
	matches = uint64(0)
	i = libc.Int32FromUint32(rowEntries - libc.Uint32FromInt32(chunkSize))
	if MEM_isLittleEndian(tls) != 0 { /* runtime check so have two loops */
		extractMagic = xFF / uint64(0x7F) >> chunkSize
		for cond := true; cond; cond = i >= 0 {
			chunk = MEM_readST(tls, src+uintptr(i))
			chunk = chunk ^ splatChar
			chunk = (chunk | x80 - x01 | chunk) & x80
			matches = matches << libc.Uint64FromInt32(chunkSize)
			matches = matches | chunk*extractMagic>>shiftAmount
			i = i - chunkSize
		}
	} else { /* big endian: reverse bits during extraction */
		msb = xFF ^ xFF>>libc.Int32FromInt32(1)
		extractMagic1 = msb/uint64(0x1FF) | msb
		for cond := true; cond; cond = i >= 0 {
			chunk1 = MEM_readST(tls, src+uintptr(i))
			chunk1 = chunk1 ^ splatChar
			chunk1 = (chunk1 | x80 - x01 | chunk1) & x80
			matches = matches << libc.Uint64FromInt32(chunkSize)
			matches = matches | chunk1>>libc.Int32FromInt32(7)*extractMagic1>>shiftAmount
			i = i - chunkSize
		}
	}
	matches = ^matches
	if rowEntries == uint32(16) {
		return uint64(ZSTD_rotateRight_U16(tls, uint16(matches), headGrouped))
	} else {
		if rowEntries == uint32(32) {
			return uint64(ZSTD_rotateRight_U32(tls, uint32(matches), headGrouped))
		} else {
			return ZSTD_rotateRight_U64(tls, matches, headGrouped)
		}
	}
	return r
}

// C documentation
//
//	/* The high-level approach of the SIMD row based match finder is as follows:
//	 * - Figure out where to insert the new entry:
//	 *      - Generate a hash for current input position and split it into a one byte of tag and `rowHashLog` bits of index.
//	 *           - The hash is salted by a value that changes on every context reset, so when the same table is used
//	 *             we will avoid collisions that would otherwise slow us down by introducing phantom matches.
//	 *      - The hashTable is effectively split into groups or "rows" of 15 or 31 entries of U32, and the index determines
//	 *        which row to insert into.
//	 *      - Determine the correct position within the row to insert the entry into. Each row of 15 or 31 can
//	 *        be considered as a circular buffer with a "head" index that resides in the tagTable (overall 16 or 32 bytes
//	 *        per row).
//	 * - Use SIMD to efficiently compare the tags in the tagTable to the 1-byte tag calculated for the position and
//	 *   generate a bitfield that we can cycle through to check the collisions in the hash table.
//	 * - Pick the longest match.
//	 * - Insert the tag into the equivalent row and position in the tagTable.
//	 */
func ZSTD_RowFindBestMatch(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr, mls U32, dictMode ZSTD_dictMode_e, rowLog U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var base, cParams, dictBase, dictEnd, dms, dmsBase, dmsEnd, dmsHashTable, dmsRow, dmsTagRow, dmsTagTable, hashCache, hashTable, match, match1, match2, prefixStart, row, tagRow, tagTable, v8 uintptr
	var cappedSearchLog, curr, ddsExtraAttempts, ddsHashLog, dictLimit, dmsHash, dmsIndexDelta, dmsLowestIndex, dmsRelRow, dmsSize, dmsTag, groupWidth, hash, hashLog, headGrouped, headGrouped1, isDictionary, lowLimit, lowestValid, matchIndex, matchIndex1, matchIndex2, matchIndex3, matchPos, matchPos1, maxDistance, nbAttempts, pos, relRow, rowEntries, rowMask, tag, withinMaxDistance, v7 U32
	var currMatch, currMatch1, currentMl, currentMl1, ddsIdx, ml, numMatches, numMatches1, v6 size_t
	var hashSalt U64
	var matchBuffer, matchBuffer1 [64]U32
	var matches, matches1 ZSTD_VecMask
	var v1, v2, v3 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, cParams, cappedSearchLog, curr, currMatch, currMatch1, currentMl, currentMl1, ddsExtraAttempts, ddsHashLog, ddsIdx, dictBase, dictEnd, dictLimit, dms, dmsBase, dmsEnd, dmsHash, dmsHashTable, dmsIndexDelta, dmsLowestIndex, dmsRelRow, dmsRow, dmsSize, dmsTag, dmsTagRow, dmsTagTable, groupWidth, hash, hashCache, hashLog, hashSalt, hashTable, headGrouped, headGrouped1, isDictionary, lowLimit, lowestValid, match, match1, match2, matchBuffer, matchBuffer1, matchIndex, matchIndex1, matchIndex2, matchIndex3, matchPos, matchPos1, matches, matches1, maxDistance, ml, nbAttempts, numMatches, numMatches1, pos, prefixStart, relRow, row, rowEntries, rowMask, tag, tagRow, tagTable, withinMaxDistance, v1, v2, v3, v6, v7, v8
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	tagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FtagTable
	hashCache = ms + 64
	hashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FrowHashLog
	cParams = ms + 256
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(dictLimit)
	dictEnd = dictBase + uintptr(dictLimit)
	curr = libc.Uint32FromInt64(int64(ip) - int64(base))
	maxDistance = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	lowestValid = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit
	if curr-lowestValid > maxDistance {
		v1 = curr - maxDistance
	} else {
		v1 = lowestValid
	}
	withinMaxDistance = v1
	isDictionary = libc.BoolUint32((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FloadedDictEnd != libc.Uint32FromInt32(0))
	if isDictionary != 0 {
		v2 = lowestValid
	} else {
		v2 = withinMaxDistance
	}
	lowLimit = v2
	rowEntries = libc.Uint32FromUint32(1) << rowLog
	rowMask = rowEntries - uint32(1)
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog < rowLog {
		v3 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	} else {
		v3 = rowLog
	}
	cappedSearchLog = v3 /* nb of searches is capped at nb entries per row */
	groupWidth = ZSTD_row_matchMaskGroupWidth(tls, rowEntries)
	hashSalt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashSalt
	nbAttempts = uint32(1) << cappedSearchLog
	ml = libc.Uint64FromInt32(libc.Int32FromInt32(4) - libc.Int32FromInt32(1))
	/* DMS/DDS variables that may be referenced laster */
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	/* Initialize the following variables to satisfy static analyzer */
	ddsIdx = uint64(0)
	ddsExtraAttempts = uint32(0) /* cctx hash tables are limited in searches, but allow extra searches into DDS */
	dmsTag = uint32(0)
	dmsRow = libc.UintptrFromInt32(0)
	dmsTagRow = libc.UintptrFromInt32(0)
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		ddsHashLog = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FcParams.FhashLog - uint32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		/* Prefetch DDS hashtable entry */
		ddsIdx = ZSTD_hashPtr(tls, ip, ddsHashLog, mls) << int32(ZSTD_LAZY_DDSS_BUCKET_LOG)
		libc.X__builtin_prefetch(tls, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable+uintptr(ddsIdx)*4, libc.VaList(bp+8, 0, int32(3)))
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog > rowLog {
			v1 = uint32(1) << ((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog - rowLog)
		} else {
			v1 = uint32(0)
		}
		ddsExtraAttempts = v1
	}
	if dictMode == int32(ZSTD_dictMatchState) {
		/* Prefetch DMS rows */
		dmsHashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable
		dmsTagTable = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FtagTable
		dmsHash = uint32(ZSTD_hashPtr(tls, ip, (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FrowHashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls))
		dmsRelRow = dmsHash >> libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
		dmsTag = dmsHash & (libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) - libc.Uint32FromInt32(1))
		dmsTagRow = dmsTagTable + uintptr(dmsRelRow)
		dmsRow = dmsHashTable + uintptr(dmsRelRow)*4
		ZSTD_row_prefetch(tls, dmsHashTable, dmsTagTable, dmsRelRow, rowLog)
	}
	/* Update the hashTable and tagTable up to (but not including) ip */
	if !((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping != 0) {
		ZSTD_row_update_internal(tls, ms, ip, mls, rowLog, rowMask, uint32(1))
		hash = ZSTD_row_nextCachedHash(tls, hashCache, hashTable, tagTable, base, curr, hashLog, rowLog, mls, hashSalt)
	} else {
		/* Stop inserting every position when in the lazy skipping mode.
		 * The hash cache is also not kept up to date in this mode.
		 */
		hash = uint32(ZSTD_hashPtrSalted(tls, ip, hashLog+uint32(ZSTD_ROW_HASH_TAG_BITS), mls, hashSalt))
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr
	}
	*(*U32)(unsafe.Pointer(ms + 104)) += hash /* collect salt entropy */
	/* Get the hash for ip, compute the appropriate row */
	relRow = hash >> int32(ZSTD_ROW_HASH_TAG_BITS) << rowLog
	tag = hash & (libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_ROW_HASH_TAG_BITS) - libc.Uint32FromInt32(1))
	row = hashTable + uintptr(relRow)*4
	tagRow = tagTable + uintptr(relRow)
	headGrouped = uint32(*(*BYTE)(unsafe.Pointer(tagRow))) & rowMask * groupWidth
	numMatches = uint64(0)
	currMatch = uint64(0)
	matches = ZSTD_row_getMatchMask(tls, tagRow, uint8(tag), headGrouped, rowEntries)
	/* Cycle through the matches and prefetch */
	for {
		if !(matches > uint64(0) && nbAttempts > uint32(0)) {
			break
		}
		matchPos = (headGrouped + ZSTD_VecMask_next(tls, matches)) / groupWidth & rowMask
		matchIndex = *(*U32)(unsafe.Pointer(row + uintptr(matchPos)*4))
		if matchPos == uint32(0) {
			goto _5
		}
		if matchIndex < lowLimit {
			break
		}
		if dictMode != int32(ZSTD_extDict) || matchIndex >= dictLimit {
			libc.X__builtin_prefetch(tls, base+uintptr(matchIndex), libc.VaList(bp+8, 0, int32(3)))
		} else {
			libc.X__builtin_prefetch(tls, dictBase+uintptr(matchIndex), libc.VaList(bp+8, 0, int32(3)))
		}
		v6 = numMatches
		numMatches = numMatches + 1
		matchBuffer[v6] = matchIndex
		nbAttempts = nbAttempts - 1
		goto _5
	_5:
		;
		matches = matches & (matches - uint64(1))
	}
	/* Speed opt: insert current byte into hashtable too. This allows us to avoid one iteration of the loop
	   in ZSTD_row_update_internal() at the next search. */
	pos = ZSTD_row_nextIndex(tls, tagRow, rowMask)
	*(*BYTE)(unsafe.Pointer(tagRow + uintptr(pos))) = uint8(tag)
	v8 = ms + 44
	v7 = *(*U32)(unsafe.Pointer(v8))
	*(*U32)(unsafe.Pointer(v8)) = *(*U32)(unsafe.Pointer(v8)) + 1
	*(*U32)(unsafe.Pointer(row + uintptr(pos)*4)) = v7
	/* Return the longest match */
	for {
		if !(currMatch < numMatches) {
			break
		}
		matchIndex1 = matchBuffer[currMatch]
		currentMl = uint64(0)
		if dictMode != int32(ZSTD_extDict) || matchIndex1 >= dictLimit {
			match = base + uintptr(matchIndex1)
			/* ensures this is true if dictMode != ZSTD_extDict */
			/* read 4B starting from (match + ml + 1 - sizeof(U32)) */
			if MEM_read32(tls, match+uintptr(ml)-uintptr(3)) == MEM_read32(tls, ip+uintptr(ml)-uintptr(3)) { /* potentially better */
				currentMl = ZSTD_count(tls, ip, match, iLimit)
			}
		} else {
			match1 = dictBase + uintptr(matchIndex1)
			if MEM_read32(tls, match1) == MEM_read32(tls, ip) { /* assumption : matchIndex <= dictLimit-4 (by table construction) */
				currentMl = ZSTD_count_2segments(tls, ip+uintptr(4), match1+uintptr(4), iLimit, dictEnd, prefixStart) + uint64(4)
			}
		}
		/* Save best solution */
		if currentMl > ml {
			ml = currentMl
			*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - matchIndex1 + libc.Uint32FromInt32(ZSTD_REP_NUM))
			if ip+uintptr(currentMl) == iLimit {
				break
			} /* best possible, avoids read overflow on next attempt */
		}
		goto _9
	_9:
		;
		currMatch = currMatch + 1
	}
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dedicatedDictSearch) {
		ml = ZSTD_dedicatedDictSearch_lazy_search(tls, offsetPtr, ml, nbAttempts+ddsExtraAttempts, dms, ip, iLimit, prefixStart, curr, dictLimit, ddsIdx)
	} else {
		if dictMode == int32(ZSTD_dictMatchState) {
			/* TODO: Measure and potentially add prefetching to DMS */
			dmsLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
			dmsBase = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
			dmsEnd = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
			dmsSize = libc.Uint32FromInt64(int64(dmsEnd) - int64(dmsBase))
			dmsIndexDelta = dictLimit - dmsSize
			headGrouped1 = uint32(*(*BYTE)(unsafe.Pointer(dmsTagRow))) & rowMask * groupWidth
			numMatches1 = uint64(0)
			currMatch1 = uint64(0)
			matches1 = ZSTD_row_getMatchMask(tls, dmsTagRow, uint8(dmsTag), headGrouped1, rowEntries)
			for {
				if !(matches1 > uint64(0) && nbAttempts > uint32(0)) {
					break
				}
				matchPos1 = (headGrouped1 + ZSTD_VecMask_next(tls, matches1)) / groupWidth & rowMask
				matchIndex2 = *(*U32)(unsafe.Pointer(dmsRow + uintptr(matchPos1)*4))
				if matchPos1 == uint32(0) {
					goto _10
				}
				if matchIndex2 < dmsLowestIndex {
					break
				}
				libc.X__builtin_prefetch(tls, dmsBase+uintptr(matchIndex2), libc.VaList(bp+8, 0, int32(3)))
				v6 = numMatches1
				numMatches1 = numMatches1 + 1
				matchBuffer1[v6] = matchIndex2
				nbAttempts = nbAttempts - 1
				goto _10
			_10:
				;
				matches1 = matches1 & (matches1 - uint64(1))
			}
			/* Return the longest match */
			for {
				if !(currMatch1 < numMatches1) {
					break
				}
				matchIndex3 = matchBuffer1[currMatch1]
				currentMl1 = uint64(0)
				match2 = dmsBase + uintptr(matchIndex3)
				if MEM_read32(tls, match2) == MEM_read32(tls, ip) {
					currentMl1 = ZSTD_count_2segments(tls, ip+uintptr(4), match2+uintptr(4), iLimit, dmsEnd, prefixStart) + uint64(4)
				}
				if currentMl1 > ml {
					ml = currentMl1
					*(*size_t)(unsafe.Pointer(offsetPtr)) = uint64(curr - (matchIndex3 + dmsIndexDelta) + libc.Uint32FromInt32(ZSTD_REP_NUM))
					if ip+uintptr(currentMl1) == iLimit {
						break
					}
				}
				goto _12
			_12:
				;
				currMatch1 = currMatch1 + 1
			}
		}
	}
	return ml
}

/**
 * Generate search functions templated on (dictMode, mls, rowLog).
 * These functions are outlined for code size & compilation time.
 * ZSTD_searchMax() dispatches to the correct implementation function.
 *
 * TODO: The start of the search function involves loading and calculating a
 * bunch of constants from the ZSTD_MatchState_t. These computations could be
 * done in an initialization function, and saved somewhere in the match state.
 * Then we could pass a pointer to the saved state instead of the match state,
 * and avoid duplicate computations.
 *
 * TODO: Move the match re-winding into searchMax. This improves compression
 * ratio, and unlocks further simplifications with the next TODO.
 *
 * TODO: Try moving the repcode search into searchMax. After the re-winding
 * and repcode search are in searchMax, there is no more logic in the match
 * finder loop that requires knowledge about the dictMode. So we should be
 * able to avoid force inlining it, and we can join the extDict loop with
 * the single segment loop. It should go in searchMax instead of its own
 * function to avoid having multiple virtual function calls per search.
 */

// C documentation
//
//	/* Generate row search fns for each combination of (dictMode, mls, rowLog) */
func ZSTD_RowFindBestMatch_noDict_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict), uint32(4))
}

func ZSTD_RowFindBestMatch_noDict_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict), uint32(5))
}

func ZSTD_RowFindBestMatch_noDict_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict), uint32(6))
}

func ZSTD_RowFindBestMatch_noDict_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict), uint32(4))
}

func ZSTD_RowFindBestMatch_noDict_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict), uint32(5))
}

func ZSTD_RowFindBestMatch_noDict_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict), uint32(6))
}

func ZSTD_RowFindBestMatch_noDict_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict), uint32(4))
}

func ZSTD_RowFindBestMatch_noDict_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict), uint32(5))
}

func ZSTD_RowFindBestMatch_noDict_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict), uint32(6))
}

func ZSTD_RowFindBestMatch_extDict_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict), uint32(4))
}

func ZSTD_RowFindBestMatch_extDict_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict), uint32(5))
}

func ZSTD_RowFindBestMatch_extDict_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict), uint32(6))
}

func ZSTD_RowFindBestMatch_extDict_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict), uint32(4))
}

func ZSTD_RowFindBestMatch_extDict_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict), uint32(5))
}

func ZSTD_RowFindBestMatch_extDict_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict), uint32(6))
}

func ZSTD_RowFindBestMatch_extDict_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict), uint32(4))
}

func ZSTD_RowFindBestMatch_extDict_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict), uint32(5))
}

func ZSTD_RowFindBestMatch_extDict_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict), uint32(6))
}

func ZSTD_RowFindBestMatch_dictMatchState_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_RowFindBestMatch_dictMatchState_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_RowFindBestMatch_dictMatchState_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_RowFindBestMatch_dictMatchState_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_RowFindBestMatch_dictMatchState_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_RowFindBestMatch_dictMatchState_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_RowFindBestMatch_dictMatchState_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_RowFindBestMatch_dictMatchState_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_RowFindBestMatch_dictMatchState_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_4_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch), uint32(4))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_4_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch), uint32(5))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_4_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch), uint32(6))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_5_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch), uint32(4))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_5_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch), uint32(5))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_5_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch), uint32(6))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_6_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch), uint32(4))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_6_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch), uint32(5))
}

func ZSTD_RowFindBestMatch_dedicatedDictSearch_6_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_RowFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch), uint32(6))
}

// C documentation
//
//	/* Generate binary Tree search fns for each combination of (dictMode, mls) */
func ZSTD_BtFindBestMatch_noDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_noDict))
}

func ZSTD_BtFindBestMatch_noDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_noDict))
}

func ZSTD_BtFindBestMatch_noDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_noDict))
}

func ZSTD_BtFindBestMatch_extDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_extDict))
}

func ZSTD_BtFindBestMatch_extDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_extDict))
}

func ZSTD_BtFindBestMatch_extDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_extDict))
}

func ZSTD_BtFindBestMatch_dictMatchState_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_dictMatchState))
}

func ZSTD_BtFindBestMatch_dictMatchState_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_dictMatchState))
}

func ZSTD_BtFindBestMatch_dictMatchState_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_dictMatchState))
}

func ZSTD_BtFindBestMatch_dedicatedDictSearch_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(4), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_BtFindBestMatch_dedicatedDictSearch_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(5), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_BtFindBestMatch_dedicatedDictSearch_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offBasePtr uintptr) (r size_t) {
	return ZSTD_BtFindBestMatch(tls, ms, ip, iLimit, offBasePtr, uint32(6), int32(ZSTD_dedicatedDictSearch))
}

// C documentation
//
//	/* Generate hash chain search fns for each combination of (dictMode, mls) */
func ZSTD_HcFindBestMatch_noDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_noDict))
}

func ZSTD_HcFindBestMatch_noDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_noDict))
}

func ZSTD_HcFindBestMatch_noDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_noDict))
}

func ZSTD_HcFindBestMatch_extDict_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_extDict))
}

func ZSTD_HcFindBestMatch_extDict_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_extDict))
}

func ZSTD_HcFindBestMatch_extDict_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_extDict))
}

func ZSTD_HcFindBestMatch_dictMatchState_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dictMatchState))
}

func ZSTD_HcFindBestMatch_dictMatchState_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dictMatchState))
}

func ZSTD_HcFindBestMatch_dictMatchState_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dictMatchState))
}

func ZSTD_HcFindBestMatch_dedicatedDictSearch_4(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(4), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_HcFindBestMatch_dedicatedDictSearch_5(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(5), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_HcFindBestMatch_dedicatedDictSearch_6(tls *libc.TLS, ms uintptr, ip uintptr, iLimit uintptr, offsetPtr uintptr) (r size_t) {
	return ZSTD_HcFindBestMatch(tls, ms, ip, iLimit, offsetPtr, uint32(6), int32(ZSTD_dedicatedDictSearch))
}

type searchMethod_e = int32

const search_hashChain = 0
const search_binaryTree = 1
const search_rowHash = 2

// C documentation
//
//	/**
//	 * Searches for the longest match at @p ip.
//	 * Dispatches to the correct implementation function based on the
//	 * (searchMethod, dictMode, mls, rowLog). We use switch statements
//	 * here instead of using an indirect function call through a function
//	 * pointer because after Spectre and Meltdown mitigations, indirect
//	 * function calls can be very costly, especially in the kernel.
//	 *
//	 * NOTE: dictMode and searchMethod should be templated, so those switch
//	 * statements should be optimized out. Only the mls & rowLog switches
//	 * should be left.
//	 *
//	 * @param ms The match state.
//	 * @param ip The position to search at.
//	 * @param iend The end of the input data.
//	 * @param[out] offsetPtr Stores the match offset into this pointer.
//	 * @param mls The minimum search length, in the range [4, 6].
//	 * @param rowLog The row log (if applicable), in the range [4, 6].
//	 * @param searchMethod The search method to use (templated).
//	 * @param dictMode The dictMode (templated).
//	 *
//	 * @returns The length of the longest match found, or < mls if no match is found.
//	 * If a match is found its offset is stored in @p offsetPtr.
//	 */
func ZSTD_searchMax(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, offsetPtr uintptr, mls U32, rowLog U32, searchMethod searchMethod_e, dictMode ZSTD_dictMode_e) (r size_t) {
	if dictMode == int32(ZSTD_noDict) {
		switch searchMethod {
		case int32(search_hashChain):
			switch mls {
			case uint32(4):
				return ZSTD_HcFindBestMatch_noDict_4(tls, ms, ip, iend, offsetPtr)
			case uint32(5):
				return ZSTD_HcFindBestMatch_noDict_5(tls, ms, ip, iend, offsetPtr)
			case uint32(6):
				return ZSTD_HcFindBestMatch_noDict_6(tls, ms, ip, iend, offsetPtr)
			}
		case int32(search_binaryTree):
			switch mls {
			case uint32(4):
				return ZSTD_BtFindBestMatch_noDict_4(tls, ms, ip, iend, offsetPtr)
			case uint32(5):
				return ZSTD_BtFindBestMatch_noDict_5(tls, ms, ip, iend, offsetPtr)
			case uint32(6):
				return ZSTD_BtFindBestMatch_noDict_6(tls, ms, ip, iend, offsetPtr)
			}
		case int32(search_rowHash):
			switch mls {
			case uint32(4):
				switch rowLog {
				case uint32(4):
					return ZSTD_RowFindBestMatch_noDict_4_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_RowFindBestMatch_noDict_4_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_RowFindBestMatch_noDict_4_6(tls, ms, ip, iend, offsetPtr)
				}
				libc.X__builtin_unreachable(tls)
			case uint32(5):
				switch rowLog {
				case uint32(4):
					return ZSTD_RowFindBestMatch_noDict_5_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_RowFindBestMatch_noDict_5_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_RowFindBestMatch_noDict_5_6(tls, ms, ip, iend, offsetPtr)
				}
				libc.X__builtin_unreachable(tls)
			case uint32(6):
				switch rowLog {
				case uint32(4):
					return ZSTD_RowFindBestMatch_noDict_6_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_RowFindBestMatch_noDict_6_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_RowFindBestMatch_noDict_6_6(tls, ms, ip, iend, offsetPtr)
				}
				libc.X__builtin_unreachable(tls)
				break
			}
			break
		}
		libc.X__builtin_unreachable(tls)
	} else {
		if dictMode == int32(ZSTD_extDict) {
			switch searchMethod {
			case int32(search_hashChain):
				switch mls {
				case uint32(4):
					return ZSTD_HcFindBestMatch_extDict_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_HcFindBestMatch_extDict_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_HcFindBestMatch_extDict_6(tls, ms, ip, iend, offsetPtr)
				}
			case int32(search_binaryTree):
				switch mls {
				case uint32(4):
					return ZSTD_BtFindBestMatch_extDict_4(tls, ms, ip, iend, offsetPtr)
				case uint32(5):
					return ZSTD_BtFindBestMatch_extDict_5(tls, ms, ip, iend, offsetPtr)
				case uint32(6):
					return ZSTD_BtFindBestMatch_extDict_6(tls, ms, ip, iend, offsetPtr)
				}
			case int32(search_rowHash):
				switch mls {
				case uint32(4):
					switch rowLog {
					case uint32(4):
						return ZSTD_RowFindBestMatch_extDict_4_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_RowFindBestMatch_extDict_4_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_RowFindBestMatch_extDict_4_6(tls, ms, ip, iend, offsetPtr)
					}
					libc.X__builtin_unreachable(tls)
				case uint32(5):
					switch rowLog {
					case uint32(4):
						return ZSTD_RowFindBestMatch_extDict_5_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_RowFindBestMatch_extDict_5_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_RowFindBestMatch_extDict_5_6(tls, ms, ip, iend, offsetPtr)
					}
					libc.X__builtin_unreachable(tls)
				case uint32(6):
					switch rowLog {
					case uint32(4):
						return ZSTD_RowFindBestMatch_extDict_6_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_RowFindBestMatch_extDict_6_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_RowFindBestMatch_extDict_6_6(tls, ms, ip, iend, offsetPtr)
					}
					libc.X__builtin_unreachable(tls)
					break
				}
				break
			}
			libc.X__builtin_unreachable(tls)
		} else {
			if dictMode == int32(ZSTD_dictMatchState) {
				switch searchMethod {
				case int32(search_hashChain):
					switch mls {
					case uint32(4):
						return ZSTD_HcFindBestMatch_dictMatchState_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_HcFindBestMatch_dictMatchState_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_HcFindBestMatch_dictMatchState_6(tls, ms, ip, iend, offsetPtr)
					}
				case int32(search_binaryTree):
					switch mls {
					case uint32(4):
						return ZSTD_BtFindBestMatch_dictMatchState_4(tls, ms, ip, iend, offsetPtr)
					case uint32(5):
						return ZSTD_BtFindBestMatch_dictMatchState_5(tls, ms, ip, iend, offsetPtr)
					case uint32(6):
						return ZSTD_BtFindBestMatch_dictMatchState_6(tls, ms, ip, iend, offsetPtr)
					}
				case int32(search_rowHash):
					switch mls {
					case uint32(4):
						switch rowLog {
						case uint32(4):
							return ZSTD_RowFindBestMatch_dictMatchState_4_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_RowFindBestMatch_dictMatchState_4_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_RowFindBestMatch_dictMatchState_4_6(tls, ms, ip, iend, offsetPtr)
						}
						libc.X__builtin_unreachable(tls)
					case uint32(5):
						switch rowLog {
						case uint32(4):
							return ZSTD_RowFindBestMatch_dictMatchState_5_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_RowFindBestMatch_dictMatchState_5_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_RowFindBestMatch_dictMatchState_5_6(tls, ms, ip, iend, offsetPtr)
						}
						libc.X__builtin_unreachable(tls)
					case uint32(6):
						switch rowLog {
						case uint32(4):
							return ZSTD_RowFindBestMatch_dictMatchState_6_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_RowFindBestMatch_dictMatchState_6_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_RowFindBestMatch_dictMatchState_6_6(tls, ms, ip, iend, offsetPtr)
						}
						libc.X__builtin_unreachable(tls)
						break
					}
					break
				}
				libc.X__builtin_unreachable(tls)
			} else {
				if dictMode == int32(ZSTD_dedicatedDictSearch) {
					switch searchMethod {
					case int32(search_hashChain):
						switch mls {
						case uint32(4):
							return ZSTD_HcFindBestMatch_dedicatedDictSearch_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_HcFindBestMatch_dedicatedDictSearch_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_HcFindBestMatch_dedicatedDictSearch_6(tls, ms, ip, iend, offsetPtr)
						}
					case int32(search_binaryTree):
						switch mls {
						case uint32(4):
							return ZSTD_BtFindBestMatch_dedicatedDictSearch_4(tls, ms, ip, iend, offsetPtr)
						case uint32(5):
							return ZSTD_BtFindBestMatch_dedicatedDictSearch_5(tls, ms, ip, iend, offsetPtr)
						case uint32(6):
							return ZSTD_BtFindBestMatch_dedicatedDictSearch_6(tls, ms, ip, iend, offsetPtr)
						}
					case int32(search_rowHash):
						switch mls {
						case uint32(4):
							switch rowLog {
							case uint32(4):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_4_4(tls, ms, ip, iend, offsetPtr)
							case uint32(5):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_4_5(tls, ms, ip, iend, offsetPtr)
							case uint32(6):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_4_6(tls, ms, ip, iend, offsetPtr)
							}
							libc.X__builtin_unreachable(tls)
						case uint32(5):
							switch rowLog {
							case uint32(4):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_5_4(tls, ms, ip, iend, offsetPtr)
							case uint32(5):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_5_5(tls, ms, ip, iend, offsetPtr)
							case uint32(6):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_5_6(tls, ms, ip, iend, offsetPtr)
							}
							libc.X__builtin_unreachable(tls)
						case uint32(6):
							switch rowLog {
							case uint32(4):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_6_4(tls, ms, ip, iend, offsetPtr)
							case uint32(5):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_6_5(tls, ms, ip, iend, offsetPtr)
							case uint32(6):
								return ZSTD_RowFindBestMatch_dedicatedDictSearch_6_6(tls, ms, ip, iend, offsetPtr)
							}
							libc.X__builtin_unreachable(tls)
							break
						}
						break
					}
					libc.X__builtin_unreachable(tls)
				}
			}
		}
	}
	libc.X__builtin_unreachable(tls)
	return uint64(0)
}

/* *******************************
*  Common parser - lazy strategy
*********************************/
func ZSTD_compressBlock_lazy_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, searchMethod searchMethod_e, depth U32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, dictBase, dictEnd, dictLowest, dms, iend, ilimit, ip, istart, mStart, match, prefixLowest, repEnd2, repMatch, repMatch1, repMatch2, repMatch3, repMatchEnd, repMatchEnd1, repMatchEnd2, start, v1, v10, v11, v9 uintptr
	var curr, current2, dictAndPrefixLength, dictIndexDelta, dictLowestIndex, matchIndex, maxRep, mls, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowestIndex, repIndex, repIndex1, repIndex2, repIndex3, rowLog, windowLow U32
	var gain1, gain11, gain12, gain13, gain14, gain15, gain2, gain21, gain22, gain23, gain24, gain25, isDDS, isDMS, isDxS int32
	var litLength, matchLength, ml2, ml21, ml22, mlRep, mlRep1, mlRep2, mlRep3, offBase, step size_t
	var v12, v2, v3, v4, v5, v6, v7, v8 uint32
	var v19 bool
	var _ /* ofbCandidate at bp+16 */ size_t
	var _ /* ofbCandidate at bp+8 */ size_t
	var _ /* offbaseFound at bp+0 */ size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, curr, current2, dictAndPrefixLength, dictBase, dictEnd, dictIndexDelta, dictLowest, dictLowestIndex, dms, gain1, gain11, gain12, gain13, gain14, gain15, gain2, gain21, gain22, gain23, gain24, gain25, iend, ilimit, ip, isDDS, isDMS, isDxS, istart, litLength, mStart, match, matchIndex, matchLength, maxRep, ml2, ml21, ml22, mlRep, mlRep1, mlRep2, mlRep3, mls, offBase, offsetSaved1, offsetSaved2, offset_1, offset_2, prefixLowest, prefixLowestIndex, repEnd2, repIndex, repIndex1, repIndex2, repIndex3, repMatch, repMatch1, repMatch2, repMatch3, repMatchEnd, repMatchEnd1, repMatchEnd2, rowLog, start, step, windowLow, v1, v10, v11, v12, v19, v2, v3, v4, v5, v6, v7, v8, v9
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	if searchMethod == int32(search_rowHash) {
		v1 = iend - uintptr(8) - uintptr(ZSTD_ROW_HASH_CACHE_SIZE)
	} else {
		v1 = iend - uintptr(8)
	}
	ilimit = v1
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	prefixLowestIndex = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixLowest = base + uintptr(prefixLowestIndex)
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
		v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v3 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
	}
	if libc.Uint32FromInt32(libc.Int32FromInt32(4)) > v3 {
		v2 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
		} else {
			v4 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		v2 = v4
	}
	mls = v2
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
		v6 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
	} else {
		v6 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
	}
	if libc.Uint32FromInt32(libc.Int32FromInt32(4)) > v6 {
		v5 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v7 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
		} else {
			v7 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		v5 = v7
	}
	rowLog = v5
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	offsetSaved1 = uint32(0)
	offsetSaved2 = uint32(0)
	isDMS = libc.BoolInt32(dictMode == int32(ZSTD_dictMatchState))
	isDDS = libc.BoolInt32(dictMode == int32(ZSTD_dedicatedDictSearch))
	isDxS = libc.BoolInt32(isDMS != 0 || isDDS != 0)
	dms = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	if isDxS != 0 {
		v8 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FdictLimit
	} else {
		v8 = uint32(0)
	}
	dictLowestIndex = v8
	if isDxS != 0 {
		v9 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	} else {
		v9 = libc.UintptrFromInt32(0)
	}
	dictBase = v9
	if isDxS != 0 {
		v10 = dictBase + uintptr(dictLowestIndex)
	} else {
		v10 = libc.UintptrFromInt32(0)
	}
	dictLowest = v10
	if isDxS != 0 {
		v11 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	} else {
		v11 = libc.UintptrFromInt32(0)
	}
	dictEnd = v11
	if isDxS != 0 {
		v12 = prefixLowestIndex - libc.Uint32FromInt64(int64(dictEnd)-int64(dictBase))
	} else {
		v12 = uint32(0)
	}
	dictIndexDelta = v12
	dictAndPrefixLength = libc.Uint32FromInt64(int64(ip) - int64(prefixLowest) + (int64(dictEnd) - int64(dictLowest)))
	ip = ip + libc.BoolUintptr(dictAndPrefixLength == libc.Uint32FromInt32(0))
	if dictMode == int32(ZSTD_noDict) {
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		windowLow = ZSTD_getLowestPrefixIndex(tls, ms, curr, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FwindowLog)
		maxRep = curr - windowLow
		if offset_2 > maxRep {
			offsetSaved2 = offset_2
			offset_2 = libc.Uint32FromInt32(0)
		}
		if offset_1 > maxRep {
			offsetSaved1 = offset_1
			offset_1 = libc.Uint32FromInt32(0)
		}
	}
	if isDxS != 0 {
		/* dictMatchState repCode checks don't currently handle repCode == 0
		 * disabling. */
	}
	/* Reset the lazy skipping state */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
	if searchMethod == int32(search_rowHash) {
		ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
	}
	/* Match Loop */
	for ip < ilimit {
		matchLength = uint64(0)
		offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
		start = ip + uintptr(1)
		/* check repCode */
		if isDxS != 0 {
			repIndex = libc.Uint32FromInt64(int64(ip)-int64(base)) + uint32(1) - offset_1
			if (dictMode == int32(ZSTD_dictMatchState) || dictMode == int32(ZSTD_dedicatedDictSearch)) && repIndex < prefixLowestIndex {
				v1 = dictBase + uintptr(repIndex-dictIndexDelta)
			} else {
				v1 = base + uintptr(repIndex)
			}
			repMatch = v1
			if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex) != 0 && MEM_read32(tls, repMatch) == MEM_read32(tls, ip+uintptr(1)) {
				if repIndex < prefixLowestIndex {
					v9 = dictEnd
				} else {
					v9 = iend
				}
				repMatchEnd = v9
				matchLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repMatchEnd, prefixLowest) + uint64(4)
				if depth == uint32(0) {
					goto _storeSequence
				}
			}
		}
		if dictMode == int32(ZSTD_noDict) && libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip+uintptr(1)-uintptr(offset_1)) == MEM_read32(tls, ip+uintptr(1))) != 0 {
			matchLength = ZSTD_count(tls, ip+uintptr(1)+uintptr(4), ip+uintptr(1)+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
			if depth == uint32(0) {
				goto _storeSequence
			}
		}
		/* first search (depth 0) */
		*(*size_t)(unsafe.Pointer(bp)) = uint64(999999999)
		ml2 = ZSTD_searchMax(tls, ms, ip, iend, bp, mls, rowLog, searchMethod, dictMode)
		if ml2 > matchLength {
			matchLength = ml2
			start = ip
			offBase = *(*size_t)(unsafe.Pointer(bp))
		}
		if matchLength < uint64(4) {
			step = libc.Uint64FromInt64(int64(ip)-int64(anchor))>>libc.Int32FromInt32(kSearchStrength) + uint64(1) /* jump faster over incompressible sections */
			ip = ip + uintptr(step)
			/* Enter the lazy skipping mode once we are skipping more than 8 bytes at a time.
			 * In this mode we stop inserting every position into our tables, and only insert
			 * positions that we search, which is one in step positions.
			 * The exact cutoff is flexible, I've just chosen a number that is reasonably high,
			 * so we minimize the compression ratio loss in "normal" scenarios. This mode gets
			 * triggered once we've gone 2KB without finding any matches.
			 */
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = libc.BoolInt32(step > uint64(kLazySkippingStep))
			continue
		}
		/* let's try to find a better solution */
		if depth >= uint32(1) {
			for ip < ilimit {
				ip = ip + 1
				if dictMode == int32(ZSTD_noDict) && offBase != 0 && libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_1))) != 0 {
					mlRep = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
					gain2 = libc.Int32FromUint64(mlRep * libc.Uint64FromInt32(3))
					gain1 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(3) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
					if mlRep >= uint64(4) && gain2 > gain1 {
						matchLength = mlRep
						offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
						start = ip
					}
				}
				if isDxS != 0 {
					repIndex1 = libc.Uint32FromInt64(int64(ip)-int64(base)) - offset_1
					if repIndex1 < prefixLowestIndex {
						v1 = dictBase + uintptr(repIndex1-dictIndexDelta)
					} else {
						v1 = base + uintptr(repIndex1)
					}
					repMatch1 = v1
					if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex1) != 0 && MEM_read32(tls, repMatch1) == MEM_read32(tls, ip) {
						if repIndex1 < prefixLowestIndex {
							v9 = dictEnd
						} else {
							v9 = iend
						}
						repMatchEnd1 = v9
						mlRep1 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch1+uintptr(4), iend, repMatchEnd1, prefixLowest) + uint64(4)
						gain21 = libc.Int32FromUint64(mlRep1 * libc.Uint64FromInt32(3))
						gain11 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(3) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
						if mlRep1 >= uint64(4) && gain21 > gain11 {
							matchLength = mlRep1
							offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
							start = ip
						}
					}
				}
				*(*size_t)(unsafe.Pointer(bp + 8)) = uint64(999999999)
				ml21 = ZSTD_searchMax(tls, ms, ip, iend, bp+8, mls, rowLog, searchMethod, dictMode)
				gain22 = libc.Int32FromUint64(ml21*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 8)))))) /* raw approx */
				gain12 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(4))
				if ml21 >= uint64(4) && gain22 > gain12 {
					matchLength = ml21
					offBase = *(*size_t)(unsafe.Pointer(bp + 8))
					start = ip
					continue /* search a better one */
				}
				/* let's find an even better one */
				if depth == uint32(2) && ip < ilimit {
					ip = ip + 1
					if dictMode == int32(ZSTD_noDict) && offBase != 0 && libc.BoolInt32(offset_1 > uint32(0))&libc.BoolInt32(MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_1))) != 0 {
						mlRep2 = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_1), iend) + uint64(4)
						gain23 = libc.Int32FromUint64(mlRep2 * libc.Uint64FromInt32(4))
						gain13 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
						if mlRep2 >= uint64(4) && gain23 > gain13 {
							matchLength = mlRep2
							offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
							start = ip
						}
					}
					if isDxS != 0 {
						repIndex2 = libc.Uint32FromInt64(int64(ip)-int64(base)) - offset_1
						if repIndex2 < prefixLowestIndex {
							v1 = dictBase + uintptr(repIndex2-dictIndexDelta)
						} else {
							v1 = base + uintptr(repIndex2)
						}
						repMatch2 = v1
						if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex2) != 0 && MEM_read32(tls, repMatch2) == MEM_read32(tls, ip) {
							if repIndex2 < prefixLowestIndex {
								v9 = dictEnd
							} else {
								v9 = iend
							}
							repMatchEnd2 = v9
							mlRep3 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repMatchEnd2, prefixLowest) + uint64(4)
							gain24 = libc.Int32FromUint64(mlRep3 * libc.Uint64FromInt32(4))
							gain14 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
							if mlRep3 >= uint64(4) && gain24 > gain14 {
								matchLength = mlRep3
								offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
								start = ip
							}
						}
					}
					*(*size_t)(unsafe.Pointer(bp + 16)) = uint64(999999999)
					ml22 = ZSTD_searchMax(tls, ms, ip, iend, bp+16, mls, rowLog, searchMethod, dictMode)
					gain25 = libc.Int32FromUint64(ml22*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 16)))))) /* raw approx */
					gain15 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(7))
					if ml22 >= uint64(4) && gain25 > gain15 {
						matchLength = ml22
						offBase = *(*size_t)(unsafe.Pointer(bp + 16))
						start = ip
						continue
					}
				}
				break /* nothing found : store previous solution */
			}
		}
		/* NOTE:
		 * Pay attention that `start[-value]` can lead to strange undefined behavior
		 * notably if `value` is unsigned, resulting in a large positive `-value`.
		 */
		/* catch up */
		if offBase > uint64(ZSTD_REP_NUM) {
			if dictMode == int32(ZSTD_noDict) {
				for {
					if v19 = libc.BoolInt32(start > anchor)&libc.BoolInt32(start-uintptr(offBase-libc.Uint64FromInt32(ZSTD_REP_NUM)) > prefixLowest) != 0; v19 {
					}
					if !(v19 && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(start + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(start - uintptr(offBase-libc.Uint64FromInt32(ZSTD_REP_NUM)) + uintptr(-libc.Int32FromInt32(1)))))) {
						break
					} /* only search for offset within prefix */
					start = start - 1
					matchLength = matchLength + 1
				}
			}
			if isDxS != 0 {
				matchIndex = uint32(libc.Uint64FromInt64(int64(start)-int64(base)) - (offBase - libc.Uint64FromInt32(ZSTD_REP_NUM)))
				if matchIndex < prefixLowestIndex {
					v1 = dictBase + uintptr(matchIndex) - uintptr(dictIndexDelta)
				} else {
					v1 = base + uintptr(matchIndex)
				}
				match = v1
				if matchIndex < prefixLowestIndex {
					v9 = dictLowest
				} else {
					v9 = prefixLowest
				}
				mStart = v9
				for start > anchor && match > mStart && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(start + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
					start = start - 1
					match = match - 1
					matchLength = matchLength + 1
				} /* catch up */
			}
			offset_2 = offset_1
			offset_1 = uint32(offBase - libc.Uint64FromInt32(ZSTD_REP_NUM))
		}
		/* store sequence */
		goto _storeSequence
	_storeSequence:
		;
		litLength = libc.Uint64FromInt64(int64(start) - int64(anchor))
		ZSTD_storeSeq(tls, seqStore, litLength, anchor, iend, uint32(offBase), matchLength)
		v1 = start + uintptr(matchLength)
		ip = v1
		anchor = v1
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping != 0 {
			/* We've found a match, disable lazy skipping mode, and refill the hash cache. */
			if searchMethod == int32(search_rowHash) {
				ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
			}
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
		}
		/* check immediate repcode */
		if isDxS != 0 {
			for ip <= ilimit {
				current2 = libc.Uint32FromInt64(int64(ip) - int64(base))
				repIndex3 = current2 - offset_2
				if repIndex3 < prefixLowestIndex {
					v1 = dictBase - uintptr(dictIndexDelta) + uintptr(repIndex3)
				} else {
					v1 = base + uintptr(repIndex3)
				}
				repMatch3 = v1
				if ZSTD_index_overlap_check(tls, prefixLowestIndex, repIndex3) != 0 && MEM_read32(tls, repMatch3) == MEM_read32(tls, ip) {
					if repIndex3 < prefixLowestIndex {
						v9 = dictEnd
					} else {
						v9 = iend
					}
					repEnd2 = v9
					matchLength = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch3+uintptr(4), iend, repEnd2, prefixLowest) + uint64(4)
					offBase = uint64(offset_2)
					offset_2 = offset_1
					offset_1 = uint32(offBase) /* swap offset_2 <=> offset_1 */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), matchLength)
					ip = ip + uintptr(matchLength)
					anchor = ip
					continue
				}
				break
			}
		}
		if dictMode == int32(ZSTD_noDict) {
			for libc.BoolInt32(ip <= ilimit)&libc.BoolInt32(offset_2 > uint32(0)) != 0 && MEM_read32(tls, ip) == MEM_read32(tls, ip-uintptr(offset_2)) {
				/* store sequence */
				matchLength = ZSTD_count(tls, ip+uintptr(4), ip+uintptr(4)-uintptr(offset_2), iend) + uint64(4)
				offBase = uint64(offset_2)
				offset_2 = offset_1
				offset_1 = uint32(offBase) /* swap repcodes */
				ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), matchLength)
				ip = ip + uintptr(matchLength)
				anchor = ip
				continue /* faster when present ... (?) */
			}
		}
	}
	/* If offset_1 started invalid (offsetSaved1 != 0) and became valid (offset_1 != 0),
	 * rotate saved offsets. See comment in ZSTD_compressBlock_fast_noDict for more context. */
	if offsetSaved1 != uint32(0) && offset_1 != uint32(0) {
		v2 = offsetSaved1
	} else {
		v2 = offsetSaved2
	}
	offsetSaved2 = v2
	/* save reps for next block */
	if offset_1 != 0 {
		v2 = offset_1
	} else {
		v2 = offsetSaved1
	}
	*(*U32)(unsafe.Pointer(rep)) = v2
	if offset_2 != 0 {
		v2 = offset_2
	} else {
		v2 = offsetSaved2
	}
	*(*U32)(unsafe.Pointer(rep + 1*4)) = v2
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_greedy(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_greedy_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_greedy_dedicatedDictSearch(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_greedy_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_greedy_dictMatchState_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_greedy_dedicatedDictSearch_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy_dedicatedDictSearch(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy_dictMatchState_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy_dedicatedDictSearch_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy2_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy2_dedicatedDictSearch(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_lazy2_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_lazy2_dictMatchState_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy2_dedicatedDictSearch_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2), int32(ZSTD_dedicatedDictSearch))
}

func ZSTD_compressBlock_btlazy2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_binaryTree), uint32(2), int32(ZSTD_noDict))
}

func ZSTD_compressBlock_btlazy2_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_binaryTree), uint32(2), int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_lazy_extDict_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, searchMethod searchMethod_e, depth U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var anchor, base, dictBase, dictEnd, dictStart, iend, ilimit, ip, istart, mStart, match, prefixStart, repBase, repBase1, repBase2, repBase3, repEnd, repEnd1, repEnd2, repEnd3, repMatch, repMatch1, repMatch2, repMatch3, start, v1, v8 uintptr
	var curr, dictLimit, matchIndex, mls, offset_1, offset_2, repCurrent, repIndex, repIndex1, repIndex2, repIndex3, rowLog, windowLog, windowLow, windowLow1, windowLow2, windowLow3 U32
	var gain1, gain11, gain12, gain13, gain2, gain21, gain22, gain23 int32
	var litLength, matchLength, ml2, ml21, ml22, offBase, repLength, repLength1, step size_t
	var v2, v3, v4, v5, v6, v7 uint32
	var _ /* ofbCandidate at bp+0 */ size_t
	var _ /* ofbCandidate at bp+16 */ size_t
	var _ /* ofbCandidate at bp+8 */ size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, base, curr, dictBase, dictEnd, dictLimit, dictStart, gain1, gain11, gain12, gain13, gain2, gain21, gain22, gain23, iend, ilimit, ip, istart, litLength, mStart, match, matchIndex, matchLength, ml2, ml21, ml22, mls, offBase, offset_1, offset_2, prefixStart, repBase, repBase1, repBase2, repBase3, repCurrent, repEnd, repEnd1, repEnd2, repEnd3, repIndex, repIndex1, repIndex2, repIndex3, repLength, repLength1, repMatch, repMatch1, repMatch2, repMatch3, rowLog, start, step, windowLog, windowLow, windowLow1, windowLow2, windowLow3, v1, v2, v3, v4, v5, v6, v7, v8
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	if searchMethod == int32(search_rowHash) {
		v1 = iend - uintptr(8) - uintptr(ZSTD_ROW_HASH_CACHE_SIZE)
	} else {
		v1 = iend - uintptr(8)
	}
	ilimit = v1
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	prefixStart = base + uintptr(dictLimit)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictEnd = dictBase + uintptr(dictLimit)
	dictStart = dictBase + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit)
	windowLog = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FwindowLog
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
		v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v3 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
	}
	if libc.Uint32FromInt32(libc.Int32FromInt32(4)) > v3 {
		v2 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v4 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
		} else {
			v4 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		v2 = v4
	}
	mls = v2
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
		v6 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
	} else {
		v6 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
	}
	if libc.Uint32FromInt32(libc.Int32FromInt32(4)) > v6 {
		v5 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v7 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FsearchLog
		} else {
			v7 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		v5 = v7
	}
	rowLog = v5
	offset_1 = *(*U32)(unsafe.Pointer(rep))
	offset_2 = *(*U32)(unsafe.Pointer(rep + 1*4))
	/* Reset the lazy skipping state */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
	/* init */
	ip = ip + libc.BoolUintptr(ip == prefixStart)
	if searchMethod == int32(search_rowHash) {
		ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
	}
	/* Match Loop */
	for ip < ilimit {
		matchLength = uint64(0)
		offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
		start = ip + uintptr(1)
		curr = libc.Uint32FromInt64(int64(ip) - int64(base))
		/* check repCode */
		windowLow = ZSTD_getLowestMatchIndex(tls, ms, curr+uint32(1), windowLog)
		repIndex = curr + libc.Uint32FromInt32(1) - offset_1
		if repIndex < dictLimit {
			v1 = dictBase
		} else {
			v1 = base
		}
		repBase = v1
		repMatch = repBase + uintptr(repIndex)
		if ZSTD_index_overlap_check(tls, dictLimit, repIndex)&libc.BoolInt32(offset_1 <= curr+uint32(1)-windowLow) != 0 { /* note: we are searching at curr+1 */
			if MEM_read32(tls, ip+uintptr(1)) == MEM_read32(tls, repMatch) {
				if repIndex < dictLimit {
					v8 = dictEnd
				} else {
					v8 = iend
				}
				/* repcode detected we should take it */
				repEnd = v8
				matchLength = ZSTD_count_2segments(tls, ip+uintptr(1)+uintptr(4), repMatch+uintptr(4), iend, repEnd, prefixStart) + uint64(4)
				if depth == uint32(0) {
					goto _storeSequence
				}
			}
		}
		/* first search (depth 0) */
		*(*size_t)(unsafe.Pointer(bp)) = uint64(999999999)
		ml2 = ZSTD_searchMax(tls, ms, ip, iend, bp, mls, rowLog, searchMethod, int32(ZSTD_extDict))
		if ml2 > matchLength {
			matchLength = ml2
			start = ip
			offBase = *(*size_t)(unsafe.Pointer(bp))
		}
		if matchLength < uint64(4) {
			step = libc.Uint64FromInt64(int64(ip)-int64(anchor)) >> libc.Int32FromInt32(kSearchStrength)
			ip = ip + uintptr(step+uint64(1)) /* jump faster over incompressible sections */
			/* Enter the lazy skipping mode once we are skipping more than 8 bytes at a time.
			 * In this mode we stop inserting every position into our tables, and only insert
			 * positions that we search, which is one in step positions.
			 * The exact cutoff is flexible, I've just chosen a number that is reasonably high,
			 * so we minimize the compression ratio loss in "normal" scenarios. This mode gets
			 * triggered once we've gone 2KB without finding any matches.
			 */
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = libc.BoolInt32(step > uint64(kLazySkippingStep))
			continue
		}
		/* let's try to find a better solution */
		if depth >= uint32(1) {
			for ip < ilimit {
				ip = ip + 1
				curr = curr + 1
				/* check repCode */
				if offBase != 0 {
					windowLow1 = ZSTD_getLowestMatchIndex(tls, ms, curr, windowLog)
					repIndex1 = curr - offset_1
					if repIndex1 < dictLimit {
						v1 = dictBase
					} else {
						v1 = base
					}
					repBase1 = v1
					repMatch1 = repBase1 + uintptr(repIndex1)
					if ZSTD_index_overlap_check(tls, dictLimit, repIndex1)&libc.BoolInt32(offset_1 <= curr-windowLow1) != 0 { /* equivalent to `curr > repIndex >= windowLow` */
						if MEM_read32(tls, ip) == MEM_read32(tls, repMatch1) {
							if repIndex1 < dictLimit {
								v8 = dictEnd
							} else {
								v8 = iend
							}
							/* repcode detected */
							repEnd1 = v8
							repLength = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch1+uintptr(4), iend, repEnd1, prefixStart) + uint64(4)
							gain2 = libc.Int32FromUint64(repLength * libc.Uint64FromInt32(3))
							gain1 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(3) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
							if repLength >= uint64(4) && gain2 > gain1 {
								matchLength = repLength
								offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
								start = ip
							}
						}
					}
				}
				/* search match, depth 1 */
				*(*size_t)(unsafe.Pointer(bp + 8)) = uint64(999999999)
				ml21 = ZSTD_searchMax(tls, ms, ip, iend, bp+8, mls, rowLog, searchMethod, int32(ZSTD_extDict))
				gain21 = libc.Int32FromUint64(ml21*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 8)))))) /* raw approx */
				gain11 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(4))
				if ml21 >= uint64(4) && gain21 > gain11 {
					matchLength = ml21
					offBase = *(*size_t)(unsafe.Pointer(bp + 8))
					start = ip
					continue /* search a better one */
				}
				/* let's find an even better one */
				if depth == uint32(2) && ip < ilimit {
					ip = ip + 1
					curr = curr + 1
					/* check repCode */
					if offBase != 0 {
						windowLow2 = ZSTD_getLowestMatchIndex(tls, ms, curr, windowLog)
						repIndex2 = curr - offset_1
						if repIndex2 < dictLimit {
							v1 = dictBase
						} else {
							v1 = base
						}
						repBase2 = v1
						repMatch2 = repBase2 + uintptr(repIndex2)
						if ZSTD_index_overlap_check(tls, dictLimit, repIndex2)&libc.BoolInt32(offset_1 <= curr-windowLow2) != 0 { /* equivalent to `curr > repIndex >= windowLow` */
							if MEM_read32(tls, ip) == MEM_read32(tls, repMatch2) {
								if repIndex2 < dictLimit {
									v8 = dictEnd
								} else {
									v8 = iend
								}
								/* repcode detected */
								repEnd2 = v8
								repLength1 = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch2+uintptr(4), iend, repEnd2, prefixStart) + uint64(4)
								gain22 = libc.Int32FromUint64(repLength1 * libc.Uint64FromInt32(4))
								gain12 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(1))
								if repLength1 >= uint64(4) && gain22 > gain12 {
									matchLength = repLength1
									offBase = libc.Uint64FromInt32(libc.Int32FromInt32(1))
									start = ip
								}
							}
						}
					}
					/* search match, depth 2 */
					*(*size_t)(unsafe.Pointer(bp + 16)) = uint64(999999999)
					ml22 = ZSTD_searchMax(tls, ms, ip, iend, bp+16, mls, rowLog, searchMethod, int32(ZSTD_extDict))
					gain23 = libc.Int32FromUint64(ml22*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(*(*size_t)(unsafe.Pointer(bp + 16)))))) /* raw approx */
					gain13 = libc.Int32FromUint64(matchLength*libc.Uint64FromInt32(4) - uint64(ZSTD_highbit32(tls, uint32(offBase))) + libc.Uint64FromInt32(7))
					if ml22 >= uint64(4) && gain23 > gain13 {
						matchLength = ml22
						offBase = *(*size_t)(unsafe.Pointer(bp + 16))
						start = ip
						continue
					}
				}
				break /* nothing found : store previous solution */
			}
		}
		/* catch up */
		if offBase > uint64(ZSTD_REP_NUM) {
			matchIndex = uint32(libc.Uint64FromInt64(int64(start)-int64(base)) - (offBase - libc.Uint64FromInt32(ZSTD_REP_NUM)))
			if matchIndex < dictLimit {
				v1 = dictBase + uintptr(matchIndex)
			} else {
				v1 = base + uintptr(matchIndex)
			}
			match = v1
			if matchIndex < dictLimit {
				v8 = dictStart
			} else {
				v8 = prefixStart
			}
			mStart = v8
			for start > anchor && match > mStart && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(start + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(-libc.Int32FromInt32(1))))) {
				start = start - 1
				match = match - 1
				matchLength = matchLength + 1
			} /* catch up */
			offset_2 = offset_1
			offset_1 = uint32(offBase - libc.Uint64FromInt32(ZSTD_REP_NUM))
		}
		/* store sequence */
		goto _storeSequence
	_storeSequence:
		;
		litLength = libc.Uint64FromInt64(int64(start) - int64(anchor))
		ZSTD_storeSeq(tls, seqStore, litLength, anchor, iend, uint32(offBase), matchLength)
		v1 = start + uintptr(matchLength)
		ip = v1
		anchor = v1
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping != 0 {
			/* We've found a match, disable lazy skipping mode, and refill the hash cache. */
			if searchMethod == int32(search_rowHash) {
				ZSTD_row_fillHashCache(tls, ms, base, rowLog, mls, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate, ilimit)
			}
			(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FlazySkipping = 0
		}
		/* check immediate repcode */
		for ip <= ilimit {
			repCurrent = libc.Uint32FromInt64(int64(ip) - int64(base))
			windowLow3 = ZSTD_getLowestMatchIndex(tls, ms, repCurrent, windowLog)
			repIndex3 = repCurrent - offset_2
			if repIndex3 < dictLimit {
				v1 = dictBase
			} else {
				v1 = base
			}
			repBase3 = v1
			repMatch3 = repBase3 + uintptr(repIndex3)
			if ZSTD_index_overlap_check(tls, dictLimit, repIndex3)&libc.BoolInt32(offset_2 <= repCurrent-windowLow3) != 0 { /* equivalent to `curr > repIndex >= windowLow` */
				if MEM_read32(tls, ip) == MEM_read32(tls, repMatch3) {
					if repIndex3 < dictLimit {
						v8 = dictEnd
					} else {
						v8 = iend
					}
					/* repcode detected we should take it */
					repEnd3 = v8
					matchLength = ZSTD_count_2segments(tls, ip+uintptr(4), repMatch3+uintptr(4), iend, repEnd3, prefixStart) + uint64(4)
					offBase = uint64(offset_2)
					offset_2 = offset_1
					offset_1 = uint32(offBase) /* swap offset history */
					ZSTD_storeSeq(tls, seqStore, uint64(0), anchor, iend, libc.Uint32FromInt32(libc.Int32FromInt32(1)), matchLength)
					ip = ip + uintptr(matchLength)
					anchor = ip
					continue /* faster when present ... (?) */
				}
			}
			break
		}
	}
	/* Save reps for next block */
	*(*U32)(unsafe.Pointer(rep)) = offset_1
	*(*U32)(unsafe.Pointer(rep + 1*4)) = offset_2
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_greedy_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(0))
}

func ZSTD_compressBlock_greedy_extDict_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(0))
}

func ZSTD_compressBlock_lazy_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(1))
}

func ZSTD_compressBlock_lazy_extDict_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(1))
}

func ZSTD_compressBlock_lazy2_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_hashChain), uint32(2))
}

func ZSTD_compressBlock_lazy2_extDict_row(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_rowHash), uint32(2))
}

func ZSTD_compressBlock_btlazy2_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_lazy_extDict_generic(tls, ms, seqStore, rep, src, srcSize, int32(search_binaryTree), uint32(2))
}

/**** ended inlining compress/zstd_lazy.c ****/
/**** start inlining compress/zstd_ldm.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_ldm.h ****/

/**** skipping file: ../common/debug.h ****/
/**** skipping file: ../common/xxhash.h ****/
/**** skipping file: zstd_fast.h ****/
/**** skipping file: zstd_double_fast.h ****/
/**** start inlining zstd_ldm_geartab.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/mem.h ****/

var ZSTD_ldm_gearTab = [256]U64{
	0:   uint64(0xf5b8f72c5f77775c),
	1:   uint64(0x84935f266b7ac412),
	2:   uint64(0xb647ada9ca730ccc),
	3:   uint64(0xb065bb4b114fb1de),
	4:   uint64(0x34584e7e8c3a9fd0),
	5:   uint64(0x4e97e17c6ae26b05),
	6:   uint64(0x3a03d743bc99a604),
	7:   uint64(0xcecd042422c4044f),
	8:   uint64(0x76de76c58524259e),
	9:   uint64(0x9c8528f65badeaca),
	10:  uint64(0x86563706e2097529),
	11:  uint64(0x2902475fa375d889),
	12:  uint64(0xafb32a9739a5ebe6),
	13:  uint64(0xce2714da3883e639),
	14:  uint64(0x21eaf821722e69e),
	15:  uint64(0x37b628620b628),
	16:  uint64(0x49a8d455d88caf5),
	17:  uint64(0x8556d711e6958140),
	18:  uint64(0x4f7ae74fc605c1f),
	19:  uint64(0x829f0c3468bd3a20),
	20:  uint64(0x4ffdc885c625179e),
	21:  uint64(0x8473de048a3daf1b),
	22:  uint64(0x51008822b05646b2),
	23:  uint64(0x69d75d12b2d1cc5f),
	24:  uint64(0x8c9d4a19159154bc),
	25:  uint64(0xc3cc10f4abbd4003),
	26:  uint64(0xd06ddc1cecb97391),
	27:  uint64(0xbe48e6e7ed80302e),
	28:  uint64(0x3481db31cee03547),
	29:  uint64(0xacc3f67cdaa1d210),
	30:  uint64(0x65cb771d8c7f96cc),
	31:  uint64(0x8eb27177055723dd),
	32:  uint64(0xc789950d44cd94be),
	33:  uint64(0x934feadc3700b12b),
	34:  uint64(0x5e485f11edbdf182),
	35:  uint64(0x1e2e2a46fd64767a),
	36:  uint64(0x2969ca71d82efa7c),
	37:  uint64(0x9d46e9935ebbba2e),
	38:  uint64(0xe056b67e05e6822b),
	39:  uint64(0x94d73f55739d03a0),
	40:  uint64(0xcd7010bdb69b5a03),
	41:  uint64(0x455ef9fcd79b82f4),
	42:  uint64(0x869cb54a8749c161),
	43:  uint64(0x38d1a4fa6185d225),
	44:  uint64(0xb475166f94bbe9bb),
	45:  uint64(0xa4143548720959f1),
	46:  uint64(0x7aed4780ba6b26ba),
	47:  uint64(0xd0ce264439e02312),
	48:  uint64(0x84366d746078d508),
	49:  uint64(0xa8ce973c72ed17be),
	50:  uint64(0x21c323a29a430b01),
	51:  uint64(0x9962d617e3af80ee),
	52:  uint64(0xab0ce91d9c8cf75b),
	53:  uint64(0x530e8ee6d19a4dbc),
	54:  uint64(0x2ef68c0cf53f5d72),
	55:  uint64(0xc03a681640a85506),
	56:  uint64(0x496e4e9f9c310967),
	57:  uint64(0x78580472b59b14a0),
	58:  uint64(0x273824c23b388577),
	59:  uint64(0x66bf923ad45cb553),
	60:  uint64(0x47ae1a5a2492ba86),
	61:  uint64(0x35e304569e229659),
	62:  uint64(0x4765182a46870b6f),
	63:  uint64(0x6cbab625e9099412),
	64:  uint64(0xddac9a2e598522c1),
	65:  uint64(0x7172086e666624f2),
	66:  uint64(0xdf5003ca503b7837),
	67:  uint64(0x88c0c1db78563d09),
	68:  uint64(0x58d51865acfc289d),
	69:  uint64(0x177671aec65224f1),
	70:  uint64(0xfb79d8a241e967d7),
	71:  uint64(0x2be1e101cad9a49a),
	72:  uint64(0x6625682f6e29186b),
	73:  uint64(0x399553457ac06e50),
	74:  uint64(0x35dffb4c23abb74),
	75:  uint64(0x429db2591f54aade),
	76:  uint64(0xc52802a8037d1009),
	77:  uint64(0x6acb27381f0b25f3),
	78:  uint64(0xf45e2551ee4f823b),
	79:  uint64(0x8b0ea2d99580c2f7),
	80:  uint64(0x3bed519cbcb4e1e1),
	81:  uint64(0xff452823dbb010a),
	82:  uint64(0x9d42ed614f3dd267),
	83:  uint64(0x5b9313c06257c57b),
	84:  uint64(0xa114b8008b5e1442),
	85:  uint64(0xc1fe311c11c13d4b),
	86:  uint64(0x66e8763ea34c5568),
	87:  uint64(0x8b982af1c262f05d),
	88:  uint64(0xee8876faaa75fbb7),
	89:  uint64(0x8a62a4d0d172bb2a),
	90:  uint64(0xc13d94a3b7449a97),
	91:  uint64(0x6dbbba9dc15d037c),
	92:  uint64(0xc786101f1d92e0f1),
	93:  uint64(0xd78681a907a0b79b),
	94:  uint64(0xf61aaf2962c9abb9),
	95:  uint64(0x2cfd16fcd3cb7ad9),
	96:  uint64(0x868c5b6744624d21),
	97:  uint64(0x25e650899c74ddd7),
	98:  uint64(0xba042af4a7c37463),
	99:  uint64(0x4eb1a539465a3eca),
	100: uint64(0xbe09dbf03b05d5ca),
	101: uint64(0x774e5a362b5472ba),
	102: uint64(0x47a1221229d183cd),
	103: uint64(0x504b0ca18ef5a2df),
	104: uint64(0xdffbdfbde2456eb9),
	105: uint64(0x46cd2b2fbee34634),
	106: uint64(0xf2aef8fe819d98c3),
	107: uint64(0x357f5276d4599d61),
	108: uint64(0x24a5483879c453e3),
	109: uint64(0x88026889192b4b9),
	110: uint64(0x28da96671782dbec),
	111: uint64(0x4ef37c40588e9aaa),
	112: uint64(0x8837b90651bc9fb3),
	113: uint64(0xc164f741d3f0e5d6),
	114: uint64(0xbc135a0a704b70ba),
	115: uint64(0x69cd868f7622ada),
	116: uint64(0xbc37ba89e0b9c0ab),
	117: uint64(0x47c14a01323552f6),
	118: uint64(0x4f00794bacee98bb),
	119: uint64(0x7107de7d637a69d5),
	120: uint64(0x88af793bb6f2255e),
	121: uint64(0xf3c6466b8799b598),
	122: uint64(0xc288c616aa7f3b59),
	123: uint64(0x81ca63cf42fca3fd),
	124: uint64(0x88d85ace36a2674b),
	125: uint64(0xd056bd3792389e7),
	126: uint64(0xe55c396c4e9dd32d),
	127: uint64(0xbefb504571e6c0a6),
	128: uint64(0x96ab32115e91e8cc),
	129: uint64(0xbf8acb18de8f38d1),
	130: uint64(0x66dae58801672606),
	131: uint64(0x833b6017872317fb),
	132: uint64(0xb87c16f2d1c92864),
	133: uint64(0xdb766a74e58b669c),
	134: uint64(0x89659f85c61417be),
	135: uint64(0xc8daad856011ea0c),
	136: uint64(0x76a4b565b6fe7eae),
	137: uint64(0xa469d085f6237312),
	138: uint64(0xaaf0365683a3e96c),
	139: uint64(0x4dbb746f8424f7b8),
	140: uint64(0x638755af4e4acc1),
	141: uint64(0x3d7807f5bde64486),
	142: uint64(0x17be6d8f5bbb7639),
	143: uint64(0x903f0cd44dc35dc),
	144: uint64(0x67b672eafdf1196c),
	145: uint64(0xa676ff93ed4c82f1),
	146: uint64(0x521d1004c5053d9d),
	147: uint64(0x37ba9ad09ccc9202),
	148: uint64(0x84e54d297aacfb51),
	149: uint64(0xa0b4b776a143445),
	150: uint64(0x820d471e20b348e),
	151: uint64(0x1874383cb83d46dc),
	152: uint64(0x97edeec7a1efe11c),
	153: uint64(0xb330e50b1bdc42aa),
	154: uint64(0x1dd91955ce70e032),
	155: uint64(0xa514cdb88f2939d5),
	156: uint64(0x2791233fd90db9d3),
	157: uint64(0x7b670a4cc50f7a9b),
	158: uint64(0x77c07d2a05c6dfa5),
	159: uint64(0xe3778b6646d0a6fa),
	160: uint64(0xb39c8eda47b56749),
	161: uint64(0x933ed448addbef28),
	162: uint64(0xaf846af6ab7d0bf4),
	163: uint64(0xe5af208eb666e49),
	164: uint64(0x5e6622f73534cd6a),
	165: uint64(0x297daeca42ef5b6e),
	166: uint64(0x862daef3d35539a6),
	167: uint64(0xe68722498f8e1ea9),
	168: uint64(0x981c53093dc0d572),
	169: uint64(0xfa09b0bfbf86fbf5),
	170: uint64(0x30b1e96166219f15),
	171: uint64(0x70e7d466bdc4fb83),
	172: uint64(0x5a66736e35f2a8e9),
	173: uint64(0xcddb59d2b7c1baef),
	174: uint64(0xd6c7d247d26d8996),
	175: uint64(0xea4e39eac8de1ba3),
	176: uint64(0x539c8bb19fa3aff2),
	177: uint64(0x9f90e4c5fd508d8),
	178: uint64(0xa34e5956fbaf3385),
	179: uint64(0x2e2f8e151d3ef375),
	180: uint64(0x173691e9b83faec1),
	181: uint64(0xb85a8d56bf016379),
	182: uint64(0x8382381267408ae3),
	183: uint64(0xb90f901bbdc0096d),
	184: uint64(0x7c6ad32933bcec65),
	185: uint64(0x76bb5e2f2c8ad595),
	186: uint64(0x390f851a6cf46d28),
	187: uint64(0xc3e6064da1c2da72),
	188: uint64(0xc52a0c101cfa5389),
	189: uint64(0xd78eaf84a3fbc530),
	190: uint64(0x3781b9e2288b997e),
	191: uint64(0x73c2f6dea83d05c4),
	192: uint64(0x4228e364c5b5ed7),
	193: uint64(0x9d7a3edf0da43911),
	194: uint64(0x8edcfeda24686756),
	195: uint64(0x5e7667a7b7a9b3a1),
	196: uint64(0x4c4f389fa143791d),
	197: uint64(0xb08bc1023da7cddc),
	198: uint64(0x7ab4be3ae529b1cc),
	199: uint64(0x754e6132dbe74ff9),
	200: uint64(0x71635442a839df45),
	201: uint64(0x2f6fb1643fbe52de),
	202: uint64(0x961e0a42cf7a8177),
	203: uint64(0xf3b45d83d89ef2ea),
	204: uint64(0xee3de4cf4a6e3e9b),
	205: uint64(0xcd6848542c3295e7),
	206: uint64(0xe4cee1664c78662f),
	207: uint64(0x9947548b474c68c4),
	208: uint64(0x25d73777a5ed8b0b),
	209: uint64(0xc915b1d636b7fc),
	210: uint64(0x21c2ba75d9b0d2da),
	211: uint64(0x5f6b5dcf608a64a1),
	212: uint64(0xdcf333255ff9570c),
	213: uint64(0x633b922418ced4ee),
	214: uint64(0xc136dde0b004b34a),
	215: uint64(0x58cc83b05d4b2f5a),
	216: uint64(0x5eb424dda28e42d2),
	217: uint64(0x62df47369739cd98),
	218: uint64(0xb4e0b42485e4ce17),
	219: uint64(0x16e1f0c1f9a8d1e7),
	220: uint64(0x8ec3916707560ebf),
	221: uint64(0x62ba6e2df2cc9db3),
	222: uint64(0xcbf9f4ff77d83a16),
	223: uint64(0x78d9d7d07d2bbcc4),
	224: uint64(0xef554ce1e02c41f4),
	225: uint64(0x8d7581127eccf94d),
	226: uint64(0xa9b53336cb3c8a05),
	227: uint64(0x38c42c0bf45c4f91),
	228: uint64(0x640893cdf4488863),
	229: uint64(0x80ec34bc575ea568),
	230: uint64(0x39f324f5b48eaa40),
	231: uint64(0xe9d9ed1f8eff527f),
	232: uint64(0x9224fc058cc5a214),
	233: uint64(0xbaba00b04cfe7741),
	234: uint64(0x309a9f120fcf52af),
	235: uint64(0xa558f3ec65626212),
	236: uint64(0x424bec8b7adabe2f),
	237: uint64(0x41622513a6aea433),
	238: uint64(0xb88da2d5324ca798),
	239: uint64(0xd287733b245528a4),
	240: uint64(0x9a44697e6d68aec3),
	241: uint64(0x7b1093be2f49bb28),
	242: uint64(0x50bbec632e3d8aad),
	243: uint64(0x6cd90723e1ea8283),
	244: uint64(0x897b9e7431b02bf3),
	245: uint64(0x219efdcb338a7047),
	246: uint64(0x3b0311f0a27c0656),
	247: uint64(0xdb17bf91c0db96e7),
	248: uint64(0x8cd4fd6b4e85a5b2),
	249: uint64(0xfab071054ba6409d),
	250: uint64(0x40d6fe831fa9dfd9),
	251: uint64(0xaf358debad7d791e),
	252: uint64(0xeb8d0e25a65e3e58),
	253: uint64(0xbbcbd3df14e08580),
	254: uint64(0xcf751f27ecdab2b),
	255: uint64(0x2b4da14f2613d8f4),
}

/**** ended inlining zstd_ldm_geartab.h ****/

type ldmRollingHashState_t = struct {
	Frolling  U64
	FstopMask U64
}

// C documentation
//
//	/** ZSTD_ldm_gear_init():
//	 *
//	 * Initializes the rolling hash state such that it will honor the
//	 * settings in params. */
func ZSTD_ldm_gear_init(tls *libc.TLS, state uintptr, params uintptr) {
	var hashRateLog, maxBitsInMask, v1 uint32
	_, _, _ = hashRateLog, maxBitsInMask, v1
	if (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength < libc.Uint32FromInt32(libc.Int32FromInt32(64)) {
		v1 = (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(64))
	}
	maxBitsInMask = v1
	hashRateLog = (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog
	(*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling = uint64(^libc.Uint32FromInt32(0))
	/* The choice of the splitting criterion is subject to two conditions:
	 *   1. it has to trigger on average every 2^(hashRateLog) bytes;
	 *   2. ideally, it has to depend on a window of minMatchLength bytes.
	 *
	 * In the gear hash algorithm, bit n depends on the last n bytes;
	 * so in order to obtain a good quality splitting criterion it is
	 * preferable to use bits with high weight.
	 *
	 * To match condition 1 we use a mask with hashRateLog bits set
	 * and, because of the previous remark, we make sure these bits
	 * have the highest possible weight while still respecting
	 * condition 2.
	 */
	if hashRateLog > uint32(0) && hashRateLog <= maxBitsInMask {
		(*ldmRollingHashState_t)(unsafe.Pointer(state)).FstopMask = (libc.Uint64FromInt32(1)<<hashRateLog - uint64(1)) << (maxBitsInMask - hashRateLog)
	} else {
		/* In this degenerate case we simply honor the hash rate. */
		(*ldmRollingHashState_t)(unsafe.Pointer(state)).FstopMask = libc.Uint64FromInt32(1)<<hashRateLog - uint64(1)
	}
}

// C documentation
//
//	/** ZSTD_ldm_gear_reset()
//	 * Feeds [data, data + minMatchLength) into the hash without registering any
//	 * splits. This effectively resets the hash state. This is used when skipping
//	 * over data, either at the beginning of a block, or skipping sections.
//	 */
func ZSTD_ldm_gear_reset(tls *libc.TLS, state uintptr, data uintptr, minMatchLength size_t) {
	var hash U64
	var n size_t
	_, _ = hash, n
	hash = (*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling
	n = uint64(0)
	for n+uint64(3) < minMatchLength {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
	}
	for n < minMatchLength {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
	}
}

// C documentation
//
//	/** ZSTD_ldm_gear_feed():
//	 *
//	 * Registers in the splits array all the split points found in the first
//	 * size bytes following the data pointer. This function terminates when
//	 * either all the data has been processed or LDM_BATCH_SIZE splits are
//	 * present in the splits array.
//	 *
//	 * Precondition: The splits array must not be full.
//	 * Returns: The number of bytes processed. */
func ZSTD_ldm_gear_feed(tls *libc.TLS, state uintptr, data uintptr, size size_t, splits uintptr, numSplits uintptr) (r size_t) {
	var hash, mask U64
	var n size_t
	_, _, _ = hash, mask, n
	hash = (*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling
	mask = (*ldmRollingHashState_t)(unsafe.Pointer(state)).FstopMask
	n = uint64(0)
	for n+uint64(3) < size {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt64(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt64(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt64(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt64(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
	}
	for n < size {
		hash = hash<<libc.Int32FromInt32(1) + ZSTD_ldm_gearTab[libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(data + uintptr(n))))&int32(0xff)]
		n = n + uint64(1)
		if libc.BoolInt64(hash&mask == libc.Uint64FromInt32(0)) != 0 {
			*(*size_t)(unsafe.Pointer(splits + uintptr(*(*uint32)(unsafe.Pointer(numSplits)))*8)) = n
			*(*uint32)(unsafe.Pointer(numSplits)) += uint32(1)
			if *(*uint32)(unsafe.Pointer(numSplits)) == uint32(LDM_BATCH_SIZE) {
				goto done
			}
		}
	}
	goto done
done:
	;
	(*ldmRollingHashState_t)(unsafe.Pointer(state)).Frolling = hash
	return n
}

func ZSTD_ldm_adjustParameters(tls *libc.TLS, params uintptr, cParams uintptr) {
	var v1, v2, v3 uint32
	_, _, _ = v1, v2, v3
	(*ldmParams_t)(unsafe.Pointer(params)).FwindowLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog
	_ = libc.Uint64FromInt64(1)
	if (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog == uint32(0) {
		if (*ldmParams_t)(unsafe.Pointer(params)).FhashLog > uint32(0) {
			/* if params->hashLog is set, derive hashRateLog from it */
			if (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog > (*ldmParams_t)(unsafe.Pointer(params)).FhashLog {
				(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog = (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog - (*ldmParams_t)(unsafe.Pointer(params)).FhashLog
			}
		} else {
			/* mapping from [fast, rate7] to [btultra2, rate4] */
			(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog = libc.Uint32FromInt32(int32(7) - (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy/int32(3))
		}
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FhashLog == uint32(0) {
		if (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog-(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog < libc.Uint32FromInt32(libc.Int32FromInt32(30)) {
			v2 = (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog - (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog
		} else {
			v2 = libc.Uint32FromInt32(libc.Int32FromInt32(30))
		}
		if libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HASHLOG_MIN)) > v2 {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HASHLOG_MIN))
		} else {
			if (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog-(*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog < libc.Uint32FromInt32(libc.Int32FromInt32(30)) {
				v3 = (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog - (*ldmParams_t)(unsafe.Pointer(params)).FhashRateLog
			} else {
				v3 = libc.Uint32FromInt32(libc.Int32FromInt32(30))
			}
			v1 = v3
		}
		(*ldmParams_t)(unsafe.Pointer(params)).FhashLog = v1
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength == uint32(0) {
		(*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength = uint32(LDM_MIN_MATCH_LENGTH)
		if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btultra) {
			*(*U32)(unsafe.Pointer(params + 12)) /= uint32(2)
		}
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog == uint32(0) {
		if libc.Uint32FromInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy) < libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX)) {
			v2 = libc.Uint32FromInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy)
		} else {
			v2 = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX))
		}
		if libc.Uint32FromInt32(libc.Int32FromInt32(LDM_BUCKET_SIZE_LOG)) > v2 {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(LDM_BUCKET_SIZE_LOG))
		} else {
			if libc.Uint32FromInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy) < libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX)) {
				v3 = libc.Uint32FromInt32((*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy)
			} else {
				v3 = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_LDM_BUCKETSIZELOG_MAX))
			}
			v1 = v3
		}
		(*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog = v1
	}
	if (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog < (*ldmParams_t)(unsafe.Pointer(params)).FhashLog {
		v1 = (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	} else {
		v1 = (*ldmParams_t)(unsafe.Pointer(params)).FhashLog
	}
	(*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog = v1
}

func ZSTD_ldm_getTableSize(tls *libc.TLS, params ldmParams_t) (r size_t) {
	var ldmBucketSize, ldmBucketSizeLog, ldmHSize, totalSize size_t
	var v1 uint32
	var v2 uint64
	_, _, _, _, _, _ = ldmBucketSize, ldmBucketSizeLog, ldmHSize, totalSize, v1, v2
	ldmHSize = libc.Uint64FromInt32(1) << params.FhashLog
	if params.FbucketSizeLog < params.FhashLog {
		v1 = params.FbucketSizeLog
	} else {
		v1 = params.FhashLog
	}
	ldmBucketSizeLog = uint64(v1)
	ldmBucketSize = libc.Uint64FromInt32(1) << (uint64(params.FhashLog) - ldmBucketSizeLog)
	totalSize = ZSTD_cwksp_alloc_size(tls, ldmBucketSize) + ZSTD_cwksp_alloc_size(tls, ldmHSize*uint64(8))
	if params.FenableLdm == int32(ZSTD_ps_enable) {
		v2 = totalSize
	} else {
		v2 = uint64(0)
	}
	return v2
}

func ZSTD_ldm_getMaxNbSeq(tls *libc.TLS, params ldmParams_t, maxChunkSize size_t) (r size_t) {
	var v1 uint64
	_ = v1
	if params.FenableLdm == int32(ZSTD_ps_enable) {
		v1 = maxChunkSize / uint64(params.FminMatchLength)
	} else {
		v1 = uint64(0)
	}
	return v1
}

// C documentation
//
//	/** ZSTD_ldm_getBucket() :
//	 *  Returns a pointer to the start of the bucket associated with hash. */
func ZSTD_ldm_getBucket(tls *libc.TLS, ldmState uintptr, hash size_t, bucketSizeLog U32) (r uintptr) {
	return (*ldmState_t)(unsafe.Pointer(ldmState)).FhashTable + uintptr(hash<<bucketSizeLog)*8
}

// C documentation
//
//	/** ZSTD_ldm_insertEntry() :
//	 *  Insert the entry with corresponding hash into the hash table */
func ZSTD_ldm_insertEntry(tls *libc.TLS, ldmState uintptr, hash size_t, entry ldmEntry_t, bucketSizeLog U32) {
	var offset uint32
	var pOffset uintptr
	_, _ = offset, pOffset
	pOffset = (*ldmState_t)(unsafe.Pointer(ldmState)).FbucketOffsets + uintptr(hash)
	offset = uint32(*(*BYTE)(unsafe.Pointer(pOffset)))
	*(*ldmEntry_t)(unsafe.Pointer(ZSTD_ldm_getBucket(tls, ldmState, hash, bucketSizeLog) + uintptr(offset)*8)) = entry
	*(*BYTE)(unsafe.Pointer(pOffset)) = uint8((offset + libc.Uint32FromInt32(1)) & (libc.Uint32FromUint32(1)<<bucketSizeLog - libc.Uint32FromInt32(1)))
}

// C documentation
//
//	/** ZSTD_ldm_countBackwardsMatch() :
//	 *  Returns the number of bytes that match backwards before pIn and pMatch.
//	 *
//	 *  We count only bytes where pMatch >= pBase and pIn >= pAnchor. */
func ZSTD_ldm_countBackwardsMatch(tls *libc.TLS, pIn uintptr, pAnchor uintptr, pMatch uintptr, pMatchBase uintptr) (r size_t) {
	var matchLength size_t
	_ = matchLength
	matchLength = uint64(0)
	for pIn > pAnchor && pMatch > pMatchBase && libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(pIn + uintptr(-libc.Int32FromInt32(1))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(pMatch + uintptr(-libc.Int32FromInt32(1))))) {
		pIn = pIn - 1
		pMatch = pMatch - 1
		matchLength = matchLength + 1
	}
	return matchLength
}

// C documentation
//
//	/** ZSTD_ldm_countBackwardsMatch_2segments() :
//	 *  Returns the number of bytes that match backwards from pMatch,
//	 *  even with the backwards match spanning 2 different segments.
//	 *
//	 *  On reaching `pMatchBase`, start counting from mEnd */
func ZSTD_ldm_countBackwardsMatch_2segments(tls *libc.TLS, pIn uintptr, pAnchor uintptr, pMatch uintptr, pMatchBase uintptr, pExtDictStart uintptr, pExtDictEnd uintptr) (r size_t) {
	var matchLength size_t
	_ = matchLength
	matchLength = ZSTD_ldm_countBackwardsMatch(tls, pIn, pAnchor, pMatch, pMatchBase)
	if pMatch-uintptr(matchLength) != pMatchBase || pMatchBase == pExtDictStart {
		/* If backwards match is entirely in the extDict or prefix, immediately return */
		return matchLength
	}
	matchLength = matchLength + ZSTD_ldm_countBackwardsMatch(tls, pIn-uintptr(matchLength), pAnchor, pExtDictEnd, pExtDictStart)
	return matchLength
}

// C documentation
//
//	/** ZSTD_ldm_fillFastTables() :
//	 *
//	 *  Fills the relevant tables for the ZSTD_fast and ZSTD_dfast strategies.
//	 *  This is similar to ZSTD_loadDictionaryContent.
//	 *
//	 *  The tables for the other strategies are filled within their
//	 *  block compressors. */
func ZSTD_ldm_fillFastTables(tls *libc.TLS, ms uintptr, end uintptr) (r size_t) {
	var iend uintptr
	_ = iend
	iend = end
	switch (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.Fstrategy {
	case int32(ZSTD_fast):
		ZSTD_fillHashTable(tls, ms, iend, int32(ZSTD_dtlm_fast), int32(ZSTD_tfp_forCCtx))
	case int32(ZSTD_dfast):
		ZSTD_fillDoubleHashTable(tls, ms, iend, int32(ZSTD_dtlm_fast), int32(ZSTD_tfp_forCCtx))
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_lazy2):
		fallthrough
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_btopt):
		fallthrough
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btultra2):
	default:
		/* not possible : not a valid strategy id */
	}
	return uint64(0)
}

func ZSTD_ldm_fillHashTable(tls *libc.TLS, ldmState uintptr, ip uintptr, iend uintptr, params uintptr) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var base, istart, split, splits uintptr
	var bucketSizeLog, hBits, hash, minMatchLength U32
	var entry ldmEntry_t
	var hashed size_t
	var n uint32
	var xxhash U64
	var _ /* hashState at bp+0 */ ldmRollingHashState_t
	var _ /* numSplits at bp+16 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _ = base, bucketSizeLog, entry, hBits, hash, hashed, istart, minMatchLength, n, split, splits, xxhash
	minMatchLength = (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength
	bucketSizeLog = (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	hBits = (*ldmParams_t)(unsafe.Pointer(params)).FhashLog - bucketSizeLog
	base = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.Fbase
	istart = ip
	splits = ldmState + 64
	ZSTD_ldm_gear_init(tls, bp, params)
	for ip < iend {
		*(*uint32)(unsafe.Pointer(bp + 16)) = uint32(0)
		hashed = ZSTD_ldm_gear_feed(tls, bp, ip, libc.Uint64FromInt64(int64(iend)-int64(ip)), splits, bp+16)
		n = uint32(0)
		for {
			if !(n < *(*uint32)(unsafe.Pointer(bp + 16))) {
				break
			}
			if ip+uintptr(*(*size_t)(unsafe.Pointer(splits + uintptr(n)*8))) >= istart+uintptr(minMatchLength) {
				split = ip + uintptr(*(*size_t)(unsafe.Pointer(splits + uintptr(n)*8))) - uintptr(minMatchLength)
				xxhash = XXH_INLINE_XXH64(tls, split, uint64(minMatchLength), uint64(0))
				hash = uint32(xxhash & uint64(libc.Uint32FromInt32(1)<<hBits-libc.Uint32FromInt32(1)))
				entry.Foffset = libc.Uint32FromInt64(int64(split) - int64(base))
				entry.Fchecksum = uint32(xxhash >> libc.Int32FromInt32(32))
				ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash), entry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
			}
			goto _1
		_1:
			;
			n = n + 1
		}
		ip = ip + uintptr(hashed)
	}
}

// C documentation
//
//	/** ZSTD_ldm_limitTableUpdate() :
//	 *
//	 *  Sets cctx->nextToUpdate to a position corresponding closer to anchor
//	 *  if it is far way
//	 *  (after a long match, only update tables a limited amount). */
func ZSTD_ldm_limitTableUpdate(tls *libc.TLS, ms uintptr, anchor uintptr) {
	var curr U32
	var v1 uint32
	_, _ = curr, v1
	curr = libc.Uint32FromInt64(int64(anchor) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	if curr > (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate+uint32(1024) {
		if libc.Uint32FromInt32(libc.Int32FromInt32(512)) < curr-(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate-uint32(1024) {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(512))
		} else {
			v1 = curr - (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate - uint32(1024)
		}
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr - v1
	}
}

func ZSTD_ldm_generateSequences_internal(tls *libc.TLS, ldmState uintptr, rawSeqStore uintptr, params uintptr, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var anchor, base, bestEntry, bucket, candidates, cur, curMatchBase, dictBase, dictEnd, dictStart, iend, ilimit, ip, istart, lowMatchPtr, lowPrefixPtr, matchEnd, pMatch, pMatch1, seq, split, split1, splits, v2, v3, v4 uintptr
	var backwardMatchLength, bestMatchLength, curBackwardMatchLength, curForwardMatchLength, curTotalMatchLength, forwardMatchLength, hashed, mLength size_t
	var checksum, dictLimit, entsPerBucket, hBits, hash, hash1, lowestIndex, minMatchLength, offset U32
	var extDict int32
	var n, v1 uint32
	var newEntry ldmEntry_t
	var xxhash U64
	var _ /* hashState at bp+0 */ ldmRollingHashState_t
	var _ /* numSplits at bp+16 */ uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = anchor, backwardMatchLength, base, bestEntry, bestMatchLength, bucket, candidates, checksum, cur, curBackwardMatchLength, curForwardMatchLength, curMatchBase, curTotalMatchLength, dictBase, dictEnd, dictLimit, dictStart, entsPerBucket, extDict, forwardMatchLength, hBits, hash, hash1, hashed, iend, ilimit, ip, istart, lowMatchPtr, lowPrefixPtr, lowestIndex, mLength, matchEnd, minMatchLength, n, newEntry, offset, pMatch, pMatch1, seq, split, split1, splits, xxhash, v1, v2, v3, v4
	/* LDM parameters */
	extDict = libc.Int32FromUint32(ZSTD_window_hasExtDict(tls, (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow))
	minMatchLength = (*ldmParams_t)(unsafe.Pointer(params)).FminMatchLength
	entsPerBucket = uint32(1) << (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	hBits = (*ldmParams_t)(unsafe.Pointer(params)).FhashLog - (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog
	/* Prefix and extDict parameters */
	dictLimit = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.FdictLimit
	if extDict != 0 {
		v1 = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.FlowLimit
	} else {
		v1 = dictLimit
	}
	lowestIndex = v1
	base = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.Fbase
	if extDict != 0 {
		v2 = (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow.FdictBase
	} else {
		v2 = libc.UintptrFromInt32(0)
	}
	dictBase = v2
	if extDict != 0 {
		v3 = dictBase + uintptr(lowestIndex)
	} else {
		v3 = libc.UintptrFromInt32(0)
	}
	dictStart = v3
	if extDict != 0 {
		v4 = dictBase + uintptr(dictLimit)
	} else {
		v4 = libc.UintptrFromInt32(0)
	}
	dictEnd = v4
	lowPrefixPtr = base + uintptr(dictLimit)
	/* Input bounds */
	istart = src
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(HASH_READ_SIZE)
	/* Input positions */
	anchor = istart
	ip = istart
	/* Arrays for staged-processing */
	splits = ldmState + 64
	candidates = ldmState + 576
	if srcSize < uint64(minMatchLength) {
		return libc.Uint64FromInt64(int64(iend) - int64(anchor))
	}
	/* Initialize the rolling hash state with the first minMatchLength bytes */
	ZSTD_ldm_gear_init(tls, bp, params)
	ZSTD_ldm_gear_reset(tls, bp, ip, uint64(minMatchLength))
	ip = ip + uintptr(minMatchLength)
	for ip < ilimit {
		*(*uint32)(unsafe.Pointer(bp + 16)) = uint32(0)
		hashed = ZSTD_ldm_gear_feed(tls, bp, ip, libc.Uint64FromInt64(int64(ilimit)-int64(ip)), splits, bp+16)
		n = uint32(0)
		for {
			if !(n < *(*uint32)(unsafe.Pointer(bp + 16))) {
				break
			}
			split = ip + uintptr(*(*size_t)(unsafe.Pointer(splits + uintptr(n)*8))) - uintptr(minMatchLength)
			xxhash = XXH_INLINE_XXH64(tls, split, uint64(minMatchLength), uint64(0))
			hash = uint32(xxhash & uint64(libc.Uint32FromInt32(1)<<hBits-libc.Uint32FromInt32(1)))
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fsplit = split
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fhash = hash
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fchecksum = uint32(xxhash >> libc.Int32FromInt32(32))
			(*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fbucket = ZSTD_ldm_getBucket(tls, ldmState, uint64(hash), (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
			libc.X__builtin_prefetch(tls, (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fbucket, libc.VaList(bp+32, 0, int32(3)))
			goto _5
		_5:
			;
			n = n + 1
		}
		n = uint32(0)
		for {
			if !(n < *(*uint32)(unsafe.Pointer(bp + 16))) {
				break
			}
			forwardMatchLength = uint64(0)
			backwardMatchLength = uint64(0)
			bestMatchLength = uint64(0)
			split1 = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fsplit
			checksum = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fchecksum
			hash1 = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fhash
			bucket = (*(*ldmMatchCandidate_t)(unsafe.Pointer(candidates + uintptr(n)*24))).Fbucket
			bestEntry = libc.UintptrFromInt32(0)
			newEntry.Foffset = libc.Uint32FromInt64(int64(split1) - int64(base))
			newEntry.Fchecksum = checksum
			/* If a split point would generate a sequence overlapping with
			 * the previous one, we merely register it in the hash table and
			 * move on */
			if split1 < anchor {
				ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash1), newEntry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
				goto _6
			}
			cur = bucket
			for {
				if !(cur < bucket+uintptr(entsPerBucket)*8) {
					break
				}
				if (*ldmEntry_t)(unsafe.Pointer(cur)).Fchecksum != checksum || (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset <= lowestIndex {
					goto _7
				}
				if extDict != 0 {
					if (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset < dictLimit {
						v2 = dictBase
					} else {
						v2 = base
					}
					curMatchBase = v2
					pMatch = curMatchBase + uintptr((*ldmEntry_t)(unsafe.Pointer(cur)).Foffset)
					if (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset < dictLimit {
						v3 = dictEnd
					} else {
						v3 = iend
					}
					matchEnd = v3
					if (*ldmEntry_t)(unsafe.Pointer(cur)).Foffset < dictLimit {
						v4 = dictStart
					} else {
						v4 = lowPrefixPtr
					}
					lowMatchPtr = v4
					curForwardMatchLength = ZSTD_count_2segments(tls, split1, pMatch, iend, matchEnd, lowPrefixPtr)
					if curForwardMatchLength < uint64(minMatchLength) {
						goto _7
					}
					curBackwardMatchLength = ZSTD_ldm_countBackwardsMatch_2segments(tls, split1, anchor, pMatch, lowMatchPtr, dictStart, dictEnd)
				} else { /* !extDict */
					pMatch1 = base + uintptr((*ldmEntry_t)(unsafe.Pointer(cur)).Foffset)
					curForwardMatchLength = ZSTD_count(tls, split1, pMatch1, iend)
					if curForwardMatchLength < uint64(minMatchLength) {
						goto _7
					}
					curBackwardMatchLength = ZSTD_ldm_countBackwardsMatch(tls, split1, anchor, pMatch1, lowPrefixPtr)
				}
				curTotalMatchLength = curForwardMatchLength + curBackwardMatchLength
				if curTotalMatchLength > bestMatchLength {
					bestMatchLength = curTotalMatchLength
					forwardMatchLength = curForwardMatchLength
					backwardMatchLength = curBackwardMatchLength
					bestEntry = cur
				}
				goto _7
			_7:
				;
				cur += 8
			}
			/* No match found -- insert an entry into the hash table
			 * and process the next candidate match */
			if bestEntry == libc.UintptrFromInt32(0) {
				ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash1), newEntry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
				goto _6
			}
			/* Match found */
			offset = libc.Uint32FromInt64(int64(split1)-int64(base)) - (*ldmEntry_t)(unsafe.Pointer(bestEntry)).Foffset
			mLength = forwardMatchLength + backwardMatchLength
			seq = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize)*12
			/* Out of sequence storage */
			if (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize == (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fcapacity {
				return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
			}
			(*rawSeq)(unsafe.Pointer(seq)).FlitLength = libc.Uint32FromInt64(int64(split1-uintptr(backwardMatchLength)) - int64(anchor))
			(*rawSeq)(unsafe.Pointer(seq)).FmatchLength = uint32(mLength)
			(*rawSeq)(unsafe.Pointer(seq)).Foffset = offset
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize + 1
			/* Insert the current entry into the hash table --- it must be
			 * done after the previous block to avoid clobbering bestEntry */
			ZSTD_ldm_insertEntry(tls, ldmState, uint64(hash1), newEntry, (*ldmParams_t)(unsafe.Pointer(params)).FbucketSizeLog)
			anchor = split1 + uintptr(forwardMatchLength)
			/* If we find a match that ends after the data that we've hashed
			 * then we have a repeating, overlapping, pattern. E.g. all zeros.
			 * If one repetition of the pattern matches our `stopMask` then all
			 * repetitions will. We don't need to insert them all into out table,
			 * only the first one. So skip over overlapping matches.
			 * This is a major speed boost (20x) for compressing a single byte
			 * repeated, when that byte ends up in the table.
			 */
			if anchor > ip+uintptr(hashed) {
				ZSTD_ldm_gear_reset(tls, bp, anchor-uintptr(minMatchLength), uint64(minMatchLength))
				/* Continue the outer loop at anchor (ip + hashed == anchor). */
				ip = anchor - uintptr(hashed)
				break
			}
			goto _6
		_6:
			;
			n = n + 1
		}
		ip = ip + uintptr(hashed)
	}
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
}

// C documentation
//
//	/*! ZSTD_ldm_reduceTable() :
//	 *  reduce table indexes by `reducerValue` */
func ZSTD_ldm_reduceTable(tls *libc.TLS, table uintptr, size U32, reducerValue U32) {
	var u U32
	_ = u
	u = uint32(0)
	for {
		if !(u < size) {
			break
		}
		if (*(*ldmEntry_t)(unsafe.Pointer(table + uintptr(u)*8))).Foffset < reducerValue {
			(*(*ldmEntry_t)(unsafe.Pointer(table + uintptr(u)*8))).Foffset = uint32(0)
		} else {
			(*(*ldmEntry_t)(unsafe.Pointer(table + uintptr(u)*8))).Foffset -= reducerValue
		}
		goto _1
	_1:
		;
		u = u + 1
	}
}

func ZSTD_ldm_generateSequences(tls *libc.TLS, ldmState uintptr, sequences uintptr, params uintptr, src uintptr, srcSize size_t) (r size_t) {
	var chunk, chunkSize, kMaxChunkSize, leftoverSize, nbChunks, newLeftoverSize, prevSize, remaining size_t
	var chunkEnd, chunkStart, iend, istart, v2 uintptr
	var correction, ldmHSize, maxDist U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = chunk, chunkEnd, chunkSize, chunkStart, correction, iend, istart, kMaxChunkSize, ldmHSize, leftoverSize, maxDist, nbChunks, newLeftoverSize, prevSize, remaining, v2
	maxDist = uint32(1) << (*ldmParams_t)(unsafe.Pointer(params)).FwindowLog
	istart = src
	iend = istart + uintptr(srcSize)
	kMaxChunkSize = libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
	nbChunks = srcSize/kMaxChunkSize + libc.BoolUint64(srcSize%kMaxChunkSize != libc.Uint64FromInt32(0))
	leftoverSize = uint64(0)
	/* Check that ZSTD_window_update() has been called for this chunk prior
	 * to passing it to this function.
	 */
	/* The input could be very large (in zstdmt), so it must be broken up into
	 * chunks to enforce the maximum distance and handle overflow correction.
	 */
	chunk = uint64(0)
	for {
		if !(chunk < nbChunks && (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fsize < (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fcapacity) {
			break
		}
		chunkStart = istart + uintptr(chunk*kMaxChunkSize)
		remaining = libc.Uint64FromInt64(int64(iend) - int64(chunkStart))
		if remaining < kMaxChunkSize {
			v2 = iend
		} else {
			v2 = chunkStart + uintptr(kMaxChunkSize)
		}
		chunkEnd = v2
		chunkSize = libc.Uint64FromInt64(int64(chunkEnd) - int64(chunkStart))
		prevSize = (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fsize
		/* 1. Perform overflow correction if necessary. */
		if ZSTD_window_needOverflowCorrection(tls, (*ldmState_t)(unsafe.Pointer(ldmState)).Fwindow, uint32(0), maxDist, (*ldmState_t)(unsafe.Pointer(ldmState)).FloadedDictEnd, chunkStart, chunkEnd) != 0 {
			ldmHSize = uint32(1) << (*ldmParams_t)(unsafe.Pointer(params)).FhashLog
			correction = ZSTD_window_correctOverflow(tls, ldmState, uint32(0), maxDist, chunkStart)
			ZSTD_ldm_reduceTable(tls, (*ldmState_t)(unsafe.Pointer(ldmState)).FhashTable, ldmHSize, correction)
			/* invalidate dictionaries on overflow correction */
			(*ldmState_t)(unsafe.Pointer(ldmState)).FloadedDictEnd = uint32(0)
		}
		/* 2. We enforce the maximum offset allowed.
		 *
		 * kMaxChunkSize should be small enough that we don't lose too much of
		 * the window through early invalidation.
		 * TODO: * Test the chunk size.
		 *       * Try invalidation after the sequence generation and test the
		 *         offset against maxDist directly.
		 *
		 * NOTE: Because of dictionaries + sequence splitting we MUST make sure
		 * that any offset used is valid at the END of the sequence, since it may
		 * be split into two sequences. This condition holds when using
		 * ZSTD_window_enforceMaxDist(), but if we move to checking offsets
		 * against maxDist directly, we'll have to carefully handle that case.
		 */
		ZSTD_window_enforceMaxDist(tls, ldmState, chunkEnd, maxDist, ldmState+48, libc.UintptrFromInt32(0))
		/* 3. Generate the sequences for the chunk, and get newLeftoverSize. */
		newLeftoverSize = ZSTD_ldm_generateSequences_internal(tls, ldmState, sequences, params, chunkStart, chunkSize)
		if ZSTD_isError(tls, newLeftoverSize) != 0 {
			return newLeftoverSize
		}
		/* 4. We add the leftover literals from previous iterations to the first
		 *    newly generated sequence, or add the `newLeftoverSize` if none are
		 *    generated.
		 */
		/* Prepend the leftover literals from the last call */
		if prevSize < (*RawSeqStore_t)(unsafe.Pointer(sequences)).Fsize {
			(*(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(sequences)).Fseq + uintptr(prevSize)*12))).FlitLength += uint32(leftoverSize)
			leftoverSize = newLeftoverSize
		} else {
			leftoverSize = leftoverSize + chunkSize
		}
		goto _1
	_1:
		;
		chunk = chunk + 1
	}
	return uint64(0)
}

func ZSTD_ldm_skipSequences(tls *libc.TLS, rawSeqStore uintptr, srcSize size_t, minMatch U32) {
	var seq uintptr
	_ = seq
	for srcSize > uint64(0) && (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		seq = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12
		if srcSize <= uint64((*rawSeq)(unsafe.Pointer(seq)).FlitLength) {
			/* Skip past srcSize literals */
			*(*U32)(unsafe.Pointer(seq + 4)) -= uint32(srcSize)
			return
		}
		srcSize = srcSize - uint64((*rawSeq)(unsafe.Pointer(seq)).FlitLength)
		(*rawSeq)(unsafe.Pointer(seq)).FlitLength = uint32(0)
		if srcSize < uint64((*rawSeq)(unsafe.Pointer(seq)).FmatchLength) {
			/* Skip past the first srcSize of the match */
			*(*U32)(unsafe.Pointer(seq + 8)) -= uint32(srcSize)
			if (*rawSeq)(unsafe.Pointer(seq)).FmatchLength < minMatch {
				/* The match is too short, omit it */
				if (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos+uint64(1) < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
					(*(*rawSeq)(unsafe.Pointer(seq + 1*12))).FlitLength += (*(*rawSeq)(unsafe.Pointer(seq))).FmatchLength
				}
				(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
			}
			return
		}
		srcSize = srcSize - uint64((*rawSeq)(unsafe.Pointer(seq)).FmatchLength)
		(*rawSeq)(unsafe.Pointer(seq)).FmatchLength = uint32(0)
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
	}
}

// C documentation
//
//	/**
//	 * If the sequence length is longer than remaining then the sequence is split
//	 * between this block and the next.
//	 *
//	 * Returns the current sequence to handle, or if the rest of the block should
//	 * be literals, it returns a sequence with offset == 0.
//	 */
func maybeSplitSequence(tls *libc.TLS, rawSeqStore uintptr, remaining U32, minMatch U32) (r rawSeq) {
	var sequence rawSeq
	_ = sequence
	sequence = *(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12))
	/* Likely: No partial sequence */
	if remaining >= sequence.FlitLength+sequence.FmatchLength {
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
		return sequence
	}
	/* Cut the sequence short (offset == 0 ==> rest is literals). */
	if remaining <= sequence.FlitLength {
		sequence.Foffset = uint32(0)
	} else {
		if remaining < sequence.FlitLength+sequence.FmatchLength {
			sequence.FmatchLength = remaining - sequence.FlitLength
			if sequence.FmatchLength < minMatch {
				sequence.Foffset = uint32(0)
			}
		}
	}
	/* Skip past `remaining` bytes for the future sequences. */
	ZSTD_ldm_skipSequences(tls, rawSeqStore, uint64(remaining), minMatch)
	return sequence
}

func ZSTD_ldm_skipRawSeqStoreBytes(tls *libc.TLS, rawSeqStore uintptr, nbBytes size_t) {
	var currPos U32
	var currSeq rawSeq
	_, _ = currPos, currSeq
	currPos = uint32((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence + nbBytes)
	for currPos != 0 && (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		currSeq = *(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12))
		if currPos >= currSeq.FlitLength+currSeq.FmatchLength {
			currPos = currPos - (currSeq.FlitLength + currSeq.FmatchLength)
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
		} else {
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(currPos)
			break
		}
	}
	if currPos == uint32(0) || (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos == (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(0)
	}
}

func ZSTD_ldm_blockCompress(tls *libc.TLS, rawSeqStore uintptr, ms uintptr, seqStore uintptr, rep uintptr, useRowMatchFinder ZSTD_ParamSwitch_e, src uintptr, srcSize size_t) (r size_t) {
	var blockCompressor ZSTD_BlockCompressor_f
	var cParams, iend, ip, istart uintptr
	var i int32
	var lastLLSize, newLitLength size_t
	var minMatch uint32
	var sequence rawSeq
	_, _, _, _, _, _, _, _, _, _ = blockCompressor, cParams, i, iend, ip, istart, lastLLSize, minMatch, newLitLength, sequence
	cParams = ms + 256
	minMatch = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch
	blockCompressor = ZSTD_selectBlockCompressor(tls, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy, useRowMatchFinder, ZSTD_matchState_dictMode(tls, ms))
	/* Input bounds */
	istart = src
	iend = istart + uintptr(srcSize)
	/* Input positions */
	ip = istart
	/* If using opt parser, use LDMs only as candidates rather than always accepting them */
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).Fstrategy >= int32(ZSTD_btopt) {
		(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore = rawSeqStore
		lastLLSize = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, seqStore, rep, src, srcSize)
		ZSTD_ldm_skipRawSeqStoreBytes(tls, rawSeqStore, srcSize)
		return lastLLSize
	}
	/* Loop through each sequence and apply the block compressor to the literals */
	for (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize && ip < iend {
		/* maybeSplitSequence updates rawSeqStore->pos */
		sequence = maybeSplitSequence(tls, rawSeqStore, libc.Uint32FromInt64(int64(iend)-int64(ip)), minMatch)
		/* End signal */
		if sequence.Foffset == uint32(0) {
			break
		}
		/* Fill tables for block compressor */
		ZSTD_ldm_limitTableUpdate(tls, ms, ip)
		ZSTD_ldm_fillFastTables(tls, ms, ip)
		/* Run the block compressor */
		newLitLength = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, seqStore, rep, ip, uint64(sequence.FlitLength))
		ip = ip + uintptr(sequence.FlitLength)
		/* Update the repcodes */
		i = libc.Int32FromInt32(ZSTD_REP_NUM) - libc.Int32FromInt32(1)
		for {
			if !(i > 0) {
				break
			}
			*(*U32)(unsafe.Pointer(rep + uintptr(i)*4)) = *(*U32)(unsafe.Pointer(rep + uintptr(i-int32(1))*4))
			goto _1
		_1:
			;
			i = i - 1
		}
		*(*U32)(unsafe.Pointer(rep)) = sequence.Foffset
		/* Store the sequence */
		ZSTD_storeSeq(tls, seqStore, newLitLength, ip-uintptr(newLitLength), iend, sequence.Foffset+libc.Uint32FromInt32(ZSTD_REP_NUM), uint64(sequence.FmatchLength))
		ip = ip + uintptr(sequence.FmatchLength)
	}
	/* Fill the tables for the block compressor */
	ZSTD_ldm_limitTableUpdate(tls, ms, ip)
	ZSTD_ldm_fillFastTables(tls, ms, ip)
	/* Compress the last literals */
	return (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, size_t) size_t)(unsafe.Pointer(&struct{ uintptr }{blockCompressor})))(tls, ms, seqStore, rep, ip, libc.Uint64FromInt64(int64(iend)-int64(ip)))
}

/**** ended inlining compress/zstd_ldm.c ****/
/**** start inlining compress/zstd_opt.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: hist.h ****/
/**** skipping file: zstd_opt.h ****/

/*-*************************************
*  Price functions for optimal parser
***************************************/

// C documentation
//
//	/* ZSTD_bitWeight() :
//	 * provide estimated "cost" of a stat in full bits only */
func ZSTD_bitWeight(tls *libc.TLS, stat U32) (r U32) {
	return ZSTD_highbit32(tls, stat+uint32(1)) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
}

// C documentation
//
//	/* ZSTD_fracWeight() :
//	 * provide fractional-bit "cost" of a stat,
//	 * using linear interpolation approximation */
func ZSTD_fracWeight(tls *libc.TLS, rawStat U32) (r U32) {
	var BWeight, FWeight, hb, stat, weight U32
	_, _, _, _, _ = BWeight, FWeight, hb, stat, weight
	stat = rawStat + uint32(1)
	hb = ZSTD_highbit32(tls, stat)
	BWeight = hb * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	/* Fweight was meant for "Fractional weight"
	 * but it's effectively a value between 1 and 2
	 * using fixed point arithmetic */
	FWeight = stat << libc.Int32FromInt32(BITCOST_ACCURACY) >> hb
	weight = BWeight + FWeight
	return weight
}

func ZSTD_compressedLiterals(tls *libc.TLS, optPtr uintptr) (r int32) {
	return libc.BoolInt32((*optState_t)(unsafe.Pointer(optPtr)).FliteralCompressionMode != int32(ZSTD_ps_disable))
}

func ZSTD_setBasePrices(tls *libc.TLS, optPtr uintptr, optLevel int32) {
	var v1 uint32
	_ = v1
	if ZSTD_compressedLiterals(tls, optPtr) != 0 {
		if optLevel != 0 {
			v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitSum)
		} else {
			v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitSum)
		}
		(*optState_t)(unsafe.Pointer(optPtr)).FlitSumBasePrice = v1
	}
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum)
	} else {
		v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum)
	}
	(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSumBasePrice = v1
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum)
	} else {
		v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum)
	}
	(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSumBasePrice = v1
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum)
	} else {
		v1 = ZSTD_bitWeight(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum)
	}
	(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSumBasePrice = v1
}

func sum_u32(tls *libc.TLS, table uintptr, nbElts size_t) (r U32) {
	var n size_t
	var total U32
	_, _ = n, total
	total = uint32(0)
	n = uint64(0)
	for {
		if !(n < nbElts) {
			break
		}
		total = total + *(*uint32)(unsafe.Pointer(table + uintptr(n)*4))
		goto _1
	_1:
		;
		n = n + 1
	}
	return total
}

type base_directive_e = int32

const base_0possible = 0
const base_1guaranteed = 1

func ZSTD_downscaleStats(tls *libc.TLS, table uintptr, lastEltIndex U32, shift U32, base1 base_directive_e) (r U32) {
	var base, newStat uint32
	var s, sum U32
	var v2 int32
	_, _, _, _, _ = base, newStat, s, sum, v2
	sum = uint32(0)
	s = uint32(0)
	for {
		if !(s < lastEltIndex+uint32(1)) {
			break
		}
		if base1 != 0 {
			v2 = int32(1)
		} else {
			v2 = libc.BoolInt32(*(*uint32)(unsafe.Pointer(table + uintptr(s)*4)) > uint32(0))
		}
		base = libc.Uint32FromInt32(v2)
		newStat = base + *(*uint32)(unsafe.Pointer(table + uintptr(s)*4))>>shift
		sum = sum + newStat
		*(*uint32)(unsafe.Pointer(table + uintptr(s)*4)) = newStat
		goto _1
	_1:
		;
		s = s + 1
	}
	return sum
}

// C documentation
//
//	/* ZSTD_scaleStats() :
//	 * reduce all elt frequencies in table if sum too large
//	 * return the resulting sum of elements */
func ZSTD_scaleStats(tls *libc.TLS, table uintptr, lastEltIndex U32, logTarget U32) (r U32) {
	var factor, prevsum U32
	_, _ = factor, prevsum
	prevsum = sum_u32(tls, table, uint64(lastEltIndex+uint32(1)))
	factor = prevsum >> logTarget
	if factor <= uint32(1) {
		return prevsum
	}
	return ZSTD_downscaleStats(tls, table, lastEltIndex, ZSTD_highbit32(tls, factor), int32(base_1guaranteed))
}

// C documentation
//
//	/* ZSTD_rescaleFreqs() :
//	 * if first block (detected by optPtr->litLengthSum == 0) : init statistics
//	 *    take hints from dictionary if there is one
//	 *    and init from zero if there is none,
//	 *    using src for literals stats, and baseline stats for sequence symbols
//	 * otherwise downscale existing stats, to be used as seed for next block.
//	 */
func ZSTD_rescaleFreqs(tls *libc.TLS, optPtr uintptr, src uintptr, srcSize size_t, optLevel int32) {
	bp := tls.Alloc(384)
	defer tls.Free(384)
	var bitCost, bitCost1, bitCost2, bitCost3, scaleLog, scaleLog1, scaleLog2, scaleLog3 U32
	var compressedLiterals, v2 int32
	var lit, ll, ml, ml1, of uint32
	var _ /* baseLLfreqs at bp+100 */ [36]uint32
	var _ /* baseOFCfreqs at bp+244 */ [32]uint32
	var _ /* lit at bp+96 */ uint32
	var _ /* llstate at bp+0 */ FSE_CState_t
	var _ /* mlstate at bp+32 */ FSE_CState_t
	var _ /* ofstate at bp+64 */ FSE_CState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bitCost, bitCost1, bitCost2, bitCost3, compressedLiterals, lit, ll, ml, ml1, of, scaleLog, scaleLog1, scaleLog2, scaleLog3, v2
	compressedLiterals = ZSTD_compressedLiterals(tls, optPtr)
	(*optState_t)(unsafe.Pointer(optPtr)).FpriceType = int32(zop_dynamic)
	if (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum == uint32(0) { /* no literals stats collected -> first block assumed -> init */
		/* heuristic: use pre-defined stats for too small inputs */
		if srcSize <= uint64(ZSTD_PREDEF_THRESHOLD) {
			(*optState_t)(unsafe.Pointer(optPtr)).FpriceType = int32(zop_predef)
		}
		if (*ZSTD_entropyCTables_t)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts)).Fhuf.FrepeatMode == int32(HUF_repeat_valid) {
			/* huffman stats covering the full value set : table presumed generated by dictionary */
			(*optState_t)(unsafe.Pointer(optPtr)).FpriceType = int32(zop_dynamic)
			if compressedLiterals != 0 {
				(*optState_t)(unsafe.Pointer(optPtr)).FlitSum = uint32(0)
				lit = uint32(0)
				for {
					if !(lit <= libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits)-libc.Int32FromInt32(1))) {
						break
					}
					scaleLog = uint32(11) /* scale to 2K */
					bitCost = HUF_getNbBitsFromCTable(tls, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts, lit)
					if bitCost != 0 {
						v2 = int32(1) << (scaleLog - bitCost)
					} else {
						v2 = int32(1)
					}
					*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(lit)*4)) = libc.Uint32FromInt32(v2)
					*(*U32)(unsafe.Pointer(optPtr + 48)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(lit)*4))
					goto _1
				_1:
					;
					lit = lit + 1
				}
			}
			FSE_initCState(tls, bp, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts+2064+2224)
			(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = uint32(0)
			ll = uint32(0)
			for {
				if !(ll <= uint32(MaxLL)) {
					break
				}
				scaleLog1 = uint32(10) /* scale to 1K */
				bitCost1 = FSE_getMaxNbBits(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp))).FsymbolTT, ll)
				if bitCost1 != 0 {
					v2 = int32(1) << (scaleLog1 - bitCost1)
				} else {
					v2 = int32(1)
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(ll)*4)) = libc.Uint32FromInt32(v2)
				*(*U32)(unsafe.Pointer(optPtr + 52)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(ll)*4))
				goto _3
			_3:
				;
				ll = ll + 1
			}
			FSE_initCState(tls, bp+32, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts+2064+772)
			(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = uint32(0)
			ml = uint32(0)
			for {
				if !(ml <= uint32(MaxML)) {
					break
				}
				scaleLog2 = uint32(10)
				bitCost2 = FSE_getMaxNbBits(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp + 32))).FsymbolTT, ml)
				if bitCost2 != 0 {
					v2 = int32(1) << (scaleLog2 - bitCost2)
				} else {
					v2 = int32(1)
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(ml)*4)) = libc.Uint32FromInt32(v2)
				*(*U32)(unsafe.Pointer(optPtr + 56)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(ml)*4))
				goto _5
			_5:
				;
				ml = ml + 1
			}
			FSE_initCState(tls, bp+64, (*optState_t)(unsafe.Pointer(optPtr)).FsymbolCosts+2064)
			(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = uint32(0)
			of = uint32(0)
			for {
				if !(of <= uint32(MaxOff)) {
					break
				}
				scaleLog3 = uint32(10)
				bitCost3 = FSE_getMaxNbBits(tls, (*(*FSE_CState_t)(unsafe.Pointer(bp + 64))).FsymbolTT, of)
				if bitCost3 != 0 {
					v2 = int32(1) << (scaleLog3 - bitCost3)
				} else {
					v2 = int32(1)
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(of)*4)) = libc.Uint32FromInt32(v2)
				*(*U32)(unsafe.Pointer(optPtr + 60)) += *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(of)*4))
				goto _7
			_7:
				;
				of = of + 1
			}
		} else { /* first block, no dictionary */
			if compressedLiterals != 0 {
				/* base initial cost of literals on direct frequency within src */
				*(*uint32)(unsafe.Pointer(bp + 96)) = libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits) - libc.Int32FromInt32(1))
				HIST_count_simple(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitFreq, bp+96, src, srcSize) /* use raw first block to init statistics */
				(*optState_t)(unsafe.Pointer(optPtr)).FlitSum = ZSTD_downscaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitFreq, libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits)-libc.Int32FromInt32(1)), uint32(8), int32(base_0possible))
			}
			*(*[36]uint32)(unsafe.Pointer(bp + 100)) = [36]uint32{
				0:  uint32(4),
				1:  uint32(2),
				2:  uint32(1),
				3:  uint32(1),
				4:  uint32(1),
				5:  uint32(1),
				6:  uint32(1),
				7:  uint32(1),
				8:  uint32(1),
				9:  uint32(1),
				10: uint32(1),
				11: uint32(1),
				12: uint32(1),
				13: uint32(1),
				14: uint32(1),
				15: uint32(1),
				16: uint32(1),
				17: uint32(1),
				18: uint32(1),
				19: uint32(1),
				20: uint32(1),
				21: uint32(1),
				22: uint32(1),
				23: uint32(1),
				24: uint32(1),
				25: uint32(1),
				26: uint32(1),
				27: uint32(1),
				28: uint32(1),
				29: uint32(1),
				30: uint32(1),
				31: uint32(1),
				32: uint32(1),
				33: uint32(1),
				34: uint32(1),
				35: uint32(1),
			}
			libc.Xmemcpy(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq, bp+100, libc.Uint64FromInt64(144))
			(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = sum_u32(tls, bp+100, libc.Uint64FromInt32(libc.Int32FromInt32(MaxLL)+libc.Int32FromInt32(1)))
			ml1 = uint32(0)
			for {
				if !(ml1 <= uint32(MaxML)) {
					break
				}
				*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(ml1)*4)) = uint32(1)
				goto _9
			_9:
				;
				ml1 = ml1 + 1
			}
			(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = libc.Uint32FromInt32(libc.Int32FromInt32(MaxML) + libc.Int32FromInt32(1))
			*(*[32]uint32)(unsafe.Pointer(bp + 244)) = [32]uint32{
				0:  uint32(6),
				1:  uint32(2),
				2:  uint32(1),
				3:  uint32(1),
				4:  uint32(2),
				5:  uint32(3),
				6:  uint32(4),
				7:  uint32(4),
				8:  uint32(4),
				9:  uint32(3),
				10: uint32(2),
				11: uint32(1),
				12: uint32(1),
				13: uint32(1),
				14: uint32(1),
				15: uint32(1),
				16: uint32(1),
				17: uint32(1),
				18: uint32(1),
				19: uint32(1),
				20: uint32(1),
				21: uint32(1),
				22: uint32(1),
				23: uint32(1),
				24: uint32(1),
				25: uint32(1),
				26: uint32(1),
				27: uint32(1),
				28: uint32(1),
				29: uint32(1),
				30: uint32(1),
				31: uint32(1),
			}
			libc.Xmemcpy(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq, bp+244, libc.Uint64FromInt64(128))
			(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = sum_u32(tls, bp+244, libc.Uint64FromInt32(libc.Int32FromInt32(MaxOff)+libc.Int32FromInt32(1)))
		}
	} else { /* new block : scale down accumulated statistics */
		if compressedLiterals != 0 {
			(*optState_t)(unsafe.Pointer(optPtr)).FlitSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitFreq, libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(Litbits)-libc.Int32FromInt32(1)), uint32(12))
		}
		(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq, uint32(MaxLL), uint32(11))
		(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq, uint32(MaxML), uint32(11))
		(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = ZSTD_scaleStats(tls, (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq, uint32(MaxOff), uint32(11))
	}
	ZSTD_setBasePrices(tls, optPtr, optLevel)
}

// C documentation
//
//	/* ZSTD_rawLiteralsCost() :
//	 * price of literals (only) in specified segment (which length can be 0).
//	 * does not include price of literalLength symbol */
func ZSTD_rawLiteralsCost(tls *libc.TLS, literals uintptr, litLength U32, optPtr uintptr, optLevel int32) (r U32) {
	var litPrice, litPriceMax, price, u U32
	var v2 uint32
	_, _, _, _, _ = litPrice, litPriceMax, price, u, v2
	if litLength == uint32(0) {
		return uint32(0)
	}
	if !(ZSTD_compressedLiterals(tls, optPtr) != 0) {
		return litLength << libc.Int32FromInt32(3) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* Uncompressed - 8 bytes per literal. */
	if (*optState_t)(unsafe.Pointer(optPtr)).FpriceType == int32(zop_predef) {
		return litLength * uint32(6) * libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* 6 bit per literal - no statistic used */
	/* dynamic statistics */
	price = (*optState_t)(unsafe.Pointer(optPtr)).FlitSumBasePrice * litLength
	litPriceMax = (*optState_t)(unsafe.Pointer(optPtr)).FlitSumBasePrice - libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	u = uint32(0)
	for {
		if !(u < litLength) {
			break
		}
		if optLevel != 0 {
			v2 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(*(*BYTE)(unsafe.Pointer(literals + uintptr(u))))*4)))
		} else {
			v2 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(*(*BYTE)(unsafe.Pointer(literals + uintptr(u))))*4)))
		}
		litPrice = v2
		if libc.BoolInt64(litPrice > litPriceMax) != 0 {
			litPrice = litPriceMax
		}
		price = price - litPrice
		goto _1
	_1:
		;
		u = u + 1
	}
	return price
	return r
}

// C documentation
//
//	/* ZSTD_litLengthPrice() :
//	 * cost of literalLength symbol */
func ZSTD_litLengthPrice(tls *libc.TLS, litLength U32, optPtr uintptr, optLevel int32) (r U32) {
	var llCode U32
	var v1 uint32
	_, _ = llCode, v1
	if (*optState_t)(unsafe.Pointer(optPtr)).FpriceType == int32(zop_predef) {
		if optLevel != 0 {
			v1 = ZSTD_fracWeight(tls, litLength)
		} else {
			v1 = ZSTD_bitWeight(tls, litLength)
		}
		return v1
	}
	/* ZSTD_LLcode() can't compute litLength price for sizes >= ZSTD_BLOCKSIZE_MAX
	 * because it isn't representable in the zstd format.
	 * So instead just pretend it would cost 1 bit more than ZSTD_BLOCKSIZE_MAX - 1.
	 * In such a case, the block would be all literals.
	 */
	if litLength == libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		return libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)) + ZSTD_litLengthPrice(tls, libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)-libc.Int32FromInt32(1)), optPtr, optLevel)
	}
	/* dynamic statistics */
	llCode = ZSTD_LLcode(tls, litLength)
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)))
	} else {
		v1 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)))
	}
	return libc.Uint32FromInt32(libc.Int32FromUint8(LL_bits[llCode])*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))) + (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSumBasePrice - v1
	return r
}

// C documentation
//
//	/* ZSTD_getMatchPrice() :
//	 * Provides the cost of the match part (offset + matchLength) of a sequence.
//	 * Must be combined with ZSTD_fullLiteralsCost() to get the full cost of a sequence.
//	 * @offBase : sumtype, representing an offset or a repcode, and using numeric representation of ZSTD_storeSeq()
//	 * @optLevel: when <2, favors small offset for decompression speed (improved cache efficiency)
//	 */
func ZSTD_getMatchPrice(tls *libc.TLS, offBase U32, matchLength U32, optPtr uintptr, optLevel int32) (r U32) {
	var mlBase, mlCode, offCode, price U32
	var v1 uint32
	_, _, _, _, _ = mlBase, mlCode, offCode, price, v1
	offCode = ZSTD_highbit32(tls, offBase)
	mlBase = matchLength - uint32(MINMATCH)
	if (*optState_t)(unsafe.Pointer(optPtr)).FpriceType == int32(zop_predef) { /* fixed scheme, does not use statistics */
		if optLevel != 0 {
			v1 = ZSTD_fracWeight(tls, mlBase)
		} else {
			v1 = ZSTD_bitWeight(tls, mlBase)
		}
		return v1 + (uint32(16)+offCode)*libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* emulated offset cost */
	/* dynamic statistics */
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)))
	} else {
		v1 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)))
	}
	price = offCode*libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)) + ((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSumBasePrice - v1)
	if optLevel < int32(2) && offCode >= uint32(20) {
		price = price + (offCode-uint32(19))*uint32(2)*libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))
	} /* handicap for long distance offsets, favor decompression speed */
	/* match Length */
	mlCode = ZSTD_MLcode(tls, mlBase)
	if optLevel != 0 {
		v1 = ZSTD_fracWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)))
	} else {
		v1 = ZSTD_bitWeight(tls, *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)))
	}
	price = price + (libc.Uint32FromInt32(libc.Int32FromUint8(ML_bits[mlCode])*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY))) + ((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSumBasePrice - v1))
	price = price + libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)/libc.Int32FromInt32(5)) /* heuristic : make matches a bit more costly to favor less sequences -> faster decompression speed */
	return price
}

// C documentation
//
//	/* ZSTD_updateStats() :
//	 * assumption : literals + litLength <= iend */
func ZSTD_updateStats(tls *libc.TLS, optPtr uintptr, litLength U32, literals uintptr, offBase U32, matchLength U32) {
	var llCode, mlBase, mlCode, offCode, u U32
	_, _, _, _, _ = llCode, mlBase, mlCode, offCode, u
	/* literals */
	if ZSTD_compressedLiterals(tls, optPtr) != 0 {
		u = uint32(0)
		for {
			if !(u < litLength) {
				break
			}
			*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitFreq + uintptr(*(*BYTE)(unsafe.Pointer(literals + uintptr(u))))*4)) += uint32(ZSTD_LITFREQ_ADD)
			goto _1
		_1:
			;
			u = u + 1
		}
		*(*U32)(unsafe.Pointer(optPtr + 48)) += litLength * uint32(ZSTD_LITFREQ_ADD)
	}
	/* literal Length */
	llCode = ZSTD_LLcode(tls, litLength)
	*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)) = *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FlitLengthFreq + uintptr(llCode)*4)) + 1
	(*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum = (*optState_t)(unsafe.Pointer(optPtr)).FlitLengthSum + 1
	/* offset code : follows storeSeq() numeric representation */
	offCode = ZSTD_highbit32(tls, offBase)
	*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)) = *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FoffCodeFreq + uintptr(offCode)*4)) + 1
	(*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum = (*optState_t)(unsafe.Pointer(optPtr)).FoffCodeSum + 1
	/* match Length */
	mlBase = matchLength - uint32(MINMATCH)
	mlCode = ZSTD_MLcode(tls, mlBase)
	*(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)) = *(*uint32)(unsafe.Pointer((*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthFreq + uintptr(mlCode)*4)) + 1
	(*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum = (*optState_t)(unsafe.Pointer(optPtr)).FmatchLengthSum + 1
}

// C documentation
//
//	/* ZSTD_readMINMATCH() :
//	 * function safe only for comparisons
//	 * assumption : memPtr must be at least 4 bytes before end of buffer */
func ZSTD_readMINMATCH(tls *libc.TLS, memPtr uintptr, length U32) (r U32) {
	switch length {
	default:
		fallthrough
	case uint32(4):
		return MEM_read32(tls, memPtr)
	case uint32(3):
		if MEM_isLittleEndian(tls) != 0 {
			return MEM_read32(tls, memPtr) << int32(8)
		} else {
			return MEM_read32(tls, memPtr) >> int32(8)
		}
	}
	return r
}

// C documentation
//
//	/* Update hashTable3 up to ip (excluded)
//	   Assumption : always within prefix (i.e. not within extDict) */
func ZSTD_insertAndFindFirstIndexHash3(tls *libc.TLS, ms uintptr, nextToUpdate3 uintptr, ip uintptr) (r U32) {
	var base, hashTable3 uintptr
	var hash3 size_t
	var hashLog3, idx, target U32
	_, _, _, _, _, _ = base, hash3, hashLog3, hashTable3, idx, target
	hashTable3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable3
	hashLog3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashLog3
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	idx = *(*U32)(unsafe.Pointer(nextToUpdate3))
	target = libc.Uint32FromInt64(int64(ip) - int64(base))
	hash3 = ZSTD_hash3Ptr(tls, ip, hashLog3)
	for idx < target {
		*(*U32)(unsafe.Pointer(hashTable3 + uintptr(ZSTD_hash3Ptr(tls, base+uintptr(idx), hashLog3))*4)) = idx
		idx = idx + 1
	}
	*(*U32)(unsafe.Pointer(nextToUpdate3)) = target
	return *(*U32)(unsafe.Pointer(hashTable3 + uintptr(hash3)*4))
}

// C documentation
//
//	/*-*************************************
//	*  Binary Tree search
//	***************************************/
//	/** ZSTD_insertBt1() : add one or multiple positions to tree.
//	 * @param ip assumed <= iend-8 .
//	 * @param target The target of ZSTD_updateTree_internal() - we are filling to this position
//	 * @return : nb of positions added */
func ZSTD_insertBt1(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, target U32, mls U32, extDict int32) (r U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, hashTable, largerPtr, match, nextPtr, prefixStart, smallerPtr uintptr
	var bestLength, commonLengthLarger, commonLengthSmaller, h, matchLength size_t
	var btLog, btLow, btMask, curr, dictLimit, hashLog, matchEndIdx, matchIndex, nbCompares, positions, windowLow, v4 U32
	var v1 uint32
	var v3 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bestLength, bt, btLog, btLow, btMask, cParams, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictEnd, dictLimit, h, hashLog, hashTable, largerPtr, match, matchEndIdx, matchIndex, matchLength, nbCompares, nextPtr, positions, prefixStart, smallerPtr, windowLow, v1, v3, v4
	cParams = ms + 256
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = libc.Uint32FromInt32(int32(1)<<btLog - int32(1))
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	curr = libc.Uint32FromInt64(int64(ip) - int64(base))
	if btMask >= curr {
		v1 = uint32(0)
	} else {
		v1 = curr - btMask
	}
	btLow = v1
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = smallerPtr + uintptr(1)*4 /* to be nullified at the end */
	/* windowLow is based on target because
	 * we only need positions that will be in the window at the end of the tree update.
	 */
	windowLow = ZSTD_getLowestMatchIndex(tls, ms, target, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	matchEndIdx = curr + uint32(8) + uint32(1)
	bestLength = uint64(8)
	nbCompares = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	/* required for h calculation */
	*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = curr /* Update Hash Table */
	for {
		if !(nbCompares != 0 && matchIndex >= windowLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v3 = commonLengthSmaller
		} else {
			v3 = commonLengthLarger
		}
		matchLength = v3 /* guaranteed minimum nb of common bytes */
		if !(extDict != 0) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
			/* might be wrong if actually extDict */
			match = base + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend)
		} else {
			match = dictBase + uintptr(matchIndex)
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match+uintptr(matchLength), iend, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match = base + uintptr(matchIndex)
			} /* to prepare for next usage of match[matchLength] */
		}
		if matchLength > bestLength {
			bestLength = matchLength
			if matchLength > uint64(matchEndIdx-matchIndex) {
				matchEndIdx = matchIndex + uint32(matchLength)
			}
		}
		if ip+uintptr(matchLength) == iend { /* equal : no way to know if inf or sup */
			break /* drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt tree */
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match + uintptr(matchLength)))) < libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) { /* necessarily within buffer */
			/* match is smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop searching */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new "candidate" => larger than match, which was smaller than target */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex, larger than previous and closer to current */
		} else {
			/* match is larger than current */
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop searching */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _2
	_2:
		;
		nbCompares = nbCompares - 1
	}
	v4 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v4
	*(*U32)(unsafe.Pointer(smallerPtr)) = v4
	positions = uint32(0)
	if bestLength > uint64(384) {
		if libc.Uint32FromInt32(libc.Int32FromInt32(192)) < uint32(bestLength-libc.Uint64FromInt32(384)) {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(192))
		} else {
			v1 = uint32(bestLength - libc.Uint64FromInt32(384))
		}
		positions = v1
	} /* speed optimization */
	if positions > matchEndIdx-(curr+uint32(8)) {
		v1 = positions
	} else {
		v1 = matchEndIdx - (curr + uint32(8))
	}
	return v1
	return r
}

func ZSTD_updateTree_internal(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr, mls U32, dictMode ZSTD_dictMode_e) {
	var base uintptr
	var forward, idx, target U32
	_, _, _, _ = base, forward, idx, target
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	target = libc.Uint32FromInt64(int64(ip) - int64(base))
	idx = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	for idx < target {
		forward = ZSTD_insertBt1(tls, ms, base+uintptr(idx), iend, target, mls, libc.BoolInt32(dictMode == int32(ZSTD_extDict)))
		idx = idx + forward
	}
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = target
}

func ZSTD_updateTree(tls *libc.TLS, ms uintptr, ip uintptr, iend uintptr) {
	ZSTD_updateTree_internal(tls, ms, ip, iend, (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch, int32(ZSTD_noDict))
}

func ZSTD_insertBtAndGetAllMatches(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iLimit uintptr, dictMode ZSTD_dictMode_e, rep uintptr, ll0 U32, lengthToBeat U32, mls U32) (r U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var base, bt, cParams, dictBase, dictEnd, dms, dmsBase, dmsBt, dmsCParams, dmsEnd, hashTable, largerPtr, match, match1, match2, match3, nextPtr, nextPtr1, prefixStart, repMatch, smallerPtr, v5, v6, v7, v8 uintptr
	var bestLength, commonLengthLarger, commonLengthSmaller, dmsH, h, matchLength, matchLength1, mlen, v22 size_t
	var btLog, btLow, btMask, curr, dictLimit, dictMatchIndex, dmsBtLog, dmsBtLow, dmsBtMask, dmsHashLog, dmsHighLimit, dmsIndexDelta, dmsLowLimit, hashLog, lastR, matchEndIdx, matchIndex, matchIndex3, matchLow, minMatch, mnum, nbCompares, repCode, repIndex, repLen, repOffset, sufficient_len, windowLow, v21 U32
	var v1, v10, v11, v12, v13, v14, v15, v3, v4, v9 uint32
	var v2 int32
	var v20 uint64
	var _ /* dummy32 at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = base, bestLength, bt, btLog, btLow, btMask, cParams, commonLengthLarger, commonLengthSmaller, curr, dictBase, dictEnd, dictLimit, dictMatchIndex, dms, dmsBase, dmsBt, dmsBtLog, dmsBtLow, dmsBtMask, dmsCParams, dmsEnd, dmsH, dmsHashLog, dmsHighLimit, dmsIndexDelta, dmsLowLimit, h, hashLog, hashTable, largerPtr, lastR, match, match1, match2, match3, matchEndIdx, matchIndex, matchIndex3, matchLength, matchLength1, matchLow, minMatch, mlen, mnum, nbCompares, nextPtr, nextPtr1, prefixStart, repCode, repIndex, repLen, repMatch, repOffset, smallerPtr, sufficient_len, windowLow, v1, v10, v11, v12, v13, v14, v15, v2, v20, v21, v22, v3, v4, v5, v6, v7, v8, v9
	cParams = ms + 256
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength < libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)-libc.Int32FromInt32(1)) {
		v1 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12) - libc.Int32FromInt32(1))
	}
	sufficient_len = v1
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	curr = libc.Uint32FromInt64(int64(ip) - int64(base))
	hashLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FhashLog
	if mls == uint32(3) {
		v2 = int32(3)
	} else {
		v2 = int32(4)
	}
	minMatch = libc.Uint32FromInt32(v2)
	hashTable = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FhashTable
	h = ZSTD_hashPtr(tls, ip, hashLog, mls)
	matchIndex = *(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4))
	bt = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FchainTable
	btLog = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FchainLog - uint32(1)
	btMask = uint32(1)<<btLog - uint32(1)
	commonLengthSmaller = uint64(0)
	commonLengthLarger = uint64(0)
	dictBase = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictBase
	dictLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	dictEnd = dictBase + uintptr(dictLimit)
	prefixStart = base + uintptr(dictLimit)
	if btMask >= curr {
		v3 = uint32(0)
	} else {
		v3 = curr - btMask
	}
	btLow = v3
	windowLow = ZSTD_getLowestMatchIndex(tls, ms, curr, (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FwindowLog)
	if windowLow != 0 {
		v4 = windowLow
	} else {
		v4 = uint32(1)
	}
	matchLow = v4
	smallerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4
	largerPtr = bt + uintptr(uint32(2)*(curr&btMask))*4 + uintptr(1)*4
	matchEndIdx = curr + uint32(8) + uint32(1) /* to be nullified at the end */
	mnum = uint32(0)
	nbCompares = uint32(1) << (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FsearchLog
	if dictMode == int32(ZSTD_dictMatchState) {
		v5 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FdictMatchState
	} else {
		v5 = libc.UintptrFromInt32(0)
	}
	dms = v5
	if dictMode == int32(ZSTD_dictMatchState) {
		v6 = dms + 256
	} else {
		v6 = libc.UintptrFromInt32(0)
	}
	dmsCParams = v6
	if dictMode == int32(ZSTD_dictMatchState) {
		v7 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.Fbase
	} else {
		v7 = libc.UintptrFromInt32(0)
	}
	dmsBase = v7
	if dictMode == int32(ZSTD_dictMatchState) {
		v8 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FnextSrc
	} else {
		v8 = libc.UintptrFromInt32(0)
	}
	dmsEnd = v8
	if dictMode == int32(ZSTD_dictMatchState) {
		v9 = libc.Uint32FromInt64(int64(dmsEnd) - int64(dmsBase))
	} else {
		v9 = uint32(0)
	}
	dmsHighLimit = v9
	if dictMode == int32(ZSTD_dictMatchState) {
		v10 = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).Fwindow.FlowLimit
	} else {
		v10 = uint32(0)
	}
	dmsLowLimit = v10
	if dictMode == int32(ZSTD_dictMatchState) {
		v11 = windowLow - dmsHighLimit
	} else {
		v11 = uint32(0)
	}
	dmsIndexDelta = v11
	if dictMode == int32(ZSTD_dictMatchState) {
		v12 = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FhashLog
	} else {
		v12 = hashLog
	}
	dmsHashLog = v12
	if dictMode == int32(ZSTD_dictMatchState) {
		v13 = (*ZSTD_compressionParameters)(unsafe.Pointer(dmsCParams)).FchainLog - uint32(1)
	} else {
		v13 = btLog
	}
	dmsBtLog = v13
	if dictMode == int32(ZSTD_dictMatchState) {
		v14 = uint32(1)<<dmsBtLog - uint32(1)
	} else {
		v14 = uint32(0)
	}
	dmsBtMask = v14
	if dictMode == int32(ZSTD_dictMatchState) && dmsBtMask < dmsHighLimit-dmsLowLimit {
		v15 = dmsHighLimit - dmsBtMask
	} else {
		v15 = dmsLowLimit
	}
	dmsBtLow = v15
	bestLength = uint64(lengthToBeat - uint32(1))
	/* check repCode */
	/* necessarily 1 or 0 */
	lastR = uint32(ZSTD_REP_NUM) + ll0
	repCode = ll0
	for {
		if !(repCode < lastR) {
			break
		}
		if repCode == uint32(ZSTD_REP_NUM) {
			v1 = *(*U32)(unsafe.Pointer(rep)) - uint32(1)
		} else {
			v1 = *(*U32)(unsafe.Pointer(rep + uintptr(repCode)*4))
		}
		repOffset = v1
		repIndex = curr - repOffset
		repLen = uint32(0)
		if repOffset-uint32(1) < curr-dictLimit { /* equivalent to `curr > repIndex >= dictLimit` */
			/* We must validate the repcode offset because when we're using a dictionary the
			 * valid offset range shrinks when the dictionary goes out of bounds.
			 */
			if libc.BoolInt32(repIndex >= windowLow)&libc.BoolInt32(ZSTD_readMINMATCH(tls, ip, minMatch) == ZSTD_readMINMATCH(tls, ip-uintptr(repOffset), minMatch)) != 0 {
				repLen = uint32(ZSTD_count(tls, ip+uintptr(minMatch), ip+uintptr(minMatch)-uintptr(repOffset), iLimit)) + minMatch
			}
		} else {
			if dictMode == int32(ZSTD_dictMatchState) {
				v5 = dmsBase + uintptr(repIndex) - uintptr(dmsIndexDelta)
			} else {
				v5 = dictBase + uintptr(repIndex)
			} /* repIndex < dictLimit || repIndex >= curr */
			repMatch = v5
			if dictMode == int32(ZSTD_extDict) && libc.BoolInt32(repOffset-uint32(1) < curr-windowLow)&ZSTD_index_overlap_check(tls, dictLimit, repIndex) != 0 && ZSTD_readMINMATCH(tls, ip, minMatch) == ZSTD_readMINMATCH(tls, repMatch, minMatch) {
				repLen = uint32(ZSTD_count_2segments(tls, ip+uintptr(minMatch), repMatch+uintptr(minMatch), iLimit, dictEnd, prefixStart)) + minMatch
			}
			if dictMode == int32(ZSTD_dictMatchState) && libc.BoolInt32(repOffset-uint32(1) < curr-(dmsLowLimit+dmsIndexDelta))&ZSTD_index_overlap_check(tls, dictLimit, repIndex) != 0 && ZSTD_readMINMATCH(tls, ip, minMatch) == ZSTD_readMINMATCH(tls, repMatch, minMatch) {
				repLen = uint32(ZSTD_count_2segments(tls, ip+uintptr(minMatch), repMatch+uintptr(minMatch), iLimit, dmsEnd, prefixStart)) + minMatch
			}
		}
		/* save longer solution */
		if uint64(repLen) > bestLength {
			bestLength = uint64(repLen)
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Foff = repCode - ll0 + libc.Uint32FromInt32(1) /* expect value between 1 and 3 */
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Flen1 = repLen
			mnum = mnum + 1
			if libc.BoolInt32(repLen > sufficient_len)|libc.BoolInt32(ip+uintptr(repLen) == iLimit) != 0 { /* best possible */
				return mnum
			}
		}
		goto _16
	_16:
		;
		repCode = repCode + 1
	}
	/* HC3 match finder */
	if mls == uint32(3) && bestLength < uint64(mls) {
		matchIndex3 = ZSTD_insertAndFindFirstIndexHash3(tls, ms, nextToUpdate3, ip)
		if libc.BoolInt32(matchIndex3 >= matchLow)&libc.BoolInt32(curr-matchIndex3 < libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(18))) != 0 {
			if dictMode == int32(ZSTD_noDict) || dictMode == int32(ZSTD_dictMatchState) || matchIndex3 >= dictLimit {
				match = base + uintptr(matchIndex3)
				mlen = ZSTD_count(tls, ip, match, iLimit)
			} else {
				match1 = dictBase + uintptr(matchIndex3)
				mlen = ZSTD_count_2segments(tls, ip, match1, iLimit, dictEnd, prefixStart)
			}
			/* save best solution */
			if mlen >= uint64(mls) {
				bestLength = mlen
				/* no prior solution */
				(*(*ZSTD_match_t)(unsafe.Pointer(matches))).Foff = curr - matchIndex3 + libc.Uint32FromInt32(ZSTD_REP_NUM)
				(*(*ZSTD_match_t)(unsafe.Pointer(matches))).Flen1 = uint32(mlen)
				mnum = uint32(1)
				if libc.BoolInt32(mlen > uint64(sufficient_len))|libc.BoolInt32(ip+uintptr(mlen) == iLimit) != 0 { /* best possible length */
					(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = curr + uint32(1) /* skip insertion */
					return uint32(1)
				}
			}
		}
		/* no dictMatchState lookup: dicts don't have a populated HC3 table */
	} /* if (mls == 3) */
	*(*U32)(unsafe.Pointer(hashTable + uintptr(h)*4)) = curr /* Update Hash Table */
	for {
		if !(nbCompares != 0 && matchIndex >= matchLow) {
			break
		}
		nextPtr = bt + uintptr(uint32(2)*(matchIndex&btMask))*4
		if commonLengthSmaller < commonLengthLarger {
			v20 = commonLengthSmaller
		} else {
			v20 = commonLengthLarger
		}
		matchLength = v20 /* guaranteed minimum nb of common bytes */
		if dictMode == int32(ZSTD_noDict) || dictMode == int32(ZSTD_dictMatchState) || uint64(matchIndex)+matchLength >= uint64(dictLimit) {
			/* ensure the condition is correct when !extDict */
			match2 = base + uintptr(matchIndex)
			if matchIndex >= dictLimit {
			} /* ensure early section of match is equal as expected */
			matchLength = matchLength + ZSTD_count(tls, ip+uintptr(matchLength), match2+uintptr(matchLength), iLimit)
		} else {
			match2 = dictBase + uintptr(matchIndex)
			/* ensure early section of match is equal as expected */
			matchLength = matchLength + ZSTD_count_2segments(tls, ip+uintptr(matchLength), match2+uintptr(matchLength), iLimit, dictEnd, prefixStart)
			if uint64(matchIndex)+matchLength >= uint64(dictLimit) {
				match2 = base + uintptr(matchIndex)
			} /* prepare for match[matchLength] read */
		}
		if matchLength > bestLength {
			if matchLength > uint64(matchEndIdx-matchIndex) {
				matchEndIdx = matchIndex + uint32(matchLength)
			}
			bestLength = matchLength
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Foff = curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM)
			(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Flen1 = uint32(matchLength)
			mnum = mnum + 1
			if libc.BoolInt32(matchLength > libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)))|libc.BoolInt32(ip+uintptr(matchLength) == iLimit) != 0 {
				if dictMode == int32(ZSTD_dictMatchState) {
					nbCompares = uint32(0)
				} /* break should also skip searching dms */
				break /* drop, to preserve bt consistency (miss a little bit of compression) */
			}
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match2 + uintptr(matchLength)))) < libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength)))) {
			/* match smaller than current */
			*(*U32)(unsafe.Pointer(smallerPtr)) = matchIndex /* update smaller idx */
			commonLengthSmaller = matchLength                /* all smaller will now have at least this guaranteed common length */
			if matchIndex <= btLow {
				smallerPtr = bp
				break
			} /* beyond tree size, stop the search */
			smallerPtr = nextPtr + uintptr(1)*4                 /* new candidate => larger than match, which was smaller than current */
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr + 1*4)) /* new matchIndex, larger than previous, closer to current */
		} else {
			*(*U32)(unsafe.Pointer(largerPtr)) = matchIndex
			commonLengthLarger = matchLength
			if matchIndex <= btLow {
				largerPtr = bp
				break
			} /* beyond tree size, stop the search */
			largerPtr = nextPtr
			matchIndex = *(*U32)(unsafe.Pointer(nextPtr))
		}
		goto _19
	_19:
		;
		nbCompares = nbCompares - 1
	}
	v21 = libc.Uint32FromInt32(0)
	*(*U32)(unsafe.Pointer(largerPtr)) = v21
	*(*U32)(unsafe.Pointer(smallerPtr)) = v21
	/* Check we haven't underflowed. */
	if dictMode == int32(ZSTD_dictMatchState) && nbCompares != 0 {
		dmsH = ZSTD_hashPtr(tls, ip, dmsHashLog, mls)
		dictMatchIndex = *(*U32)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FhashTable + uintptr(dmsH)*4))
		dmsBt = (*ZSTD_MatchState_t)(unsafe.Pointer(dms)).FchainTable
		v22 = libc.Uint64FromInt32(0)
		commonLengthLarger = v22
		commonLengthSmaller = v22
		for {
			if !(nbCompares != 0 && dictMatchIndex > dmsLowLimit) {
				break
			}
			nextPtr1 = dmsBt + uintptr(uint32(2)*(dictMatchIndex&dmsBtMask))*4
			if commonLengthSmaller < commonLengthLarger {
				v20 = commonLengthSmaller
			} else {
				v20 = commonLengthLarger
			}
			matchLength1 = v20 /* guaranteed minimum nb of common bytes */
			match3 = dmsBase + uintptr(dictMatchIndex)
			matchLength1 = matchLength1 + ZSTD_count_2segments(tls, ip+uintptr(matchLength1), match3+uintptr(matchLength1), iLimit, dmsEnd, prefixStart)
			if uint64(dictMatchIndex)+matchLength1 >= uint64(dmsHighLimit) {
				match3 = base + uintptr(dictMatchIndex) + uintptr(dmsIndexDelta)
			} /* to prepare for next usage of match[matchLength] */
			if matchLength1 > bestLength {
				matchIndex = dictMatchIndex + dmsIndexDelta
				if matchLength1 > uint64(matchEndIdx-matchIndex) {
					matchEndIdx = matchIndex + uint32(matchLength1)
				}
				bestLength = matchLength1
				(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Foff = curr - matchIndex + libc.Uint32FromInt32(ZSTD_REP_NUM)
				(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(mnum)*8))).Flen1 = uint32(matchLength1)
				mnum = mnum + 1
				if libc.BoolInt32(matchLength1 > libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)))|libc.BoolInt32(ip+uintptr(matchLength1) == iLimit) != 0 {
					break /* drop, to guarantee consistency (miss a little bit of compression) */
				}
			}
			if dictMatchIndex <= dmsBtLow {
				break
			} /* beyond tree size, stop the search */
			if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(match3 + uintptr(matchLength1)))) < libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip + uintptr(matchLength1)))) {
				commonLengthSmaller = matchLength1                       /* all smaller will now have at least this guaranteed common length */
				dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr1 + 1*4)) /* new matchIndex larger than previous (closer to current) */
			} else {
				/* match is larger than current */
				commonLengthLarger = matchLength1
				dictMatchIndex = *(*U32)(unsafe.Pointer(nextPtr1))
			}
			goto _23
		_23:
			;
			nbCompares = nbCompares - 1
		}
	} /* if (dictMode == ZSTD_dictMatchState) */
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = matchEndIdx - uint32(8) /* skip repetitive patterns */
	return mnum
}

type ZSTD_getAllMatchesFn = uintptr

func ZSTD_btGetAllMatches_internal(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32, dictMode ZSTD_dictMode_e, mls U32) (r U32) {
	if ip < (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase+uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate) {
		return uint32(0)
	} /* skipped area */
	ZSTD_updateTree_internal(tls, ms, ip, iHighLimit, mls, dictMode)
	return ZSTD_insertBtAndGetAllMatches(tls, matches, ms, nextToUpdate3, ip, iHighLimit, dictMode, rep, ll0, lengthToBeat, mls)
}

func ZSTD_btGetAllMatches_noDict_3(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(3))
}

func ZSTD_btGetAllMatches_noDict_4(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(4))
}

func ZSTD_btGetAllMatches_noDict_5(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(5))
}

func ZSTD_btGetAllMatches_noDict_6(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_noDict), uint32(6))
}

func ZSTD_btGetAllMatches_extDict_3(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(3))
}

func ZSTD_btGetAllMatches_extDict_4(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(4))
}

func ZSTD_btGetAllMatches_extDict_5(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(5))
}

func ZSTD_btGetAllMatches_extDict_6(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_extDict), uint32(6))
}

func ZSTD_btGetAllMatches_dictMatchState_3(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(3))
}

func ZSTD_btGetAllMatches_dictMatchState_4(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(4))
}

func ZSTD_btGetAllMatches_dictMatchState_5(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(5))
}

func ZSTD_btGetAllMatches_dictMatchState_6(tls *libc.TLS, matches uintptr, ms uintptr, nextToUpdate3 uintptr, ip uintptr, iHighLimit uintptr, rep uintptr, ll0 U32, lengthToBeat U32) (r U32) {
	return ZSTD_btGetAllMatches_internal(tls, matches, ms, nextToUpdate3, ip, iHighLimit, rep, ll0, lengthToBeat, int32(ZSTD_dictMatchState), uint32(6))
}

func ZSTD_selectBtGetAllMatches(tls *libc.TLS, ms uintptr, dictMode ZSTD_dictMode_e) (r ZSTD_getAllMatchesFn) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var mls U32
	var v1, v2, v3 uint32
	var _ /* getAllMatchesFns at bp+0 */ [3][4]ZSTD_getAllMatchesFn
	_, _, _, _ = mls, v1, v2, v3
	*(*[3][4]ZSTD_getAllMatchesFn)(unsafe.Pointer(bp)) = [3][4]ZSTD_getAllMatchesFn{
		0: {
			0: __ccgo_fp(ZSTD_btGetAllMatches_noDict_3),
			1: __ccgo_fp(ZSTD_btGetAllMatches_noDict_4),
			2: __ccgo_fp(ZSTD_btGetAllMatches_noDict_5),
			3: __ccgo_fp(ZSTD_btGetAllMatches_noDict_6),
		},
		1: {
			0: __ccgo_fp(ZSTD_btGetAllMatches_extDict_3),
			1: __ccgo_fp(ZSTD_btGetAllMatches_extDict_4),
			2: __ccgo_fp(ZSTD_btGetAllMatches_extDict_5),
			3: __ccgo_fp(ZSTD_btGetAllMatches_extDict_6),
		},
		2: {
			0: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_3),
			1: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_4),
			2: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_5),
			3: __ccgo_fp(ZSTD_btGetAllMatches_dictMatchState_6),
		},
	}
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
		v2 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
	} else {
		v2 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
	}
	if libc.Uint32FromInt32(libc.Int32FromInt32(3)) > v2 {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(3))
	} else {
		if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch < libc.Uint32FromInt32(libc.Int32FromInt32(6)) {
			v3 = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FcParams.FminMatch
		} else {
			v3 = libc.Uint32FromInt32(libc.Int32FromInt32(6))
		}
		v1 = v3
	}
	mls = v1
	return *(*ZSTD_getAllMatchesFn)(unsafe.Pointer(bp + uintptr(dictMode)*32 + uintptr(mls-uint32(3))*8))
}

/*************************
*  LDM helper functions  *
*************************/

// C documentation
//
//	/* Struct containing info needed to make decision about ldm inclusion */
type ZSTD_optLdm_t = struct {
	FseqStore        RawSeqStore_t
	FstartPosInBlock U32
	FendPosInBlock   U32
	Foffset          U32
}

// C documentation
//
//	/* ZSTD_optLdm_skipRawSeqStoreBytes():
//	 * Moves forward in @rawSeqStore by @nbBytes,
//	 * which will update the fields 'pos' and 'posInSequence'.
//	 */
func ZSTD_optLdm_skipRawSeqStoreBytes(tls *libc.TLS, rawSeqStore uintptr, nbBytes size_t) {
	var currPos U32
	var currSeq rawSeq
	_, _ = currPos, currSeq
	currPos = uint32((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence + nbBytes)
	for currPos != 0 && (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos < (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		currSeq = *(*rawSeq)(unsafe.Pointer((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fseq + uintptr((*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos)*12))
		if currPos >= currSeq.FlitLength+currSeq.FmatchLength {
			currPos = currPos - (currSeq.FlitLength + currSeq.FmatchLength)
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos = (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos + 1
		} else {
			(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(currPos)
			break
		}
	}
	if currPos == uint32(0) || (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fpos == (*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).Fsize {
		(*RawSeqStore_t)(unsafe.Pointer(rawSeqStore)).FposInSequence = uint64(0)
	}
}

// C documentation
//
//	/* ZSTD_opt_getNextMatchAndUpdateSeqStore():
//	 * Calculates the beginning and end of the next match in the current block.
//	 * Updates 'pos' and 'posInSequence' of the ldmSeqStore.
//	 */
func ZSTD_opt_getNextMatchAndUpdateSeqStore(tls *libc.TLS, optLdm uintptr, currPosInBlock U32, blockBytesRemaining U32) {
	var currBlockEndPos, literalsBytesRemaining, matchBytesRemaining U32
	var currSeq rawSeq
	var v1 uint32
	_, _, _, _, _ = currBlockEndPos, currSeq, literalsBytesRemaining, matchBytesRemaining, v1
	/* Setting match end position to MAX to ensure we never use an LDM during this block */
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize == uint64(0) || (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fpos >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize {
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock = uint32(0xffffffff)
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = uint32(0xffffffff)
		return
	}
	/* Calculate appropriate bytes left in matchLength and litLength
	 * after adjusting based on ldmSeqStore->posInSequence */
	currSeq = *(*rawSeq)(unsafe.Pointer((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fseq + uintptr((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fpos)*12))
	currBlockEndPos = currPosInBlock + blockBytesRemaining
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.FposInSequence < uint64(currSeq.FlitLength) {
		v1 = currSeq.FlitLength - uint32((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.FposInSequence)
	} else {
		v1 = uint32(0)
	}
	literalsBytesRemaining = v1
	if literalsBytesRemaining == uint32(0) {
		v1 = currSeq.FmatchLength - (uint32((*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.FposInSequence) - currSeq.FlitLength)
	} else {
		v1 = currSeq.FmatchLength
	}
	matchBytesRemaining = v1
	/* If there are more literal bytes than bytes remaining in block, no ldm is possible */
	if literalsBytesRemaining >= blockBytesRemaining {
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock = uint32(0xffffffff)
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = uint32(0xffffffff)
		ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(blockBytesRemaining))
		return
	}
	/* Matches may be < minMatch by this process. In that case, we will reject them
	   when we are deciding whether or not to add the ldm */
	(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock = currPosInBlock + literalsBytesRemaining
	(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock + matchBytesRemaining
	(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).Foffset = currSeq.Foffset
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock > currBlockEndPos {
		/* Match ends after the block ends, we can't use the whole match */
		(*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock = currBlockEndPos
		ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(currBlockEndPos-currPosInBlock))
	} else {
		/* Consume nb of bytes equal to size of sequence left */
		ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(literalsBytesRemaining+matchBytesRemaining))
	}
}

// C documentation
//
//	/* ZSTD_optLdm_maybeAddMatch():
//	 * Adds a match if it's long enough,
//	 * based on it's 'matchStartPosInBlock' and 'matchEndPosInBlock',
//	 * into 'matches'. Maintains the correct ordering of 'matches'.
//	 */
func ZSTD_optLdm_maybeAddMatch(tls *libc.TLS, matches uintptr, nbMatches uintptr, optLdm uintptr, currPosInBlock U32, minMatch U32) {
	var candidateMatchLength, candidateOffBase, posDiff U32
	_, _, _ = candidateMatchLength, candidateOffBase, posDiff
	posDiff = currPosInBlock - (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock
	/* Note: ZSTD_match_t actually contains offBase and matchLength (before subtracting MINMATCH) */
	candidateMatchLength = (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock - (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock - posDiff
	/* Ensure that current block position is not outside of the match */
	if currPosInBlock < (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FstartPosInBlock || currPosInBlock >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock || candidateMatchLength < minMatch {
		return
	}
	if *(*U32)(unsafe.Pointer(nbMatches)) == uint32(0) || candidateMatchLength > (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(nbMatches))-uint32(1))*8))).Flen1 && *(*U32)(unsafe.Pointer(nbMatches)) < libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)) {
		candidateOffBase = (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).Foffset + libc.Uint32FromInt32(ZSTD_REP_NUM)
		(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(nbMatches)))*8))).Flen1 = candidateMatchLength
		(*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(nbMatches)))*8))).Foff = candidateOffBase
		*(*U32)(unsafe.Pointer(nbMatches)) = *(*U32)(unsafe.Pointer(nbMatches)) + 1
	}
}

// C documentation
//
//	/* ZSTD_optLdm_processMatchCandidate():
//	 * Wrapper function to update ldm seq store and call ldm functions as necessary.
//	 */
func ZSTD_optLdm_processMatchCandidate(tls *libc.TLS, optLdm uintptr, matches uintptr, nbMatches uintptr, currPosInBlock U32, remainingBytes U32, minMatch U32) {
	var posOvershoot U32
	_ = posOvershoot
	if (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize == uint64(0) || (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fpos >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FseqStore.Fsize {
		return
	}
	if currPosInBlock >= (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock {
		if currPosInBlock > (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock {
			/* The position at which ZSTD_optLdm_processMatchCandidate() is called is not necessarily
			 * at the end of a match from the ldm seq store, and will often be some bytes
			 * over beyond matchEndPosInBlock. As such, we need to correct for these "overshoots"
			 */
			posOvershoot = currPosInBlock - (*ZSTD_optLdm_t)(unsafe.Pointer(optLdm)).FendPosInBlock
			ZSTD_optLdm_skipRawSeqStoreBytes(tls, optLdm, uint64(posOvershoot))
		}
		ZSTD_opt_getNextMatchAndUpdateSeqStore(tls, optLdm, currPosInBlock, remainingBytes)
	}
	ZSTD_optLdm_maybeAddMatch(tls, matches, nbMatches, optLdm, currPosInBlock, minMatch)
}

/*-*******************************
*  Optimal parser
*********************************/
func ZSTD_compressBlock_opt_generic(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, optLevel int32, dictMode ZSTD_dictMode_e) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	var advance, cur, end, lastML, last_pos, litlen, litlen1, ll0, ll01, llen, longestML, matchNb, matchNb1, maxML, maxOffBase, minMatch, mlen, mlen1, offBase, offBase1, offset, pos, pos1, prev, prev1, startML, storeEnd, storePos, storeStart, stretchPos, sufficient_len, v4, v5 U32
	var anchor, base, cParams, iend, ilimit, inr, ip, istart, matches, opt, optStatePtr, prefixStart uintptr
	var basePrice, matchPrice, previousPrice, price, price1, sequencePrice, with1literal, withMoreLiterals, v2 int32
	var getAllMatches ZSTD_getAllMatchesFn
	var nextStretch, prevMatch ZSTD_optimal_t
	var v1 uint32
	var v3 RawSeqStore_t
	var _ /* lastStretch at bp+4 */ ZSTD_optimal_t
	var _ /* nbMatches at bp+116 */ U32
	var _ /* nbMatches at bp+88 */ U32
	var _ /* newReps at bp+104 */ Repcodes_t
	var _ /* newReps at bp+92 */ Repcodes_t
	var _ /* nextToUpdate3 at bp+0 */ U32
	var _ /* optLdm at bp+32 */ ZSTD_optLdm_t
	var _ /* reps at bp+120 */ Repcodes_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = advance, anchor, base, basePrice, cParams, cur, end, getAllMatches, iend, ilimit, inr, ip, istart, lastML, last_pos, litlen, litlen1, ll0, ll01, llen, longestML, matchNb, matchNb1, matchPrice, matches, maxML, maxOffBase, minMatch, mlen, mlen1, nextStretch, offBase, offBase1, offset, opt, optStatePtr, pos, pos1, prefixStart, prev, prev1, prevMatch, previousPrice, price, price1, sequencePrice, startML, storeEnd, storePos, storeStart, stretchPos, sufficient_len, with1literal, withMoreLiterals, v1, v2, v3, v4, v5
	optStatePtr = ms + 144
	istart = src
	ip = istart
	anchor = istart
	iend = istart + uintptr(srcSize)
	ilimit = iend - uintptr(8)
	base = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase
	prefixStart = base + uintptr((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit)
	cParams = ms + 256
	getAllMatches = ZSTD_selectBtGetAllMatches(tls, ms, dictMode)
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength < libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)-libc.Int32FromInt32(1)) {
		v1 = (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FtargetLength
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12) - libc.Int32FromInt32(1))
	}
	sufficient_len = v1
	if (*ZSTD_compressionParameters)(unsafe.Pointer(cParams)).FminMatch == uint32(3) {
		v2 = int32(3)
	} else {
		v2 = int32(4)
	}
	minMatch = libc.Uint32FromInt32(v2)
	*(*U32)(unsafe.Pointer(bp)) = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate
	opt = (*optState_t)(unsafe.Pointer(optStatePtr)).FpriceTable
	matches = (*optState_t)(unsafe.Pointer(optStatePtr)).FmatchTable
	libc.Xmemset(tls, bp+4, 0, libc.Uint64FromInt64(28))
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore != 0 {
		v3 = *(*RawSeqStore_t)(unsafe.Pointer((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FldmSeqStore))
	} else {
		v3 = kNullRawSeqStore
	}
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).FseqStore = v3
	v5 = libc.Uint32FromInt32(0)
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).Foffset = v5
	v4 = v5
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).FstartPosInBlock = v4
	(*(*ZSTD_optLdm_t)(unsafe.Pointer(bp + 32))).FendPosInBlock = v4
	ZSTD_opt_getNextMatchAndUpdateSeqStore(tls, bp+32, libc.Uint32FromInt64(int64(ip)-int64(istart)), libc.Uint32FromInt64(int64(iend)-int64(ip)))
	/* init */
	ZSTD_rescaleFreqs(tls, optStatePtr, src, srcSize, optLevel)
	ip = ip + libc.BoolUintptr(ip == prefixStart)
	/* Match Loop */
	for ip < ilimit {
		last_pos = uint32(0)
		/* find first match */
		litlen = libc.Uint32FromInt64(int64(ip) - int64(anchor))
		ll0 = libc.BoolUint32(!(litlen != 0))
		*(*U32)(unsafe.Pointer(bp + 88)) = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, uintptr, uintptr, U32, U32) U32)(unsafe.Pointer(&struct{ uintptr }{getAllMatches})))(tls, matches, ms, bp, ip, iend, rep, ll0, minMatch)
		ZSTD_optLdm_processMatchCandidate(tls, bp+32, matches, bp+88, libc.Uint32FromInt64(int64(ip)-int64(istart)), libc.Uint32FromInt64(int64(iend)-int64(ip)), minMatch)
		if !(*(*U32)(unsafe.Pointer(bp + 88)) != 0) {
			ip = ip + 1
			continue
		}
		/* Match found: let's store this solution, and eventually find more candidates.
		 * During this forward pass, @opt is used to store stretches,
		 * defined as "a match followed by N literals".
		 * Note how this is different from a Sequence, which is "N literals followed by a match".
		 * Storing stretches allows us to store different match predecessors
		 * for each literal position part of a literals run. */
		/* initialize opt[0] */
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Fmlen = uint32(0) /* there are only literals so far */
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Flitlen = litlen
		/* No need to include the actual price of the literals before the first match
		 * because it is static for the duration of the forward pass, and is included
		 * in every subsequent price. But, we include the literal length because
		 * the cost variation of litlen depends on the value of litlen.
		 */
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Fprice = libc.Int32FromUint32(ZSTD_litLengthPrice(tls, litlen, optStatePtr, optLevel))
		_ = libc.Uint64FromInt64(1)
		libc.Xmemcpy(tls, opt+16, rep, libc.Uint64FromInt64(12))
		/* large match -> immediate encoding */
		maxML = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 88))-uint32(1))*8))).Flen1
		maxOffBase = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 88))-uint32(1))*8))).Foff
		if maxML > sufficient_len {
			(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen = uint32(0)
			(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen = maxML
			(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Foff = maxOffBase
			cur = uint32(0)
			last_pos = maxML
			goto _shortestPath
		}
		/* set prices for first matches starting position == 0 */
		pos = uint32(1)
		for {
			if !(pos < minMatch) {
				break
			}
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fmlen = uint32(0)
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Flitlen = litlen + pos
			goto _6
		_6:
			;
			pos = pos + 1
		}
		matchNb = uint32(0)
		for {
			if !(matchNb < *(*U32)(unsafe.Pointer(bp + 88))) {
				break
			}
			offBase = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb)*8))).Foff
			end = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb)*8))).Flen1
			for {
				if !(pos <= end) {
					break
				}
				matchPrice = libc.Int32FromUint32(ZSTD_getMatchPrice(tls, offBase, pos, optStatePtr, optLevel))
				sequencePrice = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt))).Fprice + matchPrice
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fmlen = pos
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Foff = offBase
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Flitlen = uint32(0) /* end of match */
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fprice = sequencePrice + libc.Int32FromUint32(ZSTD_litLengthPrice(tls, uint32(0), optStatePtr, optLevel))
				goto _8
			_8:
				;
				pos = pos + 1
			}
			goto _7
		_7:
			;
			matchNb = matchNb + 1
		}
		last_pos = pos - uint32(1)
		(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos)*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
		/* check further positions */
		cur = uint32(1)
		for {
			if !(cur <= last_pos) {
				break
			}
			inr = ip + uintptr(cur)
			/* Fix current position with one literal if cheaper */
			litlen1 = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur-uint32(1))*28))).Flitlen + uint32(1)
			price = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur-uint32(1))*28))).Fprice + libc.Int32FromUint32(ZSTD_rawLiteralsCost(tls, ip+uintptr(cur)-uintptr(1), uint32(1), optStatePtr, optLevel)) + (libc.Int32FromUint32(ZSTD_litLengthPrice(tls, litlen1, optStatePtr, optLevel)) - libc.Int32FromUint32(ZSTD_litLengthPrice(tls, litlen1-uint32(1), optStatePtr, optLevel)))
			/* overflow check */
			if price <= (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice {
				prevMatch = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))
				*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28)) = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur-uint32(1))*28))
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen = litlen1
				(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice = price
				if optLevel >= int32(1) && prevMatch.Flitlen == uint32(0) && libc.Int32FromUint32(ZSTD_litLengthPrice(tls, uint32(1), optStatePtr, optLevel))-libc.Int32FromUint32(ZSTD_litLengthPrice(tls, libc.Uint32FromInt32(libc.Int32FromInt32(1)-libc.Int32FromInt32(1)), optStatePtr, optLevel)) < 0 && libc.BoolInt64(ip+uintptr(cur) < iend) != 0 {
					/* check next position, in case it would be cheaper */
					with1literal = prevMatch.Fprice + libc.Int32FromUint32(ZSTD_rawLiteralsCost(tls, ip+uintptr(cur), uint32(1), optStatePtr, optLevel)) + (libc.Int32FromUint32(ZSTD_litLengthPrice(tls, uint32(1), optStatePtr, optLevel)) - libc.Int32FromUint32(ZSTD_litLengthPrice(tls, libc.Uint32FromInt32(libc.Int32FromInt32(1)-libc.Int32FromInt32(1)), optStatePtr, optLevel)))
					withMoreLiterals = price + libc.Int32FromUint32(ZSTD_rawLiteralsCost(tls, ip+uintptr(cur), uint32(1), optStatePtr, optLevel)) + (libc.Int32FromUint32(ZSTD_litLengthPrice(tls, litlen1+uint32(1), optStatePtr, optLevel)) - libc.Int32FromUint32(ZSTD_litLengthPrice(tls, litlen1+uint32(1)-uint32(1), optStatePtr, optLevel)))
					if with1literal < withMoreLiterals && with1literal < (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Fprice {
						/* update offset history - before it disappears */
						prev = cur - prevMatch.Fmlen
						*(*Repcodes_t)(unsafe.Pointer(bp + 92)) = ZSTD_newRep(tls, opt+uintptr(prev)*28+16, prevMatch.Foff, libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(prev)*28))).Flitlen == uint32(0)))
						*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28)) = prevMatch /* mlen & offbase */
						libc.Xmemcpy(tls, opt+uintptr(cur+uint32(1))*28+16, bp+92, libc.Uint64FromInt64(12))
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Flitlen = uint32(1)
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Fprice = with1literal
						if last_pos < cur+uint32(1) {
							last_pos = cur + uint32(1)
						}
					}
				}
			} else {
			}
			/* Offset history is not updated during match comparison.
			 * Do it here, now that the match is selected and confirmed.
			 */
			_ = libc.Uint64FromInt64(1)
			if (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen == uint32(0) {
				/* just finished a match => alter offset history */
				prev1 = cur - (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fmlen
				*(*Repcodes_t)(unsafe.Pointer(bp + 104)) = ZSTD_newRep(tls, opt+uintptr(prev1)*28+16, (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Foff, libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(prev1)*28))).Flitlen == uint32(0)))
				libc.Xmemcpy(tls, opt+uintptr(cur)*28+16, bp+104, libc.Uint64FromInt64(12))
			}
			/* last match must start at a minimum distance of 8 from oend */
			if inr > ilimit {
				goto _9
			}
			if cur == last_pos {
				break
			}
			if optLevel == 0 && (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur+uint32(1))*28))).Fprice <= (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice+libc.Int32FromInt32(1)<<libc.Int32FromInt32(BITCOST_ACCURACY)/libc.Int32FromInt32(2) {
				goto _9 /* skip unpromising positions; about ~+6% speed, -0.01 ratio */
			}
			ll01 = libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen == libc.Uint32FromInt32(0))
			previousPrice = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Fprice
			basePrice = previousPrice + libc.Int32FromUint32(ZSTD_litLengthPrice(tls, uint32(0), optStatePtr, optLevel))
			*(*U32)(unsafe.Pointer(bp + 116)) = (*(*func(*libc.TLS, uintptr, uintptr, uintptr, uintptr, uintptr, uintptr, U32, U32) U32)(unsafe.Pointer(&struct{ uintptr }{getAllMatches})))(tls, matches, ms, bp, inr, iend, opt+uintptr(cur)*28+16, ll01, minMatch)
			ZSTD_optLdm_processMatchCandidate(tls, bp+32, matches, bp+116, libc.Uint32FromInt64(int64(inr)-int64(istart)), libc.Uint32FromInt64(int64(iend)-int64(inr)), minMatch)
			if !(*(*U32)(unsafe.Pointer(bp + 116)) != 0) {
				goto _9
			}
			longestML = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 116))-uint32(1))*8))).Flen1
			if longestML > sufficient_len || cur+longestML >= libc.Uint32FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(12)) || ip+uintptr(cur)+uintptr(longestML) >= iend {
				(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen = longestML
				(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Foff = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(*(*U32)(unsafe.Pointer(bp + 116))-uint32(1))*8))).Foff
				(*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen = uint32(0)
				last_pos = cur + longestML
				goto _shortestPath
			}
			/* set prices using matches found at position == cur */
			matchNb1 = uint32(0)
			for {
				if !(matchNb1 < *(*U32)(unsafe.Pointer(bp + 116))) {
					break
				}
				offset = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb1)*8))).Foff
				lastML = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb1)*8))).Flen1
				if matchNb1 > uint32(0) {
					v1 = (*(*ZSTD_match_t)(unsafe.Pointer(matches + uintptr(matchNb1-uint32(1))*8))).Flen1 + uint32(1)
				} else {
					v1 = minMatch
				}
				startML = v1
				mlen = lastML
				for {
					if !(mlen >= startML) {
						break
					} /* scan downward */
					pos1 = cur + mlen
					price1 = basePrice + libc.Int32FromUint32(ZSTD_getMatchPrice(tls, offset, mlen, optStatePtr, optLevel))
					if pos1 > last_pos || price1 < (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Fprice {
						for last_pos < pos1 {
							/* fill empty positions, for future comparisons */
							last_pos = last_pos + 1
							(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos)*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
							(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos)*28))).Flitlen = libc.BoolUint32(!(libc.Int32FromInt32(0) != 0)) /* just needs to be != 0, to mean "not an end of match" */
						}
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Fmlen = mlen
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Foff = offset
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Flitlen = uint32(0)
						(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(pos1)*28))).Fprice = price1
					} else {
						if optLevel == 0 {
							break
						} /* early update abort; gets ~+10% speed for about -0.01 ratio loss */
					}
					goto _12
				_12:
					;
					mlen = mlen - 1
				}
				goto _10
			_10:
				;
				matchNb1 = matchNb1 + 1
			}
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos+uint32(1))*28))).Fprice = libc.Int32FromInt32(1) << libc.Int32FromInt32(30)
			goto _9
		_9:
			;
			cur = cur + 1
		} /* for (cur = 1; cur <= last_pos; cur++) */
		*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4)) = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(last_pos)*28))
		cur = last_pos - (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen
		goto _shortestPath
	_shortestPath:
		; /* cur, last_pos, best_mlen, best_off have to be set */
		if (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Fmlen == uint32(0) {
			/* no solution : all matches have been converted into literals */
			ip = ip + uintptr(last_pos)
			continue
		}
		/* Update offset history */
		if (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen == uint32(0) {
			/* finishing on a match : update offset history */
			*(*Repcodes_t)(unsafe.Pointer(bp + 120)) = ZSTD_newRep(tls, opt+uintptr(cur)*28+16, (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Foff, libc.BoolUint32((*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(cur)*28))).Flitlen == uint32(0)))
			libc.Xmemcpy(tls, rep, bp+120, libc.Uint64FromInt64(12))
		} else {
			libc.Xmemcpy(tls, rep, bp+4+16, libc.Uint64FromInt64(12))
			cur = cur - (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen
		}
		/* Let's write the shortest path solution.
		 * It is stored in @opt in reverse order,
		 * starting from @storeEnd (==cur+2),
		 * effectively partially @opt overwriting.
		 * Content is changed too:
		 * - So far, @opt stored stretches, aka a match followed by literals
		 * - Now, it will store sequences, aka literals followed by a match
		 */
		storeEnd = cur + uint32(2)
		storeStart = storeEnd
		stretchPos = cur
		_ = last_pos
		if (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen > uint32(0) {
			/* last "sequence" is unfinished: just a bunch of literals */
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeEnd)*28))).Flitlen = (*(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))).Flitlen
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeEnd)*28))).Fmlen = uint32(0)
			storeStart = storeEnd - uint32(1)
			*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeStart)*28)) = *(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4))
		}
		*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeEnd)*28)) = *(*ZSTD_optimal_t)(unsafe.Pointer(bp + 4)) /* note: litlen will be fixed */
		storeStart = storeEnd
		for int32(1) != 0 {
			nextStretch = *(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(stretchPos)*28))
			(*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeStart)*28))).Flitlen = nextStretch.Flitlen
			if nextStretch.Fmlen == uint32(0) {
				/* reaching beginning of segment */
				break
			}
			storeStart = storeStart - 1
			*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storeStart)*28)) = nextStretch /* note: litlen will be fixed */
			stretchPos = stretchPos - (nextStretch.Flitlen + nextStretch.Fmlen)
		}
		/* save sequences */
		storePos = storeStart
		for {
			if !(storePos <= storeEnd) {
				break
			}
			llen = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storePos)*28))).Flitlen
			mlen1 = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storePos)*28))).Fmlen
			offBase1 = (*(*ZSTD_optimal_t)(unsafe.Pointer(opt + uintptr(storePos)*28))).Foff
			advance = llen + mlen1
			if mlen1 == uint32(0) { /* only literals => must be last "sequence", actually starting a new stream of sequences */
				/* must be last sequence */
				ip = anchor + uintptr(llen) /* last "sequence" is a bunch of literals => don't progress anchor */
				goto _13                    /* will finish */
			}
			ZSTD_updateStats(tls, optStatePtr, llen, anchor, offBase1, mlen1)
			ZSTD_storeSeq(tls, seqStore, uint64(llen), anchor, iend, offBase1, uint64(mlen1))
			anchor = anchor + uintptr(advance)
			ip = anchor
			goto _13
		_13:
			;
			storePos = storePos + 1
		}
		/* update all costs */
		ZSTD_setBasePrices(tls, optStatePtr, optLevel)
	} /* while (ip < ilimit) */
	/* Return the last literals size */
	return libc.Uint64FromInt64(int64(iend) - int64(anchor))
}

func ZSTD_compressBlock_opt0(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, dictMode ZSTD_dictMode_e) (r size_t) {
	return ZSTD_compressBlock_opt_generic(tls, ms, seqStore, rep, src, srcSize, 0, dictMode)
}

func ZSTD_compressBlock_opt2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t, dictMode ZSTD_dictMode_e) (r size_t) {
	return ZSTD_compressBlock_opt_generic(tls, ms, seqStore, rep, src, srcSize, int32(2), dictMode)
}

func ZSTD_compressBlock_btopt(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt0(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_noDict))
}

// C documentation
//
//	/* ZSTD_initStats_ultra():
//	 * make a first compression pass, just to seed stats with more accurate starting values.
//	 * only works on first block, with no dictionary and no ldm.
//	 * this function cannot error out, its narrow contract must be respected.
//	 */
func ZSTD_initStats_ultra(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* tmpRep at bp+0 */ [3]U32 /* updated rep codes will sink here */
	libc.Xmemcpy(tls, bp, rep, libc.Uint64FromInt64(12))
	/* first block */
	/* no ldm */
	/* no dictionary */
	/* no prefix (note: intentional overflow, defined as 2-complement) */
	ZSTD_compressBlock_opt2(tls, ms, seqStore, bp, src, srcSize, int32(ZSTD_noDict)) /* generate stats into ms->opt*/
	/* invalidate first scan from history, only keep entropy stats */
	ZSTD_resetSeqStore(tls, seqStore)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase -= uintptr(srcSize)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit += uint32(srcSize)
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
	(*ZSTD_MatchState_t)(unsafe.Pointer(ms)).FnextToUpdate = (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit
}

func ZSTD_compressBlock_btultra(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_noDict))
}

func ZSTD_compressBlock_btultra2(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	var curr U32
	_ = curr
	curr = libc.Uint32FromInt64(int64(src) - int64((*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.Fbase))
	/* 2-passes strategy:
	 * this strategy makes a first pass over first block to collect statistics
	 * in order to seed next round's statistics with it.
	 * After 1st pass, function forgets history, and starts a new block.
	 * Consequently, this can only work if no data has been previously loaded in tables,
	 * aka, no dictionary, no prefix, no ldm preprocessing.
	 * The compression ratio gain is generally small (~0.5% on first block),
	 * the cost is 2x cpu time on first block. */
	if (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fopt.FlitLengthSum == uint32(0) && (*SeqStore_t)(unsafe.Pointer(seqStore)).Fsequences == (*SeqStore_t)(unsafe.Pointer(seqStore)).FsequencesStart && (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit == (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FlowLimit && curr == (*ZSTD_MatchState_t)(unsafe.Pointer(ms)).Fwindow.FdictLimit && srcSize > uint64(ZSTD_PREDEF_THRESHOLD) {
		ZSTD_initStats_ultra(tls, ms, seqStore, rep, src, srcSize)
	}
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_noDict))
}

func ZSTD_compressBlock_btopt_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt0(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_btopt_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt0(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_extDict))
}

func ZSTD_compressBlock_btultra_dictMatchState(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_dictMatchState))
}

func ZSTD_compressBlock_btultra_extDict(tls *libc.TLS, ms uintptr, seqStore uintptr, rep uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_compressBlock_opt2(tls, ms, seqStore, rep, src, srcSize, int32(ZSTD_extDict))
}

/* note : no btultra2 variant for extDict nor dictMatchState,
 * because btultra2 is not meant to work with dictionaries
 * and is only specific for the first block (no prefix) */
/**** ended inlining compress/zstd_opt.c ****/
/**** start inlining compress/zstdmt_compress.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ======   Compiler specifics   ====== */

/* ======   Dependencies   ====== */
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/pool.h ****/
/**** skipping file: ../common/threading.h ****/
/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_ldm.h ****/
/**** skipping file: zstdmt_compress.h ****/

/* Guards code to support resizing the SeqPool.
 * We will want to resize the SeqPool to save memory in the future.
 * Until then, comment the code out since it is unused.
 */

/* ======   Debug   ====== */

/* =====   Buffer Pool   ===== */
/* a single Buffer Pool can be invoked from multiple threads in parallel */

type Buffer = struct {
	Fstart    uintptr
	Fcapacity size_t
}

/* note : no btultra2 variant for extDict nor dictMatchState,
 * because btultra2 is not meant to work with dictionaries
 * and is only specific for the first block (no prefix) */
/**** ended inlining compress/zstd_opt.c ****/
/**** start inlining compress/zstdmt_compress.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ======   Compiler specifics   ====== */

/* ======   Dependencies   ====== */
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/pool.h ****/
/**** skipping file: ../common/threading.h ****/
/**** skipping file: zstd_compress_internal.h ****/
/**** skipping file: zstd_ldm.h ****/
/**** skipping file: zstdmt_compress.h ****/

/* Guards code to support resizing the SeqPool.
 * We will want to resize the SeqPool to save memory in the future.
 * Until then, comment the code out since it is unused.
 */

/* ======   Debug   ====== */

/* =====   Buffer Pool   ===== */
/* a single Buffer Pool can be invoked from multiple threads in parallel */

type buffer_s = Buffer

var g_nullBuffer = Buffer{}

type ZSTDMT_bufferPool = struct {
	FpoolMutex    pthread_mutex_t
	FbufferSize   size_t
	FtotalBuffers uint32
	FnbBuffers    uint32
	FcMem         ZSTD_customMem
	Fbuffers      uintptr
}

type ZSTDMT_bufferPool_s = ZSTDMT_bufferPool

func ZSTDMT_freeBufferPool(tls *libc.TLS, bufPool uintptr) {
	var u uint32
	_ = u
	if !(bufPool != 0) {
		return
	} /* compatibility with free on NULL */
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers != 0 {
		u = uint32(0)
		for {
			if !(u < (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers) {
				break
			}
			ZSTD_customFree(tls, (*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(u)*16))).Fstart, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
			goto _1
		_1:
			;
			u = u + 1
		}
		ZSTD_customFree(tls, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
	}
	libc.Xpthread_mutex_destroy(tls, bufPool)
	ZSTD_customFree(tls, bufPool, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
}

func ZSTDMT_createBufferPool(tls *libc.TLS, maxNbBuffers uint32, cMem ZSTD_customMem) (r uintptr) {
	var bufPool uintptr
	_ = bufPool
	bufPool = ZSTD_customCalloc(tls, uint64(88), cMem)
	if bufPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	if libc.Xpthread_mutex_init(tls, bufPool, libc.UintptrFromInt32(0)) != 0 {
		ZSTD_customFree(tls, bufPool, cMem)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers = ZSTD_customCalloc(tls, uint64(maxNbBuffers)*uint64(16), cMem)
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers == libc.UintptrFromInt32(0) {
		ZSTDMT_freeBufferPool(tls, bufPool)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FbufferSize = libc.Uint64FromInt32(libc.Int32FromInt32(64) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers = maxNbBuffers
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers = uint32(0)
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem = cMem
	return bufPool
}

// C documentation
//
//	/* only works at initialization, not during compression */
func ZSTDMT_sizeof_bufferPool(tls *libc.TLS, bufPool uintptr) (r size_t) {
	var arraySize, poolSize, totalBufferSize size_t
	var u uint32
	_, _, _, _ = arraySize, poolSize, totalBufferSize, u
	poolSize = uint64(88)
	arraySize = uint64((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers) * uint64(16)
	totalBufferSize = uint64(0)
	libc.Xpthread_mutex_lock(tls, bufPool)
	u = uint32(0)
	for {
		if !(u < (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers) {
			break
		}
		totalBufferSize = totalBufferSize + (*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(u)*16))).Fcapacity
		goto _1
	_1:
		;
		u = u + 1
	}
	libc.Xpthread_mutex_unlock(tls, bufPool)
	return poolSize + arraySize + totalBufferSize
}

// C documentation
//
//	/* ZSTDMT_setBufferSize() :
//	 * all future buffers provided by this buffer pool will have _at least_ this size
//	 * note : it's better for all buffers to have same size,
//	 * as they become freely interchangeable, reducing malloc/free usages and memory fragmentation */
func ZSTDMT_setBufferSize(tls *libc.TLS, bufPool uintptr, bSize size_t) {
	libc.Xpthread_mutex_lock(tls, bufPool)
	(*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FbufferSize = bSize
	libc.Xpthread_mutex_unlock(tls, bufPool)
}

func ZSTDMT_expandBufferPool(tls *libc.TLS, srcBufPool uintptr, maxNbBuffers uint32) (r uintptr) {
	var bSize size_t
	var cMem ZSTD_customMem
	var newBufPool uintptr
	_, _, _ = bSize, cMem, newBufPool
	if srcBufPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(srcBufPool)).FtotalBuffers >= maxNbBuffers { /* good enough */
		return srcBufPool
	}
	/* need a larger buffer pool */
	cMem = (*ZSTDMT_bufferPool)(unsafe.Pointer(srcBufPool)).FcMem
	bSize = (*ZSTDMT_bufferPool)(unsafe.Pointer(srcBufPool)).FbufferSize
	ZSTDMT_freeBufferPool(tls, srcBufPool)
	newBufPool = ZSTDMT_createBufferPool(tls, maxNbBuffers, cMem)
	if newBufPool == libc.UintptrFromInt32(0) {
		return newBufPool
	}
	ZSTDMT_setBufferSize(tls, newBufPool, bSize)
	return newBufPool
	return r
}

// C documentation
//
//	/** ZSTDMT_getBuffer() :
//	 *  assumption : bufPool must be valid
//	 * @return : a buffer, with start pointer and size
//	 *  note: allocation may fail, in this case, start==NULL and size==0 */
func ZSTDMT_getBuffer(tls *libc.TLS, bufPool uintptr) (r Buffer) {
	var availBufferSize, bSize size_t
	var buf, buffer Buffer
	var start, v2 uintptr
	var v1 uint32
	var v3 uint64
	_, _, _, _, _, _, _, _ = availBufferSize, bSize, buf, buffer, start, v1, v2, v3
	bSize = (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FbufferSize
	libc.Xpthread_mutex_lock(tls, bufPool)
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers != 0 {
		v2 = bufPool + 52
		*(*uint32)(unsafe.Pointer(v2)) = *(*uint32)(unsafe.Pointer(v2)) - 1
		v1 = *(*uint32)(unsafe.Pointer(v2)) /* try to use an existing buffer */
		buf = *(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(v1)*16))
		availBufferSize = buf.Fcapacity
		*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers)*16)) = g_nullBuffer
		if libc.BoolInt32(availBufferSize >= bSize)&libc.BoolInt32(availBufferSize>>libc.Int32FromInt32(3) <= bSize) != 0 {
			/* large enough, but not too much */
			libc.Xpthread_mutex_unlock(tls, bufPool)
			return buf
		}
		/* size conditions not respected : scratch this buffer, create new one */
		ZSTD_customFree(tls, buf.Fstart, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
	}
	libc.Xpthread_mutex_unlock(tls, bufPool)
	/* create new buffer */
	start = ZSTD_customMalloc(tls, bSize, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
	buffer.Fstart = start /* note : start can be NULL if malloc fails ! */
	if start == libc.UintptrFromInt32(0) {
		v3 = uint64(0)
	} else {
		v3 = bSize
	}
	buffer.Fcapacity = v3
	if start == libc.UintptrFromInt32(0) {
	} else {
	}
	return buffer
	return r
}

// C documentation
//
//	/* store buffer for later re-use, up to pool capacity */
func ZSTDMT_releaseBuffer(tls *libc.TLS, bufPool uintptr, buf Buffer) {
	var v1 uint32
	var v2 uintptr
	_, _ = v1, v2
	if buf.Fstart == libc.UintptrFromInt32(0) {
		return
	} /* compatible with release on NULL */
	libc.Xpthread_mutex_lock(tls, bufPool)
	if (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FnbBuffers < (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FtotalBuffers {
		v2 = bufPool + 52
		v1 = *(*uint32)(unsafe.Pointer(v2))
		*(*uint32)(unsafe.Pointer(v2)) = *(*uint32)(unsafe.Pointer(v2)) + 1
		*(*Buffer)(unsafe.Pointer((*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).Fbuffers + uintptr(v1)*16)) = buf /* stored for later use */
		libc.Xpthread_mutex_unlock(tls, bufPool)
		return
	}
	libc.Xpthread_mutex_unlock(tls, bufPool)
	/* Reached bufferPool capacity (note: should not happen) */
	ZSTD_customFree(tls, buf.Fstart, (*ZSTDMT_bufferPool)(unsafe.Pointer(bufPool)).FcMem)
}

/* We need 2 output buffers per worker since each dstBuff must be flushed after it is released.
 * The 3 additional buffers are as follows:
 *   1 buffer for input loading
 *   1 buffer for "next input" when submitting current one
 *   1 buffer stuck in queue */

/* After a worker releases its rawSeqStore, it is immediately ready for reuse.
 * So we only need one seq buffer per worker. */

/* =====   Seq Pool Wrapper   ====== */

type ZSTDMT_seqPool = struct {
	FpoolMutex    pthread_mutex_t
	FbufferSize   size_t
	FtotalBuffers uint32
	FnbBuffers    uint32
	FcMem         ZSTD_customMem
	Fbuffers      uintptr
}

func ZSTDMT_sizeof_seqPool(tls *libc.TLS, seqPool uintptr) (r size_t) {
	return ZSTDMT_sizeof_bufferPool(tls, seqPool)
}

func bufferToSeq(tls *libc.TLS, buffer Buffer) (r RawSeqStore_t) {
	var seq RawSeqStore_t
	_ = seq
	seq = kNullRawSeqStore
	seq.Fseq = buffer.Fstart
	seq.Fcapacity = buffer.Fcapacity / uint64(12)
	return seq
}

func seqToBuffer(tls *libc.TLS, seq RawSeqStore_t) (r Buffer) {
	var buffer Buffer
	_ = buffer
	buffer.Fstart = seq.Fseq
	buffer.Fcapacity = seq.Fcapacity * uint64(12)
	return buffer
}

func ZSTDMT_getSeq(tls *libc.TLS, seqPool uintptr) (r RawSeqStore_t) {
	if (*ZSTDMT_seqPool)(unsafe.Pointer(seqPool)).FbufferSize == uint64(0) {
		return kNullRawSeqStore
	}
	return bufferToSeq(tls, ZSTDMT_getBuffer(tls, seqPool))
}

func ZSTDMT_releaseSeq(tls *libc.TLS, seqPool uintptr, seq RawSeqStore_t) {
	ZSTDMT_releaseBuffer(tls, seqPool, seqToBuffer(tls, seq))
}

func ZSTDMT_setNbSeq(tls *libc.TLS, seqPool uintptr, nbSeq size_t) {
	ZSTDMT_setBufferSize(tls, seqPool, nbSeq*uint64(12))
}

func ZSTDMT_createSeqPool(tls *libc.TLS, nbWorkers uint32, cMem ZSTD_customMem) (r uintptr) {
	var seqPool uintptr
	_ = seqPool
	seqPool = ZSTDMT_createBufferPool(tls, nbWorkers, cMem)
	if seqPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTDMT_setNbSeq(tls, seqPool, uint64(0))
	return seqPool
}

func ZSTDMT_freeSeqPool(tls *libc.TLS, seqPool uintptr) {
	ZSTDMT_freeBufferPool(tls, seqPool)
}

func ZSTDMT_expandSeqPool(tls *libc.TLS, pool uintptr, nbWorkers U32) (r uintptr) {
	return ZSTDMT_expandBufferPool(tls, pool, nbWorkers)
}

/* =====   CCtx Pool   ===== */
/* a single CCtx Pool can be invoked from multiple threads in parallel */

type ZSTDMT_CCtxPool = struct {
	FpoolMutex pthread_mutex_t
	FtotalCCtx int32
	FavailCCtx int32
	FcMem      ZSTD_customMem
	Fcctxs     uintptr
}

// C documentation
//
//	/* note : all CCtx borrowed from the pool must be reverted back to the pool _before_ freeing the pool */
func ZSTDMT_freeCCtxPool(tls *libc.TLS, pool uintptr) {
	var cid int32
	_ = cid
	if !(pool != 0) {
		return
	}
	libc.Xpthread_mutex_destroy(tls, pool)
	if (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs != 0 {
		cid = 0
		for {
			if !(cid < (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FtotalCCtx) {
				break
			}
			ZSTD_freeCCtx(tls, *(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs + uintptr(cid)*8)))
			goto _1
		_1:
			;
			cid = cid + 1
		} /* free compatible with NULL */
		ZSTD_customFree(tls, (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs, (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FcMem)
	}
	ZSTD_customFree(tls, pool, (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FcMem)
}

// C documentation
//
//	/* ZSTDMT_createCCtxPool() :
//	 * implies nbWorkers >= 1 , checked by caller ZSTDMT_createCCtx() */
func ZSTDMT_createCCtxPool(tls *libc.TLS, nbWorkers int32, cMem ZSTD_customMem) (r uintptr) {
	var cctxPool uintptr
	_ = cctxPool
	cctxPool = ZSTD_customCalloc(tls, uint64(80), cMem)
	if !(cctxPool != 0) {
		return libc.UintptrFromInt32(0)
	}
	if libc.Xpthread_mutex_init(tls, cctxPool, libc.UintptrFromInt32(0)) != 0 {
		ZSTD_customFree(tls, cctxPool, cMem)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FtotalCCtx = nbWorkers
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs = ZSTD_customCalloc(tls, libc.Uint64FromInt32(nbWorkers)*uint64(8), cMem)
	if !((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs != 0) {
		ZSTDMT_freeCCtxPool(tls, cctxPool)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FcMem = cMem
	*(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs)) = ZSTD_createCCtx_advanced(tls, cMem)
	if !(*(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs)) != 0) {
		ZSTDMT_freeCCtxPool(tls, cctxPool)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx = int32(1) /* at least one cctx for single-thread mode */
	return cctxPool
}

func ZSTDMT_expandCCtxPool(tls *libc.TLS, srcPool uintptr, nbWorkers int32) (r uintptr) {
	var cMem ZSTD_customMem
	_ = cMem
	if srcPool == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	if nbWorkers <= (*ZSTDMT_CCtxPool)(unsafe.Pointer(srcPool)).FtotalCCtx {
		return srcPool
	} /* good enough */
	/* need a larger cctx pool */
	cMem = (*ZSTDMT_CCtxPool)(unsafe.Pointer(srcPool)).FcMem
	ZSTDMT_freeCCtxPool(tls, srcPool)
	return ZSTDMT_createCCtxPool(tls, nbWorkers, cMem)
	return r
}

// C documentation
//
//	/* only works during initialization phase, not during compression */
func ZSTDMT_sizeof_CCtxPool(tls *libc.TLS, cctxPool uintptr) (r size_t) {
	var arraySize, poolSize, totalCCtxSize size_t
	var nbWorkers, u uint32
	_, _, _, _, _ = arraySize, nbWorkers, poolSize, totalCCtxSize, u
	libc.Xpthread_mutex_lock(tls, cctxPool)
	nbWorkers = libc.Uint32FromInt32((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FtotalCCtx)
	poolSize = uint64(80)
	arraySize = libc.Uint64FromInt32((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FtotalCCtx) * uint64(8)
	totalCCtxSize = uint64(0)
	u = uint32(0)
	for {
		if !(u < nbWorkers) {
			break
		}
		totalCCtxSize = totalCCtxSize + ZSTD_sizeof_CCtx(tls, *(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs + uintptr(u)*8)))
		goto _1
	_1:
		;
		u = u + 1
	}
	libc.Xpthread_mutex_unlock(tls, cctxPool)
	return poolSize + arraySize + totalCCtxSize
	return r
}

func ZSTDMT_getCCtx(tls *libc.TLS, cctxPool uintptr) (r uintptr) {
	var cctx uintptr
	_ = cctx
	libc.Xpthread_mutex_lock(tls, cctxPool)
	if (*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx != 0 {
		(*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx = (*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx - 1
		cctx = *(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).Fcctxs + uintptr((*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FavailCCtx)*8))
		libc.Xpthread_mutex_unlock(tls, cctxPool)
		return cctx
	}
	libc.Xpthread_mutex_unlock(tls, cctxPool)
	return ZSTD_createCCtx_advanced(tls, (*ZSTDMT_CCtxPool)(unsafe.Pointer(cctxPool)).FcMem) /* note : can be NULL, when creation fails ! */
}

func ZSTDMT_releaseCCtx(tls *libc.TLS, pool uintptr, cctx uintptr) {
	var v1 int32
	var v2 uintptr
	_, _ = v1, v2
	if cctx == libc.UintptrFromInt32(0) {
		return
	} /* compatibility with release on NULL */
	libc.Xpthread_mutex_lock(tls, pool)
	if (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FavailCCtx < (*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).FtotalCCtx {
		v2 = pool + 44
		v1 = *(*int32)(unsafe.Pointer(v2))
		*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(v2)) + 1
		*(*uintptr)(unsafe.Pointer((*ZSTDMT_CCtxPool)(unsafe.Pointer(pool)).Fcctxs + uintptr(v1)*8)) = cctx
	} else {
		/* pool overflow : should not happen, since totalCCtx==nbWorkers */
		ZSTD_freeCCtx(tls, cctx)
	}
	libc.Xpthread_mutex_unlock(tls, pool)
}

/* ====   Serial State   ==== */

type Range = struct {
	Fstart uintptr
	Fsize  size_t
}

type SerialState = struct {
	Fmutex          pthread_mutex_t
	Fcond           pthread_cond_t
	Fparams         ZSTD_CCtx_params
	FldmState       ldmState_t
	FxxhState       XXH_NAMESPACEXXH64_state_t
	FnextJobID      uint32
	FldmWindowMutex pthread_mutex_t
	FldmWindowCond  pthread_cond_t
	FldmWindow      ZSTD_window_t
}

func ZSTDMT_serialState_reset(tls *libc.TLS, serialState uintptr, seqPool uintptr, _params ZSTD_CCtx_params, jobSize size_t, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e) (r int32) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var bucketLog, hashLog, prevBucketLog, v1 uint32
	var cMem ZSTD_customMem
	var dictEnd uintptr
	var hashSize, numBuckets size_t
	_, _, _, _, _, _, _, _ = bucketLog, cMem, dictEnd, hashLog, hashSize, numBuckets, prevBucketLog, v1
	/* Adjust parameters */
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		ZSTD_ldm_adjustParameters(tls, bp+96, bp+4)
	} else {
		libc.Xmemset(tls, bp+96, 0, libc.Uint64FromInt64(24))
	}
	(*SerialState)(unsafe.Pointer(serialState)).FnextJobID = uint32(0)
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams.FchecksumFlag != 0 {
		XXH_INLINE_XXH64_reset(tls, serialState+2424, uint64(0))
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		cMem = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcustomMem
		hashLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FhashLog
		hashSize = libc.Uint64FromInt32(1) << hashLog * uint64(8)
		bucketLog = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FhashLog - (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FbucketSizeLog
		prevBucketLog = (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FhashLog - (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FbucketSizeLog
		numBuckets = libc.Uint64FromInt32(1) << bucketLog
		/* Size the seq pool tables */
		ZSTDMT_setNbSeq(tls, seqPool, ZSTD_ldm_getMaxNbSeq(tls, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams, jobSize))
		/* Reset the window */
		ZSTD_window_init(tls, serialState+312)
		/* Resize tables and output space if necessary. */
		if (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable == libc.UintptrFromInt32(0) || (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FhashLog < hashLog {
			ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable, cMem)
			(*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable = ZSTD_customMalloc(tls, hashSize, cMem)
		}
		if (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets == libc.UintptrFromInt32(0) || prevBucketLog < bucketLog {
			ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets, cMem)
			(*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets = ZSTD_customMalloc(tls, numBuckets, cMem)
		}
		if !((*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable != 0) || !((*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets != 0) {
			return int32(1)
		}
		/* Zero the tables */
		libc.Xmemset(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable, 0, hashSize)
		libc.Xmemset(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets, 0, numBuckets)
		/* Update window state and fill hash table with dict */
		(*SerialState)(unsafe.Pointer(serialState)).FldmState.FloadedDictEnd = uint32(0)
		if dictSize > uint64(0) {
			if dictContentType == int32(ZSTD_dct_rawContent) {
				dictEnd = dict + uintptr(dictSize)
				ZSTD_window_update(tls, serialState+312, dict, dictSize, 0)
				ZSTD_ldm_fillHashTable(tls, serialState+312, dict, dictEnd, bp+96)
				if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FforceWindow != 0 {
					v1 = uint32(0)
				} else {
					v1 = libc.Uint32FromInt64(int64(dictEnd) - int64((*SerialState)(unsafe.Pointer(serialState)).FldmState.Fwindow.Fbase))
				}
				(*SerialState)(unsafe.Pointer(serialState)).FldmState.FloadedDictEnd = v1
			} else {
				/* don't even load anything */
			}
		}
		/* Initialize serialState's copy of ldmWindow. */
		(*SerialState)(unsafe.Pointer(serialState)).FldmWindow = (*SerialState)(unsafe.Pointer(serialState)).FldmState.Fwindow
	}
	(*SerialState)(unsafe.Pointer(serialState)).Fparams = *(*ZSTD_CCtx_params)(unsafe.Pointer(bp))
	(*SerialState)(unsafe.Pointer(serialState)).Fparams.FjobSize = uint64(uint32(jobSize))
	return 0
}

func ZSTDMT_serialState_init(tls *libc.TLS, serialState uintptr) (r int32) {
	var initError int32
	_ = initError
	initError = 0
	libc.Xmemset(tls, serialState, 0, libc.Uint64FromInt64(2648))
	initError = initError | libc.Xpthread_mutex_init(tls, serialState, libc.UintptrFromInt32(0))
	initError = initError | libc.Xpthread_cond_init(tls, serialState+40, libc.UintptrFromInt32(0))
	initError = initError | libc.Xpthread_mutex_init(tls, serialState+2520, libc.UintptrFromInt32(0))
	initError = initError | libc.Xpthread_cond_init(tls, serialState+2560, libc.UintptrFromInt32(0))
	return initError
}

func ZSTDMT_serialState_free(tls *libc.TLS, serialState uintptr) {
	var cMem ZSTD_customMem
	_ = cMem
	cMem = (*SerialState)(unsafe.Pointer(serialState)).Fparams.FcustomMem
	libc.Xpthread_mutex_destroy(tls, serialState)
	libc.Xpthread_cond_destroy(tls, serialState+40)
	libc.Xpthread_mutex_destroy(tls, serialState+2520)
	libc.Xpthread_cond_destroy(tls, serialState+2560)
	ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FhashTable, cMem)
	ZSTD_customFree(tls, (*SerialState)(unsafe.Pointer(serialState)).FldmState.FbucketOffsets, cMem)
}

func ZSTDMT_serialState_genSequences(tls *libc.TLS, serialState uintptr, seqStore uintptr, src Range, jobID uint32) {
	var error1 size_t
	_ = error1
	/* Wait for our turn */
	libc.Xpthread_mutex_lock(tls, serialState)
	for (*SerialState)(unsafe.Pointer(serialState)).FnextJobID < jobID {
		libc.Xpthread_cond_wait(tls, serialState+40, serialState)
	}
	/* A future job may error and skip our job */
	if (*SerialState)(unsafe.Pointer(serialState)).FnextJobID == jobID {
		/* It is now our turn, do any processing necessary */
		if (*SerialState)(unsafe.Pointer(serialState)).Fparams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
			ZSTD_window_update(tls, serialState+312, src.Fstart, src.Fsize, 0)
			error1 = ZSTD_ldm_generateSequences(tls, serialState+312, seqStore, serialState+88+96, src.Fstart, src.Fsize)
			/* We provide a large enough buffer to never fail. */
			_ = error1
			/* Update ldmWindow to match the ldmState.window and signal the main
			 * thread if it is waiting for a buffer.
			 */
			libc.Xpthread_mutex_lock(tls, serialState+2520)
			(*SerialState)(unsafe.Pointer(serialState)).FldmWindow = (*SerialState)(unsafe.Pointer(serialState)).FldmState.Fwindow
			libc.Xpthread_cond_signal(tls, serialState+2560)
			libc.Xpthread_mutex_unlock(tls, serialState+2520)
		}
		if (*SerialState)(unsafe.Pointer(serialState)).Fparams.FfParams.FchecksumFlag != 0 && src.Fsize > uint64(0) {
			XXH_INLINE_XXH64_update(tls, serialState+2424, src.Fstart, src.Fsize)
		}
	}
	/* Now it is the next jobs turn */
	(*SerialState)(unsafe.Pointer(serialState)).FnextJobID = (*SerialState)(unsafe.Pointer(serialState)).FnextJobID + 1
	libc.Xpthread_cond_broadcast(tls, serialState+40)
	libc.Xpthread_mutex_unlock(tls, serialState)
}

func ZSTDMT_serialState_applySequences(tls *libc.TLS, serialState uintptr, jobCCtx uintptr, seqStore uintptr) {
	if (*RawSeqStore_t)(unsafe.Pointer(seqStore)).Fsize > uint64(0) {
		_ = serialState
		ZSTD_referenceExternalSequences(tls, jobCCtx, (*RawSeqStore_t)(unsafe.Pointer(seqStore)).Fseq, (*RawSeqStore_t)(unsafe.Pointer(seqStore)).Fsize)
	}
}

func ZSTDMT_serialState_ensureFinished(tls *libc.TLS, serialState uintptr, jobID uint32, cSize size_t) {
	libc.Xpthread_mutex_lock(tls, serialState)
	if (*SerialState)(unsafe.Pointer(serialState)).FnextJobID <= jobID {
		_ = cSize
		(*SerialState)(unsafe.Pointer(serialState)).FnextJobID = jobID + uint32(1)
		libc.Xpthread_cond_broadcast(tls, serialState+40)
		libc.Xpthread_mutex_lock(tls, serialState+2520)
		ZSTD_window_clear(tls, serialState+2608)
		libc.Xpthread_cond_signal(tls, serialState+2560)
		libc.Xpthread_mutex_unlock(tls, serialState+2520)
	}
	libc.Xpthread_mutex_unlock(tls, serialState)
}

/* ------------------------------------------ */
/* =====          Worker thread         ===== */
/* ------------------------------------------ */

var kNullRange = Range{}

type ZSTDMT_jobDescription = struct {
	Fconsumed            size_t
	FcSize               size_t
	Fjob_mutex           pthread_mutex_t
	Fjob_cond            pthread_cond_t
	FcctxPool            uintptr
	FbufPool             uintptr
	FseqPool             uintptr
	Fserial              uintptr
	FdstBuff             Buffer
	Fprefix              Range
	Fsrc                 Range
	FjobID               uint32
	FfirstJob            uint32
	FlastJob             uint32
	Fparams              ZSTD_CCtx_params
	Fcdict               uintptr
	FfullFrameSize       uint64
	FdstFlushed          size_t
	FframeChecksumNeeded uint32
}

// C documentation
//
//	/* ZSTDMT_compressionJob() is a POOL_function type */
func ZSTDMT_compressionJob(tls *libc.TLS, jobDescription uintptr) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var cSize, cSize1, chunkSize, err, forceWindowError, hSize, initError, initError1, lastBlockSize, lastBlockSize1, lastCBlockSize size_t
	var cctx, ip, job, oend, op, ostart uintptr
	var chunkNb, nbChunks int32
	var dstBuff Buffer
	var pledgedSrcSize U64
	var v1, v3 uint64
	var _ /* jobParams at bp+0 */ ZSTD_CCtx_params
	var _ /* rawSeqStore at bp+224 */ RawSeqStore_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cSize, cSize1, cctx, chunkNb, chunkSize, dstBuff, err, forceWindowError, hSize, initError, initError1, ip, job, lastBlockSize, lastBlockSize1, lastCBlockSize, nbChunks, oend, op, ostart, pledgedSrcSize, v1, v3
	job = jobDescription
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fparams /* do not modify job->params ! copy it, modify the copy */
	cctx = ZSTDMT_getCCtx(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcctxPool)
	*(*RawSeqStore_t)(unsafe.Pointer(bp + 224)) = ZSTDMT_getSeq(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FseqPool)
	dstBuff = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff
	lastCBlockSize = uint64(0)
	/* resources */
	if cctx == libc.UintptrFromInt32(0) {
		libc.Xpthread_mutex_lock(tls, job+16)
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		libc.Xpthread_mutex_unlock(tls, job+16)
		goto _endJob
	}
	if dstBuff.Fstart == libc.UintptrFromInt32(0) { /* streaming job : doesn't provide a dstBuffer */
		dstBuff = ZSTDMT_getBuffer(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FbufPool)
		if dstBuff.Fstart == libc.UintptrFromInt32(0) {
			libc.Xpthread_mutex_lock(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
			libc.Xpthread_mutex_unlock(tls, job+16)
			goto _endJob
		}
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff = dstBuff /* this value can be read in ZSTDMT_flush, when it copies the whole job */
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm == int32(ZSTD_ps_enable) && (*(*RawSeqStore_t)(unsafe.Pointer(bp + 224))).Fseq == libc.UintptrFromInt32(0) {
		libc.Xpthread_mutex_lock(tls, job+16)
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		libc.Xpthread_mutex_unlock(tls, job+16)
		goto _endJob
	}
	/* Don't compute the checksum for chunks, since we compute it externally,
	 * but write it in the header.
	 */
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FjobID != uint32(0) {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FfParams.FchecksumFlag = 0
	}
	/* Don't run LDM for the chunks, since we handle it externally */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FldmParams.FenableLdm = int32(ZSTD_ps_disable)
	/* Correct nbWorkers to 0. */
	(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers = 0
	/* init */
	/* Perform serial step as early as possible */
	ZSTDMT_serialState_genSequences(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fserial, bp+224, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FjobID)
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fcdict != 0 {
		initError = ZSTD_compressBegin_advanced_internal(tls, cctx, libc.UintptrFromInt32(0), uint64(0), int32(ZSTD_dct_auto), int32(ZSTD_dtlm_fast), (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fcdict, bp, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfullFrameSize)
		/* only allowed for first job */
		if ZSTD_isError(tls, initError) != 0 {
			libc.Xpthread_mutex_lock(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = initError
			libc.Xpthread_mutex_unlock(tls, job+16)
			goto _endJob
		}
	} else {
		if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0 {
			v1 = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfullFrameSize
		} else {
			v1 = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize
		}
		pledgedSrcSize = v1
		forceWindowError = ZSTD_CCtxParams_setParameter(tls, bp, int32(ZSTD_c_experimentalParam3), libc.BoolInt32(!((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0)))
		if ZSTD_isError(tls, forceWindowError) != 0 {
			libc.Xpthread_mutex_lock(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = forceWindowError
			libc.Xpthread_mutex_unlock(tls, job+16)
			goto _endJob
		}
		if !((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0) {
			err = ZSTD_CCtxParams_setParameter(tls, bp, int32(ZSTD_c_experimentalParam15), 0)
			if ZSTD_isError(tls, err) != 0 {
				libc.Xpthread_mutex_lock(tls, job+16)
				(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = err
				libc.Xpthread_mutex_unlock(tls, job+16)
				goto _endJob
			}
		}
		initError1 = ZSTD_compressBegin_advanced_internal(tls, cctx, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fprefix.Fstart, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fprefix.Fsize, int32(ZSTD_dct_rawContent), int32(ZSTD_dtlm_fast), libc.UintptrFromInt32(0), bp, pledgedSrcSize)
		if ZSTD_isError(tls, initError1) != 0 {
			libc.Xpthread_mutex_lock(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = initError1
			libc.Xpthread_mutex_unlock(tls, job+16)
			goto _endJob
		}
	}
	/* External Sequences can only be applied after CCtx initialization */
	ZSTDMT_serialState_applySequences(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fserial, cctx, bp+224)
	if !((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0) { /* flush and overwrite frame header when it's not first job */
		hSize = ZSTD_compressContinue_public(tls, cctx, dstBuff.Fstart, dstBuff.Fcapacity, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fstart, uint64(0))
		if ZSTD_isError(tls, hSize) != 0 {
			libc.Xpthread_mutex_lock(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = hSize
			libc.Xpthread_mutex_unlock(tls, job+16)
			goto _endJob
		}
		ZSTD_invalidateRepCodes(tls, cctx)
	}
	/* compress the entire job by smaller chunks, for better granularity */
	chunkSize = libc.Uint64FromInt32(libc.Int32FromInt32(4) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)))
	nbChunks = libc.Int32FromUint64(((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize + (chunkSize - libc.Uint64FromInt32(1))) / chunkSize)
	ip = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fstart
	ostart = dstBuff.Fstart
	op = ostart
	oend = op + uintptr(dstBuff.Fcapacity)
	if uint64(8) > uint64(4) {
	} /* check overflow */
	chunkNb = int32(1)
	for {
		if !(chunkNb < nbChunks) {
			break
		}
		cSize = ZSTD_compressContinue_public(tls, cctx, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, chunkSize)
		if ZSTD_isError(tls, cSize) != 0 {
			libc.Xpthread_mutex_lock(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = cSize
			libc.Xpthread_mutex_unlock(tls, job+16)
			goto _endJob
		}
		ip = ip + uintptr(chunkSize)
		op = op + uintptr(cSize)
		/* stats */
		libc.Xpthread_mutex_lock(tls, job+16)
		*(*size_t)(unsafe.Pointer(job + 8)) += cSize
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fconsumed = chunkSize * libc.Uint64FromInt32(chunkNb)
		libc.Xpthread_cond_signal(tls, job+56) /* warns some more data is ready to be flushed */
		libc.Xpthread_mutex_unlock(tls, job+16)
		goto _2
	_2:
		;
		chunkNb = chunkNb + 1
	}
	/* last block */
	/* chunkSize must be power of 2 for mask==(chunkSize-1) to work */
	if libc.BoolUint32(nbChunks > libc.Int32FromInt32(0))|(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FlastJob != 0 {
		lastBlockSize1 = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize & (chunkSize - uint64(1))
		if libc.BoolInt32(lastBlockSize1 == uint64(0))&libc.BoolInt32((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize >= chunkSize) != 0 {
			v1 = chunkSize
		} else {
			v1 = lastBlockSize1
		}
		lastBlockSize = v1
		if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FlastJob != 0 {
			v3 = ZSTD_compressEnd_public(tls, cctx, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, lastBlockSize)
		} else {
			v3 = ZSTD_compressContinue_public(tls, cctx, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, lastBlockSize)
		}
		cSize1 = v3
		if ZSTD_isError(tls, cSize1) != 0 {
			libc.Xpthread_mutex_lock(tls, job+16)
			(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = cSize1
			libc.Xpthread_mutex_unlock(tls, job+16)
			goto _endJob
		}
		lastCBlockSize = cSize1
	}
	if !((*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FfirstJob != 0) {
		/* Double check that we don't have an ext-dict, because then our
		 * repcode invalidation doesn't work.
		 */
	}
	ZSTD_CCtx_trace(tls, cctx, uint64(0))
	goto _endJob
_endJob:
	;
	ZSTDMT_serialState_ensureFinished(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fserial, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FjobID, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize)
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fprefix.Fsize > uint64(0) {
	}
	/* release resources */
	ZSTDMT_releaseSeq(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FseqPool, *(*RawSeqStore_t)(unsafe.Pointer(bp + 224)))
	ZSTDMT_releaseCCtx(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcctxPool, cctx)
	/* report */
	libc.Xpthread_mutex_lock(tls, job+16)
	if ZSTD_isError(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize) != 0 {
	}
	*(*size_t)(unsafe.Pointer(job + 8)) += lastCBlockSize
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fconsumed = (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc.Fsize /* when job->consumed == job->src.size , compression job is presumed completed */
	libc.Xpthread_cond_signal(tls, job+56)
	libc.Xpthread_mutex_unlock(tls, job+16)
}

/* ------------------------------------------ */
/* =====   Multi-threaded compression   ===== */
/* ------------------------------------------ */

type InBuff_t = struct {
	Fprefix Range
	Fbuffer Buffer
	Ffilled size_t
}

type RoundBuff_t = struct {
	Fbuffer   uintptr
	Fcapacity size_t
	Fpos      size_t
}

var kNullRoundBuff = RoundBuff_t{}

/* Don't create chunks smaller than the zstd block size.
 * This stops us from regressing compression ratio too much,
 * and ensures our output fits in ZSTD_compressBound().
 *
 * If this is shrunk < ZSTD_BLOCKSIZELOG_MIN then
 * ZSTD_COMPRESSBOUND() will need to be updated.
 */

type RSyncState_t = struct {
	Fhash       U64
	FhitMask    U64
	FprimePower U64
}

func ZSTDMT_freeJobsTable(tls *libc.TLS, jobTable uintptr, nbJobs U32, cMem ZSTD_customMem) {
	var jobNb U32
	_ = jobNb
	if jobTable == libc.UintptrFromInt32(0) {
		return
	}
	jobNb = uint32(0)
	for {
		if !(jobNb < nbJobs) {
			break
		}
		libc.Xpthread_mutex_destroy(tls, jobTable+uintptr(jobNb)*456+16)
		libc.Xpthread_cond_destroy(tls, jobTable+uintptr(jobNb)*456+56)
		goto _1
	_1:
		;
		jobNb = jobNb + 1
	}
	ZSTD_customFree(tls, jobTable, cMem)
}

// C documentation
//
//	/* ZSTDMT_allocJobsTable()
//	 * allocate and init a job table.
//	 * update *nbJobsPtr to next power of 2 value, as size of table */
func ZSTDMT_createJobsTable(tls *libc.TLS, nbJobsPtr uintptr, cMem ZSTD_customMem) (r uintptr) {
	var initError int32
	var jobNb, nbJobs, nbJobsLog2 U32
	var jobTable uintptr
	_, _, _, _, _ = initError, jobNb, jobTable, nbJobs, nbJobsLog2
	nbJobsLog2 = ZSTD_highbit32(tls, *(*U32)(unsafe.Pointer(nbJobsPtr))) + uint32(1)
	nbJobs = libc.Uint32FromInt32(int32(1) << nbJobsLog2)
	jobTable = ZSTD_customCalloc(tls, uint64(nbJobs)*uint64(456), cMem)
	initError = 0
	if jobTable == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	*(*U32)(unsafe.Pointer(nbJobsPtr)) = nbJobs
	jobNb = uint32(0)
	for {
		if !(jobNb < nbJobs) {
			break
		}
		initError = initError | libc.Xpthread_mutex_init(tls, jobTable+uintptr(jobNb)*456+16, libc.UintptrFromInt32(0))
		initError = initError | libc.Xpthread_cond_init(tls, jobTable+uintptr(jobNb)*456+56, libc.UintptrFromInt32(0))
		goto _1
	_1:
		;
		jobNb = jobNb + 1
	}
	if initError != 0 {
		ZSTDMT_freeJobsTable(tls, jobTable, nbJobs, cMem)
		return libc.UintptrFromInt32(0)
	}
	return jobTable
}

func ZSTDMT_expandJobsTable(tls *libc.TLS, mtctx uintptr, nbWorkers U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* nbJobs at bp+0 */ U32
	*(*U32)(unsafe.Pointer(bp)) = nbWorkers + uint32(2)
	if *(*U32)(unsafe.Pointer(bp)) > (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+uint32(1) { /* need more job capacity */
		ZSTDMT_freeJobsTable(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+uint32(1), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask = uint32(0)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs = ZSTDMT_createJobsTable(tls, bp, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs == libc.UintptrFromInt32(0) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		/* ensure nbJobs is a power of 2 */
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask = *(*U32)(unsafe.Pointer(bp)) - uint32(1)
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTDMT_CCtxParam_setNbWorkers():
//	 * Internal use only */
func ZSTDMT_CCtxParam_setNbWorkers(tls *libc.TLS, params uintptr, nbWorkers uint32) (r size_t) {
	return ZSTD_CCtxParams_setParameter(tls, params, int32(ZSTD_c_nbWorkers), libc.Int32FromUint32(nbWorkers))
}

func ZSTDMT_createCCtx_advanced_internal(tls *libc.TLS, nbWorkers uint32, cMem ZSTD_customMem, pool uintptr) (r uintptr) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var initError int32
	var mtctx uintptr
	var v1 uint32
	var _ /* nbJobs at bp+0 */ U32
	_, _, _ = initError, mtctx, v1
	*(*U32)(unsafe.Pointer(bp)) = nbWorkers + uint32(2)
	if nbWorkers < uint32(1) {
		return libc.UintptrFromInt32(0)
	}
	if nbWorkers < libc.Uint32FromInt32(libc.Int32FromInt32(256)) {
		v1 = nbWorkers
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(256))
	}
	nbWorkers = v1
	if libc.BoolInt32(cMem.FcustomAlloc != libc.UintptrFromInt32(0))^libc.BoolInt32(cMem.FcustomFree != libc.UintptrFromInt32(0)) != 0 {
		/* invalid custom allocator */
		return libc.UintptrFromInt32(0)
	}
	mtctx = ZSTD_customCalloc(tls, uint64(3120), cMem)
	if !(mtctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	ZSTDMT_CCtxParam_setNbWorkers(tls, mtctx+40, nbWorkers)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem = cMem
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(1)
	if pool != libc.UintptrFromInt32(0) {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory = pool
		libc.SetBitFieldPtr8Uint32(mtctx+3112, libc.Uint32FromInt32(1), 0, 0x1)
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory = POOL_create_advanced(tls, uint64(nbWorkers), uint64(0), cMem)
		libc.SetBitFieldPtr8Uint32(mtctx+3112, libc.Uint32FromInt32(0), 0, 0x1)
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs = ZSTDMT_createJobsTable(tls, bp, cMem)
	/* ensure nbJobs is a power of 2 */
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask = *(*U32)(unsafe.Pointer(bp)) - uint32(1)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool = ZSTDMT_createBufferPool(tls, uint32(2)*nbWorkers+uint32(3), cMem)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool = ZSTDMT_createCCtxPool(tls, libc.Int32FromUint32(nbWorkers), cMem)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool = ZSTDMT_createSeqPool(tls, nbWorkers, cMem)
	initError = ZSTDMT_serialState_init(tls, mtctx+352)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff = kNullRoundBuff
	if libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool != 0))|libc.BoolInt32(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool != 0))|initError != 0 {
		ZSTDMT_freeCCtx(tls, mtctx)
		return libc.UintptrFromInt32(0)
	}
	return mtctx
}

func ZSTDMT_createCCtx_advanced(tls *libc.TLS, nbWorkers uint32, cMem ZSTD_customMem, pool uintptr) (r uintptr) {
	return ZSTDMT_createCCtx_advanced_internal(tls, nbWorkers, cMem, pool)
}

// C documentation
//
//	/* ZSTDMT_releaseAllJobResources() :
//	 * note : ensure all workers are killed first ! */
func ZSTDMT_releaseAllJobResources(tls *libc.TLS, mtctx uintptr) {
	var cond pthread_cond_t
	var jobID uint32
	var mutex pthread_mutex_t
	_, _, _ = cond, jobID, mutex
	jobID = uint32(0)
	for {
		if !(jobID <= (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask) {
			break
		}
		/* Copy the mutex/cond out */
		mutex = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fjob_mutex
		cond = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fjob_cond
		ZSTDMT_releaseBuffer(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FdstBuff)
		/* Clear the job description, but keep the mutex/cond */
		libc.Xmemset(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*456, 0, libc.Uint64FromInt64(456))
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fjob_mutex = mutex
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fjob_cond = cond
		goto _1
	_1:
		;
		jobID = jobID + 1
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = g_nullBuffer
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(1)
}

func ZSTDMT_waitForAllJobsCompleted(tls *libc.TLS, mtctx uintptr) {
	var jobID uint32
	_ = jobID
	for (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		jobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
		libc.Xpthread_mutex_lock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*456+16)
		for (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fconsumed < (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fsrc.Fsize {
			/* we want to block when waiting for data to flush */
			libc.Xpthread_cond_wait(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*456+56, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*456+16)
		}
		libc.Xpthread_mutex_unlock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*456+16)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID + 1
	}
}

func ZSTDMT_freeCCtx(tls *libc.TLS, mtctx uintptr) (r size_t) {
	if mtctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* compatible with free on NULL */
	if !(int32(uint32(*(*uint8)(unsafe.Pointer(mtctx + 3112))&0x1>>0)) != 0) {
		POOL_free(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory)
	} /* stop and free worker threads */
	ZSTDMT_releaseAllJobResources(tls, mtctx) /* release job resources into pools first */
	ZSTDMT_freeJobsTable(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+uint32(1), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
	ZSTDMT_freeBufferPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool)
	ZSTDMT_freeCCtxPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool)
	ZSTDMT_freeSeqPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool)
	ZSTDMT_serialState_free(tls, mtctx+352)
	ZSTD_freeCDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal)
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer != 0 {
		ZSTD_customFree(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
	}
	ZSTD_customFree(tls, mtctx, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
	return uint64(0)
}

func ZSTDMT_sizeof_CCtx(tls *libc.TLS, mtctx uintptr) (r size_t) {
	if mtctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* supports sizeof NULL */
	return uint64(3120) + POOL_sizeof(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory) + ZSTDMT_sizeof_bufferPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool) + uint64((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask+libc.Uint32FromInt32(1))*uint64(456) + ZSTDMT_sizeof_CCtxPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool) + ZSTDMT_sizeof_seqPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool) + ZSTD_sizeof_CDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal) + (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity
}

// C documentation
//
//	/* ZSTDMT_resize() :
//	 * @return : error code if fails, 0 on success */
func ZSTDMT_resize(tls *libc.TLS, mtctx uintptr, nbWorkers uint32) (r size_t) {
	var err_code size_t
	_ = err_code
	if POOL_resize(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory, uint64(nbWorkers)) != 0 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	err_code = ZSTDMT_expandJobsTable(tls, mtctx, nbWorkers)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool = ZSTDMT_expandBufferPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, uint32(2)*nbWorkers+uint32(3))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool == libc.UintptrFromInt32(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool = ZSTDMT_expandCCtxPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool, libc.Int32FromUint32(nbWorkers))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool == libc.UintptrFromInt32(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool = ZSTDMT_expandSeqPool(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool, nbWorkers)
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool == libc.UintptrFromInt32(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	ZSTDMT_CCtxParam_setNbWorkers(tls, mtctx+40, nbWorkers)
	return uint64(0)
}

// C documentation
//
//	/*! ZSTDMT_updateCParams_whileCompressing() :
//	 *  Updates a selected set of compression parameters, remaining compatible with currently active frame.
//	 *  New parameters will be applied to next compression job. */
func ZSTDMT_updateCParams_whileCompressing(tls *libc.TLS, mtctx uintptr, cctxParams uintptr) {
	var cParams ZSTD_compressionParameters
	var compressionLevel int32
	var saved_wlog U32
	_, _, _ = cParams, compressionLevel, saved_wlog
	saved_wlog = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcParams.FwindowLog /* Do not modify windowLog while compressing */
	compressionLevel = (*ZSTD_CCtx_params)(unsafe.Pointer(cctxParams)).FcompressionLevel
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcompressionLevel = compressionLevel
	cParams = ZSTD_getCParamsFromCCtxParams(tls, cctxParams, uint64(libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1)), uint64(0), int32(ZSTD_cpm_noAttachDict))
	cParams.FwindowLog = saved_wlog
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcParams = cParams
}

// C documentation
//
//	/* ZSTDMT_getFrameProgression():
//	 * tells how much data has been consumed (input) and produced (output) for current frame.
//	 * able to count progression inside worker threads.
//	 * Note : mutex will be acquired during statistics collection inside workers. */
func ZSTDMT_getFrameProgression(tls *libc.TLS, mtctx uintptr) (r ZSTD_frameProgression) {
	var cResult, flushed, produced size_t
	var fps ZSTD_frameProgression
	var jobNb, lastJobNb, wJobID uint32
	var jobPtr uintptr
	var v1, v3 uint64
	_, _, _, _, _, _, _, _, _, _ = cResult, flushed, fps, jobNb, jobPtr, lastJobNb, produced, wJobID, v1, v3
	fps.Fingested = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fconsumed + (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
	fps.Fconsumed = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fconsumed
	v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fproduced
	fps.Fflushed = v1
	fps.Fproduced = v1
	fps.FcurrentJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID
	fps.FnbActiveWorkers = uint32(0)
	lastJobNb = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID + libc.Uint32FromInt32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady)
	jobNb = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID
	for {
		if !(jobNb < lastJobNb) {
			break
		}
		wJobID = jobNb & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
		jobPtr = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456
		libc.Xpthread_mutex_lock(tls, jobPtr+16)
		cResult = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FcSize
		if ZSTD_isError(tls, cResult) != 0 {
			v1 = uint64(0)
		} else {
			v1 = cResult
		}
		produced = v1
		if ZSTD_isError(tls, cResult) != 0 {
			v3 = uint64(0)
		} else {
			v3 = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FdstFlushed
		}
		flushed = v3
		fps.Fingested += (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fsrc.Fsize
		fps.Fconsumed += (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fconsumed
		fps.Fproduced += produced
		fps.Fflushed += flushed
		fps.FnbActiveWorkers += libc.BoolUint32((*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fconsumed < (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).Fsrc.Fsize)
		libc.Xpthread_mutex_unlock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+16)
		goto _2
	_2:
		;
		jobNb = jobNb + 1
	}
	return fps
}

func ZSTDMT_toFlushNow(tls *libc.TLS, mtctx uintptr) (r size_t) {
	var cResult, flushed, produced, toFlush size_t
	var jobID, wJobID uint32
	var jobPtr uintptr
	var v1, v2 uint64
	_, _, _, _, _, _, _, _, _ = cResult, flushed, jobID, jobPtr, produced, toFlush, wJobID, v1, v2
	jobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID
	if jobID == (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		return uint64(0)
	} /* no active job => nothing to flush */
	/* look into oldest non-fully-flushed job */
	wJobID = jobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
	jobPtr = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456
	libc.Xpthread_mutex_lock(tls, jobPtr+16)
	cResult = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FcSize
	if ZSTD_isError(tls, cResult) != 0 {
		v1 = uint64(0)
	} else {
		v1 = cResult
	}
	produced = v1
	if ZSTD_isError(tls, cResult) != 0 {
		v2 = uint64(0)
	} else {
		v2 = (*ZSTDMT_jobDescription)(unsafe.Pointer(jobPtr)).FdstFlushed
	}
	flushed = v2
	toFlush = produced - flushed
	/* if toFlush==0, nothing is available to flush.
	 * However, jobID is expected to still be active:
	 * if jobID was already completed and fully flushed,
	 * ZSTDMT_flushProduced() should have already moved onto next job.
	 * Therefore, some input has not yet been consumed. */
	if toFlush == uint64(0) {
	}
	libc.Xpthread_mutex_unlock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+16)
	return toFlush
}

/* ------------------------------------------ */
/* =====   Multi-threaded compression   ===== */
/* ------------------------------------------ */

func ZSTDMT_computeTargetJobLog(tls *libc.TLS, params uintptr) (r uint32) {
	var jobLog, v1 uint32
	var v4, v5 int32
	_, _, _, _ = jobLog, v1, v4, v5
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* In Long Range Mode, the windowLog is typically oversized.
		 * In which case, it's preferable to determine the jobSize
		 * based on cycleLog instead. */
		if libc.Uint32FromInt32(libc.Int32FromInt32(21)) > ZSTD_cycleLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy)+uint32(3) {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(21))
		} else {
			v1 = ZSTD_cycleLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FchainLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy) + uint32(3)
		}
		jobLog = v1
	} else {
		if libc.Uint32FromInt32(libc.Int32FromInt32(20)) > (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog+uint32(2) {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(20))
		} else {
			v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog + uint32(2)
		}
		jobLog = v1
	}
	if MEM_32bits(tls) != 0 {
		v4 = int32(29)
	} else {
		v4 = int32(30)
	}
	if jobLog < libc.Uint32FromInt32(v4) {
		v1 = jobLog
	} else {
		if MEM_32bits(tls) != 0 {
			v5 = int32(29)
		} else {
			v5 = int32(30)
		}
		v1 = libc.Uint32FromInt32(v5)
	}
	return v1
}

func ZSTDMT_overlapLog_default(tls *libc.TLS, strat ZSTD_strategy) (r int32) {
	switch strat {
	case int32(ZSTD_btultra2):
		return int32(9)
	case int32(ZSTD_btultra):
		fallthrough
	case int32(ZSTD_btopt):
		return int32(8)
	case int32(ZSTD_btlazy2):
		fallthrough
	case int32(ZSTD_lazy2):
		return int32(7)
	case int32(ZSTD_lazy):
		fallthrough
	case int32(ZSTD_greedy):
		fallthrough
	case int32(ZSTD_dfast):
		fallthrough
	case int32(ZSTD_fast):
		fallthrough
	default:
	}
	return int32(6)
}

func ZSTDMT_overlapLog(tls *libc.TLS, ovlog int32, strat ZSTD_strategy) (r int32) {
	if ovlog == 0 {
		return ZSTDMT_overlapLog_default(tls, strat)
	}
	return ovlog
}

func ZSTDMT_computeOverlapSize(tls *libc.TLS, params uintptr) (r size_t) {
	var ovLog, overlapRLog int32
	var v1 uint32
	var v3 uint64
	_, _, _, _ = ovLog, overlapRLog, v1, v3
	overlapRLog = int32(9) - ZSTDMT_overlapLog(tls, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FoverlapLog, (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.Fstrategy)
	if overlapRLog >= int32(8) {
		v1 = uint32(0)
	} else {
		v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog - libc.Uint32FromInt32(overlapRLog)
	}
	ovLog = libc.Int32FromUint32(v1)
	if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		/* In Long Range Mode, the windowLog is typically oversized.
		 * In which case, it's preferable to determine the jobSize
		 * based on chainLog instead.
		 * Then, ovLog becomes a fraction of the jobSize, rather than windowSize */
		if (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog < ZSTDMT_computeTargetJobLog(tls, params)-uint32(2) {
			v1 = (*ZSTD_CCtx_params)(unsafe.Pointer(params)).FcParams.FwindowLog
		} else {
			v1 = ZSTDMT_computeTargetJobLog(tls, params) - uint32(2)
		}
		ovLog = libc.Int32FromUint32(v1 - libc.Uint32FromInt32(overlapRLog))
	}
	if ovLog == 0 {
		v3 = uint64(0)
	} else {
		v3 = libc.Uint64FromInt32(1) << ovLog
	}
	return v3
}

/* ====================================== */
/* =======      Streaming API     ======= */
/* ====================================== */

func ZSTDMT_initCStream_internal(tls *libc.TLS, mtctx uintptr, dict uintptr, dictSize size_t, dictContentType ZSTD_dictContentType_e, cdict uintptr, _params ZSTD_CCtx_params, pledgedSrcSize uint64) (r size_t) {
	bp := tls.Alloc(224)
	defer tls.Free(224)
	*(*ZSTD_CCtx_params)(unsafe.Pointer(bp)) = _params
	var capacity, err_code, nbSlackBuffers, nbWorkers, sectionsSize, slackSize, windowSize size_t
	var jobSizeKB, rsyncBits U32
	var v1, v2 int32
	var v3 uint32
	var v5 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _ = capacity, err_code, jobSizeKB, nbSlackBuffers, nbWorkers, rsyncBits, sectionsSize, slackSize, windowSize, v1, v2, v3, v5
	/* params supposed partially fully validated at this point */
	/* either dict or cdict, not both */
	/* init */
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers != (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FnbWorkers {
		err_code = ZSTDMT_resize(tls, mtctx, libc.Uint32FromInt32((*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FnbWorkers))
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize != uint64(0) && (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize < libc.Uint64FromInt32(libc.Int32FromInt32(512)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10))) {
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize = libc.Uint64FromInt32(libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(10)))
	}
	if MEM_32bits(tls) != 0 {
		v1 = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
	} else {
		v1 = libc.Int32FromInt32(1024) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize > libc.Uint64FromInt32(v1) {
		if MEM_32bits(tls) != 0 {
			v2 = libc.Int32FromInt32(512) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		} else {
			v2 = libc.Int32FromInt32(1024) * (libc.Int32FromInt32(1) << libc.Int32FromInt32(20))
		}
		(*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize = libc.Uint64FromInt32(v2)
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted == uint32(0) { /* previous compression not correctly finished */
		ZSTDMT_waitForAllJobsCompleted(tls, mtctx)
		ZSTDMT_releaseAllJobResources(tls, mtctx)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(1)
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams = *(*ZSTD_CCtx_params)(unsafe.Pointer(bp))
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeContentSize = pledgedSrcSize
	ZSTD_freeCDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal)
	if dict != 0 {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byCopy), dictContentType, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal == libc.UintptrFromInt32(0) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = libc.UintptrFromInt32(0)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = cdict
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize = ZSTDMT_computeOverlapSize(tls, bp)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize = (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FjobSize
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize == uint64(0) {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize = uint64(uint64(1) << ZSTDMT_computeTargetJobLog(tls, bp))
	}
	if (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).Frsyncable != 0 {
		/* Aim for the targetsectionSize as the average job size. */
		jobSizeKB = uint32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize >> libc.Int32FromInt32(10))
		rsyncBits = ZSTD_highbit32(tls, jobSizeKB) + libc.Uint32FromInt32(10)
		/* We refuse to create jobs < RSYNC_MIN_BLOCK_SIZE bytes, so make sure our
		 * expected job size is at least 4x larger. */
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.Fhash = uint64(0)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FhitMask = uint64(uint64(1)<<rsyncBits - uint64(1))
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FprimePower = ZSTD_rollingHash_primePower(tls, uint32(RSYNC_LENGTH))
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize
	} /* job size must be >= overlap size */
	ZSTDMT_setBufferSize(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, ZSTD_compressBound(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		v3 = uint32(1) << (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FcParams.FwindowLog
	} else {
		v3 = uint32(0)
	}
	/* If ldm is enabled we need windowSize space. */
	windowSize = uint64(v3)
	/* Two buffers of slack, plus extra space for the overlap
	 * This is the minimum slack that LDM works with. One extra because
	 * flush might waste up to targetSectionSize-1 bytes. Another extra
	 * for the overlap (if > 0), then one to fill which doesn't overlap
	 * with the LDM window.
	 */
	nbSlackBuffers = libc.Uint64FromInt32(int32(2) + libc.BoolInt32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize > uint64(0)))
	slackSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize * nbSlackBuffers
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FnbWorkers > int32(1) {
		v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FnbWorkers
	} else {
		v1 = int32(1)
	}
	/* Compute the total size, and always have enough slack */
	nbWorkers = libc.Uint64FromInt32(v1)
	sectionsSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize * nbWorkers
	if windowSize > sectionsSize {
		v5 = windowSize
	} else {
		v5 = sectionsSize
	}
	capacity = v5 + slackSize
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity < capacity {
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer != 0 {
			ZSTD_customFree(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		}
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer = ZSTD_customMalloc(tls, capacity, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer == libc.UintptrFromInt32(0) {
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity = uint64(0)
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity = capacity
	}
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = g_nullBuffer
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix = kNullRange
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = uint32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fconsumed = uint64(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fproduced = uint64(0)
	/* update dictionary */
	ZSTD_freeCDict(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = libc.UintptrFromInt32(0)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = libc.UintptrFromInt32(0)
	if dict != 0 {
		if dictContentType == int32(ZSTD_dct_rawContent) {
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart = dict
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fsize = dictSize
		} else {
			/* note : a loadPrefix becomes an internal CDict */
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal = ZSTD_createCDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byRef), dictContentType, (*(*ZSTD_CCtx_params)(unsafe.Pointer(bp))).FcParams, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcMem)
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal
			if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcdictLocal == libc.UintptrFromInt32(0) {
				return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
			}
		}
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict = cdict
	}
	if ZSTDMT_serialState_reset(tls, mtctx+352, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool, *(*ZSTD_CCtx_params)(unsafe.Pointer(bp)), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize, dict, dictSize, dictContentType) != 0 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTDMT_writeLastEmptyBlock()
//	 * Write a single empty block with an end-of-frame to finish a frame.
//	 * Job must be created from streaming variant.
//	 * This function is always successful if expected conditions are fulfilled.
//	 */
func ZSTDMT_writeLastEmptyBlock(tls *libc.TLS, job uintptr) {
	/* last job is empty -> will be simplified into a last empty block */
	/* cannot be first job, as it also needs to create frame header */
	/* invoked from streaming variant only (otherwise, dstBuff might be user's output) */
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff = ZSTDMT_getBuffer(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FbufPool)
	if (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff.Fstart == libc.UintptrFromInt32(0) {
		(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		return
	}
	/* no buffer should ever be that small */
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).Fsrc = kNullRange
	(*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FcSize = ZSTD_writeLastEmptyBlock(tls, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff.Fstart, (*ZSTDMT_jobDescription)(unsafe.Pointer(job)).FdstBuff.Fcapacity)
}

func ZSTDMT_createCompressionJob(tls *libc.TLS, mtctx uintptr, srcSize size_t, endOp ZSTD_EndDirective) (r size_t) {
	var endFrame int32
	var jobID uint32
	var newPrefixSize size_t
	var src, v1 uintptr
	var v2 uint64
	_, _, _, _, _, _ = endFrame, jobID, newPrefixSize, src, v1, v2
	jobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
	endFrame = libc.BoolInt32(endOp == int32(ZSTD_e_end))
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID > (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID+(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask {
		return uint64(0)
	}
	if !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0) {
		src = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fsrc.Fstart = src
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fsrc.Fsize = srcSize
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fprefix = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fconsumed = uint64(0)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FcSize = uint64(0)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fparams = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID == uint32(0) {
			v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fcdict
		} else {
			v1 = libc.UintptrFromInt32(0)
		}
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fcdict = v1
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FfullFrameSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeContentSize
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FdstBuff = g_nullBuffer
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FcctxPool = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FcctxPool
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FbufPool = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FseqPool = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FseqPool
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).Fserial = mtctx + 352
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FjobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FfirstJob = libc.BoolUint32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID == libc.Uint32FromInt32(0))
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FlastJob = libc.Uint32FromInt32(endFrame)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FframeChecksumNeeded = libc.BoolUint32((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FfParams.FchecksumFlag != 0 && endFrame != 0 && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID > uint32(0))
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(jobID)*456))).FdstFlushed = uint64(0)
		/* Update the round buffer pos and clear the input buffer to be reset */
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos += srcSize
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = g_nullBuffer
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
		/* Set the prefix for next job */
		if !(endFrame != 0) {
			if srcSize < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize {
				v2 = srcSize
			} else {
				v2 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetPrefixSize
			}
			newPrefixSize = v2
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart = src + uintptr(srcSize) - uintptr(newPrefixSize)
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fsize = newPrefixSize
		} else { /* endFrame==1 => no need for another input buffer */
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix = kNullRange
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded = libc.Uint32FromInt32(endFrame)
			if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID == uint32(0) {
				/* single job exception : checksum is already calculated directly within worker thread */
				(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FfParams.FchecksumFlag = 0
			}
		}
		if srcSize == uint64(0) && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID > uint32(0) {
			/* only possible case : need to end the frame with an empty last block */
			ZSTDMT_writeLastEmptyBlock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*456)
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID + 1
			return uint64(0)
		}
	}
	if POOL_tryAdd(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Ffactory, __ccgo_fp(ZSTDMT_compressionJob), (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(jobID)*456) != 0 {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID + 1
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady = 0
	} else {
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady = int32(1)
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTDMT_flushProduced() :
//	 *  flush whatever data has been produced but not yet flushed in current job.
//	 *  move to next job if current one is fully flushed.
//	 * `output` : `pos` will be updated with amount of data flushed .
//	 * `blockToFlush` : if >0, the function will block and wait if there is no data available to flush .
//	 * @return : amount of data remaining within internal buffer, 0 if no more, 1 if unknown but > 0, or an error code */
func ZSTDMT_flushProduced(tls *libc.TLS, mtctx uintptr, output uintptr, blockToFlush uint32, end ZSTD_EndDirective) (r size_t) {
	var cSize, srcConsumed, srcSize, toFlush size_t
	var checksum U32
	var wJobID uint32
	var v1 uint64
	_, _, _, _, _, _, _ = cSize, checksum, srcConsumed, srcSize, toFlush, wJobID, v1
	wJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
	libc.Xpthread_mutex_lock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+16)
	if blockToFlush != 0 && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		for (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed == (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FcSize { /* nothing to flush */
			if (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fconsumed == (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fsrc.Fsize {
				break
			}
			libc.Xpthread_cond_wait(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+56, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+16) /* block when nothing to flush but some to come */
		}
	}
	/* try to flush something */
	cSize = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FcSize          /* shared */
	srcConsumed = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fconsumed /* shared */
	srcSize = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fsrc.Fsize    /* read-only, could be done after mutex lock, but no-declaration-after-statement */
	libc.Xpthread_mutex_unlock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+16)
	if ZSTD_isError(tls, cSize) != 0 {
		ZSTDMT_waitForAllJobsCompleted(tls, mtctx)
		ZSTDMT_releaseAllJobResources(tls, mtctx)
		return cSize
	}
	/* add frame checksum if necessary (can only happen once) */
	if srcConsumed == srcSize && (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FframeChecksumNeeded != 0 {
		checksum = uint32(XXH_INLINE_XXH64_digest(tls, mtctx+352+2424))
		MEM_writeLE32(tls, (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstBuff.Fstart+uintptr((*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FcSize), checksum)
		cSize = cSize + uint64(4)
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FcSize += uint64(4) /* can write this shared value, as worker is no longer active */
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FframeChecksumNeeded = uint32(0)
	}
	if cSize > uint64(0) {
		if cSize-(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed < (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize-(*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos {
			v1 = cSize - (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed
		} else {
			v1 = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize - (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos
		} /* compression is ongoing or completed */
		toFlush = v1
		if toFlush > uint64(0) {
			libc.Xmemcpy(tls, (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst+uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos), (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstBuff.Fstart+uintptr((*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed), toFlush)
		}
		*(*size_t)(unsafe.Pointer(output + 16)) += toFlush
		(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed += toFlush                              /* can write : this value is only used by mtctx */
		if srcConsumed == srcSize && (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed == cSize { /* output buffer fully flushed => free this job position */
			ZSTDMT_releaseBuffer(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FbufPool, (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstBuff)
			(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstBuff = g_nullBuffer
			(*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FcSize = uint64(0) /* ensure this job slot is considered "not started" in future check */
			*(*uint64)(unsafe.Pointer(mtctx + 3056)) += srcSize
			*(*uint64)(unsafe.Pointer(mtctx + 3064)) += cSize
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID + 1
		}
	}
	/* return value : how many bytes left in buffer ; fake it to 1 when unknown but >0 */
	if cSize > (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed {
		return cSize - (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).FdstFlushed
	}
	if srcSize > srcConsumed {
		return uint64(1)
	} /* current job not completely compressed */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID {
		return uint64(1)
	} /* some more jobs ongoing */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0 {
		return uint64(1)
	} /* one job is ready to push, just not yet in the list */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled > uint64(0) {
		return uint64(1)
	} /* input is not empty, and still needs to be converted into a job */
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FallJobsCompleted = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded /* all jobs are entirely flushed => if this one is last one, frame is completed */
	if end == int32(ZSTD_e_end) {
		return libc.BoolUint64(!((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded != 0))
	} /* for ZSTD_e_end, question becomes : is frame completed ? instead of : are internal buffers fully flushed ? */
	return uint64(0) /* internal buffers fully flushed */
}

// C documentation
//
//	/**
//	 * Returns the range of data used by the earliest job that is not yet complete.
//	 * If the data of the first job is broken up into two segments, we cover both
//	 * sections.
//	 */
func ZSTDMT_getInputDataInUse(tls *libc.TLS, mtctx uintptr) (r Range) {
	var consumed, nbJobs1stRoundMin, roundBuffCapacity size_t
	var firstJobID, jobID, lastJobID, wJobID uint32
	var range1 Range
	_, _, _, _, _, _, _, _ = consumed, firstJobID, jobID, lastJobID, nbJobs1stRoundMin, range1, roundBuffCapacity, wJobID
	firstJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FdoneJobID
	lastJobID = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FnextJobID
	/* no need to check during first round */
	roundBuffCapacity = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity
	nbJobs1stRoundMin = roundBuffCapacity / (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize
	if uint64(lastJobID) < nbJobs1stRoundMin {
		return kNullRange
	}
	jobID = firstJobID
	for {
		if !(jobID < lastJobID) {
			break
		}
		wJobID = jobID & (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobIDMask
		libc.Xpthread_mutex_lock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+16)
		consumed = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fconsumed
		libc.Xpthread_mutex_unlock(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs+uintptr(wJobID)*456+16)
		if consumed < (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fsrc.Fsize {
			range1 = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fprefix
			if range1.Fsize == uint64(0) {
				/* Empty prefix */
				range1 = (*(*ZSTDMT_jobDescription)(unsafe.Pointer((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fjobs + uintptr(wJobID)*456))).Fsrc
			}
			/* Job source in multiple segments not supported yet */
			return range1
		}
		goto _1
	_1:
		;
		jobID = jobID + 1
	}
	return kNullRange
}

// C documentation
//
//	/**
//	 * Returns non-zero iff buffer and range overlap.
//	 */
func ZSTDMT_isOverlapped(tls *libc.TLS, buffer Buffer, range1 Range) (r int32) {
	var bufferEnd, bufferStart, rangeEnd, rangeStart uintptr
	_, _, _, _ = bufferEnd, bufferStart, rangeEnd, rangeStart
	bufferStart = buffer.Fstart
	rangeStart = range1.Fstart
	if rangeStart == libc.UintptrFromInt32(0) || bufferStart == libc.UintptrFromInt32(0) {
		return 0
	}
	bufferEnd = bufferStart + uintptr(buffer.Fcapacity)
	rangeEnd = rangeStart + uintptr(range1.Fsize)
	/* Empty ranges cannot overlap */
	if bufferStart == bufferEnd || rangeStart == rangeEnd {
		return 0
	}
	return libc.BoolInt32(bufferStart < rangeEnd && rangeStart < bufferEnd)
	return r
}

func ZSTDMT_doesOverlapWindow(tls *libc.TLS, buffer Buffer, window ZSTD_window_t) (r int32) {
	var extDict, prefix Range
	_, _ = extDict, prefix
	extDict.Fstart = window.FdictBase + uintptr(window.FlowLimit)
	extDict.Fsize = uint64(window.FdictLimit - window.FlowLimit)
	prefix.Fstart = window.Fbase + uintptr(window.FdictLimit)
	prefix.Fsize = libc.Uint64FromInt64(int64(window.FnextSrc) - int64(window.Fbase+uintptr(window.FdictLimit)))
	return libc.BoolInt32(ZSTDMT_isOverlapped(tls, buffer, extDict) != 0 || ZSTDMT_isOverlapped(tls, buffer, prefix) != 0)
}

func ZSTDMT_waitForLdmComplete(tls *libc.TLS, mtctx uintptr, buffer Buffer) {
	var mutex uintptr
	_ = mutex
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.FldmParams.FenableLdm == int32(ZSTD_ps_enable) {
		mutex = mtctx + 352 + 2520
		libc.Xpthread_mutex_lock(tls, mutex)
		for ZSTDMT_doesOverlapWindow(tls, buffer, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fserial.FldmWindow) != 0 {
			libc.Xpthread_cond_wait(tls, mtctx+352+2560, mutex)
		}
		libc.Xpthread_mutex_unlock(tls, mutex)
	}
}

// C documentation
//
//	/**
//	 * Attempts to set the inBuff to the next section to fill.
//	 * If any part of the new section is still in use we give up.
//	 * Returns non-zero if the buffer is filled.
//	 */
func ZSTDMT_tryGetInputRange(tls *libc.TLS, mtctx uintptr) (r int32) {
	var buffer Buffer
	var inUse Range
	var prefixSize, spaceLeft, spaceNeeded size_t
	var start uintptr
	_, _, _, _, _, _ = buffer, inUse, prefixSize, spaceLeft, spaceNeeded, start
	inUse = ZSTDMT_getInputDataInUse(tls, mtctx)
	spaceLeft = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fcapacity - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos
	spaceNeeded = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize
	if spaceLeft < spaceNeeded {
		/* ZSTD_invalidateRepCodes() doesn't work for extDict variants.
		 * Simply copy the prefix to the beginning in that case.
		 */
		start = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer
		prefixSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fsize
		buffer.Fstart = start
		buffer.Fcapacity = prefixSize
		if ZSTDMT_isOverlapped(tls, buffer, inUse) != 0 {
			return 0
		}
		ZSTDMT_waitForLdmComplete(tls, mtctx, buffer)
		libc.Xmemmove(tls, start, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart, prefixSize)
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fprefix.Fstart = start
		(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos = prefixSize
	}
	buffer.Fstart = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fbuffer + uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FroundBuff.Fpos)
	buffer.Fcapacity = spaceNeeded
	if ZSTDMT_isOverlapped(tls, buffer, inUse) != 0 {
		return 0
	}
	ZSTDMT_waitForLdmComplete(tls, mtctx, buffer)
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer = buffer
	(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled = uint64(0)
	return int32(1)
}

type SyncPoint = struct {
	FtoLoad size_t
	Fflush  int32
}

// C documentation
//
//	/**
//	 * Searches through the input for a synchronization point. If one is found, we
//	 * will instruct the caller to flush, and return the number of bytes to load.
//	 * Otherwise, we will load as many bytes as possible and instruct the caller
//	 * to continue as normal.
//	 */
func findSynchronizationPoint(tls *libc.TLS, mtctx uintptr, input ZSTD_inBuffer) (r SyncPoint) {
	var hash, hitMask, primePower U64
	var istart, prev uintptr
	var pos size_t
	var syncPoint SyncPoint
	var toRemove BYTE
	var v1 uint64
	var v3 int32
	_, _, _, _, _, _, _, _, _, _ = hash, hitMask, istart, pos, prev, primePower, syncPoint, toRemove, v1, v3
	istart = input.Fsrc + uintptr(input.Fpos)
	primePower = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FprimePower
	hitMask = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Frsync.FhitMask
	if input.Fsize-input.Fpos < (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize-(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled {
		v1 = input.Fsize - input.Fpos
	} else {
		v1 = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
	}
	syncPoint.FtoLoad = v1
	syncPoint.Fflush = 0
	if !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).Fparams.Frsyncable != 0) {
		/* Rsync is disabled. */
		return syncPoint
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled+input.Fsize-input.Fpos < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		/* We don't emit synchronization points if it would produce too small blocks.
		 * We don't have enough input to find a synchronization point, so don't look.
		 */
		return syncPoint
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled+syncPoint.FtoLoad < uint64(RSYNC_LENGTH) {
		/* Not enough to compute the hash.
		 * We will miss any synchronization points in this RSYNC_LENGTH byte
		 * window. However, since it depends only in the internal buffers, if the
		 * state is already synchronized, we will remain synchronized.
		 * Additionally, the probability that we miss a synchronization point is
		 * low: RSYNC_LENGTH / targetSectionSize.
		 */
		return syncPoint
	}
	/* Initialize the loop variables. */
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		/* We don't need to scan the first RSYNC_MIN_BLOCK_SIZE positions
		 * because they can't possibly be a sync point. So we can start
		 * part way through the input buffer.
		 */
		pos = libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
		if pos >= uint64(RSYNC_LENGTH) {
			prev = istart + uintptr(pos) - uintptr(RSYNC_LENGTH)
			hash = ZSTD_rollingHash_compute(tls, prev, uint64(RSYNC_LENGTH))
		} else {
			prev = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart + uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled) - uintptr(RSYNC_LENGTH)
			hash = ZSTD_rollingHash_compute(tls, prev+uintptr(pos), libc.Uint64FromInt32(RSYNC_LENGTH)-pos)
			hash = ZSTD_rollingHash_append(tls, hash, istart, pos)
		}
	} else {
		/* We have enough bytes buffered to initialize the hash,
		 * and have processed enough bytes to find a sync point.
		 * Start scanning at the beginning of the input.
		 */
		pos = uint64(0)
		prev = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart + uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled) - uintptr(RSYNC_LENGTH)
		hash = ZSTD_rollingHash_compute(tls, prev, uint64(RSYNC_LENGTH))
		if hash&hitMask == hitMask {
			/* We're already at a sync point so don't load any more until
			 * we're able to flush this sync point.
			 * This likely happened because the job table was full so we
			 * couldn't add our job.
			 */
			syncPoint.FtoLoad = uint64(0)
			syncPoint.Fflush = int32(1)
			return syncPoint
		}
	}
	/* Starting with the hash of the previous RSYNC_LENGTH bytes, roll
	 * through the input. If we hit a synchronization point, then cut the
	 * job off, and tell the compressor to flush the job. Otherwise, load
	 * all the bytes and continue as normal.
	 * If we go too long without a synchronization point (targetSectionSize)
	 * then a block will be emitted anyways, but this is okay, since if we
	 * are already synchronized we will remain synchronized.
	 */
	for {
		if !(pos < syncPoint.FtoLoad) {
			break
		}
		if pos < uint64(RSYNC_LENGTH) {
			v3 = libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(prev + uintptr(pos))))
		} else {
			v3 = libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart + uintptr(pos-uint64(RSYNC_LENGTH)))))
		}
		toRemove = libc.Uint8FromInt32(v3)
		/* This assert is very expensive, and Debian compiles with asserts enabled.
		 * So disable it for now. We can get similar coverage by checking it at the
		 * beginning & end of the loop.
		 * assert(pos < RSYNC_LENGTH || ZSTD_rollingHash_compute(istart + pos - RSYNC_LENGTH, RSYNC_LENGTH) == hash);
		 */
		hash = ZSTD_rollingHash_rotate(tls, hash, toRemove, *(*BYTE)(unsafe.Pointer(istart + uintptr(pos))), primePower)
		if hash&hitMask == hitMask {
			syncPoint.FtoLoad = pos + uint64(1)
			syncPoint.Fflush = int32(1)
			pos = pos + 1 /* for assert */
			break
		}
		goto _2
	_2:
		;
		pos = pos + 1
	}
	return syncPoint
}

func ZSTDMT_nextInputSizeHint(tls *libc.TLS, mtctx uintptr) (r size_t) {
	var hintInSize size_t
	_ = hintInSize
	hintInSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize - (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
	if hintInSize == uint64(0) {
		hintInSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize
	}
	return hintInSize
}

// C documentation
//
//	/** ZSTDMT_compressStream_generic() :
//	 *  internal use only - exposed to be invoked from zstd_compress.c
//	 *  assumption : output and input are valid (pos <= size)
//	 * @return : minimum amount of data remaining to flush, 0 if none */
func ZSTDMT_compressStream_generic(tls *libc.TLS, mtctx uintptr, output uintptr, input uintptr, endOp ZSTD_EndDirective) (r size_t) {
	var err_code, jobSize, remainingToFlush size_t
	var forwardInputProgress uint32
	var syncPoint SyncPoint
	var v1 uint64
	_, _, _, _, _, _ = err_code, forwardInputProgress, jobSize, remainingToFlush, syncPoint, v1
	forwardInputProgress = uint32(0)
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded != 0 && endOp == int32(ZSTD_e_continue) {
		/* current frame being ended. Only flush/end are allowed */
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	/* fill input buffer */
	if !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0) && (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize > (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos { /* support NULL input */
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart == libc.UintptrFromInt32(0) {
			/* Can't fill an empty buffer */
			if !(ZSTDMT_tryGetInputRange(tls, mtctx) != 0) {
				/* It is only possible for this operation to fail if there are
				 * still compression jobs ongoing.
				 */
			} else {
			}
		}
		if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart != libc.UintptrFromInt32(0) {
			syncPoint = findSynchronizationPoint(tls, mtctx, *(*ZSTD_inBuffer)(unsafe.Pointer(input)))
			if syncPoint.Fflush != 0 && endOp == int32(ZSTD_e_continue) {
				endOp = int32(ZSTD_e_flush)
			}
			libc.Xmemcpy(tls, (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Fbuffer.Fstart+uintptr((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled), (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc+uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos), syncPoint.FtoLoad)
			*(*size_t)(unsafe.Pointer(input + 16)) += syncPoint.FtoLoad
			(*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled += syncPoint.FtoLoad
			forwardInputProgress = libc.BoolUint32(syncPoint.FtoLoad > uint64(0))
		}
	}
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos < (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize && endOp == int32(ZSTD_e_end) {
		/* Can't end yet because the input is not fully consumed.
		 * We are in one of these cases:
		 * - mtctx->inBuff is NULL & empty: we couldn't get an input buffer so don't create a new job.
		 * - We filled the input buffer: flush this job but don't end the frame.
		 * - We hit a synchronization point: flush this job but don't end the frame.
		 */
		endOp = int32(ZSTD_e_flush)
	}
	if (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FjobReady != 0 || (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled >= (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FtargetSectionSize || endOp != int32(ZSTD_e_continue) && (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled > uint64(0) || endOp == int32(ZSTD_e_end) && !((*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FframeEnded != 0) { /* must finish the frame with a zero-size block */
		jobSize = (*ZSTDMT_CCtx)(unsafe.Pointer(mtctx)).FinBuff.Ffilled
		err_code = ZSTDMT_createCompressionJob(tls, mtctx, jobSize, endOp)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
	}
	/* check for potential compressed data ready to be flushed */
	remainingToFlush = ZSTDMT_flushProduced(tls, mtctx, output, libc.BoolUint32(!(forwardInputProgress != 0)), endOp) /* block if there was no forward input progress */
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos < (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
		if remainingToFlush > libc.Uint64FromInt32(libc.Int32FromInt32(1)) {
			v1 = remainingToFlush
		} else {
			v1 = libc.Uint64FromInt32(libc.Int32FromInt32(1))
		}
		return v1
	} /* input not consumed : do not end flush yet */
	return remainingToFlush
	return r
}

/**** ended inlining compress/zstdmt_compress.c ****/

/**** start inlining decompress/huf_decompress.c ****/
/* ******************************************************************
 * huff0 huffman decoder,
 * part of Finite State Entropy library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 *  You can contact the author at :
 *  - FSE+HUF source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/* **************************************************************
*  Dependencies
****************************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/bitstream.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/bits.h ****/

/* **************************************************************
*  Constants
****************************************************************/

/* **************************************************************
*  Macros
****************************************************************/

/* These two optional macros force the use one way or another of the two
 * Huffman decompression implementations. You can't force in both directions
 * at the same time.
 */

/* When DYNAMIC_BMI2 is enabled, fast decoders are only called when bmi2 is
 * supported at runtime, so we can add the BMI2 target attribute.
 * When it is disabled, we will still get BMI2 if it is enabled statically.
 */

/* **************************************************************
*  Error Management
****************************************************************/

/* **************************************************************
*  Byte alignment for workSpace management
****************************************************************/

// C documentation
//
//	/* **************************************************************
//	*  BMI2 Variant Wrappers
//	****************************************************************/
type HUF_DecompressUsingDTableFn = uintptr

// C documentation
//
//	/*-***************************/
//	/*  generic DTableDesc       */
//	/*-***************************/
type DTableDesc = struct {
	FmaxTableLog BYTE
	FtableType   BYTE
	FtableLog    BYTE
	Freserved    BYTE
}

func HUF_getDTableDesc(tls *libc.TLS, table uintptr) (r DTableDesc) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* dtd at bp+0 */ DTableDesc
	libc.Xmemcpy(tls, bp, table, libc.Uint64FromInt64(4))
	return *(*DTableDesc)(unsafe.Pointer(bp))
}

func HUF_initFastDStream(tls *libc.TLS, ip uintptr) (r size_t) {
	var bitsConsumed, value size_t
	var lastByte BYTE
	var v1 uint32
	_, _, _, _ = bitsConsumed, lastByte, value, v1
	lastByte = *(*BYTE)(unsafe.Pointer(ip + 7))
	if lastByte != 0 {
		v1 = uint32(8) - ZSTD_highbit32(tls, uint32(lastByte))
	} else {
		v1 = uint32(0)
	}
	bitsConsumed = uint64(v1)
	value = MEM_readLEST(tls, ip) | uint64(1)
	return value << bitsConsumed
}

// C documentation
//
//	/**
//	 * The input/output arguments to the Huffman fast decoding loop:
//	 *
//	 * ip [in/out] - The input pointers, must be updated to reflect what is consumed.
//	 * op [in/out] - The output pointers, must be updated to reflect what is written.
//	 * bits [in/out] - The bitstream containers, must be updated to reflect the current state.
//	 * dt [in] - The decoding table.
//	 * ilowest [in] - The beginning of the valid range of the input. Decoders may read
//	 *                down to this pointer. It may be below iend[0].
//	 * oend [in] - The end of the output stream. op[3] must not cross oend.
//	 * iend [in] - The end of each input stream. ip[i] may cross iend[i],
//	 *             as long as it is above ilowest, but that indicates corruption.
//	 */
type HUF_DecompressFastArgs = struct {
	Fip      [4]uintptr
	Fop      [4]uintptr
	Fbits    [4]U64
	Fdt      uintptr
	Filowest uintptr
	Foend    uintptr
	Fiend    [4]uintptr
}

type HUF_DecompressFastLoopFn = uintptr

// C documentation
//
//	/**
//	 * Initializes args for the fast decoding loop.
//	 * @returns 1 on success
//	 *          0 if the fallback implementation should be used.
//	 *          Or an error code on failure.
//	 */
func HUF_DecompressFastArgs_init(tls *libc.TLS, args uintptr, dst uintptr, dstSize size_t, src uintptr, srcSize size_t, DTable uintptr) (r size_t) {
	var dt, istart, oend uintptr
	var dtLog U32
	var length1, length2, length3, length4 size_t
	_, _, _, _, _, _, _, _ = dt, dtLog, istart, length1, length2, length3, length4, oend
	dt = DTable + uintptr(1)*4
	dtLog = uint32(HUF_getDTableDesc(tls, DTable).FtableLog)
	istart = src
	oend = ZSTD_maybeNullPtrAdd(tls, dst, libc.Int64FromUint64(dstSize))
	/* The fast decoding loop assumes 64-bit little-endian.
	 * This condition is false on x32.
	 */
	if !(MEM_isLittleEndian(tls) != 0) || MEM_32bits(tls) != 0 {
		return uint64(0)
	}
	/* Avoid nullptr addition */
	if dstSize == uint64(0) {
		return uint64(0)
	}
	/* strict minimum : jump table + 1 byte per stream */
	if srcSize < uint64(10) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* Must have at least 8 bytes per stream because we don't handle initializing smaller bit containers.
	 * If table log is not correct at this point, fallback to the old decoder.
	 * On small inputs we don't have enough data to trigger the fast loop, so use the old decoder.
	 */
	if dtLog != uint32(HUF_DECODER_FAST_TABLELOG) {
		return uint64(0)
	}
	/* Read the jump table. */
	length1 = uint64(MEM_readLE16(tls, istart))
	length2 = uint64(MEM_readLE16(tls, istart+uintptr(2)))
	length3 = uint64(MEM_readLE16(tls, istart+uintptr(4)))
	length4 = srcSize - (length1 + length2 + length3 + uint64(6))
	*(*uintptr)(unsafe.Pointer(args + 120)) = istart + uintptr(6) /* jumpTable */
	*(*uintptr)(unsafe.Pointer(args + 120 + 1*8)) = *(*uintptr)(unsafe.Pointer(args + 120)) + uintptr(length1)
	*(*uintptr)(unsafe.Pointer(args + 120 + 2*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 1*8)) + uintptr(length2)
	*(*uintptr)(unsafe.Pointer(args + 120 + 3*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 2*8)) + uintptr(length3)
	/* HUF_initFastDStream() requires this, and this small of an input
	 * won't benefit from the ASM loop anyways.
	 */
	if length1 < uint64(8) || length2 < uint64(8) || length3 < uint64(8) || length4 < uint64(8) {
		return uint64(0)
	}
	if length4 > srcSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	/* ip[] contains the position that is currently loaded into bits[]. */
	*(*uintptr)(unsafe.Pointer(args)) = *(*uintptr)(unsafe.Pointer(args + 120 + 1*8)) - uintptr(8)
	*(*uintptr)(unsafe.Pointer(args + 1*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 2*8)) - uintptr(8)
	*(*uintptr)(unsafe.Pointer(args + 2*8)) = *(*uintptr)(unsafe.Pointer(args + 120 + 3*8)) - uintptr(8)
	*(*uintptr)(unsafe.Pointer(args + 3*8)) = src + uintptr(srcSize) - uintptr(8)
	/* op[] contains the output pointers. */
	*(*uintptr)(unsafe.Pointer(args + 32)) = dst
	*(*uintptr)(unsafe.Pointer(args + 32 + 1*8)) = *(*uintptr)(unsafe.Pointer(args + 32)) + uintptr((dstSize+uint64(3))/uint64(4))
	*(*uintptr)(unsafe.Pointer(args + 32 + 2*8)) = *(*uintptr)(unsafe.Pointer(args + 32 + 1*8)) + uintptr((dstSize+uint64(3))/uint64(4))
	*(*uintptr)(unsafe.Pointer(args + 32 + 3*8)) = *(*uintptr)(unsafe.Pointer(args + 32 + 2*8)) + uintptr((dstSize+uint64(3))/uint64(4))
	/* No point to call the ASM loop for tiny outputs. */
	if *(*uintptr)(unsafe.Pointer(args + 32 + 3*8)) >= oend {
		return uint64(0)
	}
	/* bits[] is the bit container.
	 * It is read from the MSB down to the LSB.
	 * It is shifted left as it is read, and zeros are
	 * shifted in. After the lowest valid bit a 1 is
	 * set, so that CountTrailingZeros(bits[]) can be used
	 * to count how many bits we've consumed.
	 */
	*(*U64)(unsafe.Pointer(args + 64)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args)))
	*(*U64)(unsafe.Pointer(args + 64 + 1*8)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args + 1*8)))
	*(*U64)(unsafe.Pointer(args + 64 + 2*8)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args + 2*8)))
	*(*U64)(unsafe.Pointer(args + 64 + 3*8)) = HUF_initFastDStream(tls, *(*uintptr)(unsafe.Pointer(args + 3*8)))
	/* The decoders must be sure to never read beyond ilowest.
	 * This is lower than iend[0], but allowing decoders to read
	 * down to ilowest can allow an extra iteration or two in the
	 * fast loop.
	 */
	(*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest = istart
	(*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Foend = oend
	(*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Fdt = dt
	return uint64(1)
}

func HUF_initRemainingDStream(tls *libc.TLS, bit uintptr, args uintptr, stream int32, segmentEnd uintptr) (r size_t) {
	/* Validate that we haven't overwritten. */
	if *(*uintptr)(unsafe.Pointer(args + 32 + uintptr(stream)*8)) > segmentEnd {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* Validate that we haven't read beyond iend[].
	 * Note that ip[] may be < iend[] because the MSB is
	 * the next bit to read, and we may have consumed 100%
	 * of the stream, so down to iend[i] - 8 is valid.
	 */
	if *(*uintptr)(unsafe.Pointer(args + uintptr(stream)*8)) < *(*uintptr)(unsafe.Pointer(args + 120 + uintptr(stream)*8))-uintptr(8) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* Construct the BIT_DStream_t. */
	(*BIT_DStream_t)(unsafe.Pointer(bit)).FbitContainer = MEM_readLEST(tls, *(*uintptr)(unsafe.Pointer(args + uintptr(stream)*8)))
	(*BIT_DStream_t)(unsafe.Pointer(bit)).FbitsConsumed = ZSTD_countTrailingZeros64(tls, *(*U64)(unsafe.Pointer(args + 64 + uintptr(stream)*8)))
	(*BIT_DStream_t)(unsafe.Pointer(bit)).Fstart = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest
	(*BIT_DStream_t)(unsafe.Pointer(bit)).FlimitPtr = (*BIT_DStream_t)(unsafe.Pointer(bit)).Fstart + uintptr(8)
	(*BIT_DStream_t)(unsafe.Pointer(bit)).Fptr = *(*uintptr)(unsafe.Pointer(args + uintptr(stream)*8))
	return uint64(0)
}

/* Calls X(N) for each stream 0, 1, 2, 3. */

/* Calls X(N, var) for each stream 0, 1, 2, 3. */

// C documentation
//
//	/*-***************************/
//	/*  single-symbol decoding   */
//	/*-***************************/
type HUF_DEltX1 = struct {
	FnbBits BYTE
	Fbyte1  BYTE
} /* single-symbol decoding */

// C documentation
//
//	/**
//	 * Packs 4 HUF_DEltX1 structs into a U64. This is used to lay down 4 entries at
//	 * a time.
//	 */
func HUF_DEltX1_set4(tls *libc.TLS, symbol BYTE, nbBits BYTE) (r U64) {
	var D4 U64
	_ = D4
	if MEM_isLittleEndian(tls) != 0 {
		D4 = libc.Uint64FromInt32(libc.Int32FromUint8(symbol)<<libc.Int32FromInt32(8) + libc.Int32FromUint8(nbBits))
	} else {
		D4 = libc.Uint64FromInt32(libc.Int32FromUint8(symbol) + libc.Int32FromUint8(nbBits)<<libc.Int32FromInt32(8))
	}
	D4 = uint64(D4 * libc.Uint64FromUint64(0x0001000100010001))
	return D4
}

// C documentation
//
//	/**
//	 * Increase the tableLog to targetTableLog and rescales the stats.
//	 * If tableLog > targetTableLog this is a no-op.
//	 * @returns New tableLog
//	 */
func HUF_rescaleStats(tls *libc.TLS, huffWeight uintptr, rankVal uintptr, nbSymbols U32, tableLog U32, targetTableLog U32) (r U32) {
	var s, scale U32
	var v2 uintptr
	var v3 uint32
	_, _, _, _ = s, scale, v2, v3
	if tableLog > targetTableLog {
		return tableLog
	}
	if tableLog < targetTableLog {
		scale = targetTableLog - tableLog
		/* Increase the weight for all non-zero probability symbols by scale. */
		s = uint32(0)
		for {
			if !(s < nbSymbols) {
				break
			}
			v2 = huffWeight + uintptr(s)
			if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(huffWeight + uintptr(s)))) == 0 {
				v3 = uint32(0)
			} else {
				v3 = scale
			}
			*(*BYTE)(unsafe.Pointer(v2)) = BYTE(int32(*(*BYTE)(unsafe.Pointer(v2))) + libc.Int32FromUint8(uint8(v3)))
			goto _1
		_1:
			;
			s = s + 1
		}
		/* Update rankVal to reflect the new weights.
		 * All weights except 0 get moved to weight + scale.
		 * Weights [1, scale] are empty.
		 */
		s = targetTableLog
		for {
			if !(s > scale) {
				break
			}
			*(*U32)(unsafe.Pointer(rankVal + uintptr(s)*4)) = *(*U32)(unsafe.Pointer(rankVal + uintptr(s-scale)*4))
			goto _4
		_4:
			;
			s = s - 1
		}
		s = scale
		for {
			if !(s > uint32(0)) {
				break
			}
			*(*U32)(unsafe.Pointer(rankVal + uintptr(s)*4)) = uint32(0)
			goto _5
		_5:
			;
			s = s - 1
		}
	}
	return targetTableLog
}

type HUF_ReadDTableX1_Workspace = struct {
	FrankVal    [13]U32
	FrankStart  [13]U32
	FstatsWksp  [219]U32
	Fsymbols    [256]BYTE
	FhuffWeight [256]BYTE
}

func HUF_readDTableX1_wksp(tls *libc.TLS, DTable uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var D, D1 HUF_DEltX1
	var D4, D41, D42 U64
	var curr, maxTableLog, nextRankStart, targetTableLog, w2, v5 U32
	var dt, dtPtr, wksp, v6 uintptr
	var iSize, w, w1 size_t
	var length, n, nLimit, rankStart, s, symbol, symbolCount, u, u1, uStart, unroll int32
	var nbBits BYTE
	var v1 uint32
	var _ /* dtd at bp+8 */ DTableDesc
	var _ /* nbSymbols at bp+4 */ U32
	var _ /* tableLog at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = D, D1, D4, D41, D42, curr, dt, dtPtr, iSize, length, maxTableLog, n, nLimit, nbBits, nextRankStart, rankStart, s, symbol, symbolCount, targetTableLog, u, u1, uStart, unroll, w, w1, w2, wksp, v1, v5, v6
	*(*U32)(unsafe.Pointer(bp)) = uint32(0)
	*(*U32)(unsafe.Pointer(bp + 4)) = uint32(0)
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	wksp = workSpace
	_ = libc.Uint64FromInt64(1)
	if uint64(1492) > wkspSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	_ = libc.Uint64FromInt64(1)
	/* ZSTD_memset(huffWeight, 0, sizeof(huffWeight)); */ /* is not necessary, even though some analyzer complain ... */
	iSize = HUF_readStats_wksp(tls, wksp+1236, libc.Uint64FromInt32(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)), wksp, bp+4, bp, src, srcSize, wksp+104, uint64(876), flags)
	if ERR_isError(tls, iSize) != 0 {
		return iSize
	}
	/* Table header */
	*(*DTableDesc)(unsafe.Pointer(bp + 8)) = HUF_getDTableDesc(tls, DTable)
	maxTableLog = libc.Uint32FromInt32(libc.Int32FromUint8((*(*DTableDesc)(unsafe.Pointer(bp + 8))).FmaxTableLog) + int32(1))
	if maxTableLog < libc.Uint32FromInt32(libc.Int32FromInt32(HUF_DECODER_FAST_TABLELOG)) {
		v1 = maxTableLog
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(HUF_DECODER_FAST_TABLELOG))
	}
	targetTableLog = v1
	*(*U32)(unsafe.Pointer(bp)) = HUF_rescaleStats(tls, wksp+1236, wksp, *(*U32)(unsafe.Pointer(bp + 4)), *(*U32)(unsafe.Pointer(bp)), targetTableLog)
	if *(*U32)(unsafe.Pointer(bp)) > libc.Uint32FromInt32(libc.Int32FromUint8((*(*DTableDesc)(unsafe.Pointer(bp + 8))).FmaxTableLog)+libc.Int32FromInt32(1)) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	} /* DTable too small, Huffman tree cannot fit in */
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableType = uint8(0)
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableLog = uint8(*(*U32)(unsafe.Pointer(bp)))
	libc.Xmemcpy(tls, DTable, bp+8, libc.Uint64FromInt64(4))
	/* Compute symbols and rankStart given rankVal:
	 *
	 * rankVal already contains the number of values of each weight.
	 *
	 * symbols contains the symbols ordered by weight. First are the rankVal[0]
	 * weight 0 symbols, followed by the rankVal[1] weight 1 symbols, and so on.
	 * symbols[0] is filled (but unused) to avoid a branch.
	 *
	 * rankStart contains the offset where each rank belongs in the DTable.
	 * rankStart[0] is not filled because there are no entries in the table for
	 * weight 0.
	 */
	nextRankStart = uint32(0)
	unroll = int32(4)
	nLimit = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(bp + 4))) - unroll + int32(1)
	n = 0
	for {
		if !(n < libc.Int32FromUint32(*(*U32)(unsafe.Pointer(bp)))+int32(1)) {
			break
		}
		curr = nextRankStart
		nextRankStart = nextRankStart + *(*U32)(unsafe.Pointer(wksp + uintptr(n)*4))
		*(*U32)(unsafe.Pointer(wksp + 52 + uintptr(n)*4)) = curr
		goto _2
	_2:
		;
		n = n + 1
	}
	n = 0
	for {
		if !(n < nLimit) {
			break
		}
		u = 0
		for {
			if !(u < unroll) {
				break
			}
			w = uint64(*(*BYTE)(unsafe.Pointer(wksp + 1236 + uintptr(n+u))))
			v6 = wksp + 52 + uintptr(w)*4
			v5 = *(*U32)(unsafe.Pointer(v6))
			*(*U32)(unsafe.Pointer(v6)) = *(*U32)(unsafe.Pointer(v6)) + 1
			*(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(v5))) = libc.Uint8FromInt32(n + u)
			goto _4
		_4:
			;
			u = u + 1
		}
		goto _3
	_3:
		;
		n = n + unroll
	}
	for {
		if !(n < libc.Int32FromUint32(*(*U32)(unsafe.Pointer(bp + 4)))) {
			break
		}
		w1 = uint64(*(*BYTE)(unsafe.Pointer(wksp + 1236 + uintptr(n))))
		v6 = wksp + 52 + uintptr(w1)*4
		v5 = *(*U32)(unsafe.Pointer(v6))
		*(*U32)(unsafe.Pointer(v6)) = *(*U32)(unsafe.Pointer(v6)) + 1
		*(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(v5))) = libc.Uint8FromInt32(n)
		goto _7
	_7:
		;
		n = n + 1
	}
	/* fill DTable
	 * We fill all entries of each weight in order.
	 * That way length is a constant for each iteration of the outer loop.
	 * We can switch based on the length to a different inner loop which is
	 * optimized for that particular case.
	 */
	symbol = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(wksp)))
	rankStart = 0
	w2 = uint32(1)
	for {
		if !(w2 < *(*U32)(unsafe.Pointer(bp))+uint32(1)) {
			break
		}
		symbolCount = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(wksp + uintptr(w2)*4)))
		length = int32(1) << w2 >> int32(1)
		uStart = rankStart
		nbBits = uint8(*(*U32)(unsafe.Pointer(bp)) + libc.Uint32FromInt32(1) - w2)
		switch length {
		case int32(1):
			goto _11
		case int32(2):
			goto _12
		case int32(4):
			goto _13
		case int32(8):
			goto _14
		default:
			goto _15
		}
		goto _16
	_11:
		;
		s = 0
	_19:
		;
		if !(s < symbolCount) {
			goto _17
		}
		D.Fbyte1 = *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s)))
		D.FnbBits = nbBits
		*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(uStart)*2)) = D
		uStart = uStart + int32(1)
		goto _18
	_18:
		;
		s = s + 1
		goto _19
		goto _17
	_17:
		;
		goto _16
	_12:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D1.Fbyte1 = *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s)))
			D1.FnbBits = nbBits
			*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(uStart+0)*2)) = D1
			*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(uStart+int32(1))*2)) = D1
			uStart = uStart + int32(2)
			goto _20
		_20:
			;
			s = s + 1
		}
		goto _16
	_13:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D4 = HUF_DEltX1_set4(tls, *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s))), nbBits)
			MEM_write64(tls, dt+uintptr(uStart)*2, D4)
			uStart = uStart + int32(4)
			goto _21
		_21:
			;
			s = s + 1
		}
		goto _16
	_14:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D41 = HUF_DEltX1_set4(tls, *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s))), nbBits)
			MEM_write64(tls, dt+uintptr(uStart)*2, D41)
			MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(4)*2, D41)
			uStart = uStart + int32(8)
			goto _22
		_22:
			;
			s = s + 1
		}
		goto _16
	_15:
		;
		s = 0
		for {
			if !(s < symbolCount) {
				break
			}
			D42 = HUF_DEltX1_set4(tls, *(*BYTE)(unsafe.Pointer(wksp + 980 + uintptr(symbol+s))), nbBits)
			u1 = 0
			for {
				if !(u1 < length) {
					break
				}
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(0)*2, D42)
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(4)*2, D42)
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(8)*2, D42)
				MEM_write64(tls, dt+uintptr(uStart)*2+uintptr(u1)*2+uintptr(12)*2, D42)
				goto _24
			_24:
				;
				u1 = u1 + int32(16)
			}
			uStart = uStart + length
			goto _23
		_23:
			;
			s = s + 1
		}
		goto _16
	_16:
		;
		symbol = symbol + symbolCount
		rankStart = rankStart + symbolCount*length
		goto _10
	_10:
		;
		w2 = w2 + 1
	}
	return iSize
}

func HUF_decodeSymbolX1(tls *libc.TLS, Dstream uintptr, dt uintptr, dtLog U32) (r BYTE) {
	var c BYTE
	var val size_t
	_, _ = c, val
	val = BIT_lookBitsFast(tls, Dstream, dtLog) /* note : dtLog >= 1 */
	c = (*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(val)*2))).Fbyte1
	BIT_skipBits(tls, Dstream, uint32((*(*HUF_DEltX1)(unsafe.Pointer(dt + uintptr(val)*2))).FnbBits))
	return c
}

func HUF_decodeStreamX1(tls *libc.TLS, p uintptr, bitDPtr uintptr, pEnd uintptr, dt uintptr, dtLog U32) (r size_t) {
	var pStart, v1 uintptr
	_, _ = pStart, v1
	pStart = p
	/* up to 4 symbols at a time */
	if int64(pEnd)-int64(p) > int64(3) {
		for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd-uintptr(3)) != 0 {
			if MEM_64bits(tls) != 0 {
				v1 = p
				p = p + 1
				*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v1 = p
				p = p + 1
				*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v1 = p
				p = p + 1
				*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
			}
			v1 = p
			p = p + 1
			*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
		}
	} else {
		BIT_reloadDStream(tls, bitDPtr)
	}
	/* [0-3] symbols remaining */
	if MEM_32bits(tls) != 0 {
		for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd) != 0 {
			v1 = p
			p = p + 1
			*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
		}
	}
	/* no more data to retrieve from bitstream, no need to reload */
	for p < pEnd {
		v1 = p
		p = p + 1
		*(*BYTE)(unsafe.Pointer(v1)) = HUF_decodeSymbolX1(tls, bitDPtr, dt, dtLog)
	}
	return libc.Uint64FromInt64(int64(pEnd) - int64(pStart))
}

func HUF_decompress1X1_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var _var_err__ size_t
	var dt, dtPtr, oend, op uintptr
	var dtLog U32
	var dtd DTableDesc
	var _ /* bitD at bp+0 */ BIT_DStream_t
	_, _, _, _, _, _, _ = _var_err__, dt, dtLog, dtPtr, dtd, oend, op
	op = dst
	oend = ZSTD_maybeNullPtrAdd(tls, op, libc.Int64FromUint64(dstSize))
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	dtd = HUF_getDTableDesc(tls, DTable)
	dtLog = uint32(dtd.FtableLog)
	_var_err__ = BIT_initDStream(tls, bp, cSrc, cSrcSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	HUF_decodeStreamX1(tls, op, bp, oend, dt, dtLog)
	if !(BIT_endOfDStream(tls, bp) != 0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	return dstSize
}

// C documentation
//
//	/* HUF_decompress4X1_usingDTable_internal_body():
//	 * Conditions :
//	 * @dstSize >= 6
//	 */
func HUF_decompress4X1_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(160)
	defer tls.Free(160)
	var _var_err__, _var_err__1, _var_err__2, _var_err__3, length1, length2, length3, length4, segmentSize size_t
	var dt, dtPtr, istart, istart1, istart2, istart3, istart4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart, v2 uintptr
	var dtLog, endCheck, endSignal U32
	var dtd DTableDesc
	var _ /* bitD1 at bp+0 */ BIT_DStream_t
	var _ /* bitD2 at bp+40 */ BIT_DStream_t
	var _ /* bitD3 at bp+80 */ BIT_DStream_t
	var _ /* bitD4 at bp+120 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _var_err__, _var_err__1, _var_err__2, _var_err__3, dt, dtLog, dtPtr, dtd, endCheck, endSignal, istart, istart1, istart2, istart3, istart4, length1, length2, length3, length4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart, segmentSize, v2
	/* Check */
	if cSrcSize < uint64(10) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* strict minimum : jump table + 1 byte per stream */
	if dstSize < uint64(6) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* stream 4-split doesn't work */
	istart = cSrc
	ostart = dst
	oend = ostart + uintptr(dstSize)
	olimit = oend - uintptr(3)
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	length1 = uint64(MEM_readLE16(tls, istart))
	length2 = uint64(MEM_readLE16(tls, istart+uintptr(2)))
	length3 = uint64(MEM_readLE16(tls, istart+uintptr(4)))
	length4 = cSrcSize - (length1 + length2 + length3 + uint64(6))
	istart1 = istart + uintptr(6) /* jumpTable */
	istart2 = istart1 + uintptr(length1)
	istart3 = istart2 + uintptr(length2)
	istart4 = istart3 + uintptr(length3)
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	opStart2 = ostart + uintptr(segmentSize)
	opStart3 = opStart2 + uintptr(segmentSize)
	opStart4 = opStart3 + uintptr(segmentSize)
	op1 = ostart
	op2 = opStart2
	op3 = opStart3
	op4 = opStart4
	dtd = HUF_getDTableDesc(tls, DTable)
	dtLog = uint32(dtd.FtableLog)
	endSignal = uint32(1)
	if length4 > cSrcSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	if opStart4 > oend {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	/* validated above */
	_var_err__ = BIT_initDStream(tls, bp, istart1, length1)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	_var_err__1 = BIT_initDStream(tls, bp+40, istart2, length2)
	if ERR_isError(tls, _var_err__1) != 0 {
		return _var_err__1
	}
	_var_err__2 = BIT_initDStream(tls, bp+80, istart3, length3)
	if ERR_isError(tls, _var_err__2) != 0 {
		return _var_err__2
	}
	_var_err__3 = BIT_initDStream(tls, bp+120, istart4, length4)
	if ERR_isError(tls, _var_err__3) != 0 {
		return _var_err__3
	}
	/* up to 16 symbols per loop (4 symbols per stream) in 64-bit mode */
	if libc.Uint64FromInt64(int64(oend)-int64(op4)) >= uint64(8) {
		for {
			if !(endSignal&libc.BoolUint32(op4 < olimit) != 0) {
				break
			}
			if MEM_64bits(tls) != 0 {
				v2 = op1
				op1 = op1 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op2
				op2 = op2 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op3
				op3 = op3 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op4
				op4 = op4 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op1
				op1 = op1 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op2
				op2 = op2 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op3
				op3 = op3 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				v2 = op4
				op4 = op4 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op1
				op1 = op1 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op2
				op2 = op2 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op3
				op3 = op3 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			}
			if MEM_64bits(tls) != 0 {
				v2 = op4
				op4 = op4 + 1
				*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			}
			v2 = op1
			op1 = op1 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp, dt, dtLog)
			v2 = op2
			op2 = op2 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+40, dt, dtLog)
			v2 = op3
			op3 = op3 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+80, dt, dtLog)
			v2 = op4
			op4 = op4 + 1
			*(*BYTE)(unsafe.Pointer(v2)) = HUF_decodeSymbolX1(tls, bp+120, dt, dtLog)
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp) == int32(BIT_DStream_unfinished))
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+40) == int32(BIT_DStream_unfinished))
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+80) == int32(BIT_DStream_unfinished))
			endSignal = endSignal & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+120) == int32(BIT_DStream_unfinished))
			goto _1
		_1:
		}
	}
	/* check corruption */
	/* note : should not be necessary : op# advance in lock step, and we control op4.
	 *        but curiously, binary generated by gcc 7.2 & 7.3 with -mbmi2 runs faster when >=1 test is present */
	if op1 > opStart2 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if op2 > opStart3 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if op3 > opStart4 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* note : op4 supposed already verified within main loop */
	/* finish bitStreams one by one */
	HUF_decodeStreamX1(tls, op1, bp, opStart2, dt, dtLog)
	HUF_decodeStreamX1(tls, op2, bp+40, opStart3, dt, dtLog)
	HUF_decodeStreamX1(tls, op3, bp+80, opStart4, dt, dtLog)
	HUF_decodeStreamX1(tls, op4, bp+120, oend, dt, dtLog)
	/* check */
	endCheck = BIT_endOfDStream(tls, bp) & BIT_endOfDStream(tls, bp+40) & BIT_endOfDStream(tls, bp+80) & BIT_endOfDStream(tls, bp+120)
	if !(endCheck != 0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* decoded size */
	return dstSize
	return r
}

func HUF_decompress4X1_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_usingDTable_internal_fast_c_loop(tls *libc.TLS, args uintptr) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var ctz, ctz1, ctz2, ctz3, entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry3, entry4, entry5, entry6, entry7, entry8, entry9, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index3, index4, index5, index6, index7, index8, index9, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, stream int32
	var dtable, ilowest, oend, olimit uintptr
	var iiters, iters, oiters, symbols size_t
	var v3 uint64
	var _ /* bits at bp+0 */ [4]U64
	var _ /* ip at bp+32 */ [4]uintptr
	var _ /* op at bp+64 */ [4]uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = ctz, ctz1, ctz2, ctz3, dtable, entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry3, entry4, entry5, entry6, entry7, entry8, entry9, iiters, ilowest, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index3, index4, index5, index6, index7, index8, index9, iters, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, oend, oiters, olimit, stream, symbols, v3
	dtable = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Fdt
	oend = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Foend
	ilowest = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest
	/* Copy the arguments to local variables */
	libc.Xmemcpy(tls, bp, args+64, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+32, args, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+64, args+32, libc.Uint64FromInt64(32))
	for {
		/* Assert loop preconditions */
		stream = 0
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _2
		_2:
			;
			stream = stream + 1
		}
		/* Compute olimit */
		/* Each iteration produces 5 output symbols per stream */
		oiters = libc.Uint64FromInt64(int64(oend)-int64((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)])) / uint64(5)
		/* Each iteration consumes up to 11 bits * 5 = 55 bits < 7 bytes
		 * per stream.
		 */
		iiters = libc.Uint64FromInt64(int64((*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0])-int64(ilowest)) / uint64(7)
		if oiters < iiters {
			v3 = oiters
		} else {
			v3 = iiters
		}
		/* We can safely run iters iterations before running bounds checks */
		iters = v3
		symbols = iters * uint64(5)
		/* We can simply check that op[3] < olimit, instead of checking all
		 * of our bounds, since we can't hit the other bounds until we've run
		 * iters iterations, which only happens when op[3] == olimit.
		 */
		olimit = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + uintptr(symbols)
		/* Exit fast decoding loop once we reach the end. */
		if (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] == olimit {
			break
		}
		/* Exit the decoding loop if any input pointer has crossed the
		 * previous one. This indicates corruption, and a precondition
		 * to our loop is that ip[i] >= ip[0].
		 */
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			if (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream] < (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream-int32(1)] {
				goto _out
			}
			goto _4
		_4:
			;
			stream = stream + 1
		}
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _5
		_5:
			;
			stream = stream + 1
		}
		/* Manually unroll the loop because compilers don't consistently
		 * unroll the inner loops, which destroys performance.
		 */
		for cond := true; cond; cond = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] < olimit {
			/* Decode 5 symbols in each of the 4 streams */
			index = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(entry & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0])) = libc.Uint8FromInt32(entry >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index1 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry1 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index1)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(entry1 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)])) = libc.Uint8FromInt32(entry1 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index2 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry2 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index2)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(entry2 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)])) = libc.Uint8FromInt32(entry2 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index3 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry3 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index3)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(entry3 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)])) = libc.Uint8FromInt32(entry3 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index4 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry4 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index4)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(entry4 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 1)) = libc.Uint8FromInt32(entry4 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index5 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry5 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index5)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(entry5 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 1)) = libc.Uint8FromInt32(entry5 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index6 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry6 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index6)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(entry6 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 1)) = libc.Uint8FromInt32(entry6 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index7 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry7 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index7)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(entry7 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 1)) = libc.Uint8FromInt32(entry7 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index8 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry8 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index8)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(entry8 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 2)) = libc.Uint8FromInt32(entry8 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index9 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry9 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index9)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(entry9 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 2)) = libc.Uint8FromInt32(entry9 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index10 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry10 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index10)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(entry10 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 2)) = libc.Uint8FromInt32(entry10 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index11 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry11 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index11)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(entry11 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 2)) = libc.Uint8FromInt32(entry11 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index12 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry12 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index12)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(entry12 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 3)) = libc.Uint8FromInt32(entry12 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index13 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry13 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index13)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(entry13 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 3)) = libc.Uint8FromInt32(entry13 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index14 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry14 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index14)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(entry14 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 3)) = libc.Uint8FromInt32(entry14 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index15 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry15 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index15)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(entry15 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 3)) = libc.Uint8FromInt32(entry15 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index16 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
			entry16 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index16)*2)))
			*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(entry16 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0] + 4)) = libc.Uint8FromInt32(entry16 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index17 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
			entry17 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index17)*2)))
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(entry17 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)] + 4)) = libc.Uint8FromInt32(entry17 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index18 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
			entry18 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index18)*2)))
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(entry18 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)] + 4)) = libc.Uint8FromInt32(entry18 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			index19 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
			entry19 = libc.Int32FromUint16(*(*U16)(unsafe.Pointer(dtable + uintptr(index19)*2)))
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(entry19 & libc.Int32FromInt32(0x3F))
			*(*BYTE)(unsafe.Pointer((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + 4)) = libc.Uint8FromInt32(entry19 >> libc.Int32FromInt32(8) & libc.Int32FromInt32(0xFF))
			/* Reload each of the 4 the bitstreams */
			ctz = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[0]))
			nbBits = ctz & int32(7)
			nbBytes = ctz >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32)) -= uintptr(nbBytes)
			(*(*[4]U64)(unsafe.Pointer(bp)))[0] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(nbBits)
			ctz1 = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)]))
			nbBits1 = ctz1 & int32(7)
			nbBytes1 = ctz1 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 1*8)) -= uintptr(nbBytes1)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(1)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(nbBits1)
			ctz2 = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)]))
			nbBits2 = ctz2 & int32(7)
			nbBytes2 = ctz2 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 2*8)) -= uintptr(nbBytes2)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(2)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(nbBits2)
			ctz3 = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)]))
			nbBits3 = ctz3 & int32(7)
			nbBytes3 = ctz3 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(5)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 3*8)) -= uintptr(nbBytes3)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(3)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(nbBits3)
		}
		goto _1
	_1:
	}
	goto _out
_out:
	;
	/* Save the final values of each of the state variables back to args. */
	libc.Xmemcpy(tls, args+64, bp, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args, bp+32, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args+32, bp+64, libc.Uint64FromInt64(32))
}

// C documentation
//
//	/**
//	 * @returns @p dstSize on success (>= 6)
//	 *          0 if the fallback implementation should be used
//	 *          An error if an error occurred
//	 */
func HUF_decompress4X1_usingDTable_internal_fast(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, __ccgo_fp_loopFn HUF_DecompressFastLoopFn) (r size_t) {
	bp := tls.Alloc(192)
	defer tls.Free(192)
	var dt, ilowest, oend, segmentEnd uintptr
	var err_code, err_code1, ret, segmentSize size_t
	var i int32
	var _ /* args at bp+0 */ HUF_DecompressFastArgs
	var _ /* bit at bp+152 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _ = dt, err_code, err_code1, i, ilowest, oend, ret, segmentEnd, segmentSize
	dt = DTable + uintptr(1)*4
	ilowest = cSrc
	oend = ZSTD_maybeNullPtrAdd(tls, dst, libc.Int64FromUint64(dstSize))
	ret = HUF_DecompressFastArgs_init(tls, bp, dst, dstSize, cSrc, cSrcSize, DTable)
	err_code = ret
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6721, 0)
		}
		return err_code
	}
	if ret == uint64(0) {
		return uint64(0)
	}
	(*(*func(*libc.TLS, uintptr))(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_loopFn})))(tls, bp)
	/* Our loop guarantees that ip[] >= ilowest and that we haven't
	 * overwritten any op[].
	 */
	_ = ilowest
	/* finish bit streams one by one. */
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	segmentEnd = dst
	i = 0
	for {
		if !(i < int32(4)) {
			break
		}
		if segmentSize <= libc.Uint64FromInt64(int64(oend)-int64(segmentEnd)) {
			segmentEnd = segmentEnd + uintptr(segmentSize)
		} else {
			segmentEnd = oend
		}
		err_code1 = HUF_initRemainingDStream(tls, bp+152, bp, i, segmentEnd)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6751, 0)
			}
			return err_code1
		}
		/* Decompress and validate that we've produced exactly the expected length. */
		*(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) += uintptr(HUF_decodeStreamX1(tls, *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)), bp+152, segmentEnd, dt, uint32(HUF_DECODER_FAST_TABLELOG)))
		if *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) != segmentEnd {
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	/* decoded size */
	return dstSize
}

func HUF_decompress1X1_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X1_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X1_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X1_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_decompress1X1_usingDTable_internal_bmi2(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	return HUF_decompress1X1_usingDTable_internal_default(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var fallbackFn HUF_DecompressUsingDTableFn
	var loopFn HUF_DecompressFastLoopFn
	var ret size_t
	_, _, _ = fallbackFn, loopFn, ret
	fallbackFn = __ccgo_fp(HUF_decompress4X1_usingDTable_internal_default)
	loopFn = __ccgo_fp(HUF_decompress4X1_usingDTable_internal_fast_c_loop)
	if flags&int32(HUF_flags_bmi2) != 0 {
		fallbackFn = __ccgo_fp(HUF_decompress4X1_usingDTable_internal_bmi2)
	} else {
		return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	if libc.Bool(int32(HUF_ENABLE_FAST_DECODE) != 0) && !(flags&int32(HUF_flags_disableFast) != 0) {
		ret = HUF_decompress4X1_usingDTable_internal_fast(tls, dst, dstSize, cSrc, cSrcSize, DTable, loopFn)
		if ret != uint64(0) {
			return ret
		}
	}
	return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X1_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX1_wksp(tls, dctx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress4X1_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, dctx, flags)
}

/* *************************/
/* double-symbols decoding */
/* *************************/

type HUF_DEltX2 = struct {
	Fsequence U16
	FnbBits   BYTE
	Flength   BYTE
}

/* double-symbols decoding */
type sortedSymbol_t = struct {
	Fsymbol BYTE
}

type rankValCol_t = [13]U32

type rankVal_t = [12]rankValCol_t

// C documentation
//
//	/**
//	 * Constructs a HUF_DEltX2 in a U32.
//	 */
func HUF_buildDEltX2U32(tls *libc.TLS, symbol U32, nbBits U32, baseSeq U32, level int32) (r U32) {
	var seq U32
	var v1 uint32
	_, _ = seq, v1
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	if MEM_isLittleEndian(tls) != 0 {
		if level == int32(1) {
			v1 = symbol
		} else {
			v1 = baseSeq + symbol<<libc.Int32FromInt32(8)
		}
		seq = v1
		return seq + nbBits<<libc.Int32FromInt32(16) + libc.Uint32FromInt32(level)<<libc.Int32FromInt32(24)
	} else {
		if level == int32(1) {
			v1 = symbol << libc.Int32FromInt32(8)
		} else {
			v1 = baseSeq<<libc.Int32FromInt32(8) + symbol
		}
		seq = v1
		return seq<<libc.Int32FromInt32(16) + nbBits<<libc.Int32FromInt32(8) + libc.Uint32FromInt32(level)
	}
	return r
}

// C documentation
//
//	/**
//	 * Constructs a HUF_DEltX2.
//	 */
func HUF_buildDEltX2(tls *libc.TLS, symbol U32, nbBits U32, baseSeq U32, level int32) (r HUF_DEltX2) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* DElt at bp+0 */ HUF_DEltX2
	var _ /* val at bp+4 */ U32
	*(*U32)(unsafe.Pointer(bp + 4)) = HUF_buildDEltX2U32(tls, symbol, nbBits, baseSeq, level)
	_ = libc.Uint64FromInt64(1)
	libc.Xmemcpy(tls, bp, bp+4, libc.Uint64FromInt64(4))
	return *(*HUF_DEltX2)(unsafe.Pointer(bp))
}

// C documentation
//
//	/**
//	 * Constructs 2 HUF_DEltX2s and packs them into a U64.
//	 */
func HUF_buildDEltX2U64(tls *libc.TLS, symbol U32, nbBits U32, baseSeq U16, level int32) (r U64) {
	var DElt U32
	_ = DElt
	DElt = HUF_buildDEltX2U32(tls, symbol, nbBits, uint32(baseSeq), level)
	return uint64(DElt) + uint64(DElt)<<libc.Int32FromInt32(32)
}

// C documentation
//
//	/**
//	 * Fills the DTable rank with all the symbols from [begin, end) that are each
//	 * nbBits long.
//	 *
//	 * @param DTableRank The start of the rank in the DTable.
//	 * @param begin The first symbol to fill (inclusive).
//	 * @param end The last symbol to fill (exclusive).
//	 * @param nbBits Each symbol is nbBits long.
//	 * @param tableLog The table log.
//	 * @param baseSeq If level == 1 { 0 } else { the first level symbol }
//	 * @param level The level in the table. Must be 1 or 2.
//	 */
func HUF_fillDTableX2ForWeight(tls *libc.TLS, DTableRank uintptr, begin uintptr, end uintptr, nbBits U32, tableLog U32, baseSeq U16, level int32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var DElt, DElt1 HUF_DEltX2
	var DTableRankEnd, ptr, v10 uintptr
	var length U32
	var _ /* DEltX2 at bp+0 */ U64
	var _ /* DEltX2 at bp+16 */ U64
	var _ /* DEltX2 at bp+8 */ U64
	_, _, _, _, _, _ = DElt, DElt1, DTableRankEnd, length, ptr, v10
	length = uint32(1) << ((tableLog - nbBits) & uint32(0x1F))
	switch length {
	case uint32(1):
		goto _1
	case uint32(2):
		goto _2
	case uint32(4):
		goto _3
	case uint32(8):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
	ptr = begin
_9:
	;
	if !(ptr != end) {
		goto _7
	}
	DElt = HUF_buildDEltX2(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, uint32(baseSeq), level)
	v10 = DTableRank
	DTableRank += 4
	*(*HUF_DEltX2)(unsafe.Pointer(v10)) = DElt
	goto _8
_8:
	;
	ptr = ptr + 1
	goto _9
	goto _7
_7:
	;
	goto _6
_2:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		DElt1 = HUF_buildDEltX2(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, uint32(baseSeq), level)
		*(*HUF_DEltX2)(unsafe.Pointer(DTableRank)) = DElt1
		*(*HUF_DEltX2)(unsafe.Pointer(DTableRank + 1*4)) = DElt1
		DTableRank = DTableRank + uintptr(2)*4
		goto _11
	_11:
		;
		ptr = ptr + 1
	}
	goto _6
_3:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		*(*U64)(unsafe.Pointer(bp)) = HUF_buildDEltX2U64(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, baseSeq, level)
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(0)*4, bp, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(2)*4, bp, libc.Uint64FromInt64(8))
		DTableRank = DTableRank + uintptr(4)*4
		goto _12
	_12:
		;
		ptr = ptr + 1
	}
	goto _6
_4:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		*(*U64)(unsafe.Pointer(bp + 8)) = HUF_buildDEltX2U64(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, baseSeq, level)
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(0)*4, bp+8, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(2)*4, bp+8, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(4)*4, bp+8, libc.Uint64FromInt64(8))
		libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(6)*4, bp+8, libc.Uint64FromInt64(8))
		DTableRank = DTableRank + uintptr(8)*4
		goto _13
	_13:
		;
		ptr = ptr + 1
	}
	goto _6
_5:
	;
	ptr = begin
	for {
		if !(ptr != end) {
			break
		}
		*(*U64)(unsafe.Pointer(bp + 16)) = HUF_buildDEltX2U64(tls, uint32((*sortedSymbol_t)(unsafe.Pointer(ptr)).Fsymbol), nbBits, baseSeq, level)
		DTableRankEnd = DTableRank + uintptr(length)*4
		for {
			if !(DTableRank != DTableRankEnd) {
				break
			}
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(0)*4, bp+16, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(2)*4, bp+16, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(4)*4, bp+16, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTableRank+libc.UintptrFromInt32(6)*4, bp+16, libc.Uint64FromInt64(8))
			goto _15
		_15:
			;
			DTableRank = DTableRank + uintptr(8)*4
		}
		goto _14
	_14:
		;
		ptr = ptr + 1
	}
	goto _6
_6:
}

// C documentation
//
//	/* HUF_fillDTableX2Level2() :
//	 * `rankValOrigin` must be a table of at least (HUF_TABLELOG_MAX + 1) U32 */
func HUF_fillDTableX2Level2(tls *libc.TLS, DTable uintptr, targetLog U32, consumedBits U32, rankVal uintptr, minWeight int32, maxWeight1 int32, sortedSymbols uintptr, rankStart uintptr, nbBitsBaseline U32, baseSeq U16) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var begin, end, i, skipSize, w int32
	var length, nbBits, totalBits U32
	var _ /* DEltX2 at bp+0 */ U64
	_, _, _, _, _, _, _, _ = begin, end, i, length, nbBits, skipSize, totalBits, w
	/* Fill skipped values (all positions up to rankVal[minWeight]).
	 * These are positions only get a single symbol because the combined weight
	 * is too large.
	 */
	if minWeight > int32(1) {
		length = uint32(1) << ((targetLog - consumedBits) & uint32(0x1F))
		*(*U64)(unsafe.Pointer(bp)) = HUF_buildDEltX2U64(tls, uint32(baseSeq), consumedBits, uint16(0), int32(1))
		skipSize = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(rankVal + uintptr(minWeight)*4)))
		switch length {
		case uint32(2):
			libc.Xmemcpy(tls, DTable, bp, libc.Uint64FromInt64(8))
		case uint32(4):
			libc.Xmemcpy(tls, DTable+libc.UintptrFromInt32(0)*4, bp, libc.Uint64FromInt64(8))
			libc.Xmemcpy(tls, DTable+libc.UintptrFromInt32(2)*4, bp, libc.Uint64FromInt64(8))
		default:
			i = 0
			for {
				if !(i < skipSize) {
					break
				}
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(0)*4, bp, libc.Uint64FromInt64(8))
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(2)*4, bp, libc.Uint64FromInt64(8))
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(4)*4, bp, libc.Uint64FromInt64(8))
				libc.Xmemcpy(tls, DTable+uintptr(i)*4+libc.UintptrFromInt32(6)*4, bp, libc.Uint64FromInt64(8))
				goto _1
			_1:
				;
				i = i + int32(8)
			}
		}
	}
	/* Fill each of the second level symbols by weight. */
	w = minWeight
	for {
		if !(w < maxWeight1) {
			break
		}
		begin = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w)*4)))
		end = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w+int32(1))*4)))
		nbBits = nbBitsBaseline - libc.Uint32FromInt32(w)
		totalBits = nbBits + consumedBits
		HUF_fillDTableX2ForWeight(tls, DTable+uintptr(*(*U32)(unsafe.Pointer(rankVal + uintptr(w)*4)))*4, sortedSymbols+uintptr(begin), sortedSymbols+uintptr(end), totalBits, targetLog, baseSeq, int32(2))
		goto _2
	_2:
		;
		w = w + 1
	}
}

func HUF_fillDTableX2(tls *libc.TLS, DTable uintptr, targetLog U32, sortedList uintptr, rankStart uintptr, rankValOrigin uintptr, maxWeight U32, nbBitsBaseline U32) {
	var begin, end, minWeight, s, scaleLog, start, w, wEnd int32
	var length, minBits, nbBits U32
	var rankVal uintptr
	_, _, _, _, _, _, _, _, _, _, _, _ = begin, end, length, minBits, minWeight, nbBits, rankVal, s, scaleLog, start, w, wEnd
	rankVal = rankValOrigin
	scaleLog = libc.Int32FromUint32(nbBitsBaseline - targetLog) /* note : targetLog >= srcLog, hence scaleLog <= 1 */
	minBits = nbBitsBaseline - maxWeight
	wEnd = libc.Int32FromUint32(maxWeight) + int32(1)
	/* Fill DTable in order of weight. */
	w = int32(1)
	for {
		if !(w < wEnd) {
			break
		}
		begin = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w)*4)))
		end = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(rankStart + uintptr(w+int32(1))*4)))
		nbBits = nbBitsBaseline - libc.Uint32FromInt32(w)
		if targetLog-nbBits >= minBits {
			/* Enough room for a second symbol. */
			start = libc.Int32FromUint32(*(*U32)(unsafe.Pointer(rankVal + uintptr(w)*4)))
			length = uint32(1) << ((targetLog - nbBits) & uint32(0x1F))
			minWeight = libc.Int32FromUint32(nbBits + libc.Uint32FromInt32(scaleLog))
			if minWeight < int32(1) {
				minWeight = int32(1)
			}
			/* Fill the DTable for every symbol of weight w.
			 * These symbols get at least 1 second symbol.
			 */
			s = begin
			for {
				if !(s != end) {
					break
				}
				HUF_fillDTableX2Level2(tls, DTable+uintptr(start)*4, targetLog, nbBits, rankValOrigin+uintptr(nbBits)*52, minWeight, wEnd, sortedList, rankStart, nbBitsBaseline, uint16((*(*sortedSymbol_t)(unsafe.Pointer(sortedList + uintptr(s)))).Fsymbol))
				start = libc.Int32FromUint32(uint32(start) + length)
				goto _2
			_2:
				;
				s = s + 1
			}
		} else {
			/* Only a single symbol. */
			HUF_fillDTableX2ForWeight(tls, DTable+uintptr(*(*U32)(unsafe.Pointer(rankVal + uintptr(w)*4)))*4, sortedList+uintptr(begin), sortedList+uintptr(end), nbBits, targetLog, uint16(0), int32(1))
		}
		goto _1
	_1:
		;
		w = w + 1
	}
}

type HUF_ReadDTableX2_Workspace = struct {
	FrankVal      [12]rankValCol_t
	FrankStats    [13]U32
	FrankStart0   [15]U32
	FsortedSymbol [256]sortedSymbol_t
	FweightList   [256]BYTE
	FcalleeWksp   [219]U32
}

func HUF_readDTableX2_wksp(tls *libc.TLS, DTable uintptr, src uintptr, srcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r1 size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var consumed, curr, curr1, maxTableLog, maxW, minBits, nextRankStart, nextRankVal, r, s, w, w1, w2, w3, v4 U32
	var dt, dtPtr, rankStart, rankVal0, rankValPtr, wksp, v5 uintptr
	var iSize size_t
	var rescale int32
	var _ /* dtd at bp+8 */ DTableDesc
	var _ /* nbSymbols at bp+4 */ U32
	var _ /* tableLog at bp+0 */ U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = consumed, curr, curr1, dt, dtPtr, iSize, maxTableLog, maxW, minBits, nextRankStart, nextRankVal, r, rankStart, rankVal0, rankValPtr, rescale, s, w, w1, w2, w3, wksp, v4, v5
	*(*DTableDesc)(unsafe.Pointer(bp + 8)) = HUF_getDTableDesc(tls, DTable)
	maxTableLog = uint32((*(*DTableDesc)(unsafe.Pointer(bp + 8))).FmaxTableLog)
	dtPtr = DTable + uintptr(1)*4 /* force compiler to avoid strict-aliasing */
	dt = dtPtr
	wksp = workSpace
	if uint64(2124) > wkspSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	rankStart = wksp + 676 + uintptr(1)*4
	libc.Xmemset(tls, wksp+624, 0, libc.Uint64FromInt64(52))
	libc.Xmemset(tls, wksp+676, 0, libc.Uint64FromInt64(60))
	_ = libc.Uint64FromInt64(1) /* if compiler fails here, assertion is wrong */
	if maxTableLog > uint32(HUF_TABLELOG_MAX) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	}
	/* ZSTD_memset(weightList, 0, sizeof(weightList)); */ /* is not necessary, even though some analyzer complain ... */
	iSize = HUF_readStats_wksp(tls, wksp+992, libc.Uint64FromInt32(libc.Int32FromInt32(HUF_SYMBOLVALUE_MAX)+libc.Int32FromInt32(1)), wksp+624, bp+4, bp, src, srcSize, wksp+1248, uint64(876), flags)
	if ERR_isError(tls, iSize) != 0 {
		return iSize
	}
	/* check result */
	if *(*U32)(unsafe.Pointer(bp)) > maxTableLog {
		return libc.Uint64FromInt32(-int32(ZSTD_error_tableLog_tooLarge))
	} /* DTable can't fit code depth */
	if *(*U32)(unsafe.Pointer(bp)) <= uint32(HUF_DECODER_FAST_TABLELOG) && maxTableLog > uint32(HUF_DECODER_FAST_TABLELOG) {
		maxTableLog = uint32(HUF_DECODER_FAST_TABLELOG)
	}
	/* find maxWeight */
	maxW = *(*U32)(unsafe.Pointer(bp))
	for {
		if !(*(*U32)(unsafe.Pointer(wksp + 624 + uintptr(maxW)*4)) == uint32(0)) {
			break
		}
		goto _1
	_1:
		;
		maxW = maxW - 1
	} /* necessarily finds a solution before 0 */
	/* Get start index of each weight */
	nextRankStart = uint32(0)
	w = uint32(1)
	for {
		if !(w < maxW+uint32(1)) {
			break
		}
		curr = nextRankStart
		nextRankStart = nextRankStart + *(*U32)(unsafe.Pointer(wksp + 624 + uintptr(w)*4))
		*(*U32)(unsafe.Pointer(rankStart + uintptr(w)*4)) = curr
		goto _2
	_2:
		;
		w = w + 1
	}
	*(*U32)(unsafe.Pointer(rankStart)) = nextRankStart /* put all 0w symbols at the end of sorted list*/
	*(*U32)(unsafe.Pointer(rankStart + uintptr(maxW+uint32(1))*4)) = nextRankStart
	/* sort symbols by weight */
	s = uint32(0)
	for {
		if !(s < *(*U32)(unsafe.Pointer(bp + 4))) {
			break
		}
		w1 = uint32(*(*BYTE)(unsafe.Pointer(wksp + 992 + uintptr(s))))
		v5 = rankStart + uintptr(w1)*4
		v4 = *(*U32)(unsafe.Pointer(v5))
		*(*U32)(unsafe.Pointer(v5)) = *(*U32)(unsafe.Pointer(v5)) + 1
		r = v4
		(*(*sortedSymbol_t)(unsafe.Pointer(wksp + 736 + uintptr(r)))).Fsymbol = uint8(s)
		goto _3
	_3:
		;
		s = s + 1
	}
	*(*U32)(unsafe.Pointer(rankStart)) = uint32(0) /* forget 0w symbols; this is beginning of weight(1) */
	/* Build rankVal */
	rankVal0 = wksp
	rescale = libc.Int32FromUint32(maxTableLog - *(*U32)(unsafe.Pointer(bp)) - uint32(1)) /* tableLog <= maxTableLog */
	nextRankVal = uint32(0)
	w2 = uint32(1)
	for {
		if !(w2 < maxW+uint32(1)) {
			break
		}
		curr1 = nextRankVal
		nextRankVal = nextRankVal + *(*U32)(unsafe.Pointer(wksp + 624 + uintptr(w2)*4))<<(w2+libc.Uint32FromInt32(rescale))
		*(*U32)(unsafe.Pointer(rankVal0 + uintptr(w2)*4)) = curr1
		goto _6
	_6:
		;
		w2 = w2 + 1
	}
	minBits = *(*U32)(unsafe.Pointer(bp)) + uint32(1) - maxW
	consumed = minBits
	for {
		if !(consumed < maxTableLog-minBits+uint32(1)) {
			break
		}
		rankValPtr = wksp + uintptr(consumed)*52
		w3 = uint32(1)
		for {
			if !(w3 < maxW+uint32(1)) {
				break
			}
			*(*U32)(unsafe.Pointer(rankValPtr + uintptr(w3)*4)) = *(*U32)(unsafe.Pointer(rankVal0 + uintptr(w3)*4)) >> consumed
			goto _8
		_8:
			;
			w3 = w3 + 1
		}
		goto _7
	_7:
		;
		consumed = consumed + 1
	}
	HUF_fillDTableX2(tls, dt, maxTableLog, wksp+736, wksp+676, wksp, maxW, *(*U32)(unsafe.Pointer(bp))+uint32(1))
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableLog = uint8(maxTableLog)
	(*(*DTableDesc)(unsafe.Pointer(bp + 8))).FtableType = uint8(1)
	libc.Xmemcpy(tls, DTable, bp+8, libc.Uint64FromInt64(4))
	return iSize
}

func HUF_decodeSymbolX2(tls *libc.TLS, op uintptr, DStream uintptr, dt uintptr, dtLog U32) (r U32) {
	var val size_t
	_ = val
	val = BIT_lookBitsFast(tls, DStream, dtLog) /* note : dtLog >= 1 */
	libc.Xmemcpy(tls, op, dt+uintptr(val)*4, libc.Uint64FromInt32(libc.Int32FromInt32(2)))
	BIT_skipBits(tls, DStream, uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).FnbBits))
	return uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).Flength)
}

func HUF_decodeLastSymbolX2(tls *libc.TLS, op uintptr, DStream uintptr, dt uintptr, dtLog U32) (r U32) {
	var val size_t
	_ = val
	val = BIT_lookBitsFast(tls, DStream, dtLog) /* note : dtLog >= 1 */
	libc.Xmemcpy(tls, op, dt+uintptr(val)*4, libc.Uint64FromInt32(libc.Int32FromInt32(1)))
	if libc.Int32FromUint8((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).Flength) == int32(1) {
		BIT_skipBits(tls, DStream, uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).FnbBits))
	} else {
		if uint64((*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed) < libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) {
			BIT_skipBits(tls, DStream, uint32((*(*HUF_DEltX2)(unsafe.Pointer(dt + uintptr(val)*4))).FnbBits))
			if uint64((*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed) > libc.Uint64FromInt64(8)*libc.Uint64FromInt32(8) {
				/* ugly hack; works only because it's the last symbol. Note : can't easily extract nbBits from just this symbol */
				(*BIT_DStream_t)(unsafe.Pointer(DStream)).FbitsConsumed = uint32(libc.Uint64FromInt64(8) * libc.Uint64FromInt32(8))
			}
		}
	}
	return uint32(1)
}

func HUF_decodeStreamX2(tls *libc.TLS, p uintptr, bitDPtr uintptr, pEnd uintptr, dt uintptr, dtLog U32) (r size_t) {
	var pStart uintptr
	_ = pStart
	pStart = p
	/* up to 8 symbols at a time */
	if libc.Uint64FromInt64(int64(pEnd)-int64(p)) >= uint64(8) {
		if dtLog <= uint32(11) && MEM_64bits(tls) != 0 {
			/* up to 10 symbols at a time */
			for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd-uintptr(9)) != 0 {
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
			}
		} else {
			/* up to 8 symbols at a time */
			for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p < pEnd-uintptr(libc.Uint64FromInt64(8)-libc.Uint64FromInt32(1))) != 0 {
				if MEM_64bits(tls) != 0 {
					p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				}
				if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
					p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				}
				if MEM_64bits(tls) != 0 {
					p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
				}
				p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
			}
		}
	} else {
		BIT_reloadDStream(tls, bitDPtr)
	}
	/* closer to end : up to 2 symbols at a time */
	if libc.Uint64FromInt64(int64(pEnd)-int64(p)) >= uint64(2) {
		for libc.BoolInt32(BIT_reloadDStream(tls, bitDPtr) == int32(BIT_DStream_unfinished))&libc.BoolInt32(p <= pEnd-uintptr(2)) != 0 {
			p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
		}
		for p <= pEnd-uintptr(2) {
			p = p + uintptr(HUF_decodeSymbolX2(tls, p, bitDPtr, dt, dtLog))
		} /* no need to reload : reached the end of DStream */
	}
	if p < pEnd {
		p = p + uintptr(HUF_decodeLastSymbolX2(tls, p, bitDPtr, dt, dtLog))
	}
	return libc.Uint64FromInt64(int64(p) - int64(pStart))
}

func HUF_decompress1X2_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var _var_err__ size_t
	var dt, dtPtr, oend, ostart uintptr
	var dtd DTableDesc
	var _ /* bitD at bp+0 */ BIT_DStream_t
	_, _, _, _, _, _ = _var_err__, dt, dtPtr, dtd, oend, ostart
	/* Init */
	_var_err__ = BIT_initDStream(tls, bp, cSrc, cSrcSize)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	/* decode */
	ostart = dst
	oend = ZSTD_maybeNullPtrAdd(tls, ostart, libc.Int64FromUint64(dstSize))
	dtPtr = DTable + uintptr(1)*4 /* force compiler to not use strict-aliasing */
	dt = dtPtr
	dtd = HUF_getDTableDesc(tls, DTable)
	HUF_decodeStreamX2(tls, ostart, bp, oend, dt, uint32(dtd.FtableLog))
	/* check */
	if !(BIT_endOfDStream(tls, bp) != 0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* decoded size */
	return dstSize
}

// C documentation
//
//	/* HUF_decompress4X2_usingDTable_internal_body():
//	 * Conditions:
//	 * @dstSize >= 6
//	 */
func HUF_decompress4X2_usingDTable_internal_body(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	bp := tls.Alloc(160)
	defer tls.Free(160)
	var _var_err__, _var_err__1, _var_err__2, _var_err__3, length1, length2, length3, length4, segmentSize size_t
	var dt, dtPtr, istart, istart1, istart2, istart3, istart4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart uintptr
	var dtLog, endCheck, endSignal U32
	var dtd DTableDesc
	var _ /* bitD1 at bp+0 */ BIT_DStream_t
	var _ /* bitD2 at bp+40 */ BIT_DStream_t
	var _ /* bitD3 at bp+80 */ BIT_DStream_t
	var _ /* bitD4 at bp+120 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _var_err__, _var_err__1, _var_err__2, _var_err__3, dt, dtLog, dtPtr, dtd, endCheck, endSignal, istart, istart1, istart2, istart3, istart4, length1, length2, length3, length4, oend, olimit, op1, op2, op3, op4, opStart2, opStart3, opStart4, ostart, segmentSize
	if cSrcSize < uint64(10) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* strict minimum : jump table + 1 byte per stream */
	if dstSize < uint64(6) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* stream 4-split doesn't work */
	istart = cSrc
	ostart = dst
	oend = ostart + uintptr(dstSize)
	olimit = oend - uintptr(libc.Uint64FromInt64(8)-libc.Uint64FromInt32(1))
	dtPtr = DTable + uintptr(1)*4
	dt = dtPtr
	length1 = uint64(MEM_readLE16(tls, istart))
	length2 = uint64(MEM_readLE16(tls, istart+uintptr(2)))
	length3 = uint64(MEM_readLE16(tls, istart+uintptr(4)))
	length4 = cSrcSize - (length1 + length2 + length3 + uint64(6))
	istart1 = istart + uintptr(6) /* jumpTable */
	istart2 = istart1 + uintptr(length1)
	istart3 = istart2 + uintptr(length2)
	istart4 = istart3 + uintptr(length3)
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	opStart2 = ostart + uintptr(segmentSize)
	opStart3 = opStart2 + uintptr(segmentSize)
	opStart4 = opStart3 + uintptr(segmentSize)
	op1 = ostart
	op2 = opStart2
	op3 = opStart3
	op4 = opStart4
	endSignal = uint32(1)
	dtd = HUF_getDTableDesc(tls, DTable)
	dtLog = uint32(dtd.FtableLog)
	if length4 > cSrcSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	if opStart4 > oend {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* overflow */
	_var_err__ = BIT_initDStream(tls, bp, istart1, length1)
	if ERR_isError(tls, _var_err__) != 0 {
		return _var_err__
	}
	_var_err__1 = BIT_initDStream(tls, bp+40, istart2, length2)
	if ERR_isError(tls, _var_err__1) != 0 {
		return _var_err__1
	}
	_var_err__2 = BIT_initDStream(tls, bp+80, istart3, length3)
	if ERR_isError(tls, _var_err__2) != 0 {
		return _var_err__2
	}
	_var_err__3 = BIT_initDStream(tls, bp+120, istart4, length4)
	if ERR_isError(tls, _var_err__3) != 0 {
		return _var_err__3
	}
	/* 16-32 symbols per loop (4-8 symbols per stream) */
	if libc.Uint64FromInt64(int64(oend)-int64(op4)) >= uint64(8) {
		for {
			if !(endSignal&libc.BoolUint32(op4 < olimit) != 0) {
				break
			}
			if MEM_64bits(tls) != 0 {
				op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 || libc.Bool(int32(HUF_TABLELOG_MAX) <= int32(12)) {
				op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			}
			if MEM_64bits(tls) != 0 {
				op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			}
			op1 = op1 + uintptr(HUF_decodeSymbolX2(tls, op1, bp, dt, dtLog))
			op2 = op2 + uintptr(HUF_decodeSymbolX2(tls, op2, bp+40, dt, dtLog))
			op3 = op3 + uintptr(HUF_decodeSymbolX2(tls, op3, bp+80, dt, dtLog))
			op4 = op4 + uintptr(HUF_decodeSymbolX2(tls, op4, bp+120, dt, dtLog))
			endSignal = libc.Uint32FromInt64(libc.Int64FromUint32(libc.BoolUint32(BIT_reloadDStreamFast(tls, bp) == int32(BIT_DStream_unfinished)) & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+40) == int32(BIT_DStream_unfinished)) & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+80) == int32(BIT_DStream_unfinished)) & libc.BoolUint32(BIT_reloadDStreamFast(tls, bp+120) == int32(BIT_DStream_unfinished))))
			goto _1
		_1:
		}
	}
	/* check corruption */
	if op1 > opStart2 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if op2 > opStart3 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if op3 > opStart4 {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* note : op4 already verified within main loop */
	/* finish bitStreams one by one */
	HUF_decodeStreamX2(tls, op1, bp, opStart2, dt, dtLog)
	HUF_decodeStreamX2(tls, op2, bp+40, opStart3, dt, dtLog)
	HUF_decodeStreamX2(tls, op3, bp+80, opStart4, dt, dtLog)
	HUF_decodeStreamX2(tls, op4, bp+120, oend, dt, dtLog)
	/* check */
	endCheck = BIT_endOfDStream(tls, bp) & BIT_endOfDStream(tls, bp+40) & BIT_endOfDStream(tls, bp+80) & BIT_endOfDStream(tls, bp+120)
	if !(endCheck != 0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* decoded size */
	return dstSize
	return r
}

func HUF_decompress4X2_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X2_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress4X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress4X2_usingDTable_internal_fast_c_loop(tls *libc.TLS, args uintptr) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var ctz, ctz1, ctz2, ctz3, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index20, index21, index22, index23, index24, index3, index4, index5, index6, index7, index8, index9, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, stream int32
	var dtable, ilowest, olimit uintptr
	var entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry20, entry21, entry22, entry23, entry24, entry3, entry4, entry5, entry6, entry7, entry8, entry9 HUF_DEltX2
	var iters, oiters size_t
	var oend [4]uintptr
	var v4 uint64
	var _ /* bits at bp+0 */ [4]U64
	var _ /* ip at bp+32 */ [4]uintptr
	var _ /* op at bp+64 */ [4]uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = ctz, ctz1, ctz2, ctz3, dtable, entry, entry1, entry10, entry11, entry12, entry13, entry14, entry15, entry16, entry17, entry18, entry19, entry2, entry20, entry21, entry22, entry23, entry24, entry3, entry4, entry5, entry6, entry7, entry8, entry9, ilowest, index, index1, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index2, index20, index21, index22, index23, index24, index3, index4, index5, index6, index7, index8, index9, iters, nbBits, nbBits1, nbBits2, nbBits3, nbBytes, nbBytes1, nbBytes2, nbBytes3, oend, oiters, olimit, stream, v4
	dtable = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Fdt
	ilowest = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Filowest
	/* Copy the arguments to local registers. */
	libc.Xmemcpy(tls, bp, args+64, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+32, args, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, bp+64, args+32, libc.Uint64FromInt64(32))
	oend[0] = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)]
	oend[int32(1)] = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)]
	oend[int32(2)] = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)]
	oend[int32(3)] = (*HUF_DecompressFastArgs)(unsafe.Pointer(args)).Foend
	for {
		/* Assert loop preconditions */
		stream = 0
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _2
		_2:
			;
			stream = stream + 1
		}
		/* Compute olimit */
		/* Each loop does 5 table lookups for each of the 4 streams.
		 * Each table lookup consumes up to 11 bits of input, and produces
		 * up to 2 bytes of output.
		 */
		/* We can consume up to 7 bytes of input per iteration per stream.
		 * We also know that each input pointer is >= ip[0]. So we can run
		 * iters loops before running out of input.
		 */
		iters = libc.Uint64FromInt64(int64((*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0])-int64(ilowest)) / uint64(7)
		/* Each iteration can produce up to 10 bytes of output per stream.
		 * Each output stream my advance at different rates. So take the
		 * minimum number of safe iterations among all the output streams.
		 */
		stream = 0
		for {
			if !(stream < int32(4)) {
				break
			}
			oiters = libc.Uint64FromInt64(int64(oend[stream])-int64((*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[stream])) / uint64(10)
			if iters < oiters {
				v4 = iters
			} else {
				v4 = oiters
			}
			iters = v4
			goto _3
		_3:
			;
			stream = stream + 1
		}
		/* Each iteration produces at least 5 output symbols. So until
		 * op[3] crosses olimit, we know we haven't executed iters
		 * iterations yet. This saves us maintaining an iters counter,
		 * at the expense of computing the remaining # of iterations
		 * more frequently.
		 */
		olimit = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] + uintptr(iters*libc.Uint64FromInt32(5))
		/* Exit the fast decoding loop once we reach the end. */
		if (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] == olimit {
			break
		}
		/* Exit the decoding loop if any input pointer has crossed the
		 * previous one. This indicates corruption, and a precondition
		 * to our loop is that ip[i] >= ip[0].
		 */
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			if (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream] < (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[stream-int32(1)] {
				goto _out
			}
			goto _5
		_5:
			;
			stream = stream + 1
		}
		stream = int32(1)
		for {
			if !(stream < int32(4)) {
				break
			}
			goto _6
		_6:
			;
			stream = stream + 1
		}
		/* Manually unroll the loop because compilers don't consistently
		 * unroll the inner loops, which destroys performance.
		 */
		for cond := true; cond; cond = (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)] < olimit {
			/* Decode 5 symbols from each of the first 3 streams.
			 * The final stream will be decoded during the reload phase
			 * to reduce register pressure.
			 */
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index1 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry1 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index1)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry1.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry1.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry1.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index2 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry2 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index2)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry2.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry2.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry2.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index3 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry3 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index3)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry3.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry3.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry3.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index4 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry4 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index4)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry4.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry4.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry4.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index5 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry5 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index5)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry5.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry5.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry5.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index6 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry6 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index6)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry6.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry6.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry6.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index7 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry7 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index7)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry7.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry7.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry7.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index8 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry8 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index8)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry8.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry8.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry8.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index9 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry9 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index9)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry9.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry9.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry9.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index10 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry10 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index10)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry10.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry10.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry10.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index11 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry11 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index11)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry11.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry11.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry11.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index12 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry12 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index12)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry12.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry12.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry12.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index13 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry13 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index13)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry13.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry13.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry13.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index14 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry14 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index14)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry14.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry14.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry14.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index15 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry15 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index15)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry15.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry15.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry15.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(0 != int32(3)) {
				index16 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[0] >> libc.Int32FromInt32(53))
				entry16 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index16)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[0], entry16.Fsequence)
				*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry16.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64)) += uintptr(entry16.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(1) != int32(3)) {
				index17 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] >> libc.Int32FromInt32(53))
				entry17 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index17)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(1)], entry17.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry17.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 1*8)) += uintptr(entry17.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(2) != int32(3)) {
				index18 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] >> libc.Int32FromInt32(53))
				entry18 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index18)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(2)], entry18.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry18.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 2*8)) += uintptr(entry18.Flength)
			}
			if libc.Bool(0 != 0) || libc.Bool(int32(3) != int32(3)) {
				index19 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry19 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index19)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry19.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry19.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry19.Flength)
			}
			/* Decode one symbol from the final stream */
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index20 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry20 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index20)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry20.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry20.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry20.Flength)
			}
			/* Decode 4 symbols from the final stream & reload bitstreams.
			 * The final stream is reloaded last, meaning that all 5 symbols
			 * are decoded from the final stream before it is reloaded.
			 */
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index21 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry21 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index21)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry21.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry21.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry21.Flength)
			}
			ctz = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[0]))
			nbBits = ctz & int32(7)
			nbBytes = ctz >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32)) -= uintptr(nbBytes)
			(*(*[4]U64)(unsafe.Pointer(bp)))[0] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[0]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp)) <<= libc.Uint64FromInt32(nbBits)
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index22 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry22 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index22)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry22.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry22.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry22.Flength)
			}
			ctz1 = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)]))
			nbBits1 = ctz1 & int32(7)
			nbBytes1 = ctz1 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 1*8)) -= uintptr(nbBytes1)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(1)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(1)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 1*8)) <<= libc.Uint64FromInt32(nbBits1)
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index23 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry23 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index23)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry23.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry23.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry23.Flength)
			}
			ctz2 = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)]))
			nbBits2 = ctz2 & int32(7)
			nbBytes2 = ctz2 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 2*8)) -= uintptr(nbBytes2)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(2)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(2)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 2*8)) <<= libc.Uint64FromInt32(nbBits2)
			if libc.Bool(int32(1) != 0) || libc.Bool(int32(3) != int32(3)) {
				index24 = libc.Int32FromUint64((*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] >> libc.Int32FromInt32(53))
				entry24 = *(*HUF_DEltX2)(unsafe.Pointer(dtable + uintptr(index24)*4))
				MEM_write16(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 64)))[int32(3)], entry24.Fsequence)
				*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(libc.Int32FromUint8(entry24.FnbBits) & int32(0x3F))
				*(*uintptr)(unsafe.Pointer(bp + 64 + 3*8)) += uintptr(entry24.Flength)
			}
			ctz3 = libc.Int32FromUint32(ZSTD_countTrailingZeros64(tls, (*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)]))
			nbBits3 = ctz3 & int32(7)
			nbBytes3 = ctz3 >> int32(3)
			*(*uintptr)(unsafe.Pointer(bp + 32 + 3*8)) -= uintptr(nbBytes3)
			(*(*[4]U64)(unsafe.Pointer(bp)))[int32(3)] = MEM_read64(tls, (*(*[4]uintptr)(unsafe.Pointer(bp + 32)))[int32(3)]) | uint64(1)
			*(*U64)(unsafe.Pointer(bp + 3*8)) <<= libc.Uint64FromInt32(nbBits3)
		}
		goto _1
	_1:
	}
	goto _out
_out:
	;
	/* Save the final values of each of the state variables back to args. */
	libc.Xmemcpy(tls, args+64, bp, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args, bp+32, libc.Uint64FromInt64(32))
	libc.Xmemcpy(tls, args+32, bp+64, libc.Uint64FromInt64(32))
}

func HUF_decompress4X2_usingDTable_internal_fast(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, __ccgo_fp_loopFn HUF_DecompressFastLoopFn) (r size_t) {
	bp := tls.Alloc(192)
	defer tls.Free(192)
	var dt, ilowest, oend, segmentEnd uintptr
	var err_code, err_code1, ret, segmentSize size_t
	var i int32
	var _ /* args at bp+0 */ HUF_DecompressFastArgs
	var _ /* bit at bp+152 */ BIT_DStream_t
	_, _, _, _, _, _, _, _, _ = dt, err_code, err_code1, i, ilowest, oend, ret, segmentEnd, segmentSize
	dt = DTable + uintptr(1)*4
	ilowest = cSrc
	oend = ZSTD_maybeNullPtrAdd(tls, dst, libc.Int64FromUint64(dstSize))
	ret = HUF_DecompressFastArgs_init(tls, bp, dst, dstSize, cSrc, cSrcSize, DTable)
	err_code = ret
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6762, 0)
		}
		return err_code
	}
	if ret == uint64(0) {
		return uint64(0)
	}
	(*(*func(*libc.TLS, uintptr))(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_loopFn})))(tls, bp)
	/* note : op4 already verified within main loop */
	_ = ilowest
	/* finish bitStreams one by one */
	segmentSize = (dstSize + uint64(3)) / uint64(4)
	segmentEnd = dst
	i = 0
	for {
		if !(i < int32(4)) {
			break
		}
		if segmentSize <= libc.Uint64FromInt64(int64(oend)-int64(segmentEnd)) {
			segmentEnd = segmentEnd + uintptr(segmentSize)
		} else {
			segmentEnd = oend
		}
		err_code1 = HUF_initRemainingDStream(tls, bp+152, bp, i, segmentEnd)
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6751, 0)
			}
			return err_code1
		}
		*(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) += uintptr(HUF_decodeStreamX2(tls, *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)), bp+152, segmentEnd, dt, uint32(HUF_DECODER_FAST_TABLELOG)))
		if *(*uintptr)(unsafe.Pointer(bp + 32 + uintptr(i)*8)) != segmentEnd {
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	/* decoded size */
	return dstSize
}

func HUF_decompress4X2_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var fallbackFn HUF_DecompressUsingDTableFn
	var loopFn HUF_DecompressFastLoopFn
	var ret size_t
	_, _, _ = fallbackFn, loopFn, ret
	fallbackFn = __ccgo_fp(HUF_decompress4X2_usingDTable_internal_default)
	loopFn = __ccgo_fp(HUF_decompress4X2_usingDTable_internal_fast_c_loop)
	if flags&int32(HUF_flags_bmi2) != 0 {
		fallbackFn = __ccgo_fp(HUF_decompress4X2_usingDTable_internal_bmi2)
	} else {
		return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	if libc.Bool(int32(HUF_ENABLE_FAST_DECODE) != 0) && !(flags&int32(HUF_flags_disableFast) != 0) {
		ret = HUF_decompress4X2_usingDTable_internal_fast(tls, dst, dstSize, cSrc, cSrcSize, DTable, loopFn)
		if ret != uint64(0) {
			return ret
		}
	}
	return (*(*func(*libc.TLS, uintptr, size_t, uintptr, size_t, uintptr) size_t)(unsafe.Pointer(&struct{ uintptr }{fallbackFn})))(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_usingDTable_internal_default(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_usingDTable_internal_bmi2(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr) (r size_t) {
	return HUF_decompress1X2_usingDTable_internal_body(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_usingDTable_internal(tls *libc.TLS, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	if flags&int32(HUF_flags_bmi2) != 0 {
		return HUF_decompress1X2_usingDTable_internal_bmi2(tls, dst, dstSize, cSrc, cSrcSize, DTable)
	}
	return HUF_decompress1X2_usingDTable_internal_default(tls, dst, dstSize, cSrc, cSrcSize, DTable)
}

func HUF_decompress1X2_DCtx_wksp(tls *libc.TLS, DCtx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX2_wksp(tls, DCtx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress1X2_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, DCtx, flags)
}

func HUF_decompress4X2_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX2_wksp(tls, dctx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress4X2_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, dctx, flags)
}

/* ***********************************/
/* Universal decompression selectors */
/* ***********************************/

type algo_time_t = struct {
	FtableTime     U32
	Fdecode256Time U32
}

var algoTime = [16][2]algo_time_t{
	0: {
		0: {},
		1: {
			FtableTime:     uint32(1),
			Fdecode256Time: uint32(1),
		},
	},
	1: {
		0: {},
		1: {
			FtableTime:     uint32(1),
			Fdecode256Time: uint32(1),
		},
	},
	2: {
		0: {
			FtableTime:     uint32(150),
			Fdecode256Time: uint32(216),
		},
		1: {
			FtableTime:     uint32(381),
			Fdecode256Time: uint32(119),
		},
	},
	3: {
		0: {
			FtableTime:     uint32(170),
			Fdecode256Time: uint32(205),
		},
		1: {
			FtableTime:     uint32(514),
			Fdecode256Time: uint32(112),
		},
	},
	4: {
		0: {
			FtableTime:     uint32(177),
			Fdecode256Time: uint32(199),
		},
		1: {
			FtableTime:     uint32(539),
			Fdecode256Time: uint32(110),
		},
	},
	5: {
		0: {
			FtableTime:     uint32(197),
			Fdecode256Time: uint32(194),
		},
		1: {
			FtableTime:     uint32(644),
			Fdecode256Time: uint32(107),
		},
	},
	6: {
		0: {
			FtableTime:     uint32(221),
			Fdecode256Time: uint32(192),
		},
		1: {
			FtableTime:     uint32(735),
			Fdecode256Time: uint32(107),
		},
	},
	7: {
		0: {
			FtableTime:     uint32(256),
			Fdecode256Time: uint32(189),
		},
		1: {
			FtableTime:     uint32(881),
			Fdecode256Time: uint32(106),
		},
	},
	8: {
		0: {
			FtableTime:     uint32(359),
			Fdecode256Time: uint32(188),
		},
		1: {
			FtableTime:     uint32(1167),
			Fdecode256Time: uint32(109),
		},
	},
	9: {
		0: {
			FtableTime:     uint32(582),
			Fdecode256Time: uint32(187),
		},
		1: {
			FtableTime:     uint32(1570),
			Fdecode256Time: uint32(114),
		},
	},
	10: {
		0: {
			FtableTime:     uint32(688),
			Fdecode256Time: uint32(187),
		},
		1: {
			FtableTime:     uint32(1712),
			Fdecode256Time: uint32(122),
		},
	},
	11: {
		0: {
			FtableTime:     uint32(825),
			Fdecode256Time: uint32(186),
		},
		1: {
			FtableTime:     uint32(1965),
			Fdecode256Time: uint32(136),
		},
	},
	12: {
		0: {
			FtableTime:     uint32(976),
			Fdecode256Time: uint32(185),
		},
		1: {
			FtableTime:     uint32(2131),
			Fdecode256Time: uint32(150),
		},
	},
	13: {
		0: {
			FtableTime:     uint32(1180),
			Fdecode256Time: uint32(186),
		},
		1: {
			FtableTime:     uint32(2070),
			Fdecode256Time: uint32(175),
		},
	},
	14: {
		0: {
			FtableTime:     uint32(1377),
			Fdecode256Time: uint32(185),
		},
		1: {
			FtableTime:     uint32(1731),
			Fdecode256Time: uint32(202),
		},
	},
	15: {
		0: {
			FtableTime:     uint32(1412),
			Fdecode256Time: uint32(185),
		},
		1: {
			FtableTime:     uint32(1695),
			Fdecode256Time: uint32(202),
		},
	},
}

// C documentation
//
//	/** HUF_selectDecoder() :
//	 *  Tells which decoder is likely to decode faster,
//	 *  based on a set of pre-computed metrics.
//	 * @return : 0==HUF_decompress4X1, 1==HUF_decompress4X2 .
//	 *  Assumption : 0 < dstSize <= 128 KB */
func HUF_selectDecoder(tls *libc.TLS, dstSize size_t, cSrcSize size_t) (r U32) {
	var D256, DTime0, DTime1, Q U32
	var v1 uint32
	_, _, _, _, _ = D256, DTime0, DTime1, Q, v1
	/* decoder timing evaluation */
	if cSrcSize >= dstSize {
		v1 = uint32(15)
	} else {
		v1 = uint32(cSrcSize * libc.Uint64FromInt32(16) / dstSize)
	}
	Q = v1 /* Q < 16 */
	D256 = uint32(dstSize >> libc.Int32FromInt32(8))
	DTime0 = (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16))).FtableTime + (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16))).Fdecode256Time*D256
	DTime1 = (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16 + 1*8))).FtableTime + (*(*algo_time_t)(unsafe.Pointer(uintptr(unsafe.Pointer(&algoTime)) + uintptr(Q)*16 + 1*8))).Fdecode256Time*D256
	DTime1 = DTime1 + DTime1>>int32(5) /* small advantage to algorithm using less memory, to reduce cache eviction */
	return libc.BoolUint32(DTime1 < DTime0)
	return r
}

func HUF_decompress1X_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var algoNb U32
	var v1 uint64
	_, _ = algoNb, v1
	/* validation checks */
	if dstSize == uint64(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if cSrcSize > dstSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* invalid */
	if cSrcSize == dstSize {
		libc.Xmemcpy(tls, dst, cSrc, dstSize)
		return dstSize
	} /* not compressed */
	if cSrcSize == uint64(1) {
		libc.Xmemset(tls, dst, libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(cSrc))), dstSize)
		return dstSize
	} /* RLE */
	algoNb = HUF_selectDecoder(tls, dstSize, cSrcSize)
	if algoNb != 0 {
		v1 = HUF_decompress1X2_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	} else {
		v1 = HUF_decompress1X1_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	}
	return v1
	return r
}

func HUF_decompress1X_usingDTable(tls *libc.TLS, dst uintptr, maxDstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var dtd DTableDesc
	var v1 uint64
	_, _ = dtd, v1
	dtd = HUF_getDTableDesc(tls, DTable)
	if dtd.FtableType != 0 {
		v1 = HUF_decompress1X2_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	} else {
		v1 = HUF_decompress1X1_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	}
	return v1
}

func HUF_decompress1X1_DCtx_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var hSize size_t
	var ip uintptr
	_, _ = hSize, ip
	ip = cSrc
	hSize = HUF_readDTableX1_wksp(tls, dctx, cSrc, cSrcSize, workSpace, wkspSize, flags)
	if ERR_isError(tls, hSize) != 0 {
		return hSize
	}
	if hSize >= cSrcSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	ip = ip + uintptr(hSize)
	cSrcSize = cSrcSize - hSize
	return HUF_decompress1X1_usingDTable_internal(tls, dst, dstSize, ip, cSrcSize, dctx, flags)
}

func HUF_decompress4X_usingDTable(tls *libc.TLS, dst uintptr, maxDstSize size_t, cSrc uintptr, cSrcSize size_t, DTable uintptr, flags int32) (r size_t) {
	var dtd DTableDesc
	var v1 uint64
	_, _ = dtd, v1
	dtd = HUF_getDTableDesc(tls, DTable)
	if dtd.FtableType != 0 {
		v1 = HUF_decompress4X2_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	} else {
		v1 = HUF_decompress4X1_usingDTable_internal(tls, dst, maxDstSize, cSrc, cSrcSize, DTable, flags)
	}
	return v1
}

func HUF_decompress4X_hufOnly_wksp(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t, cSrc uintptr, cSrcSize size_t, workSpace uintptr, wkspSize size_t, flags int32) (r size_t) {
	var algoNb U32
	var v1 uint64
	_, _ = algoNb, v1
	/* validation checks */
	if dstSize == uint64(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if cSrcSize == uint64(0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	algoNb = HUF_selectDecoder(tls, dstSize, cSrcSize)
	if algoNb != 0 {
		v1 = HUF_decompress4X2_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	} else {
		v1 = HUF_decompress4X1_DCtx_wksp(tls, dctx, dst, dstSize, cSrc, cSrcSize, workSpace, wkspSize, flags)
	}
	return v1
	return r
}

/**** ended inlining decompress/huf_decompress.c ****/
/**** start inlining decompress/zstd_ddict.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* zstd_ddict.c :
 * concentrates all logic that needs to know the internals of ZSTD_DDict object */

/*-*******************************************************
*  Dependencies
*********************************************************/
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/cpu.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** start inlining zstd_decompress_internal.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* zstd_decompress_internal:
 * objects and definitions shared within lib/decompress modules */

/*-*******************************************************
 *  Dependencies
 *********************************************************/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/zstd_internal.h ****/

// C documentation
//
//	/*-*******************************************************
//	 *  Constants
//	 *********************************************************/
var LL_base = [36]U32{
	1:  uint32(1),
	2:  uint32(2),
	3:  uint32(3),
	4:  uint32(4),
	5:  uint32(5),
	6:  uint32(6),
	7:  uint32(7),
	8:  uint32(8),
	9:  uint32(9),
	10: uint32(10),
	11: uint32(11),
	12: uint32(12),
	13: uint32(13),
	14: uint32(14),
	15: uint32(15),
	16: uint32(16),
	17: uint32(18),
	18: uint32(20),
	19: uint32(22),
	20: uint32(24),
	21: uint32(28),
	22: uint32(32),
	23: uint32(40),
	24: uint32(48),
	25: uint32(64),
	26: uint32(0x80),
	27: uint32(0x100),
	28: uint32(0x200),
	29: uint32(0x400),
	30: uint32(0x800),
	31: uint32(0x1000),
	32: uint32(0x2000),
	33: uint32(0x4000),
	34: uint32(0x8000),
	35: uint32(0x10000),
}

var OF_base = [32]U32{
	1:  uint32(1),
	2:  uint32(1),
	3:  uint32(5),
	4:  uint32(0xD),
	5:  uint32(0x1D),
	6:  uint32(0x3D),
	7:  uint32(0x7D),
	8:  uint32(0xFD),
	9:  uint32(0x1FD),
	10: uint32(0x3FD),
	11: uint32(0x7FD),
	12: uint32(0xFFD),
	13: uint32(0x1FFD),
	14: uint32(0x3FFD),
	15: uint32(0x7FFD),
	16: uint32(0xFFFD),
	17: uint32(0x1FFFD),
	18: uint32(0x3FFFD),
	19: uint32(0x7FFFD),
	20: uint32(0xFFFFD),
	21: uint32(0x1FFFFD),
	22: uint32(0x3FFFFD),
	23: uint32(0x7FFFFD),
	24: uint32(0xFFFFFD),
	25: uint32(0x1FFFFFD),
	26: uint32(0x3FFFFFD),
	27: uint32(0x7FFFFFD),
	28: uint32(0xFFFFFFD),
	29: uint32(0x1FFFFFFD),
	30: uint32(0x3FFFFFFD),
	31: uint32(0x7FFFFFFD),
}

var OF_bits = [32]U8{
	1:  uint8(1),
	2:  uint8(2),
	3:  uint8(3),
	4:  uint8(4),
	5:  uint8(5),
	6:  uint8(6),
	7:  uint8(7),
	8:  uint8(8),
	9:  uint8(9),
	10: uint8(10),
	11: uint8(11),
	12: uint8(12),
	13: uint8(13),
	14: uint8(14),
	15: uint8(15),
	16: uint8(16),
	17: uint8(17),
	18: uint8(18),
	19: uint8(19),
	20: uint8(20),
	21: uint8(21),
	22: uint8(22),
	23: uint8(23),
	24: uint8(24),
	25: uint8(25),
	26: uint8(26),
	27: uint8(27),
	28: uint8(28),
	29: uint8(29),
	30: uint8(30),
	31: uint8(31),
}

var ML_base = [53]U32{
	0:  uint32(3),
	1:  uint32(4),
	2:  uint32(5),
	3:  uint32(6),
	4:  uint32(7),
	5:  uint32(8),
	6:  uint32(9),
	7:  uint32(10),
	8:  uint32(11),
	9:  uint32(12),
	10: uint32(13),
	11: uint32(14),
	12: uint32(15),
	13: uint32(16),
	14: uint32(17),
	15: uint32(18),
	16: uint32(19),
	17: uint32(20),
	18: uint32(21),
	19: uint32(22),
	20: uint32(23),
	21: uint32(24),
	22: uint32(25),
	23: uint32(26),
	24: uint32(27),
	25: uint32(28),
	26: uint32(29),
	27: uint32(30),
	28: uint32(31),
	29: uint32(32),
	30: uint32(33),
	31: uint32(34),
	32: uint32(35),
	33: uint32(37),
	34: uint32(39),
	35: uint32(41),
	36: uint32(43),
	37: uint32(47),
	38: uint32(51),
	39: uint32(59),
	40: uint32(67),
	41: uint32(83),
	42: uint32(99),
	43: uint32(0x83),
	44: uint32(0x103),
	45: uint32(0x203),
	46: uint32(0x403),
	47: uint32(0x803),
	48: uint32(0x1003),
	49: uint32(0x2003),
	50: uint32(0x4003),
	51: uint32(0x8003),
	52: uint32(0x10003),
}

// C documentation
//
//	/*-*******************************************************
//	 *  Decompression types
//	 *********************************************************/
type ZSTD_seqSymbol_header = struct {
	FfastMode U32
	FtableLog U32
}

type ZSTD_seqSymbol = struct {
	FnextState        U16
	FnbAdditionalBits BYTE
	FnbBits           BYTE
	FbaseValue        U32
}

type ZSTD_entropyDTables_t = struct {
	FLLTable   [513]ZSTD_seqSymbol
	FOFTable   [257]ZSTD_seqSymbol
	FMLTable   [513]ZSTD_seqSymbol
	FhufTable  [4097]HUF_DTable
	Frep       [3]U32
	Fworkspace [157]U32
}

type ZSTD_dStage = int32

type ZSTD_dStreamStage = int32

type ZSTD_dictUses_e = int32

// C documentation
//
//	/* Hashset for storing references to multiple ZSTD_DDict within ZSTD_DCtx */
type ZSTD_DDictHashSet = struct {
	FddictPtrTable     uintptr
	FddictPtrTableSize size_t
	FddictPtrCount     size_t
}

/* extra buffer, compensates when dst is not large enough to store litBuffer */

type ZSTD_litLocation_e = int32 /* typedef'd to ZSTD_DCtx within "zstd.h" */
func ZSTD_DCtx_get_bmi2(tls *libc.TLS, dctx uintptr) (r int32) {
	return (*ZSTD_DCtx_s)(unsafe.Pointer(dctx)).Fbmi2
}

/* typedef'd to ZSTD_DDict within "zstd.h" */

func ZSTD_DDict_dictContent(tls *libc.TLS, ddict uintptr) (r uintptr) {
	return (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent
}

func ZSTD_DDict_dictSize(tls *libc.TLS, ddict uintptr) (r size_t) {
	return (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize
}

func ZSTD_copyDDictParameters(tls *libc.TLS, dctx uintptr, ddict uintptr) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent + uintptr((*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	if (*ZSTD_DDict)(unsafe.Pointer(ddict)).FentropyPresent != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = uint32(1)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr = ddict + 24
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr = ddict + 24 + 6160
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr = ddict + 24 + 4104
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr = ddict + 24 + 10264
		*(*U32)(unsafe.Pointer(dctx + 32 + 26652)) = *(*U32)(unsafe.Pointer(ddict + 24 + 26652))
		*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + 1*4)) = *(*U32)(unsafe.Pointer(ddict + 24 + 26652 + 1*4))
		*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + 2*4)) = *(*U32)(unsafe.Pointer(ddict + 24 + 26652 + 2*4))
	} else {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = uint32(0)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(0)
	}
}

func ZSTD_loadEntropy_intoDDict(tls *libc.TLS, ddict uintptr, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var magic U32
	_ = magic
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID = uint32(0)
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FentropyPresent = uint32(0)
	if dictContentType == int32(ZSTD_dct_rawContent) {
		return uint64(0)
	}
	if (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize < uint64(8) {
		if dictContentType == int32(ZSTD_dct_fullDict) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
		} /* only accept specified dictionaries */
		return uint64(0) /* pure content mode */
	}
	magic = MEM_readLE32(tls, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent)
	if magic != uint32(ZSTD_MAGIC_DICTIONARY) {
		if dictContentType == int32(ZSTD_dct_fullDict) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
		} /* only accept specified dictionaries */
		return uint64(0) /* pure content mode */
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID = MEM_readLE32(tls, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent+uintptr(ZSTD_FRAMEIDSIZE))
	/* load entropy tables */
	if ZSTD_isError(tls, ZSTD_loadDEntropy(tls, ddict+24, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FentropyPresent = uint32(1)
	return uint64(0)
}

func ZSTD_initDDict_internal(tls *libc.TLS, ddict uintptr, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var err_code size_t
	var internalBuffer uintptr
	_, _ = err_code, internalBuffer
	if dictLoadMethod == int32(ZSTD_dlm_byRef) || !(dict != 0) || !(dictSize != 0) {
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer = libc.UintptrFromInt32(0)
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent = dict
		if !(dict != 0) {
			dictSize = uint64(0)
		}
	} else {
		internalBuffer = ZSTD_customMalloc(tls, dictSize, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FcMem)
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer = internalBuffer
		(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictContent = internalBuffer
		if !(internalBuffer != 0) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		libc.Xmemcpy(tls, internalBuffer, dict, dictSize)
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize = dictSize
	*(*HUF_DTable)(unsafe.Pointer(ddict + 24 + 10264)) = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HUFFDTABLE_CAPACITY_LOG) * libc.Int32FromInt32(0x1000001)) /* cover both little and big endian */
	/* parse dictionary content */
	err_code = ZSTD_loadEntropy_intoDDict(tls, ddict, dictContentType)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return uint64(0)
}

func ZSTD_createDDict_advanced(tls *libc.TLS, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e, customMem ZSTD_customMem) (r uintptr) {
	var ddict uintptr
	var initResult size_t
	_, _ = ddict, initResult
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	ddict = ZSTD_customMalloc(tls, uint64(27352), customMem)
	if ddict == libc.UintptrFromInt32(0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DDict)(unsafe.Pointer(ddict)).FcMem = customMem
	initResult = ZSTD_initDDict_internal(tls, ddict, dict, dictSize, dictLoadMethod, dictContentType)
	if ZSTD_isError(tls, initResult) != 0 {
		ZSTD_freeDDict(tls, ddict)
		return libc.UintptrFromInt32(0)
	}
	return ddict
	return r
}

// C documentation
//
//	/*! ZSTD_createDDict() :
//	*   Create a digested dictionary, to start decompression without startup delay.
//	*   `dict` content is copied inside DDict.
//	*   Consequently, `dict` can be released after `ZSTD_DDict` creation */
func ZSTD_createDDict(tls *libc.TLS, dict uintptr, dictSize size_t) (r uintptr) {
	var allocator ZSTD_customMem
	_ = allocator
	allocator = ZSTD_customMem{}
	return ZSTD_createDDict_advanced(tls, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto), allocator)
}

// C documentation
//
//	/*! ZSTD_createDDict_byReference() :
//	 *  Create a digested dictionary, to start decompression without startup delay.
//	 *  Dictionary content is simply referenced, it will be accessed during decompression.
//	 *  Warning : dictBuffer must outlive DDict (DDict must be freed before dictBuffer) */
func ZSTD_createDDict_byReference(tls *libc.TLS, dictBuffer uintptr, dictSize size_t) (r uintptr) {
	var allocator ZSTD_customMem
	_ = allocator
	allocator = ZSTD_customMem{}
	return ZSTD_createDDict_advanced(tls, dictBuffer, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto), allocator)
}

func ZSTD_initStaticDDict(tls *libc.TLS, sBuffer uintptr, sBufferSize size_t, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r uintptr) {
	var ddict uintptr
	var neededSpace size_t
	var v1 uint64
	_, _, _ = ddict, neededSpace, v1
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = dictSize
	}
	neededSpace = uint64(27352) + v1
	ddict = sBuffer
	if uint64(sBuffer)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* 8-aligned */
	if sBufferSize < neededSpace {
		return libc.UintptrFromInt32(0)
	}
	if dictLoadMethod == int32(ZSTD_dlm_byCopy) {
		libc.Xmemcpy(tls, ddict+libc.UintptrFromInt32(1)*27352, dict, dictSize) /* local copy */
		dict = ddict + uintptr(1)*27352
	}
	if ZSTD_isError(tls, ZSTD_initDDict_internal(tls, ddict, dict, dictSize, int32(ZSTD_dlm_byRef), dictContentType)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	return ddict
}

func ZSTD_freeDDict(tls *libc.TLS, ddict uintptr) (r size_t) {
	var cMem ZSTD_customMem
	_ = cMem
	if ddict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	cMem = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FcMem
	ZSTD_customFree(tls, (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer, cMem)
	ZSTD_customFree(tls, ddict, cMem)
	return uint64(0)
	return r
}

// C documentation
//
//	/*! ZSTD_estimateDDictSize() :
//	 *  Estimate amount of memory that will be needed to create a dictionary for decompression.
//	 *  Note : dictionary created by reference using ZSTD_dlm_byRef are smaller */
func ZSTD_estimateDDictSize(tls *libc.TLS, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e) (r size_t) {
	var v1 uint64
	_ = v1
	if dictLoadMethod == int32(ZSTD_dlm_byRef) {
		v1 = uint64(0)
	} else {
		v1 = dictSize
	}
	return uint64(27352) + v1
}

func ZSTD_sizeof_DDict(tls *libc.TLS, ddict uintptr) (r size_t) {
	var v1 uint64
	_ = v1
	if ddict == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof on NULL */
	if (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictBuffer != 0 {
		v1 = (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictSize
	} else {
		v1 = uint64(0)
	}
	return uint64(27352) + v1
}

// C documentation
//
//	/*! ZSTD_getDictID_fromDDict() :
//	 *  Provides the dictID of the dictionary loaded into `ddict`.
//	 *  If @return == 0, the dictionary is not conformant to Zstandard specification, or empty.
//	 *  Non-conformant dictionaries can still be loaded, but as content-only dictionaries. */
func ZSTD_getDictID_fromDDict(tls *libc.TLS, ddict uintptr) (r uint32) {
	if ddict == libc.UintptrFromInt32(0) {
		return uint32(0)
	}
	return (*ZSTD_DDict)(unsafe.Pointer(ddict)).FdictID
}

/**** ended inlining decompress/zstd_ddict.c ****/
/**** start inlining decompress/zstd_decompress.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* ***************************************************************
*  Tuning parameters
*****************************************************************/
/*!
 * HEAPMODE :
 * Select how default decompression function ZSTD_decompress() allocates its context,
 * on stack (0), or into heap (1, default; requires malloc()).
 * Note that functions with explicit context such as ZSTD_decompressDCtx() are unaffected.
 */

/*!
*  LEGACY_SUPPORT :
*  if set to 1+, ZSTD_decompress() can decode older formats (v0.1+)
 */

/*!
 *  MAXWINDOWSIZE_DEFAULT :
 *  maximum window size accepted by DStream __by default__.
 *  Frames requiring more memory will be rejected.
 *  It's possible to set a different limit using ZSTD_DCtx_setMaxWindowSize().
 */

/*!
 *  NO_FORWARD_PROGRESS_MAX :
 *  maximum allowed nb of calls to ZSTD_decompressStream()
 *  without any forward progress
 *  (defined as: no byte read from input, and no byte flushed to output)
 *  before triggering an error.
 */

/*-*******************************************************
*  Dependencies
*********************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/allocations.h ****/
/**** skipping file: ../common/error_private.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/bits.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/xxhash.h ****/
/**** skipping file: zstd_decompress_internal.h ****/
/**** skipping file: zstd_ddict.h ****/
/**** start inlining zstd_decompress_block.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*******************************************************
 *  Dependencies
 *********************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../zstd.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: zstd_decompress_internal.h ****/

/* ===   Prototypes   === */

/* note: prototypes already published within `zstd.h` :
 * ZSTD_decompressBlock()
 */

/* note: prototypes already published within `zstd_internal.h` :
 * ZSTD_getcBlockSize()
 * ZSTD_decodeSeqHeaders()
 */

// C documentation
//
//	/* Streaming state is used to inform allocation of the literal buffer */
type streaming_operation = int32

const not_streaming = 0
const is_streaming = 1

/**** ended inlining zstd_decompress_block.h ****/

/*************************************
 * Multiple DDicts Hashset internals *
 *************************************/

// C documentation
//
//	/* Hash function to determine starting position of dict insertion within the table
//	 * Returns an index between [0, hashSet->ddictPtrTableSize]
//	 */
func ZSTD_DDictHashSet_getIndex(tls *libc.TLS, hashSet uintptr, _dictID U32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*U32)(unsafe.Pointer(bp)) = _dictID
	var hash U64
	_ = hash
	hash = XXH_INLINE_XXH64(tls, bp, uint64(4), uint64(0))
	/* DDict ptr table size is a multiple of 2, use size - 1 as mask to get index within [0, hashSet->ddictPtrTableSize) */
	return hash & ((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize - uint64(1))
}

// C documentation
//
//	/* Adds DDict to a hashset without resizing it.
//	 * If inserting a DDict with a dictID that already exists in the set, replaces the one in the set.
//	 * Returns 0 if successful, or a zstd error code if something went wrong.
//	 */
func ZSTD_DDictHashSet_emplaceDDict(tls *libc.TLS, hashSet uintptr, ddict uintptr) (r size_t) {
	var dictID U32
	var idx, idxRangeMask size_t
	_, _, _ = dictID, idx, idxRangeMask
	dictID = ZSTD_getDictID_fromDDict(tls, ddict)
	idx = ZSTD_DDictHashSet_getIndex(tls, hashSet, dictID)
	idxRangeMask = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize - uint64(1)
	if (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount == (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6786, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	}
	for *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8)) != libc.UintptrFromInt32(0) {
		/* Replace existing ddict if inserting ddict with same dictID */
		if ZSTD_getDictID_fromDDict(tls, *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8))) == dictID {
			*(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8)) = ddict
			return uint64(0)
		}
		idx = idx & idxRangeMask
		idx = idx + 1
	}
	*(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8)) = ddict
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount + 1
	return uint64(0)
}

// C documentation
//
//	/* Expands hash table by factor of DDICT_HASHSET_RESIZE_FACTOR and
//	 * rehashes all values, allocates new table, frees old table.
//	 * Returns 0 on success, otherwise a zstd error code.
//	 */
func ZSTD_DDictHashSet_expand(tls *libc.TLS, hashSet uintptr, customMem ZSTD_customMem) (r size_t) {
	var err_code, i, newTableSize, oldTableSize size_t
	var newTable, oldTable uintptr
	_, _, _, _, _, _ = err_code, i, newTable, newTableSize, oldTable, oldTableSize
	newTableSize = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize * uint64(DDICT_HASHSET_RESIZE_FACTOR)
	newTable = ZSTD_customCalloc(tls, uint64(8)*newTableSize, customMem)
	oldTable = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable
	oldTableSize = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize
	if !(newTable != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6804, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable = newTable
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize = newTableSize
	(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount = uint64(0)
	i = uint64(0)
	for {
		if !(i < oldTableSize) {
			break
		}
		if *(*uintptr)(unsafe.Pointer(oldTable + uintptr(i)*8)) != libc.UintptrFromInt32(0) {
			err_code = ZSTD_DDictHashSet_emplaceDDict(tls, hashSet, *(*uintptr)(unsafe.Pointer(oldTable + uintptr(i)*8)))
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code
			}
		}
		goto _1
	_1:
		;
		i = i + 1
	}
	ZSTD_customFree(tls, oldTable, customMem)
	return uint64(0)
}

// C documentation
//
//	/* Fetches a DDict with the given dictID
//	 * Returns the ZSTD_DDict* with the requested dictID. If it doesn't exist, then returns NULL.
//	 */
func ZSTD_DDictHashSet_getDDict(tls *libc.TLS, hashSet uintptr, dictID U32) (r uintptr) {
	var currDictID, idx, idxRangeMask size_t
	_, _, _ = currDictID, idx, idxRangeMask
	idx = ZSTD_DDictHashSet_getIndex(tls, hashSet, dictID)
	idxRangeMask = (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize - uint64(1)
	for {
		currDictID = uint64(ZSTD_getDictID_fromDDict(tls, *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8))))
		if currDictID == uint64(dictID) || currDictID == uint64(0) {
			/* currDictID == 0 implies a NULL ddict entry */
			break
		} else {
			idx = idx & idxRangeMask /* Goes to start of table when we reach the end */
			idx = idx + 1
		}
		goto _1
	_1:
	}
	return *(*uintptr)(unsafe.Pointer((*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable + uintptr(idx)*8))
}

// C documentation
//
//	/* Allocates space for and returns a ddict hash set
//	 * The hash set's ZSTD_DDict* table has all values automatically set to NULL to begin with.
//	 * Returns NULL if allocation failed.
//	 */
func ZSTD_createDDictHashSet(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var ret uintptr
	_ = ret
	ret = ZSTD_customMalloc(tls, uint64(24), customMem)
	if !(ret != 0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrTable = ZSTD_customCalloc(tls, libc.Uint64FromInt32(DDICT_HASHSET_TABLE_BASE_SIZE)*libc.Uint64FromInt64(8), customMem)
	if !((*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrTable != 0) {
		ZSTD_customFree(tls, ret, customMem)
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrTableSize = uint64(DDICT_HASHSET_TABLE_BASE_SIZE)
	(*ZSTD_DDictHashSet)(unsafe.Pointer(ret)).FddictPtrCount = uint64(0)
	return ret
}

// C documentation
//
//	/* Frees the table of ZSTD_DDict* within a hashset, then frees the hashset itself.
//	 * Note: The ZSTD_DDict* within the table are NOT freed.
//	 */
func ZSTD_freeDDictHashSet(tls *libc.TLS, hashSet uintptr, customMem ZSTD_customMem) {
	if hashSet != 0 && (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable != 0 {
		ZSTD_customFree(tls, (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTable, customMem)
	}
	if hashSet != 0 {
		ZSTD_customFree(tls, hashSet, customMem)
	}
}

// C documentation
//
//	/* Public function: Adds a DDict into the ZSTD_DDictHashSet, possibly triggering a resize of the hash set.
//	 * Returns 0 on success, or a ZSTD error.
//	 */
func ZSTD_DDictHashSet_addDDict(tls *libc.TLS, hashSet uintptr, ddict uintptr, customMem ZSTD_customMem) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	if (*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrCount*uint64(DDICT_HASHSET_MAX_LOAD_FACTOR_COUNT_MULT)/(*ZSTD_DDictHashSet)(unsafe.Pointer(hashSet)).FddictPtrTableSize*uint64(DDICT_HASHSET_MAX_LOAD_FACTOR_SIZE_MULT) != uint64(0) {
		err_code = ZSTD_DDictHashSet_expand(tls, hashSet, customMem)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
	}
	err_code1 = ZSTD_DDictHashSet_emplaceDDict(tls, hashSet, ddict)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return uint64(0)
}

// C documentation
//
//	/*-*************************************************************
//	*   Context management
//	***************************************************************/
func ZSTD_sizeof_DCtx(tls *libc.TLS, dctx uintptr) (r size_t) {
	if dctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support sizeof NULL */
	return uint64(95968) + ZSTD_sizeof_DDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal) + (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuffSize + (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBuffSize
}

func ZSTD_estimateDCtxSize(tls *libc.TLS) (r size_t) {
	return uint64(95968)
}

func ZSTD_startingInputLength(tls *libc.TLS, format ZSTD_format_e) (r size_t) {
	var startingInputLength size_t
	var v1 int32
	_, _ = startingInputLength, v1
	if format == int32(ZSTD_f_zstd1) {
		v1 = int32(5)
	} else {
		v1 = int32(1)
	}
	startingInputLength = libc.Uint64FromInt32(v1)
	/* only supports formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless */
	return startingInputLength
}

func ZSTD_DCtx_resetParameters(tls *libc.TLS, dctx uintptr) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat = int32(ZSTD_f_zstd1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize = uint64(libc.Uint32FromInt32(1)<<libc.Int32FromInt32(ZSTD_WINDOWLOG_LIMIT_DEFAULT) + libc.Uint32FromInt32(1))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBufferMode = int32(ZSTD_bm_buffered)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum = int32(ZSTD_d_validateChecksum)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts = int32(ZSTD_rmd_refSingleDDict)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm = 0
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam = 0
}

func ZSTD_initDCtx_internal(tls *libc.TLS, dctx uintptr) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold = 0
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_dont_use)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuffSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBuffSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage = int32(zdss_init)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FnoForwardProgress = 0
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoversizedDuration = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = int32(1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fbmi2 = ZSTD_cpuSupportsBmi2(tls)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet = libc.UintptrFromInt32(0)
	ZSTD_DCtx_resetParameters(tls, dctx)
}

func ZSTD_initStaticDCtx(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	var dctx uintptr
	_ = dctx
	dctx = workspace
	if uint64(workspace)&uint64(7) != 0 {
		return libc.UintptrFromInt32(0)
	} /* 8-aligned */
	if workspaceSize < uint64(95968) {
		return libc.UintptrFromInt32(0)
	} /* minimum size */
	ZSTD_initDCtx_internal(tls, dctx)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize = workspaceSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff = dctx + libc.UintptrFromInt32(1)*95968
	return dctx
}

func ZSTD_createDCtx_internal(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	var dctx uintptr
	_ = dctx
	if libc.BoolInt32(!(customMem.FcustomAlloc != 0))^libc.BoolInt32(!(customMem.FcustomFree != 0)) != 0 {
		return libc.UintptrFromInt32(0)
	}
	dctx = ZSTD_customMalloc(tls, uint64(95968), customMem)
	if !(dctx != 0) {
		return libc.UintptrFromInt32(0)
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem = customMem
	ZSTD_initDCtx_internal(tls, dctx)
	return dctx
	return r
}

func ZSTD_createDCtx_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, customMem)
}

func ZSTD_createDCtx(tls *libc.TLS) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, ZSTD_defaultCMem)
}

func ZSTD_clearDict(tls *libc.TLS, dctx uintptr) {
	ZSTD_freeDDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_dont_use)
}

func ZSTD_freeDCtx(tls *libc.TLS, dctx uintptr) (r size_t) {
	var cMem ZSTD_customMem
	_ = cMem
	if dctx == libc.UintptrFromInt32(0) {
		return uint64(0)
	} /* support free on NULL */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6840, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	cMem = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem
	ZSTD_clearDict(tls, dctx)
	ZSTD_customFree(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff, cMem)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FinBuff = libc.UintptrFromInt32(0)
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet != 0 {
		ZSTD_freeDDictHashSet(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet, cMem)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet = libc.UintptrFromInt32(0)
	}
	ZSTD_customFree(tls, dctx, cMem)
	return uint64(0)
	return r
}

// C documentation
//
//	/* no longer useful */
func ZSTD_copyDCtx(tls *libc.TLS, dstDCtx uintptr, srcDCtx uintptr) {
	var toCopy size_t
	_ = toCopy
	toCopy = libc.Uint64FromInt64(int64(dstDCtx+30240) - int64(dstDCtx))
	libc.Xmemcpy(tls, dstDCtx, srcDCtx, toCopy) /* no need to copy workspace */
}

// C documentation
//
//	/* Given a dctx with a digested frame params, re-selects the correct ZSTD_DDict based on
//	 * the requested dict ID from the frame. If there exists a reference to the correct ZSTD_DDict, then
//	 * accordingly sets the ddict to be used to decompress the frame.
//	 *
//	 * If no DDict is found, then no action is taken, and the ZSTD_DCtx::ddict remains as-is.
//	 *
//	 * ZSTD_d_refMultipleDDicts must be enabled for this function to be called.
//	 */
func ZSTD_DCtx_selectFrameDDict(tls *libc.TLS, dctx uintptr) {
	var frameDDict uintptr
	_ = frameDDict
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict != 0 {
		frameDDict = ZSTD_DDictHashSet_getDDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID)
		if frameDDict != 0 {
			ZSTD_clearDict(tls, dctx)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = frameDDict
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_indefinitely)
		}
	}
}

/*-*************************************************************
 *   Frame header decoding
 ***************************************************************/

// C documentation
//
//	/*! ZSTD_isFrame() :
//	 *  Tells if the content of `buffer` starts with a valid Frame Identifier.
//	 *  Note : Frame Identifier is 4 bytes. If `size < 4`, @return will always be 0.
//	 *  Note 2 : Legacy Frame Identifiers are considered valid only if Legacy Support is enabled.
//	 *  Note 3 : Skippable Frame Identifiers are considered valid. */
func ZSTD_isFrame(tls *libc.TLS, buffer uintptr, size size_t) (r uint32) {
	var magic U32
	_ = magic
	if size < uint64(ZSTD_FRAMEIDSIZE) {
		return uint32(0)
	}
	magic = MEM_readLE32(tls, buffer)
	if magic == uint32(ZSTD_MAGICNUMBER) {
		return uint32(1)
	}
	if magic&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
		return uint32(1)
	}
	return uint32(0)
}

// C documentation
//
//	/*! ZSTD_isSkippableFrame() :
//	 *  Tells if the content of `buffer` starts with a valid Frame Identifier for a skippable frame.
//	 *  Note : Frame Identifier is 4 bytes. If `size < 4`, @return will always be 0.
//	 */
func ZSTD_isSkippableFrame(tls *libc.TLS, buffer uintptr, size size_t) (r uint32) {
	var magic U32
	_ = magic
	if size < uint64(ZSTD_FRAMEIDSIZE) {
		return uint32(0)
	}
	magic = MEM_readLE32(tls, buffer)
	if magic&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
		return uint32(1)
	}
	return uint32(0)
}

// C documentation
//
//	/** ZSTD_frameHeaderSize_internal() :
//	 *  srcSize must be large enough to reach header size fields.
//	 *  note : only works for formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless.
//	 * @return : size of the Frame Header
//	 *           or an error code, which can be tested with ZSTD_isError() */
func ZSTD_frameHeaderSize_internal(tls *libc.TLS, src uintptr, srcSize size_t, format ZSTD_format_e) (r size_t) {
	var dictID, fcsId, singleSegment U32
	var fhd BYTE
	var minInputSize size_t
	_, _, _, _, _ = dictID, fcsId, fhd, minInputSize, singleSegment
	minInputSize = ZSTD_startingInputLength(tls, format)
	if srcSize < minInputSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	fhd = *(*BYTE)(unsafe.Pointer(src + uintptr(minInputSize-uint64(1))))
	dictID = libc.Uint32FromInt32(libc.Int32FromUint8(fhd) & int32(3))
	singleSegment = libc.Uint32FromInt32(libc.Int32FromUint8(fhd) >> int32(5) & int32(1))
	fcsId = libc.Uint32FromInt32(libc.Int32FromUint8(fhd) >> int32(6))
	return minInputSize + libc.BoolUint64(!(singleSegment != 0)) + ZSTD_did_fieldSize[dictID] + ZSTD_fcs_fieldSize[fcsId] + libc.BoolUint64(singleSegment != 0 && !(fcsId != 0))
	return r
}

// C documentation
//
//	/** ZSTD_frameHeaderSize() :
//	 *  srcSize must be >= ZSTD_frameHeaderSize_prefix.
//	 * @return : size of the Frame Header,
//	 *           or an error code (if srcSize is too small) */
func ZSTD_frameHeaderSize(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_frameHeaderSize_internal(tls, src, srcSize, int32(ZSTD_f_zstd1))
}

// C documentation
//
//	/** ZSTD_getFrameHeader_advanced() :
//	 *  decode Frame Header, or require larger `srcSize`.
//	 *  note : only works for formats ZSTD_f_zstd1 and ZSTD_f_zstd1_magicless
//	 * @return : 0, `zfhPtr` is correctly filled,
//	 *          >0, `srcSize` is too small, value is wanted `srcSize` amount,
//	**           or an error code, which can be tested using ZSTD_isError() */
func ZSTD_getFrameHeader_advanced(tls *libc.TLS, zfhPtr uintptr, src uintptr, srcSize size_t, format ZSTD_format_e) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var checksumFlag, dictID, dictIDSizeCode, fcsID, singleSegment, windowLog U32
	var fhdByte, wlByte BYTE
	var fhsize, minInputSize, pos, toCopy, v2 size_t
	var frameContentSize, windowSize U64
	var ip uintptr
	var v1 uint64
	var _ /* hbuf at bp+0 */ [4]uint8
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = checksumFlag, dictID, dictIDSizeCode, fcsID, fhdByte, fhsize, frameContentSize, ip, minInputSize, pos, singleSegment, toCopy, windowLog, windowSize, wlByte, v1, v2
	ip = src
	minInputSize = ZSTD_startingInputLength(tls, format)
	if srcSize > uint64(0) {
		/* note : technically could be considered an assert(), since it's an invalid entry */
		if src == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+6872, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		}
	}
	if srcSize < minInputSize {
		if srcSize > uint64(0) && format != int32(ZSTD_f_zstd1_magicless) {
			if libc.Uint64FromInt32(libc.Int32FromInt32(4)) < srcSize {
				v1 = libc.Uint64FromInt32(libc.Int32FromInt32(4))
			} else {
				v1 = srcSize
			}
			/* when receiving less than @minInputSize bytes,
			 * control these bytes at least correspond to a supported magic number
			 * in order to error out early if they don't.
			**/
			toCopy = v1
			MEM_writeLE32(tls, bp, uint32(ZSTD_MAGICNUMBER))
			libc.Xmemcpy(tls, bp, src, toCopy)
			if MEM_readLE32(tls, bp) != uint32(ZSTD_MAGICNUMBER) {
				/* not a zstd frame : let's check if it's a skippable frame */
				MEM_writeLE32(tls, bp, uint32(ZSTD_MAGIC_SKIPPABLE_START))
				libc.Xmemcpy(tls, bp, src, toCopy)
				if MEM_readLE32(tls, bp)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) != uint32(ZSTD_MAGIC_SKIPPABLE_START) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+6917, 0)
					}
					return libc.Uint64FromInt32(-int32(ZSTD_error_prefix_unknown))
				}
			}
		}
		return minInputSize
	}
	libc.Xmemset(tls, zfhPtr, 0, libc.Uint64FromInt64(48)) /* not strictly necessary, but static analyzers may not understand that zfhPtr will be read only if return value is zero, since they are 2 different signals */
	if format != int32(ZSTD_f_zstd1_magicless) && MEM_readLE32(tls, src) != uint32(ZSTD_MAGICNUMBER) {
		if MEM_readLE32(tls, src)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
			/* skippable frame */
			if srcSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) {
				return uint64(ZSTD_SKIPPABLEHEADERSIZE)
			} /* magic number + frame length */
			libc.Xmemset(tls, zfhPtr, 0, libc.Uint64FromInt64(48))
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeType = int32(ZSTD_skippableFrame)
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FdictID = MEM_readLE32(tls, src) - uint32(ZSTD_MAGIC_SKIPPABLE_START)
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FheaderSize = uint32(ZSTD_SKIPPABLEHEADERSIZE)
			(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeContentSize = uint64(MEM_readLE32(tls, src+uintptr(ZSTD_FRAMEIDSIZE)))
			return uint64(0)
		}
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_prefix_unknown))
	}
	/* ensure there is enough `srcSize` to fully read/decode frame header */
	fhsize = ZSTD_frameHeaderSize_internal(tls, src, srcSize, format)
	if srcSize < fhsize {
		return fhsize
	}
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FheaderSize = uint32(fhsize)
	fhdByte = *(*BYTE)(unsafe.Pointer(ip + uintptr(minInputSize-uint64(1))))
	pos = minInputSize
	dictIDSizeCode = libc.Uint32FromInt32(libc.Int32FromUint8(fhdByte) & int32(3))
	checksumFlag = libc.Uint32FromInt32(libc.Int32FromUint8(fhdByte) >> int32(2) & int32(1))
	singleSegment = libc.Uint32FromInt32(libc.Int32FromUint8(fhdByte) >> int32(5) & int32(1))
	fcsID = libc.Uint32FromInt32(libc.Int32FromUint8(fhdByte) >> int32(6))
	windowSize = uint64(0)
	dictID = uint32(0)
	frameContentSize = uint64(libc.Uint64FromUint64(0) - libc.Uint64FromInt32(1))
	if libc.Int32FromUint8(fhdByte)&int32(0x08) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+6976, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_unsupported))
	}
	if !(singleSegment != 0) {
		v2 = pos
		pos = pos + 1
		wlByte = *(*BYTE)(unsafe.Pointer(ip + uintptr(v2)))
		windowLog = libc.Uint32FromInt32(libc.Int32FromUint8(wlByte)>>int32(3) + int32(ZSTD_WINDOWLOG_ABSOLUTEMIN))
		if windowLog > libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_windowTooLarge))
		}
		windowSize = uint64(libc.Uint64FromUint64(1) << windowLog)
		windowSize = windowSize + windowSize>>libc.Int32FromInt32(3)*libc.Uint64FromInt32(libc.Int32FromUint8(wlByte)&libc.Int32FromInt32(7))
	}
	switch dictIDSizeCode {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
	case uint32(1):
		dictID = uint32(*(*BYTE)(unsafe.Pointer(ip + uintptr(pos))))
		pos = pos + 1
	case uint32(2):
		dictID = uint32(MEM_readLE16(tls, ip+uintptr(pos)))
		pos = pos + uint64(2)
	case uint32(3):
		dictID = MEM_readLE32(tls, ip+uintptr(pos))
		pos = pos + uint64(4)
		break
	}
	switch fcsID {
	default:
		/* impossible */
		fallthrough
	case uint32(0):
		if singleSegment != 0 {
			frameContentSize = uint64(*(*BYTE)(unsafe.Pointer(ip + uintptr(pos))))
		}
	case uint32(1):
		frameContentSize = libc.Uint64FromInt32(libc.Int32FromUint16(MEM_readLE16(tls, ip+uintptr(pos))) + int32(256))
	case uint32(2):
		frameContentSize = uint64(MEM_readLE32(tls, ip+uintptr(pos)))
	case uint32(3):
		frameContentSize = MEM_readLE64(tls, ip+uintptr(pos))
		break
	}
	if singleSegment != 0 {
		windowSize = frameContentSize
	}
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeType = int32(ZSTD_frame)
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FframeContentSize = frameContentSize
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FwindowSize = windowSize
	if windowSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		v1 = windowSize
	} else {
		v1 = libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FblockSizeMax = uint32(v1)
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FdictID = dictID
	(*ZSTD_FrameHeader)(unsafe.Pointer(zfhPtr)).FchecksumFlag = checksumFlag
	return uint64(0)
}

// C documentation
//
//	/** ZSTD_getFrameHeader() :
//	 *  decode Frame Header, or require larger `srcSize`.
//	 *  note : this function does not consume input, it only reads it.
//	 * @return : 0, `zfhPtr` is correctly filled,
//	 *          >0, `srcSize` is too small, value is wanted `srcSize` amount,
//	 *           or an error code, which can be tested using ZSTD_isError() */
func ZSTD_getFrameHeader(tls *libc.TLS, zfhPtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_getFrameHeader_advanced(tls, zfhPtr, src, srcSize, int32(ZSTD_f_zstd1))
}

// C documentation
//
//	/** ZSTD_getFrameContentSize() :
//	 *  compatible with legacy mode
//	 * @return : decompressed size of the single frame pointed to be `src` if known, otherwise
//	 *         - ZSTD_CONTENTSIZE_UNKNOWN if the size cannot be determined
//	 *         - ZSTD_CONTENTSIZE_ERROR if an error occurred (e.g. invalid magic number, srcSize too small) */
func ZSTD_getFrameContentSize(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var _ /* zfh at bp+0 */ ZSTD_FrameHeader
	if ZSTD_getFrameHeader(tls, bp, src, srcSize) != uint64(0) {
		return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
	}
	if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FframeType == int32(ZSTD_skippableFrame) {
		return uint64(0)
	} else {
		return (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FframeContentSize
	}
	return r
}

func readSkippableFrameSize(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	var sizeU32 U32
	var skippableHeaderSize, skippableSize size_t
	_, _, _ = sizeU32, skippableHeaderSize, skippableSize
	skippableHeaderSize = uint64(ZSTD_SKIPPABLEHEADERSIZE)
	if srcSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	sizeU32 = MEM_readLE32(tls, src+uintptr(ZSTD_FRAMEIDSIZE))
	if sizeU32+libc.Uint32FromInt32(ZSTD_SKIPPABLEHEADERSIZE) < sizeU32 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_unsupported))
	}
	skippableSize = skippableHeaderSize + uint64(sizeU32)
	if skippableSize > srcSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	return skippableSize
	return r
}

// C documentation
//
//	/*! ZSTD_readSkippableFrame() :
//	 * Retrieves content of a skippable frame, and writes it to dst buffer.
//	 *
//	 * The parameter magicVariant will receive the magicVariant that was supplied when the frame was written,
//	 * i.e. magicNumber - ZSTD_MAGIC_SKIPPABLE_START.  This can be NULL if the caller is not interested
//	 * in the magicVariant.
//	 *
//	 * Returns an error if destination buffer is not large enough, or if this is not a valid skippable frame.
//	 *
//	 * @return : number of bytes written or a ZSTD error.
//	 */
func ZSTD_readSkippableFrame(tls *libc.TLS, dst uintptr, dstCapacity size_t, magicVariant uintptr, src uintptr, srcSize size_t) (r size_t) {
	var magicNumber U32
	var skippableContentSize, skippableFrameSize size_t
	_, _, _ = magicNumber, skippableContentSize, skippableFrameSize
	if srcSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	magicNumber = MEM_readLE32(tls, src)
	skippableFrameSize = readSkippableFrameSize(tls, src, srcSize)
	skippableContentSize = skippableFrameSize - uint64(ZSTD_SKIPPABLEHEADERSIZE)
	/* check input validity */
	if !(ZSTD_isSkippableFrame(tls, src, srcSize) != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_unsupported))
	}
	if skippableFrameSize < uint64(ZSTD_SKIPPABLEHEADERSIZE) || skippableFrameSize > srcSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if skippableContentSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* deliver payload */
	if skippableContentSize > uint64(0) && dst != libc.UintptrFromInt32(0) {
		libc.Xmemcpy(tls, dst, src+libc.UintptrFromInt32(ZSTD_SKIPPABLEHEADERSIZE), skippableContentSize)
	}
	if magicVariant != libc.UintptrFromInt32(0) {
		*(*uint32)(unsafe.Pointer(magicVariant)) = magicNumber - uint32(ZSTD_MAGIC_SKIPPABLE_START)
	}
	return skippableContentSize
	return r
}

// C documentation
//
//	/** ZSTD_findDecompressedSize() :
//	 *  `srcSize` must be the exact length of some number of ZSTD compressed and/or
//	 *      skippable frames
//	 *  note: compatible with legacy mode
//	 * @return : decompressed size of the frames contained */
func ZSTD_findDecompressedSize(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	var fcs, totalDstSize uint64
	var frameSrcSize, skippableSize size_t
	var magicNumber U32
	_, _, _, _, _ = fcs, frameSrcSize, magicNumber, skippableSize, totalDstSize
	totalDstSize = uint64(0)
	for srcSize >= ZSTD_startingInputLength(tls, int32(ZSTD_f_zstd1)) {
		magicNumber = MEM_readLE32(tls, src)
		if magicNumber&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
			skippableSize = readSkippableFrameSize(tls, src, srcSize)
			if ZSTD_isError(tls, skippableSize) != 0 {
				return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
			}
			src = src + uintptr(skippableSize)
			srcSize = srcSize - skippableSize
			continue
		}
		fcs = ZSTD_getFrameContentSize(tls, src, srcSize)
		if fcs >= libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
			return fcs
		}
		if totalDstSize+fcs < totalDstSize {
			return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
		} /* check for overflow */
		totalDstSize = totalDstSize + fcs
		/* skip to next frame */
		frameSrcSize = ZSTD_findFrameCompressedSize(tls, src, srcSize)
		if ZSTD_isError(tls, frameSrcSize) != 0 {
			return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
		}
		src = src + uintptr(frameSrcSize)
		srcSize = srcSize - frameSrcSize
	} /* while (srcSize >= ZSTD_frameHeaderSize_prefix) */
	if srcSize != 0 {
		return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
	}
	return totalDstSize
}

// C documentation
//
//	/** ZSTD_getDecompressedSize() :
//	 *  compatible with legacy mode
//	 * @return : decompressed size if known, 0 otherwise
//	             note : 0 can mean any of the following :
//	                   - frame content is empty
//	                   - decompressed size field is not present in frame header
//	                   - frame header unknown / not supported
//	                   - frame header not complete (`srcSize` too small) */
func ZSTD_getDecompressedSize(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	var ret, v1 uint64
	_, _ = ret, v1
	ret = ZSTD_getFrameContentSize(tls, src, srcSize)
	_ = libc.Uint64FromInt64(1)
	if ret >= libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
		v1 = uint64(0)
	} else {
		v1 = ret
	}
	return v1
}

// C documentation
//
//	/** ZSTD_decodeFrameHeader() :
//	 * `headerSize` must be the size provided by ZSTD_frameHeaderSize().
//	 * If multiple DDict references are enabled, also will choose the correct DDict to use.
//	 * @return : 0 if success, or an error code, which can be tested using ZSTD_isError() */
func ZSTD_decodeFrameHeader(tls *libc.TLS, dctx uintptr, src uintptr, headerSize size_t) (r size_t) {
	var result size_t
	var v1 int32
	_, _ = result, v1
	result = ZSTD_getFrameHeader_advanced(tls, dctx+29928, src, headerSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat)
	if ZSTD_isError(tls, result) != 0 {
		return result
	} /* invalid header */
	if result > uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7004, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Reference DDict requested by frame if dctx references multiple ddicts */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts == int32(ZSTD_rmd_refMultipleDDicts) && (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet != 0 {
		ZSTD_DCtx_selectFrameDDict(tls, dctx)
	}
	/* Skip the dictID check in fuzzing mode, because it makes the search
	 * harder.
	 */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID != 0 && (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FdictID {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_wrong))
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 && !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum != 0) {
		v1 = int32(1)
	} else {
		v1 = 0
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum = libc.Uint32FromInt32(v1)
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
		XXH_INLINE_XXH64_reset(tls, dctx+30008, uint64(0))
	}
	*(*U64)(unsafe.Pointer(dctx + 29976)) += headerSize
	return uint64(0)
}

func ZSTD_errorFrameSizeInfo(tls *libc.TLS, ret size_t) (r ZSTD_frameSizeInfo) {
	var frameSizeInfo ZSTD_frameSizeInfo
	_ = frameSizeInfo
	frameSizeInfo.FcompressedSize = ret
	frameSizeInfo.FdecompressedBound = libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
	return frameSizeInfo
}

func ZSTD_findFrameSizeInfo(tls *libc.TLS, src uintptr, srcSize size_t, format ZSTD_format_e) (r ZSTD_frameSizeInfo) {
	bp := tls.Alloc(96)
	defer tls.Free(96)
	var cBlockSize, nbBlocks, remainingSize, ret size_t
	var ip, ipstart uintptr
	var v1 uint64
	var _ /* blockProperties at bp+72 */ blockProperties_t
	var _ /* frameSizeInfo at bp+0 */ ZSTD_frameSizeInfo
	var _ /* zfh at bp+24 */ ZSTD_FrameHeader
	_, _, _, _, _, _, _ = cBlockSize, ip, ipstart, nbBlocks, remainingSize, ret, v1
	libc.Xmemset(tls, bp, 0, libc.Uint64FromInt64(24))
	if format == int32(ZSTD_f_zstd1) && srcSize >= uint64(ZSTD_SKIPPABLEHEADERSIZE) && MEM_readLE32(tls, src)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FcompressedSize = readSkippableFrameSize(tls, src, srcSize)
		return *(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))
	} else {
		ip = src
		ipstart = ip
		remainingSize = srcSize
		nbBlocks = uint64(0)
		/* Extract Frame Header */
		ret = ZSTD_getFrameHeader_advanced(tls, bp+24, src, srcSize, format)
		if ZSTD_isError(tls, ret) != 0 {
			return ZSTD_errorFrameSizeInfo(tls, ret)
		}
		if ret > uint64(0) {
			return ZSTD_errorFrameSizeInfo(tls, libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong)))
		}
		ip = ip + uintptr((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FheaderSize)
		remainingSize = remainingSize - uint64((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FheaderSize)
		/* Iterate over each block */
		for int32(1) != 0 {
			cBlockSize = ZSTD_getcBlockSize(tls, ip, remainingSize, bp+72)
			if ZSTD_isError(tls, cBlockSize) != 0 {
				return ZSTD_errorFrameSizeInfo(tls, cBlockSize)
			}
			if ZSTD_blockHeaderSize+cBlockSize > remainingSize {
				return ZSTD_errorFrameSizeInfo(tls, libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong)))
			}
			ip = ip + uintptr(ZSTD_blockHeaderSize+cBlockSize)
			remainingSize = remainingSize - (ZSTD_blockHeaderSize + cBlockSize)
			nbBlocks = nbBlocks + 1
			if (*(*blockProperties_t)(unsafe.Pointer(bp + 72))).FlastBlock != 0 {
				break
			}
		}
		/* Final frame content checksum */
		if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FchecksumFlag != 0 {
			if remainingSize < uint64(4) {
				return ZSTD_errorFrameSizeInfo(tls, libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong)))
			}
			ip = ip + uintptr(4)
		}
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FnbBlocks = nbBlocks
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FcompressedSize = libc.Uint64FromInt64(int64(ip) - int64(ipstart))
		if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
			v1 = (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FframeContentSize
		} else {
			v1 = nbBlocks * uint64((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp + 24))).FblockSizeMax)
		}
		(*(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))).FdecompressedBound = v1
		return *(*ZSTD_frameSizeInfo)(unsafe.Pointer(bp))
	}
	return r
}

func ZSTD_findFrameCompressedSize_advanced(tls *libc.TLS, src uintptr, srcSize size_t, format ZSTD_format_e) (r size_t) {
	var frameSizeInfo ZSTD_frameSizeInfo
	_ = frameSizeInfo
	frameSizeInfo = ZSTD_findFrameSizeInfo(tls, src, srcSize, format)
	return frameSizeInfo.FcompressedSize
}

// C documentation
//
//	/** ZSTD_findFrameCompressedSize() :
//	 * See docs in zstd.h
//	 * Note: compatible with legacy mode */
func ZSTD_findFrameCompressedSize(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_findFrameCompressedSize_advanced(tls, src, srcSize, int32(ZSTD_f_zstd1))
}

// C documentation
//
//	/** ZSTD_decompressBound() :
//	 *  compatible with legacy mode
//	 *  `src` must point to the start of a ZSTD frame or a skippable frame
//	 *  `srcSize` must be at least as large as the frame contained
//	 *  @return : the maximum decompressed size of the compressed source
//	 */
func ZSTD_decompressBound(tls *libc.TLS, src uintptr, srcSize size_t) (r uint64) {
	var bound, decompressedBound uint64
	var compressedSize size_t
	var frameSizeInfo ZSTD_frameSizeInfo
	_, _, _, _ = bound, compressedSize, decompressedBound, frameSizeInfo
	bound = uint64(0)
	/* Iterate over each frame */
	for srcSize > uint64(0) {
		frameSizeInfo = ZSTD_findFrameSizeInfo(tls, src, srcSize, int32(ZSTD_f_zstd1))
		compressedSize = frameSizeInfo.FcompressedSize
		decompressedBound = frameSizeInfo.FdecompressedBound
		if ZSTD_isError(tls, compressedSize) != 0 || decompressedBound == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
			return libc.Uint64FromUint64(0) - libc.Uint64FromInt32(2)
		}
		src = src + uintptr(compressedSize)
		srcSize = srcSize - compressedSize
		bound = bound + decompressedBound
	}
	return bound
}

func ZSTD_decompressionMargin(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var compressedSize, err_code, margin size_t
	var decompressedBound uint64
	var frameSizeInfo ZSTD_frameSizeInfo
	var maxBlockSize, v2 uint32
	var v1 int32
	var _ /* zfh at bp+0 */ ZSTD_FrameHeader
	_, _, _, _, _, _, _, _ = compressedSize, decompressedBound, err_code, frameSizeInfo, margin, maxBlockSize, v1, v2
	margin = uint64(0)
	maxBlockSize = uint32(0)
	/* Iterate over each frame */
	for srcSize > uint64(0) {
		frameSizeInfo = ZSTD_findFrameSizeInfo(tls, src, srcSize, int32(ZSTD_f_zstd1))
		compressedSize = frameSizeInfo.FcompressedSize
		decompressedBound = frameSizeInfo.FdecompressedBound
		err_code = ZSTD_getFrameHeader(tls, bp, src, srcSize)
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
		if ZSTD_isError(tls, compressedSize) != 0 || decompressedBound == libc.Uint64FromUint64(0)-libc.Uint64FromInt32(2) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FframeType == int32(ZSTD_frame) {
			/* Add the frame header to our margin */
			margin = margin + uint64((*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FheaderSize)
			/* Add the checksum to our margin */
			if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FchecksumFlag != 0 {
				v1 = int32(4)
			} else {
				v1 = 0
			}
			margin = margin + libc.Uint64FromInt32(v1)
			/* Add 3 bytes per block */
			margin = margin + uint64(3)*frameSizeInfo.FnbBlocks
			/* Compute the max block size */
			if maxBlockSize > (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FblockSizeMax {
				v2 = maxBlockSize
			} else {
				v2 = (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FblockSizeMax
			}
			maxBlockSize = v2
		} else {
			/* Add the entire skippable frame size to our margin. */
			margin = margin + compressedSize
		}
		src = src + uintptr(compressedSize)
		srcSize = srcSize - compressedSize
	}
	/* Add the max block size back to the margin. */
	margin = margin + uint64(maxBlockSize)
	return margin
}

/*-*************************************************************
 *   Frame decoding
 ***************************************************************/

// C documentation
//
//	/** ZSTD_insertBlock() :
//	 *  insert `src` block into `dctx` history. Useful to track uncompressed blocks. */
func ZSTD_insertBlock(tls *libc.TLS, dctx uintptr, blockStart uintptr, blockSize size_t) (r size_t) {
	ZSTD_checkContinuity(tls, dctx, blockStart, blockSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = blockStart + uintptr(blockSize)
	return blockSize
}

func ZSTD_copyRawBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	if srcSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if dst == libc.UintptrFromInt32(0) {
		if srcSize == uint64(0) {
			return uint64(0)
		}
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstBuffer_null))
	}
	libc.Xmemmove(tls, dst, src, srcSize)
	return srcSize
}

func ZSTD_setRleBlock(tls *libc.TLS, dst uintptr, dstCapacity size_t, b BYTE, regenSize size_t) (r size_t) {
	if regenSize > dstCapacity {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if dst == libc.UintptrFromInt32(0) {
		if regenSize == uint64(0) {
			return uint64(0)
		}
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstBuffer_null))
	}
	libc.Xmemset(tls, dst, libc.Int32FromUint8(b), regenSize)
	return regenSize
}

func ZSTD_DCtx_trace_end(tls *libc.TLS, dctx uintptr, uncompressedSize U64, compressedSize U64, streaming int32) {
	_ = dctx
	_ = uncompressedSize
	_ = compressedSize
	_ = streaming
}

// C documentation
//
//	/*! ZSTD_decompressFrame() :
//	 * @dctx must be properly initialized
//	 *  will update *srcPtr and *srcSizePtr,
//	 *  to make *srcPtr progress by one frame. */
func ZSTD_decompressFrame(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, srcPtr uintptr, srcSizePtr uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cBlockSize, decodedSize, err_code, err_code1, frameHeaderSize, remainingSrcSize size_t
	var checkCalc, checkRead U32
	var ip, istart, oBlockEnd, oend, op, ostart, v1 uintptr
	var v2 int32
	var v4 uint32
	var _ /* blockProperties at bp+0 */ blockProperties_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = cBlockSize, checkCalc, checkRead, decodedSize, err_code, err_code1, frameHeaderSize, ip, istart, oBlockEnd, oend, op, ostart, remainingSrcSize, v1, v2, v4
	istart = *(*uintptr)(unsafe.Pointer(srcPtr))
	ip = istart
	ostart = dst
	if dstCapacity != uint64(0) {
		v1 = ostart + uintptr(dstCapacity)
	} else {
		v1 = ostart
	}
	oend = v1
	op = ostart
	remainingSrcSize = *(*size_t)(unsafe.Pointer(srcSizePtr))
	/* check */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) {
		v2 = int32(6)
	} else {
		v2 = int32(2)
	}
	if remainingSrcSize < libc.Uint64FromInt32(v2)+ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Frame Header */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) {
		v2 = int32(5)
	} else {
		v2 = int32(1)
	}
	frameHeaderSize = ZSTD_frameHeaderSize_internal(tls, ip, libc.Uint64FromInt32(v2), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat)
	if ZSTD_isError(tls, frameHeaderSize) != 0 {
		return frameHeaderSize
	}
	if remainingSrcSize < frameHeaderSize+ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	err_code = ZSTD_decodeFrameHeader(tls, dctx, ip, frameHeaderSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	ip = ip + uintptr(frameHeaderSize)
	remainingSrcSize = remainingSrcSize - frameHeaderSize
	/* Shrink the blockSizeMax if enabled */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam != 0 {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax < libc.Uint32FromInt32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam) {
			v4 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax
		} else {
			v4 = libc.Uint32FromInt32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam)
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax = v4
	}
	/* Loop on each block */
	for int32(1) != 0 {
		oBlockEnd = oend
		cBlockSize = ZSTD_getcBlockSize(tls, ip, remainingSrcSize, bp)
		if ZSTD_isError(tls, cBlockSize) != 0 {
			return cBlockSize
		}
		ip = ip + uintptr(ZSTD_blockHeaderSize)
		remainingSrcSize = remainingSrcSize - ZSTD_blockHeaderSize
		if cBlockSize > remainingSrcSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
		}
		if ip >= op && ip < oBlockEnd {
			/* We are decompressing in-place. Limit the output pointer so that we
			 * don't overwrite the block that we are currently reading. This will
			 * fail decompression if the input & output pointers aren't spaced
			 * far enough apart.
			 *
			 * This is important to set, even when the pointers are far enough
			 * apart, because ZSTD_decompressBlock_internal() can decide to store
			 * literals in the output buffer, after the block it is decompressing.
			 * Since we don't want anything to overwrite our input, we have to tell
			 * ZSTD_decompressBlock_internal to never write past ip.
			 *
			 * See ZSTD_allocateLiteralsBuffer() for reference.
			 */
			oBlockEnd = op + uintptr(int64(ip)-int64(op))
		}
		switch (*(*blockProperties_t)(unsafe.Pointer(bp))).FblockType {
		case int32(bt_compressed):
			goto _5
		case int32(bt_raw):
			goto _6
		case int32(bt_rle):
			goto _7
		default:
			goto _8
		case int32(bt_reserved):
			goto _9
		}
		goto _10
	_5:
		;
		decodedSize = ZSTD_decompressBlock_internal(tls, dctx, op, libc.Uint64FromInt64(int64(oBlockEnd)-int64(op)), ip, cBlockSize, int32(not_streaming))
		goto _10
	_6:
		;
		/* Use oend instead of oBlockEnd because this function is safe to overlap. It uses memmove. */
		decodedSize = ZSTD_copyRawBlock(tls, op, libc.Uint64FromInt64(int64(oend)-int64(op)), ip, cBlockSize)
		goto _10
	_7:
		;
		decodedSize = ZSTD_setRleBlock(tls, op, libc.Uint64FromInt64(int64(oBlockEnd)-int64(op)), *(*BYTE)(unsafe.Pointer(ip)), uint64((*(*blockProperties_t)(unsafe.Pointer(bp))).ForigSize))
		goto _10
	_9:
		;
	_8:
		;
	_13:
		;
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7025, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		goto _12
	_12:
		;
		if 0 != 0 {
			goto _13
		}
		goto _11
	_11:
		;
	_10:
		;
		err_code1 = decodedSize
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7044, 0)
			}
			return err_code1
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
			XXH_INLINE_XXH64_update(tls, dctx+30008, op, decodedSize)
		}
		if decodedSize != 0 { /* support dst = NULL,0 */
			op = op + uintptr(decodedSize)
		}
		ip = ip + uintptr(cBlockSize)
		remainingSrcSize = remainingSrcSize - cBlockSize
		if (*(*blockProperties_t)(unsafe.Pointer(bp))).FlastBlock != 0 {
			break
		}
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) {
		if uint64(libc.Uint64FromInt64(int64(op)-int64(ostart))) != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 { /* Frame content checksum verification */
		if remainingSrcSize < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_checksum_wrong))
		}
		if !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum != 0) {
			checkCalc = uint32(XXH_INLINE_XXH64_digest(tls, dctx+30008))
			checkRead = MEM_readLE32(tls, ip)
			if checkRead != checkCalc {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_checksum_wrong))
			}
		}
		ip = ip + uintptr(4)
		remainingSrcSize = remainingSrcSize - uint64(4)
	}
	ZSTD_DCtx_trace_end(tls, dctx, libc.Uint64FromInt64(int64(op)-int64(ostart)), libc.Uint64FromInt64(int64(ip)-int64(istart)), 0)
	/* Allow caller to get size read */
	*(*uintptr)(unsafe.Pointer(srcPtr)) = ip
	*(*size_t)(unsafe.Pointer(srcSizePtr)) = remainingSrcSize
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_decompressMultiFrame(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, _src uintptr, _srcSize size_t, dict uintptr, dictSize size_t, ddict uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _src
	*(*size_t)(unsafe.Pointer(bp + 8)) = _srcSize
	var dststart uintptr
	var err_code, err_code1, err_code2, res, skippableSize size_t
	var magicNumber U32
	var moreThan1Frame int32
	_, _, _, _, _, _, _, _ = dststart, err_code, err_code1, err_code2, magicNumber, moreThan1Frame, res, skippableSize
	dststart = dst
	moreThan1Frame = 0
	/* either dict or ddict set, not both */
	if ddict != 0 {
		dict = ZSTD_DDict_dictContent(tls, ddict)
		dictSize = ZSTD_DDict_dictSize(tls, ddict)
	}
	for *(*size_t)(unsafe.Pointer(bp + 8)) >= ZSTD_startingInputLength(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat) {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) && *(*size_t)(unsafe.Pointer(bp + 8)) >= uint64(4) {
			magicNumber = MEM_readLE32(tls, *(*uintptr)(unsafe.Pointer(bp)))
			if magicNumber&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) {
				/* skippable frame detected : skip it */
				skippableSize = readSkippableFrameSize(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*size_t)(unsafe.Pointer(bp + 8)))
				err_code = skippableSize
				if ERR_isError(tls, err_code) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+7072, 0)
					}
					return err_code
				}
				*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(skippableSize)
				*(*size_t)(unsafe.Pointer(bp + 8)) = *(*size_t)(unsafe.Pointer(bp + 8)) - skippableSize
				continue /* check next frame */
			}
		}
		if ddict != 0 {
			/* we were called from ZSTD_decompress_usingDDict */
			err_code1 = ZSTD_decompressBegin_usingDDict(tls, dctx, ddict)
			if ERR_isError(tls, err_code1) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code1
			}
		} else {
			/* this will initialize correctly with no dict if dict == NULL, so
			 * use this in all cases but ddict */
			err_code2 = ZSTD_decompressBegin_usingDict(tls, dctx, dict, dictSize)
			if ERR_isError(tls, err_code2) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code2
			}
		}
		ZSTD_checkContinuity(tls, dctx, dst, dstCapacity)
		res = ZSTD_decompressFrame(tls, dctx, dst, dstCapacity, bp, bp+8)
		if ZSTD_getErrorCode(tls, res) == int32(ZSTD_error_prefix_unknown) && moreThan1Frame == int32(1) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7096, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
		}
		if ZSTD_isError(tls, res) != 0 {
			return res
		}
		if res != uint64(0) {
			dst = dst + uintptr(res)
		}
		dstCapacity = dstCapacity - res
		moreThan1Frame = int32(1)
	} /* while (srcSize >= ZSTD_frameHeaderSize_prefix) */
	if *(*size_t)(unsafe.Pointer(bp + 8)) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7453, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	return libc.Uint64FromInt64(int64(dst) - int64(dststart))
}

func ZSTD_decompress_usingDict(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_decompressMultiFrame(tls, dctx, dst, dstCapacity, src, srcSize, dict, dictSize, libc.UintptrFromInt32(0))
}

func ZSTD_getDDict(tls *libc.TLS, dctx uintptr) (r uintptr) {
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses {
	default:
		fallthrough
	case int32(ZSTD_dont_use):
		ZSTD_clearDict(tls, dctx)
		return libc.UintptrFromInt32(0)
	case int32(ZSTD_use_indefinitely):
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict
	case int32(ZSTD_use_once):
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_dont_use)
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict
	}
	return r
}

func ZSTD_decompressDCtx(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_decompress_usingDDict(tls, dctx, dst, dstCapacity, src, srcSize, ZSTD_getDDict(tls, dctx))
}

func ZSTD_decompress(tls *libc.TLS, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var dctx uintptr
	var regenSize size_t
	_, _ = dctx, regenSize
	dctx = ZSTD_createDCtx_internal(tls, ZSTD_defaultCMem)
	if dctx == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1377, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	regenSize = ZSTD_decompressDCtx(tls, dctx, dst, dstCapacity, src, srcSize)
	ZSTD_freeDCtx(tls, dctx)
	return regenSize
}

// C documentation
//
//	/*-**************************************
//	*   Advanced Streaming Decompression API
//	*   Bufferless and synchronous
//	****************************************/
func ZSTD_nextSrcSizeToDecompress(tls *libc.TLS, dctx uintptr) (r size_t) {
	return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
}

// C documentation
//
//	/**
//	 * Similar to ZSTD_nextSrcSizeToDecompress(), but when a block input can be streamed, we
//	 * allow taking a partial block as the input. Currently only raw uncompressed blocks can
//	 * be streamed.
//	 *
//	 * For blocks that can be streamed, this allows us to reduce the latency until we produce
//	 * output, and avoid copying the input.
//	 *
//	 * @param inputSize - The total amount of input that the caller currently has.
//	 */
func ZSTD_nextSrcSizeToDecompressWithInputSize(tls *libc.TLS, dctx uintptr, inputSize size_t) (r size_t) {
	var v1, v2, v3 uint64
	_, _, _ = v1, v2, v3
	if !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_decompressBlock) || (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_decompressLastBlock)) {
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType != int32(bt_raw) {
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
	}
	if inputSize < (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected {
		v2 = inputSize
	} else {
		v2 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
	}
	if libc.Uint64FromInt32(libc.Int32FromInt32(1)) > v2 {
		v1 = libc.Uint64FromInt32(libc.Int32FromInt32(1))
	} else {
		if inputSize < (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected {
			v3 = inputSize
		} else {
			v3 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected
		}
		v1 = v3
	}
	return v1
}

func ZSTD_nextInputType(tls *libc.TLS, dctx uintptr) (r ZSTD_nextInputType_e) {
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage {
	default: /* should not happen */
		fallthrough
	case int32(ZSTDds_getFrameHeaderSize):
		fallthrough
	case int32(ZSTDds_decodeFrameHeader):
		return int32(ZSTDnit_frameHeader)
	case int32(ZSTDds_decodeBlockHeader):
		return int32(ZSTDnit_blockHeader)
	case int32(ZSTDds_decompressBlock):
		return int32(ZSTDnit_block)
	case int32(ZSTDds_decompressLastBlock):
		return int32(ZSTDnit_lastBlock)
	case int32(ZSTDds_checkChecksum):
		return int32(ZSTDnit_checksum)
	case int32(ZSTDds_decodeSkippableHeader):
		fallthrough
	case int32(ZSTDds_skipFrame):
		return int32(ZSTDnit_skippableFrame)
	}
	return r
}

func ZSTD_isSkipFrame(tls *libc.TLS, dctx uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_skipFrame))
}

// C documentation
//
//	/** ZSTD_decompressContinue() :
//	 *  srcSize : must be the exact nb of bytes expected (see ZSTD_nextSrcSizeToDecompress())
//	 *  @return : nb of bytes generated into `dst` (necessarily <= `dstCapacity)
//	 *            or an error code, which can be tested using ZSTD_isError() */
func ZSTD_decompressContinue(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var cBlockSize, err_code, err_code1, err_code2, rSize size_t
	var check32, h32 U32
	var v11 int32
	var _ /* bp at bp+0 */ blockProperties_t
	_, _, _, _, _, _, _, _ = cBlockSize, check32, err_code, err_code1, err_code2, h32, rSize, v11
	/* Sanity check */
	if srcSize != ZSTD_nextSrcSizeToDecompressWithInputSize(tls, dctx, srcSize) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7481, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	ZSTD_checkContinuity(tls, dctx, dst, dstCapacity)
	*(*U64)(unsafe.Pointer(dctx + 29976)) += srcSize
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage {
	case int32(ZSTDds_getFrameHeaderSize):
		goto _1
	case int32(ZSTDds_decodeFrameHeader):
		goto _2
	case int32(ZSTDds_decodeBlockHeader):
		goto _3
	case int32(ZSTDds_decompressBlock):
		goto _4
	case int32(ZSTDds_decompressLastBlock):
		goto _5
	case int32(ZSTDds_checkChecksum):
		goto _6
	case int32(ZSTDds_decodeSkippableHeader):
		goto _7
	case int32(ZSTDds_skipFrame):
		goto _8
	default:
		goto _9
	}
	goto _10
_1:
	;
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat == int32(ZSTD_f_zstd1) { /* allows header */
		/* to read skippable magic number */
		if MEM_readLE32(tls, src)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) { /* skippable frame */
			libc.Xmemcpy(tls, dctx+95940, src, srcSize)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(ZSTD_SKIPPABLEHEADERSIZE) - srcSize /* remaining to load to get full skippable frame header */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeSkippableHeader)
			return uint64(0)
		}
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize = ZSTD_frameHeaderSize_internal(tls, src, srcSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat)
	if ZSTD_isError(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize) != 0 {
		return (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize
	}
	libc.Xmemcpy(tls, dctx+95940, src, srcSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize - srcSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeFrameHeader)
	return uint64(0)
_2:
	;
	libc.Xmemcpy(tls, dctx+95940+uintptr((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize-srcSize), src, srcSize)
	err_code = ZSTD_decodeFrameHeader(tls, dctx, dctx+95940, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FheaderSize)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_blockHeaderSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeBlockHeader)
	return uint64(0)
_3:
	;
	cBlockSize = ZSTD_getcBlockSize(tls, src, ZSTD_blockHeaderSize, bp)
	if ZSTD_isError(tls, cBlockSize) != 0 {
		return cBlockSize
	}
	if cBlockSize > uint64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7493, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = cBlockSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType = (*(*blockProperties_t)(unsafe.Pointer(bp))).FblockType
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrleSize = uint64((*(*blockProperties_t)(unsafe.Pointer(bp))).ForigSize)
	if cBlockSize != 0 {
		if (*(*blockProperties_t)(unsafe.Pointer(bp))).FlastBlock != 0 {
			v11 = int32(ZSTDds_decompressLastBlock)
		} else {
			v11 = int32(ZSTDds_decompressBlock)
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = v11
		return uint64(0)
	}
	/* empty block */
	if (*(*blockProperties_t)(unsafe.Pointer(bp))).FlastBlock != 0 {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 {
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(4)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_checkChecksum)
		} else {
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* end of frame */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
		}
	} else {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_blockHeaderSize /* jump to next header */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeBlockHeader)
	}
	return uint64(0)
_5:
	;
_4:
	;
_14:
	;
	goto _13
_13:
	;
	if 0 != 0 {
		goto _14
	}
	goto _12
_12:
	;
	switch (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType {
	case int32(bt_compressed):
		goto _15
	case int32(bt_raw):
		goto _16
	case int32(bt_rle):
		goto _17
	default:
		goto _18
	case int32(bt_reserved):
		goto _19
	}
	goto _20
_15:
	;
_23:
	;
	goto _22
_22:
	;
	if 0 != 0 {
		goto _23
	}
	goto _21
_21:
	;
	rSize = ZSTD_decompressBlock_internal(tls, dctx, dst, dstCapacity, src, srcSize, int32(is_streaming))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* Streaming not supported */
	goto _20
_16:
	;
	rSize = ZSTD_copyRawBlock(tls, dst, dstCapacity, src, srcSize)
	err_code1 = rSize
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7520, 0)
		}
		return err_code1
	}
	*(*size_t)(unsafe.Pointer(dctx + 29920)) -= rSize
	goto _20
_17:
	;
	rSize = ZSTD_setRleBlock(tls, dst, dstCapacity, *(*BYTE)(unsafe.Pointer(src)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrleSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* Streaming not supported */
	goto _20
_19:
	; /* should never happen */
_18:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+7025, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
_20:
	;
	err_code2 = rSize
	if ERR_isError(tls, err_code2) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code2
	}
	if rSize > uint64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7545, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	*(*U64)(unsafe.Pointer(dctx + 29984)) += rSize
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
		XXH_INLINE_XXH64_update(tls, dctx+30008, dst, rSize)
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dst + uintptr(rSize)
	/* Stay on the same stage until we are finished streaming the block. */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected > uint64(0) {
		return rSize
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage == int32(ZSTDds_decompressLastBlock) { /* end of frame */
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FframeContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FchecksumFlag != 0 { /* another round for frame checksum */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(4)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_checkChecksum)
		} else {
			ZSTD_DCtx_trace_end(tls, dctx, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprocessedCSize, int32(1))
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0) /* ends here */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
		}
	} else {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_decodeBlockHeader)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_blockHeaderSize
	}
	return rSize
_6:
	;
	/* guaranteed by dctx->expected */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvalidateChecksum != 0 {
		h32 = uint32(XXH_INLINE_XXH64_digest(tls, dctx+30008))
		check32 = MEM_readLE32(tls, src)
		if check32 != h32 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_checksum_wrong))
		}
	}
	ZSTD_DCtx_trace_end(tls, dctx, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprocessedCSize, int32(1))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
	return uint64(0)
_7:
	;
	libc.Xmemcpy(tls, dctx+95940+uintptr(libc.Uint64FromInt32(ZSTD_SKIPPABLEHEADERSIZE)-srcSize), src, srcSize)    /* complete skippable header */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(MEM_readLE32(tls, dctx+95940+uintptr(ZSTD_FRAMEIDSIZE))) /* note : dctx->expected can grow seriously large, beyond local buffer size */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_skipFrame)
	return uint64(0)
_8:
	;
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
	return uint64(0)
_9:
	;
	/* impossible */
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1561, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC)) /* some compilers require default to do something */
_10:
	;
	return r
}

func ZSTD_refDictContent(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = dict - uintptr(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd)-int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart))
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = dict
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dict + uintptr(dictSize)
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_loadDEntropy() :
//	 *  dict : must point at beginning of a valid zstd dictionary.
//	 * @return : size of entropy tables read */
func ZSTD_loadDEntropy(tls *libc.TLS, entropy uintptr, dict uintptr, dictSize size_t) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var dictContentSize, hSize, litlengthHeaderSize, matchlengthHeaderSize, offcodeHeaderSize, workspaceSize size_t
	var dictEnd, dictPtr, workspace uintptr
	var i int32
	var rep U32
	var _ /* litlengthLog at bp+264 */ uint32
	var _ /* litlengthMaxValue at bp+260 */ uint32
	var _ /* litlengthNCount at bp+188 */ [36]int16
	var _ /* matchlengthLog at bp+184 */ uint32
	var _ /* matchlengthMaxValue at bp+180 */ uint32
	var _ /* matchlengthNCount at bp+72 */ [53]int16
	var _ /* offcodeLog at bp+68 */ uint32
	var _ /* offcodeMaxValue at bp+64 */ uint32
	var _ /* offcodeNCount at bp+0 */ [32]int16
	_, _, _, _, _, _, _, _, _, _, _ = dictContentSize, dictEnd, dictPtr, hSize, i, litlengthHeaderSize, matchlengthHeaderSize, offcodeHeaderSize, rep, workspace, workspaceSize
	dictPtr = dict
	dictEnd = dictPtr + uintptr(dictSize)
	if dictSize <= uint64(8) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7585, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	/* dict must be valid */
	dictPtr = dictPtr + uintptr(8) /* skip header = magic + dictID */
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	_ = libc.Uint64FromInt64(1)
	workspace = entropy /* use fse tables as temporary workspace; implies fse tables are grouped together */
	workspaceSize = libc.Uint64FromInt64(4104) + libc.Uint64FromInt64(2056) + libc.Uint64FromInt64(4104)
	hSize = HUF_readDTableX2_wksp(tls, entropy+10264, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)), workspace, workspaceSize, 0)
	if ERR_isError(tls, hSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	dictPtr = dictPtr + uintptr(hSize)
	*(*uint32)(unsafe.Pointer(bp + 64)) = uint32(MaxOff)
	offcodeHeaderSize = FSE_readNCount(tls, bp, bp+64, bp+68, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, offcodeHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 64)) > uint32(MaxOff) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 68)) > uint32(OffFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	ZSTD_buildFSETable(tls, entropy+4104, bp, *(*uint32)(unsafe.Pointer(bp + 64)), uintptr(unsafe.Pointer(&OF_base)), uintptr(unsafe.Pointer(&OF_bits)), *(*uint32)(unsafe.Pointer(bp + 68)), entropy+26664, uint64(628), 0)
	dictPtr = dictPtr + uintptr(offcodeHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 180)) = uint32(MaxML)
	matchlengthHeaderSize = FSE_readNCount(tls, bp+72, bp+180, bp+184, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, matchlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 180)) > uint32(MaxML) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 184)) > uint32(MLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	ZSTD_buildFSETable(tls, entropy+6160, bp+72, *(*uint32)(unsafe.Pointer(bp + 180)), uintptr(unsafe.Pointer(&ML_base)), uintptr(unsafe.Pointer(&ML_bits)), *(*uint32)(unsafe.Pointer(bp + 184)), entropy+26664, uint64(628), 0)
	dictPtr = dictPtr + uintptr(matchlengthHeaderSize)
	*(*uint32)(unsafe.Pointer(bp + 260)) = uint32(MaxLL)
	litlengthHeaderSize = FSE_readNCount(tls, bp+188, bp+260, bp+264, dictPtr, libc.Uint64FromInt64(int64(dictEnd)-int64(dictPtr)))
	if ERR_isError(tls, litlengthHeaderSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 260)) > uint32(MaxLL) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	if *(*uint32)(unsafe.Pointer(bp + 264)) > uint32(LLFSELog) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	ZSTD_buildFSETable(tls, entropy, bp+188, *(*uint32)(unsafe.Pointer(bp + 260)), uintptr(unsafe.Pointer(&LL_base)), uintptr(unsafe.Pointer(&LL_bits)), *(*uint32)(unsafe.Pointer(bp + 264)), entropy+26664, uint64(628), 0)
	dictPtr = dictPtr + uintptr(litlengthHeaderSize)
	if dictPtr+uintptr(12) > dictEnd {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	dictContentSize = libc.Uint64FromInt64(int64(dictEnd) - int64(dictPtr+libc.UintptrFromInt32(12)))
	i = 0
	for {
		if !(i < int32(3)) {
			break
		}
		rep = MEM_readLE32(tls, dictPtr)
		dictPtr = dictPtr + uintptr(4)
		if rep == uint32(0) || uint64(rep) > dictContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
		}
		*(*U32)(unsafe.Pointer(entropy + 26652 + uintptr(i)*4)) = rep
		goto _1
	_1:
		;
		i = i + 1
	}
	return libc.Uint64FromInt64(int64(dictPtr) - int64(dict))
}

func ZSTD_decompress_insertDictionary(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	var eSize size_t
	var magic, v1 U32
	_, _, _ = eSize, magic, v1
	if dictSize < uint64(8) {
		return ZSTD_refDictContent(tls, dctx, dict, dictSize)
	}
	magic = MEM_readLE32(tls, dict)
	if magic != uint32(ZSTD_MAGIC_DICTIONARY) {
		return ZSTD_refDictContent(tls, dctx, dict, dictSize) /* pure content mode */
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = MEM_readLE32(tls, dict+uintptr(ZSTD_FRAMEIDSIZE))
	/* load entropy tables */
	eSize = ZSTD_loadDEntropy(tls, dctx+32, dict, dictSize)
	if ZSTD_isError(tls, eSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	dict = dict + uintptr(eSize)
	dictSize = dictSize - eSize
	v1 = libc.Uint32FromInt32(1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = v1
	/* reference dictionary content */
	return ZSTD_refDictContent(tls, dctx, dict, dictSize)
}

func ZSTD_decompressBegin(tls *libc.TLS, dctx uintptr) (r size_t) {
	var v1 U32
	_ = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fexpected = ZSTD_startingInputLength(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat) /* dctx->format must be properly set */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fstage = int32(ZSTDds_getFrameHeaderSize)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprocessedCSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdecodedSize = uint64(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = libc.UintptrFromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = libc.UintptrFromInt32(0)
	*(*HUF_DTable)(unsafe.Pointer(dctx + 32 + 10264)) = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_HUFFDTABLE_CAPACITY_LOG) * libc.Int32FromInt32(0x1000001)) /* cover both little and big endian */
	v1 = libc.Uint32FromInt32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = v1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictID = uint32(0)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FbType = int32(bt_reserved)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = int32(1)
	_ = libc.Uint64FromInt64(1)
	libc.Xmemcpy(tls, dctx+32+26652, uintptr(unsafe.Pointer(&repStartValue)), libc.Uint64FromInt64(12)) /* initial repcodes */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr = dctx + 32
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr = dctx + 32 + 6160
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr = dctx + 32 + 4104
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr = dctx + 32 + 10264
	return uint64(0)
}

func ZSTD_decompressBegin_usingDict(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_decompressBegin(tls, dctx)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if dict != 0 && dictSize != 0 {
		if ZSTD_isError(tls, ZSTD_decompress_insertDictionary(tls, dctx, dict, dictSize)) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
		}
	}
	return uint64(0)
}

/* ======   ZSTD_DDict   ====== */

func ZSTD_decompressBegin_usingDDict(tls *libc.TLS, dctx uintptr, ddict uintptr) (r size_t) {
	var dictEnd, dictStart uintptr
	var dictSize, err_code size_t
	_, _, _, _ = dictEnd, dictSize, dictStart, err_code
	if ddict != 0 {
		dictStart = ZSTD_DDict_dictContent(tls, ddict)
		dictSize = ZSTD_DDict_dictSize(tls, ddict)
		dictEnd = dictStart + uintptr(dictSize)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold = libc.BoolInt32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd != dictEnd)
	}
	err_code = ZSTD_decompressBegin(tls, dctx)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	if ddict != 0 { /* NULL ddict is equivalent to no dictionary */
		ZSTD_copyDDictParameters(tls, dctx, ddict)
	}
	return uint64(0)
}

// C documentation
//
//	/*! ZSTD_getDictID_fromDict() :
//	 *  Provides the dictID stored within dictionary.
//	 *  if @return == 0, the dictionary is not conformant with Zstandard specification.
//	 *  It can still be loaded, but as a content-only dictionary. */
func ZSTD_getDictID_fromDict(tls *libc.TLS, dict uintptr, dictSize size_t) (r uint32) {
	if dictSize < uint64(8) {
		return uint32(0)
	}
	if MEM_readLE32(tls, dict) != uint32(ZSTD_MAGIC_DICTIONARY) {
		return uint32(0)
	}
	return MEM_readLE32(tls, dict+uintptr(ZSTD_FRAMEIDSIZE))
}

// C documentation
//
//	/*! ZSTD_getDictID_fromFrame() :
//	 *  Provides the dictID required to decompress frame stored within `src`.
//	 *  If @return == 0, the dictID could not be decoded.
//	 *  This could for one of the following reasons :
//	 *  - The frame does not require a dictionary (most common case).
//	 *  - The frame was built with dictID intentionally removed.
//	 *    Needed dictionary is a hidden piece of information.
//	 *    Note : this use case also happens when using a non-conformant dictionary.
//	 *  - `srcSize` is too small, and as a result, frame header could not be decoded.
//	 *    Note : possible if `srcSize < ZSTD_FRAMEHEADERSIZE_MAX`.
//	 *  - This is not a Zstandard frame.
//	 *  When identifying the exact failure cause, it's possible to use
//	 *  ZSTD_getFrameHeader(), which will provide a more precise error code. */
func ZSTD_getDictID_fromFrame(tls *libc.TLS, src uintptr, srcSize size_t) (r uint32) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var hError size_t
	var _ /* zfp at bp+0 */ ZSTD_FrameHeader
	_ = hError
	*(*ZSTD_FrameHeader)(unsafe.Pointer(bp)) = ZSTD_FrameHeader{}
	hError = ZSTD_getFrameHeader(tls, bp, src, srcSize)
	if ZSTD_isError(tls, hError) != 0 {
		return uint32(0)
	}
	return (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FdictID
}

// C documentation
//
//	/*! ZSTD_decompress_usingDDict() :
//	*   Decompression using a pre-digested Dictionary
//	*   Use dictionary without significant overhead. */
func ZSTD_decompress_usingDDict(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, ddict uintptr) (r size_t) {
	/* pass content and size in case legacy frames are encountered */
	return ZSTD_decompressMultiFrame(tls, dctx, dst, dstCapacity, src, srcSize, libc.UintptrFromInt32(0), uint64(0), ddict)
}

/*=====================================
*   Streaming decompression
*====================================*/

func ZSTD_createDStream(tls *libc.TLS) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, ZSTD_defaultCMem)
}

func ZSTD_initStaticDStream(tls *libc.TLS, workspace uintptr, workspaceSize size_t) (r uintptr) {
	return ZSTD_initStaticDCtx(tls, workspace, workspaceSize)
}

func ZSTD_createDStream_advanced(tls *libc.TLS, customMem ZSTD_customMem) (r uintptr) {
	return ZSTD_createDCtx_internal(tls, customMem)
}

func ZSTD_freeDStream(tls *libc.TLS, zds uintptr) (r size_t) {
	return ZSTD_freeDCtx(tls, zds)
}

/* ***  Initialization  *** */

func ZSTD_DStreamInSize(tls *libc.TLS) (r size_t) {
	return libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) + ZSTD_blockHeaderSize
}

func ZSTD_DStreamOutSize(tls *libc.TLS) (r size_t) {
	return libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
}

func ZSTD_DCtx_loadDictionary_advanced(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t, dictLoadMethod ZSTD_dictLoadMethod_e, dictContentType ZSTD_dictContentType_e) (r size_t) {
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearDict(tls, dctx)
	if dict != 0 && dictSize != uint64(0) {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal = ZSTD_createDDict_advanced(tls, dict, dictSize, dictLoadMethod, dictContentType, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem)
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal == libc.UintptrFromInt32(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1377, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictLocal
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_indefinitely)
	}
	return uint64(0)
}

func ZSTD_DCtx_loadDictionary_byReference(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_DCtx_loadDictionary_advanced(tls, dctx, dict, dictSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_auto))
}

func ZSTD_DCtx_loadDictionary(tls *libc.TLS, dctx uintptr, dict uintptr, dictSize size_t) (r size_t) {
	return ZSTD_DCtx_loadDictionary_advanced(tls, dctx, dict, dictSize, int32(ZSTD_dlm_byCopy), int32(ZSTD_dct_auto))
}

func ZSTD_DCtx_refPrefix_advanced(tls *libc.TLS, dctx uintptr, prefix uintptr, prefixSize size_t, dictContentType ZSTD_dictContentType_e) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_DCtx_loadDictionary_advanced(tls, dctx, prefix, prefixSize, int32(ZSTD_dlm_byRef), dictContentType)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_once)
	return uint64(0)
}

func ZSTD_DCtx_refPrefix(tls *libc.TLS, dctx uintptr, prefix uintptr, prefixSize size_t) (r size_t) {
	return ZSTD_DCtx_refPrefix_advanced(tls, dctx, prefix, prefixSize, int32(ZSTD_dct_rawContent))
}

// C documentation
//
//	/* ZSTD_initDStream_usingDict() :
//	 * return : expected size, aka ZSTD_startingInputLength().
//	 * this function cannot fail */
func ZSTD_initDStream_usingDict(tls *libc.TLS, zds uintptr, dict uintptr, dictSize size_t) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_DCtx_reset(tls, zds, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_DCtx_loadDictionary(tls, zds, dict, dictSize)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
}

// C documentation
//
//	/* note : this variant can't fail */
func ZSTD_initDStream(tls *libc.TLS, zds uintptr) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_DCtx_reset(tls, zds, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_DCtx_refDDict(tls, zds, libc.UintptrFromInt32(0))
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
}

// C documentation
//
//	/* ZSTD_initDStream_usingDDict() :
//	 * ddict will just be referenced, and must outlive decompression session
//	 * this function cannot fail */
func ZSTD_initDStream_usingDDict(tls *libc.TLS, dctx uintptr, ddict uintptr) (r size_t) {
	var err_code, err_code1 size_t
	_, _ = err_code, err_code1
	err_code = ZSTD_DCtx_reset(tls, dctx, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	err_code1 = ZSTD_DCtx_refDDict(tls, dctx, ddict)
	if ERR_isError(tls, err_code1) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code1
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(dctx)).Fformat)
}

// C documentation
//
//	/* ZSTD_resetDStream() :
//	 * return : expected size, aka ZSTD_startingInputLength().
//	 * this function cannot fail */
func ZSTD_resetDStream(tls *libc.TLS, dctx uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	err_code = ZSTD_DCtx_reset(tls, dctx, int32(ZSTD_reset_session_only))
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	return ZSTD_startingInputLength(tls, (*ZSTD_DStream)(unsafe.Pointer(dctx)).Fformat)
}

func ZSTD_DCtx_refDDict(tls *libc.TLS, dctx uintptr, ddict uintptr) (r size_t) {
	var err_code size_t
	_ = err_code
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	ZSTD_clearDict(tls, dctx)
	if ddict != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fddict = ddict
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictUses = int32(ZSTD_use_indefinitely)
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts == int32(ZSTD_rmd_refMultipleDDicts) {
			if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet == libc.UintptrFromInt32(0) {
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet = ZSTD_createDDictHashSet(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem)
				if !((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet != 0) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+7603, 0)
					}
					return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
				}
			}
			/* Impossible: ddictSet cannot have been allocated if static dctx */
			err_code = ZSTD_DDictHashSet_addDDict(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictSet, ddict, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FcustomMem)
			if ERR_isError(tls, err_code) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code
			}
		}
	}
	return uint64(0)
}

// C documentation
//
//	/* ZSTD_DCtx_setMaxWindowSize() :
//	 * note : no direct equivalence in ZSTD_DCtx_setParameter,
//	 * since this version sets windowSize, and the other sets windowLog */
func ZSTD_DCtx_setMaxWindowSize(tls *libc.TLS, dctx uintptr, maxWindowSize size_t) (r size_t) {
	var bounds ZSTD_bounds
	var max, min size_t
	_, _, _ = bounds, max, min
	bounds = ZSTD_dParam_getBounds(tls, int32(ZSTD_d_windowLogMax))
	min = libc.Uint64FromInt32(1) << bounds.FlowerBound
	max = libc.Uint64FromInt32(1) << bounds.FupperBound
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	if maxWindowSize < min {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if maxWindowSize > max {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize = maxWindowSize
	return uint64(0)
}

func ZSTD_DCtx_setFormat(tls *libc.TLS, dctx uintptr, format ZSTD_format_e) (r size_t) {
	return ZSTD_DCtx_setParameter(tls, dctx, int32(ZSTD_d_experimentalParam1), format)
}

func ZSTD_dParam_getBounds(tls *libc.TLS, dParam ZSTD_dParameter) (r ZSTD_bounds) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_bounds{}
	switch dParam {
	case int32(ZSTD_d_windowLogMax):
		bounds.FlowerBound = int32(ZSTD_WINDOWLOG_ABSOLUTEMIN)
		bounds.FupperBound = libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)
		return bounds
	case int32(ZSTD_d_experimentalParam1):
		bounds.FlowerBound = int32(ZSTD_f_zstd1)
		bounds.FupperBound = int32(ZSTD_f_zstd1_magicless)
		_ = libc.Uint64FromInt64(1)
		return bounds
	case int32(ZSTD_d_experimentalParam2):
		bounds.FlowerBound = int32(ZSTD_bm_buffered)
		bounds.FupperBound = int32(ZSTD_bm_stable)
		return bounds
	case int32(ZSTD_d_experimentalParam3):
		bounds.FlowerBound = int32(ZSTD_d_validateChecksum)
		bounds.FupperBound = int32(ZSTD_d_ignoreChecksum)
		return bounds
	case int32(ZSTD_d_experimentalParam4):
		bounds.FlowerBound = int32(ZSTD_rmd_refSingleDDict)
		bounds.FupperBound = int32(ZSTD_rmd_refMultipleDDicts)
		return bounds
	case int32(ZSTD_d_experimentalParam5):
		bounds.FlowerBound = 0
		bounds.FupperBound = int32(1)
		return bounds
	case int32(ZSTD_d_experimentalParam6):
		bounds.FlowerBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(10)
		bounds.FupperBound = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
		return bounds
	default:
	}
	bounds.Ferror1 = libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	return bounds
}

// C documentation
//
//	/* ZSTD_dParam_withinBounds:
//	 * @return 1 if value is within dParam bounds,
//	 * 0 otherwise */
func ZSTD_dParam_withinBounds(tls *libc.TLS, dParam ZSTD_dParameter, value int32) (r int32) {
	var bounds ZSTD_bounds
	_ = bounds
	bounds = ZSTD_dParam_getBounds(tls, dParam)
	if ZSTD_isError(tls, bounds.Ferror1) != 0 {
		return 0
	}
	if value < bounds.FlowerBound {
		return 0
	}
	if value > bounds.FupperBound {
		return 0
	}
	return int32(1)
}

func ZSTD_DCtx_getParameter(tls *libc.TLS, dctx uintptr, param ZSTD_dParameter, value uintptr) (r size_t) {
	switch param {
	case int32(ZSTD_d_windowLogMax):
		*(*int32)(unsafe.Pointer(value)) = libc.Int32FromUint32(ZSTD_highbit32(tls, uint32((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize)))
		return uint64(0)
	case int32(ZSTD_d_experimentalParam1):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat
		return uint64(0)
	case int32(ZSTD_d_experimentalParam2):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBufferMode
		return uint64(0)
	case int32(ZSTD_d_experimentalParam3):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum
		return uint64(0)
	case int32(ZSTD_d_experimentalParam4):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts
		return uint64(0)
	case int32(ZSTD_d_experimentalParam5):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm
		return uint64(0)
	case int32(ZSTD_d_experimentalParam6):
		*(*int32)(unsafe.Pointer(value)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam
		return uint64(0)
	default:
	}
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1319, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	return r
}

func ZSTD_DCtx_setParameter(tls *libc.TLS, dctx uintptr, dParam ZSTD_dParameter, value int32) (r size_t) {
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
	}
	switch dParam {
	case int32(ZSTD_d_windowLogMax):
		if value == 0 {
			value = int32(ZSTD_WINDOWLOG_LIMIT_DEFAULT)
		}
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_windowLogMax), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxWindowSize = libc.Uint64FromInt32(1) << value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam1):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam1), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).Fformat = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam2):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam2), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FoutBufferMode = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam3):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam3), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FforceIgnoreChecksum = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam4):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam4), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstaticSize != uint64(0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7643, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FrefMultipleDDicts = value
		return uint64(0)
	case int32(ZSTD_d_experimentalParam5):
		if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam5), value) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm = libc.BoolInt32(value != 0)
		return uint64(0)
	case int32(ZSTD_d_experimentalParam6):
		if value != 0 {
			if !(ZSTD_dParam_withinBounds(tls, int32(ZSTD_d_experimentalParam6), value) != 0) {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
			}
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FmaxBlockSizeParam = value
		return uint64(0)
	default:
	}
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+1319, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_unsupported))
	return r
}

func ZSTD_DCtx_reset(tls *libc.TLS, dctx uintptr, reset ZSTD_ResetDirective) (r size_t) {
	if reset == int32(ZSTD_reset_session_only) || reset == int32(ZSTD_reset_session_and_parameters) {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage = int32(zdss_init)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FnoForwardProgress = 0
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = int32(1)
	}
	if reset == int32(ZSTD_reset_parameters) || reset == int32(ZSTD_reset_session_and_parameters) {
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FstreamStage != int32(zdss_init) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_stage_wrong))
		}
		ZSTD_clearDict(tls, dctx)
		ZSTD_DCtx_resetParameters(tls, dctx)
	}
	return uint64(0)
}

func ZSTD_sizeof_DStream(tls *libc.TLS, dctx uintptr) (r size_t) {
	return ZSTD_sizeof_DCtx(tls, dctx)
}

func ZSTD_decodingBufferSize_internal(tls *libc.TLS, windowSize uint64, frameContentSize uint64, blockSizeMax size_t) (r size_t) {
	var blockSize, minRBSize size_t
	var neededRBSize, neededSize, v1, v2, v3, v4 uint64
	_, _, _, _, _, _, _, _ = blockSize, minRBSize, neededRBSize, neededSize, v1, v2, v3, v4
	if windowSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		v2 = windowSize
	} else {
		v2 = libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	if v2 < blockSizeMax {
		if windowSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
			v3 = windowSize
		} else {
			v3 = libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
		}
		v1 = v3
	} else {
		v1 = blockSizeMax
	}
	blockSize = v1
	/* We need blockSize + WILDCOPY_OVERLENGTH worth of buffer so that if a block
	 * ends at windowSize + WILDCOPY_OVERLENGTH + 1 bytes, we can start writing
	 * the block at the beginning of the output buffer, and maintain a full window.
	 *
	 * We need another blockSize worth of buffer so that we can store split
	 * literals at the end of the block without overwriting the extDict window.
	 */
	neededRBSize = windowSize + uint64(blockSize*libc.Uint64FromInt32(2)) + libc.Uint64FromInt32(libc.Int32FromInt32(WILDCOPY_OVERLENGTH)*libc.Int32FromInt32(2))
	if frameContentSize < neededRBSize {
		v4 = frameContentSize
	} else {
		v4 = neededRBSize
	}
	neededSize = v4
	minRBSize = neededSize
	if minRBSize != neededSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_windowTooLarge))
	}
	return minRBSize
}

func ZSTD_decodingBufferSize_min(tls *libc.TLS, windowSize uint64, frameContentSize uint64) (r size_t) {
	return ZSTD_decodingBufferSize_internal(tls, windowSize, frameContentSize, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)))
}

func ZSTD_estimateDStreamSize(tls *libc.TLS, windowSize size_t) (r size_t) {
	var blockSize, inBuffSize, outBuffSize size_t
	var v1 uint64
	_, _, _, _ = blockSize, inBuffSize, outBuffSize, v1
	if windowSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)) {
		v1 = windowSize
	} else {
		v1 = libc.Uint64FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	blockSize = v1
	inBuffSize = blockSize /* no block can be larger */
	outBuffSize = ZSTD_decodingBufferSize_min(tls, windowSize, libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1))
	return ZSTD_estimateDCtxSize(tls) + inBuffSize + outBuffSize
}

func ZSTD_estimateDStreamSize_fromFrame(tls *libc.TLS, src uintptr, srcSize size_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var err size_t
	var windowSizeMax U32
	var _ /* zfh at bp+0 */ ZSTD_FrameHeader
	_, _ = err, windowSizeMax
	windowSizeMax = libc.Uint32FromUint32(1) << libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_64)
	err = ZSTD_getFrameHeader(tls, bp, src, srcSize)
	if ZSTD_isError(tls, err) != 0 {
		return err
	}
	if err > uint64(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FwindowSize > uint64(windowSizeMax) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_windowTooLarge))
	}
	return ZSTD_estimateDStreamSize(tls, (*(*ZSTD_FrameHeader)(unsafe.Pointer(bp))).FwindowSize)
}

/* *****   Decompression   ***** */

func ZSTD_DCtx_isOverflow(tls *libc.TLS, zds uintptr, neededInBuffSize size_t, neededOutBuffSize size_t) (r int32) {
	return libc.BoolInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize+(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize >= (neededInBuffSize+neededOutBuffSize)*uint64(ZSTD_WORKSPACETOOLARGE_FACTOR))
}

func ZSTD_DCtx_updateOversizedDuration(tls *libc.TLS, zds uintptr, neededInBuffSize size_t, neededOutBuffSize size_t) {
	if ZSTD_DCtx_isOverflow(tls, zds, neededInBuffSize, neededOutBuffSize) != 0 {
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration + 1
	} else {
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration = uint64(0)
	}
}

func ZSTD_DCtx_isOversizedTooLong(tls *libc.TLS, zds uintptr) (r int32) {
	return libc.BoolInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FoversizedDuration >= uint64(ZSTD_WORKSPACETOOLARGE_MAXDURATION))
}

// C documentation
//
//	/* Checks that the output buffer hasn't changed if ZSTD_obm_stable is used. */
func ZSTD_checkOutBuffer(tls *libc.TLS, zds uintptr, output uintptr) (r size_t) {
	var expect ZSTD_outBuffer
	_ = expect
	expect = (*ZSTD_DStream)(unsafe.Pointer(zds)).FexpectedOutBuffer
	/* No requirement when ZSTD_obm_stable is not enabled. */
	if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode != int32(ZSTD_bm_stable) {
		return uint64(0)
	}
	/* Any buffer is allowed in zdss_init, this must be the same for every other call until
	 * the context is reset.
	 */
	if (*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage == int32(zdss_init) {
		return uint64(0)
	}
	/* The buffer must match our expectation exactly. */
	if expect.Fdst == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst && expect.Fpos == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos && expect.Fsize == (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
		return uint64(0)
	}
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+7689, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_dstBuffer_wrong))
	return r
}

// C documentation
//
//	/* Calls ZSTD_decompressContinue() with the right parameters for ZSTD_decompressStream()
//	 * and updates the stage and the output buffer state. This call is extracted so it can be
//	 * used both when reading directly from the ZSTD_inBuffer, and in buffered input mode.
//	 * NOTE: You must break after calling this function since the streamStage is modified.
//	 */
func ZSTD_decompressContinueStream(tls *libc.TLS, zds uintptr, op uintptr, oend uintptr, src uintptr, srcSize size_t) (r size_t) {
	var decodedSize, decodedSize1, dstSize, dstSize1, err_code, err_code1 size_t
	var isSkipFrame int32
	var v1 uint64
	_, _, _, _, _, _, _, _ = decodedSize, decodedSize1, dstSize, dstSize1, err_code, err_code1, isSkipFrame, v1
	isSkipFrame = ZSTD_isSkipFrame(tls, zds)
	if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode == int32(ZSTD_bm_buffered) {
		if isSkipFrame != 0 {
			v1 = uint64(0)
		} else {
			v1 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize - (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart
		}
		dstSize = v1
		decodedSize = ZSTD_decompressContinue(tls, zds, (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuff+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart), dstSize, src, srcSize)
		err_code = decodedSize
		if ERR_isError(tls, err_code) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code
		}
		if !(decodedSize != 0) && !(isSkipFrame != 0) {
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
		} else {
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart + decodedSize
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_flush)
		}
	} else {
		if isSkipFrame != 0 {
			v1 = uint64(0)
		} else {
			v1 = libc.Uint64FromInt64(int64(oend) - int64(*(*uintptr)(unsafe.Pointer(op))))
		}
		/* Write directly into the output buffer */
		dstSize1 = v1
		decodedSize1 = ZSTD_decompressContinue(tls, zds, *(*uintptr)(unsafe.Pointer(op)), dstSize1, src, srcSize)
		err_code1 = decodedSize1
		if ERR_isError(tls, err_code1) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code1
		}
		*(*uintptr)(unsafe.Pointer(op)) += uintptr(decodedSize1)
		/* Flushing is not needed. */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
	}
	return uint64(0)
}

func ZSTD_decompressStream(tls *libc.TLS, zds uintptr, output uintptr, input uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var bufferSize, cSize, decompressedSize, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, flushedSize, hSize, loadedSize, neededInBuffSize, neededInSize, neededInSize1, neededOutBuffSize, nextSrcSizeHint, remainingInput, toFlushSize, toLoad, toLoad1, v15, v16, v17 size_t
	var dst, iend, ip, istart, oend, ostart, src, v1, v2, v3, v4 uintptr
	var isSkipFrame, tooLarge, tooSmall, v19, v20 int32
	var someMoreWork U32
	var v18 uint64
	var v23 uint32
	var _ /* op at bp+0 */ uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = bufferSize, cSize, decompressedSize, dst, err_code, err_code1, err_code2, err_code3, err_code4, err_code5, flushedSize, hSize, iend, ip, isSkipFrame, istart, loadedSize, neededInBuffSize, neededInSize, neededInSize1, neededOutBuffSize, nextSrcSizeHint, oend, ostart, remainingInput, someMoreWork, src, toFlushSize, toLoad, toLoad1, tooLarge, tooSmall, v1, v15, v16, v17, v18, v19, v2, v20, v23, v3, v4
	src = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos != uint64(0) {
		v1 = src + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos)
	} else {
		v1 = src
	}
	istart = v1
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize != uint64(0) {
		v2 = src + uintptr((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize)
	} else {
		v2 = src
	}
	iend = v2
	ip = istart
	dst = (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos != uint64(0) {
		v3 = dst + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos)
	} else {
		v3 = dst
	}
	ostart = v3
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize != uint64(0) {
		v4 = dst + uintptr((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize)
	} else {
		v4 = dst
	}
	oend = v4
	*(*uintptr)(unsafe.Pointer(bp)) = ostart
	someMoreWork = uint32(1)
	if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos > (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7740, libc.VaList(bp+16, uint32((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos), uint32((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize)))
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos > (*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7777, libc.VaList(bp+16, uint32((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos), uint32((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fsize)))
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	err_code = ZSTD_checkOutBuffer(tls, zds, output)
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	for someMoreWork != 0 {
		switch (*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage {
		case int32(zdss_init):
			goto _5
		case int32(zdss_loadHeader):
			goto _6
		case int32(zdss_read):
			goto _7
		case int32(zdss_load):
			goto _8
		case int32(zdss_flush):
			goto _9
		default:
			goto _10
		}
		goto _11
	_5:
		;
	_14:
		;
		goto _13
	_13:
		;
		if 0 != 0 {
			goto _14
		}
		goto _12
	_12:
		;
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_loadHeader)
		v17 = libc.Uint64FromInt32(0)
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd = v17
		v16 = v17
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart = v16
		v15 = v16
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos = v15
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize = v15
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte = uint32(0)
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FexpectedOutBuffer = *(*ZSTD_outBuffer)(unsafe.Pointer(output))
	_6:
		;
		hSize = ZSTD_getFrameHeader_advanced(tls, zds+29928, zds+95940, (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FrefMultipleDDicts != 0 && (*ZSTD_DStream)(unsafe.Pointer(zds)).FddictSet != 0 {
			ZSTD_DCtx_selectFrameDDict(tls, zds)
		}
		if ZSTD_isError(tls, hSize) != 0 {
			return hSize /* error */
		}
		if hSize != uint64(0) { /* need more input */
			toLoad = hSize - (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize /* if hSize!=0, hSize > zds->lhSize */
			remainingInput = libc.Uint64FromInt64(int64(iend) - int64(ip))
			if toLoad > remainingInput { /* not enough input to load full header */
				if remainingInput > uint64(0) {
					libc.Xmemcpy(tls, zds+95940+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize), ip, remainingInput)
					*(*size_t)(unsafe.Pointer(zds + 30304)) += remainingInput
				}
				(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize
				/* check first few bytes */
				err_code1 = ZSTD_getFrameHeader_advanced(tls, zds+29928, zds+95940, (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
				if ERR_isError(tls, err_code1) != 0 {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+7815, 0)
					}
					return err_code1
				}
				/* return hint input size */
				if (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat == int32(ZSTD_f_zstd1) {
					v19 = int32(6)
				} else {
					v19 = int32(2)
				}
				if libc.Uint64FromInt32(v19) > hSize {
					if (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat == int32(ZSTD_f_zstd1) {
						v20 = int32(6)
					} else {
						v20 = int32(2)
					}
					v18 = libc.Uint64FromInt32(v20)
				} else {
					v18 = hSize
				}
				return v18 - (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize + ZSTD_blockHeaderSize /* remaining header bytes + next block header */
			}
			libc.Xmemcpy(tls, zds+95940+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize), ip, toLoad)
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize = hSize
			ip = ip + uintptr(toLoad)
			goto _11
		}
		/* check for single-pass mode opportunity */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeType != int32(ZSTD_skippableFrame) && uint64(libc.Uint64FromInt64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp))))) >= (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize {
			cSize = ZSTD_findFrameCompressedSize_advanced(tls, istart, libc.Uint64FromInt64(int64(iend)-int64(istart)), (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat)
			if cSize <= libc.Uint64FromInt64(int64(iend)-int64(istart)) {
				/* shortcut : using single-pass mode */
				decompressedSize = ZSTD_decompress_usingDDict(tls, zds, *(*uintptr)(unsafe.Pointer(bp)), libc.Uint64FromInt64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))), istart, cSize, ZSTD_getDDict(tls, zds))
				if ZSTD_isError(tls, decompressedSize) != 0 {
					return decompressedSize
				}
				ip = istart + uintptr(cSize)
				if *(*uintptr)(unsafe.Pointer(bp)) != 0 {
					v1 = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(decompressedSize)
				} else {
					v1 = *(*uintptr)(unsafe.Pointer(bp))
				}
				*(*uintptr)(unsafe.Pointer(bp)) = v1 /* can occur if frameContentSize = 0 (empty frame) */
				(*ZSTD_DStream)(unsafe.Pointer(zds)).Fexpected = uint64(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_init)
				someMoreWork = uint32(0)
				goto _11
			}
		}
		/* Check output buffer is large enough for ZSTD_odm_stable. */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode == int32(ZSTD_bm_stable) && (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeType != int32(ZSTD_skippableFrame) && (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize != libc.Uint64FromUint64(0)-libc.Uint64FromInt32(1) && uint64(libc.Uint64FromInt64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp))))) < (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+7850, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		/* Consume header (see ZSTDds_decodeFrameHeader) */
		err_code2 = ZSTD_decompressBegin_usingDDict(tls, zds, ZSTD_getDDict(tls, zds))
		if ERR_isError(tls, err_code2) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code2
		}
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).Fformat == int32(ZSTD_f_zstd1) && MEM_readLE32(tls, zds+95940)&uint32(ZSTD_MAGIC_SKIPPABLE_MASK) == uint32(ZSTD_MAGIC_SKIPPABLE_START) { /* skippable frame */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fexpected = uint64(MEM_readLE32(tls, zds+95940+uintptr(ZSTD_FRAMEIDSIZE)))
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fstage = int32(ZSTDds_skipFrame)
		} else {
			err_code3 = ZSTD_decodeFrameHeader(tls, zds, zds+95940, (*ZSTD_DStream)(unsafe.Pointer(zds)).FlhSize)
			if ERR_isError(tls, err_code3) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code3
			}
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fexpected = ZSTD_blockHeaderSize
			(*ZSTD_DStream)(unsafe.Pointer(zds)).Fstage = int32(ZSTDds_decodeBlockHeader)
		}
		/* control buffer memory usage */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize > uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(ZSTD_WINDOWLOG_ABSOLUTEMIN)) {
			v18 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize
		} else {
			v18 = uint64(libc.Uint32FromUint32(1) << libc.Int32FromInt32(ZSTD_WINDOWLOG_ABSOLUTEMIN))
		}
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize = v18
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize > (*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxWindowSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_frameParameter_windowTooLarge))
		}
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxBlockSizeParam != 0 {
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax < libc.Uint32FromInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxBlockSizeParam) {
				v23 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax
			} else {
				v23 = libc.Uint32FromInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FmaxBlockSizeParam)
			}
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax = v23
		}
		/* Adapt buffer sizes to frame header instructions */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax > libc.Uint32FromInt32(libc.Int32FromInt32(4)) {
			v23 = (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax
		} else {
			v23 = libc.Uint32FromInt32(libc.Int32FromInt32(4))
		}
		neededInBuffSize = uint64(v23)
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBufferMode == int32(ZSTD_bm_buffered) {
			v18 = ZSTD_decodingBufferSize_internal(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FwindowSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize, uint64((*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax))
		} else {
			v18 = uint64(0)
		}
		neededOutBuffSize = v18
		ZSTD_DCtx_updateOversizedDuration(tls, zds, neededInBuffSize, neededOutBuffSize)
		tooSmall = libc.BoolInt32((*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize < neededInBuffSize || (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize < neededOutBuffSize)
		tooLarge = ZSTD_DCtx_isOversizedTooLong(tls, zds)
		if tooSmall != 0 || tooLarge != 0 {
			bufferSize = neededInBuffSize + neededOutBuffSize
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FstaticSize != 0 { /* static DCtx */
				/* controlled at init */
				if bufferSize > (*ZSTD_DStream)(unsafe.Pointer(zds)).FstaticSize-uint64(95968) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+1319, 0)
					}
					return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
				}
			} else {
				ZSTD_customFree(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff, (*ZSTD_DStream)(unsafe.Pointer(zds)).FcustomMem)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize = uint64(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize = uint64(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff = ZSTD_customMalloc(tls, bufferSize, (*ZSTD_DStream)(unsafe.Pointer(zds)).FcustomMem)
				if (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff == libc.UintptrFromInt32(0) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+1319, 0)
					}
					return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
				}
			}
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize = neededInBuffSize
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuff = (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff + uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize)
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize = neededOutBuffSize
		}
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
	_7:
		;
		neededInSize = ZSTD_nextSrcSizeToDecompressWithInputSize(tls, zds, libc.Uint64FromInt64(int64(iend)-int64(ip)))
		if neededInSize == uint64(0) { /* end of frame */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_init)
			someMoreWork = uint32(0)
			goto _11
		}
		if libc.Uint64FromInt64(int64(iend)-int64(ip)) >= neededInSize { /* decode directly from src */
			err_code4 = ZSTD_decompressContinueStream(tls, zds, bp, oend, ip, neededInSize)
			if ERR_isError(tls, err_code4) != 0 {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return err_code4
			}
			ip = ip + uintptr(neededInSize)
			/* Function modifies the stage so we must break */
			goto _11
		}
		if ip == iend {
			someMoreWork = uint32(0)
			goto _11
		} /* no more input */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_load)
	_8:
		;
		neededInSize1 = ZSTD_nextSrcSizeToDecompress(tls, zds)
		toLoad1 = neededInSize1 - (*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos
		isSkipFrame = ZSTD_isSkipFrame(tls, zds)
		/* At this point we shouldn't be decompressing a block that we can stream. */
		if isSkipFrame != 0 {
			if toLoad1 < libc.Uint64FromInt64(int64(iend)-int64(ip)) {
				v18 = toLoad1
			} else {
				v18 = libc.Uint64FromInt64(int64(iend) - int64(ip))
			}
			loadedSize = v18
		} else {
			if toLoad1 > (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuffSize-(*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+7905, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
			}
			loadedSize = ZSTD_limitCopy(tls, (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos), toLoad1, ip, libc.Uint64FromInt64(int64(iend)-int64(ip)))
		}
		if loadedSize != uint64(0) {
			/* ip may be NULL */
			ip = ip + uintptr(loadedSize)
			*(*size_t)(unsafe.Pointer(zds + 30256)) += loadedSize
		}
		if loadedSize < toLoad1 {
			someMoreWork = uint32(0)
			goto _11
		} /* not enough input, wait for more */
		/* decode loaded input */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos = uint64(0) /* input is consumed */
		err_code5 = ZSTD_decompressContinueStream(tls, zds, bp, oend, (*ZSTD_DStream)(unsafe.Pointer(zds)).FinBuff, neededInSize1)
		if ERR_isError(tls, err_code5) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return err_code5
		}
		/* Function modifies the stage so we must break */
		goto _11
	_9:
		;
		toFlushSize = (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd - (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart
		flushedSize = ZSTD_limitCopy(tls, *(*uintptr)(unsafe.Pointer(bp)), libc.Uint64FromInt64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))), (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuff+uintptr((*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart), toFlushSize)
		if *(*uintptr)(unsafe.Pointer(bp)) != 0 {
			v1 = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(flushedSize)
		} else {
			v1 = *(*uintptr)(unsafe.Pointer(bp))
		}
		*(*uintptr)(unsafe.Pointer(bp)) = v1
		*(*size_t)(unsafe.Pointer(zds + 30288)) += flushedSize
		if flushedSize == toFlushSize { /* flush completed */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize < (*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FframeContentSize && (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart+uint64((*ZSTD_DStream)(unsafe.Pointer(zds)).FfParams.FblockSizeMax) > (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutBuffSize {
				v15 = libc.Uint64FromInt32(0)
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd = v15
				(*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart = v15
			}
			goto _11
		}
		/* cannot complete flush */
		someMoreWork = uint32(0)
		goto _11
	_10:
		;
		/* impossible */
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1561, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC)) /* some compilers require default to do something */
	_11:
	}
	/* result */
	(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = libc.Uint64FromInt64(int64(ip) - int64((*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsrc))
	(*ZSTD_outBuffer)(unsafe.Pointer(output)).Fpos = libc.Uint64FromInt64(int64(*(*uintptr)(unsafe.Pointer(bp))) - int64((*ZSTD_outBuffer)(unsafe.Pointer(output)).Fdst))
	/* Update the expected output buffer for ZSTD_obm_stable. */
	(*ZSTD_DStream)(unsafe.Pointer(zds)).FexpectedOutBuffer = *(*ZSTD_outBuffer)(unsafe.Pointer(output))
	if ip == istart && *(*uintptr)(unsafe.Pointer(bp)) == ostart { /* no forward progress */
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress = (*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress + 1
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress >= int32(ZSTD_NO_FORWARD_PROGRESS_MAX) {
			if *(*uintptr)(unsafe.Pointer(bp)) == oend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_noForwardProgress_destFull))
			}
			if ip == iend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_noForwardProgress_inputEmpty))
			}
		}
	} else {
		(*ZSTD_DStream)(unsafe.Pointer(zds)).FnoForwardProgress = 0
	}
	nextSrcSizeHint = ZSTD_nextSrcSizeToDecompress(tls, zds)
	if !(nextSrcSizeHint != 0) { /* frame fully decoded */
		if (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutEnd == (*ZSTD_DStream)(unsafe.Pointer(zds)).FoutStart { /* output fully flushed */
			if (*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte != 0 {
				if (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos >= (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fsize {
					/* can't release hostage (not present) */
					(*ZSTD_DStream)(unsafe.Pointer(zds)).FstreamStage = int32(zdss_read)
					return uint64(1)
				}
				(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos + 1 /* release hostage */
			} /* zds->hostageByte */
			return uint64(0)
		} /* zds->outEnd == zds->outStart */
		if !((*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte != 0) { /* output not fully flushed; keep last byte as hostage; will be released when all output is flushed */
			(*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos = (*ZSTD_inBuffer)(unsafe.Pointer(input)).Fpos - 1 /* note : pos > 0, otherwise, impossible to finish reading last block */
			(*ZSTD_DStream)(unsafe.Pointer(zds)).FhostageByte = uint32(1)
		}
		return uint64(1)
	} /* nextSrcSizeHint==0 */
	nextSrcSizeHint = nextSrcSizeHint + ZSTD_blockHeaderSize*libc.BoolUint64(ZSTD_nextInputType(tls, zds) == int32(ZSTDnit_block)) /* preload header of next block */
	nextSrcSizeHint = nextSrcSizeHint - (*ZSTD_DStream)(unsafe.Pointer(zds)).FinPos                                                /* part already loaded*/
	return nextSrcSizeHint
	return r
}

func ZSTD_decompressStream_simpleArgs(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, dstPos uintptr, src uintptr, srcSize size_t, srcPos uintptr) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var cErr size_t
	var _ /* input at bp+24 */ ZSTD_inBuffer
	var _ /* output at bp+0 */ ZSTD_outBuffer
	_ = cErr
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fdst = dst
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fsize = dstCapacity
	(*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos = *(*size_t)(unsafe.Pointer(dstPos))
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsrc = src
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fsize = srcSize
	(*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos = *(*size_t)(unsafe.Pointer(srcPos))
	cErr = ZSTD_decompressStream(tls, dctx, bp, bp+24)
	*(*size_t)(unsafe.Pointer(dstPos)) = (*(*ZSTD_outBuffer)(unsafe.Pointer(bp))).Fpos
	*(*size_t)(unsafe.Pointer(srcPos)) = (*(*ZSTD_inBuffer)(unsafe.Pointer(bp + 24))).Fpos
	return cErr
	return r
}

/**** ended inlining decompress/zstd_decompress.c ****/
/**** start inlining decompress/zstd_decompress_block.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/* zstd_decompress_block :
 * this module takes care of decompressing _compressed_ block */

/*-*******************************************************
*  Dependencies
*********************************************************/
/**** skipping file: ../common/zstd_deps.h ****/
/**** skipping file: ../common/compiler.h ****/
/**** skipping file: ../common/cpu.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: zstd_decompress_internal.h ****/
/**** skipping file: zstd_ddict.h ****/
/**** skipping file: zstd_decompress_block.h ****/
/**** skipping file: ../common/bits.h ****/

/*_*******************************************************
*  Macros
**********************************************************/

/* These two optional macros force the use one way or another of the two
 * ZSTD_decompressSequences implementations. You can't force in both directions
 * at the same time.
 */

// C documentation
//
//	/*_*******************************************************
//	*  Memory operations
//	**********************************************************/
func ZSTD_copy4(tls *libc.TLS, dst uintptr, src uintptr) {
	libc.Xmemcpy(tls, dst, src, libc.Uint64FromInt32(libc.Int32FromInt32(4)))
}

/*-*************************************************************
 *   Block decoding
 ***************************************************************/

func ZSTD_blockSizeMax(tls *libc.TLS, dctx uintptr) (r size_t) {
	var blockSizeMax size_t
	var v1 uint32
	_, _ = blockSizeMax, v1
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression != 0 {
		v1 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfParams.FblockSizeMax
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX))
	}
	blockSizeMax = uint64(v1)
	return blockSizeMax
}

// C documentation
//
//	/*! ZSTD_getcBlockSize() :
//	 *  Provides the size of compressed block from block header `src` */
func ZSTD_getcBlockSize(tls *libc.TLS, src uintptr, srcSize size_t, bpPtr uintptr) (r size_t) {
	var cBlockHeader, cSize U32
	_, _ = cBlockHeader, cSize
	if srcSize < ZSTD_blockHeaderSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	cBlockHeader = MEM_readLE24(tls, src)
	cSize = cBlockHeader >> int32(3)
	(*blockProperties_t)(unsafe.Pointer(bpPtr)).FlastBlock = cBlockHeader & uint32(1)
	(*blockProperties_t)(unsafe.Pointer(bpPtr)).FblockType = libc.Int32FromUint32(cBlockHeader >> libc.Int32FromInt32(1) & libc.Uint32FromInt32(3))
	(*blockProperties_t)(unsafe.Pointer(bpPtr)).ForigSize = cSize /* only useful for RLE */
	if (*blockProperties_t)(unsafe.Pointer(bpPtr)).FblockType == int32(bt_rle) {
		return uint64(1)
	}
	if (*blockProperties_t)(unsafe.Pointer(bpPtr)).FblockType == int32(bt_reserved) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	return uint64(cSize)
	return r
}

// C documentation
//
//	/* Allocate buffer for literals, either overlapping current dst, or split between dst and litExtraBuffer, or stored entirely within litExtraBuffer */
func ZSTD_allocateLiteralsBuffer(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, litSize size_t, streaming streaming_operation, expectedWriteSize size_t, splitImmediately uint32) {
	var blockSizeMax size_t
	_ = blockSizeMax
	blockSizeMax = ZSTD_blockSizeMax(tls, dctx)
	if streaming == int32(not_streaming) && dstCapacity > blockSizeMax+uint64(WILDCOPY_OVERLENGTH)+litSize+uint64(WILDCOPY_OVERLENGTH) {
		/* If we aren't streaming, we can just put the literals after the output
		 * of the current block. We don't need to worry about overwriting the
		 * extDict of our window, because it doesn't exist.
		 * So if we have space after the end of the block, just put it there.
		 */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dst + uintptr(blockSizeMax) + uintptr(WILDCOPY_OVERLENGTH)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer + uintptr(litSize)
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_in_dst)
	} else {
		if litSize <= libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)) {
			/* Literals fit entirely within the extra buffer, put them there to avoid
			 * having to split the literals.
			 */
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dctx + 30372
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer + uintptr(litSize)
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
		} else {
			/* Literals must be split between the output block and the extra lit
			 * buffer. We fill the extra lit buffer with the tail of the literals,
			 * and put the rest of the literals at the end of the block, with
			 * WILDCOPY_OVERLENGTH of buffer room to allow for overreads.
			 * This MUST not write more than our maxBlockSize beyond dst, because in
			 * streaming mode, that could overwrite part of our extDict window.
			 */
			if splitImmediately != 0 {
				/* won't fit in litExtraBuffer, so it will be split between end of dst and extra buffer */
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dst + uintptr(expectedWriteSize) - uintptr(litSize) + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)) - uintptr(WILDCOPY_OVERLENGTH)
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer + uintptr(litSize) - uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
			} else {
				/* initially this will be stored entirely in dst during huffman decoding, it will partially be shifted to litExtraBuffer after */
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer = dst + uintptr(expectedWriteSize) - uintptr(litSize)
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = dst + uintptr(expectedWriteSize)
			}
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_split)
		}
	}
}

// C documentation
//
//	/*! ZSTD_decodeLiteralsBlock() :
//	 * Where it is possible to do so without being stomped by the output during decompression, the literals block will be stored
//	 * in the dstBuffer.  If there is room to do so, it will be stored in full in the excess dst space after where the current
//	 * block will be output.  Otherwise it will be stored at the end of the current dst blockspace, with a small portion being
//	 * stored in dctx->litExtraBuffer to help keep it "ahead" of the current output write.
//	 *
//	 * @return : nb of bytes read from src (< srcSize )
//	 *  note : symbol not declared but exposed for fullbench */
func ZSTD_decodeLiteralsBlock(tls *libc.TLS, dctx uintptr, src uintptr, srcSize size_t, dst uintptr, dstCapacity size_t, streaming streaming_operation) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var _pos, _size, blockSizeMax, expectedWriteSize, expectedWriteSize1, expectedWriteSize2, hufSuccess, lhSize, lhSize1, lhSize2, litCSize, litSize, litSize1, litSize2 size_t
	var _ptr, istart uintptr
	var flags, v11, v12 int32
	var lhc, lhlCode, lhlCode1, lhlCode2, singleStream U32
	var litEncType SymbolEncodingType_e
	var v10 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = _pos, _ptr, _size, blockSizeMax, expectedWriteSize, expectedWriteSize1, expectedWriteSize2, flags, hufSuccess, istart, lhSize, lhSize1, lhSize2, lhc, lhlCode, lhlCode1, lhlCode2, litCSize, litEncType, litSize, litSize1, litSize2, singleStream, v10, v11, v12
	if srcSize < libc.Uint64FromInt32(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	istart = src
	litEncType = libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart))) & libc.Int32FromInt32(3)
	blockSizeMax = ZSTD_blockSizeMax(tls, dctx)
	switch litEncType {
	case int32(set_repeat):
		goto _1
	case int32(set_compressed):
		goto _2
	case int32(set_basic):
		goto _3
	case int32(set_rle):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
_9:
	;
	goto _8
_8:
	;
	if 0 != 0 {
		goto _9
	}
	goto _7
_7:
	;
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy == uint32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
_2:
	;
	if srcSize < uint64(5) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7925, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	singleStream = uint32(0)
	lhlCode = libc.Uint32FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart))) >> int32(2) & int32(3))
	lhc = MEM_readLE32(tls, istart)
	if blockSizeMax < dstCapacity {
		v10 = blockSizeMax
	} else {
		v10 = dstCapacity
	}
	expectedWriteSize = v10
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		v11 = int32(HUF_flags_bmi2)
	} else {
		v11 = 0
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdisableHufAsm != 0 {
		v12 = int32(HUF_flags_disableAsm)
	} else {
		v12 = 0
	}
	flags = 0 | v11 | v12
	switch lhlCode {
	case uint32(0):
		fallthrough
	case uint32(1):
		fallthrough
	default: /* note : default is impossible, since lhlCode into [0..3] */
		/* 2 - 2 - 10 - 10 */
		singleStream = libc.BoolUint32(!(lhlCode != 0))
		lhSize = uint64(3)
		litSize = uint64(lhc >> libc.Int32FromInt32(4) & uint32(0x3FF))
		litCSize = uint64(lhc >> libc.Int32FromInt32(14) & uint32(0x3FF))
	case uint32(2):
		/* 2 - 2 - 14 - 14 */
		lhSize = uint64(4)
		litSize = uint64(lhc >> libc.Int32FromInt32(4) & uint32(0x3FFF))
		litCSize = uint64(lhc >> int32(18))
	case uint32(3):
		/* 2 - 2 - 18 - 18 */
		lhSize = uint64(5)
		litSize = uint64(lhc >> libc.Int32FromInt32(4) & uint32(0x3FFFF))
		litCSize = uint64(lhc>>libc.Int32FromInt32(22)) + uint64(*(*BYTE)(unsafe.Pointer(istart + 4)))<<libc.Int32FromInt32(10)
		break
	}
	if litSize > uint64(0) && dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if litSize > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if !(singleStream != 0) {
		if litSize < uint64(MIN_LITERALS_FOR_4_STREAMS) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8007, libc.VaList(bp+8, litSize, int32(MIN_LITERALS_FOR_4_STREAMS)))
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_literals_headerWrong))
		}
	}
	if litCSize+lhSize > srcSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if expectedWriteSize < litSize {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_allocateLiteralsBuffer(tls, dctx, dst, dstCapacity, litSize, streaming, expectedWriteSize, uint32(0))
	/* prefetch huffman table if cold */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold != 0 && litSize > uint64(768) {
		_ptr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr
		_size = libc.Uint64FromInt64(16388)
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+8, 0, int32(2)))
			goto _13
		_13:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
	}
	if litEncType == int32(set_repeat) {
		if singleStream != 0 {
			hufSuccess = HUF_decompress1X_usingDTable(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr, flags)
		} else {
			hufSuccess = HUF_decompress4X_usingDTable(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr, flags)
		}
	} else {
		if singleStream != 0 {
			hufSuccess = HUF_decompress1X1_DCtx_wksp(tls, dctx+32+10264, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, dctx+27324, uint64(2560), flags)
		} else {
			hufSuccess = HUF_decompress4X_hufOnly_wksp(tls, dctx+32+10264, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize, istart+uintptr(lhSize), litCSize, dctx+27324, uint64(2560), flags)
		}
	}
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		libc.Xmemcpy(tls, dctx+30372, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd-uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)), libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		libc.Xmemmove(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer+uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))-libc.UintptrFromInt32(WILDCOPY_OVERLENGTH), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, litSize-libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		*(*uintptr)(unsafe.Pointer(dctx + 30352)) += uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16) - libc.Int32FromInt32(WILDCOPY_OVERLENGTH))
		*(*uintptr)(unsafe.Pointer(dctx + 30360)) -= uintptr(WILDCOPY_OVERLENGTH)
	}
	if ERR_isError(tls, hufSuccess) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitEntropy = uint32(1)
	if litEncType == int32(set_compressed) {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FHUFptr = dctx + 32 + 10264
	}
	return litCSize + lhSize
_3:
	;
	lhlCode1 = libc.Uint32FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart))) >> int32(2) & int32(3))
	if blockSizeMax < dstCapacity {
		v10 = blockSizeMax
	} else {
		v10 = dstCapacity
	}
	expectedWriteSize1 = v10
	switch lhlCode1 {
	case uint32(0):
		fallthrough
	case uint32(2):
		fallthrough
	default: /* note : default is impossible, since lhlCode into [0..3] */
		lhSize1 = uint64(1)
		litSize1 = libc.Uint64FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart))) >> int32(3))
	case uint32(1):
		lhSize1 = uint64(2)
		litSize1 = libc.Uint64FromInt32(libc.Int32FromUint16(MEM_readLE16(tls, istart)) >> int32(4))
	case uint32(3):
		lhSize1 = uint64(3)
		if srcSize < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8065, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		litSize1 = uint64(MEM_readLE24(tls, istart) >> int32(4))
		break
	}
	if litSize1 > uint64(0) && dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if litSize1 > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if expectedWriteSize1 < litSize1 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_allocateLiteralsBuffer(tls, dctx, dst, dstCapacity, litSize1, streaming, expectedWriteSize1, uint32(1))
	if lhSize1+litSize1+uint64(WILDCOPY_OVERLENGTH) > srcSize { /* risk reading beyond src buffer with wildcopy */
		if litSize1+lhSize1 > srcSize {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
			libc.Xmemcpy(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, istart+uintptr(lhSize1), litSize1-libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
			libc.Xmemcpy(tls, dctx+30372, istart+uintptr(lhSize1)+uintptr(litSize1)-uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)), libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		} else {
			libc.Xmemcpy(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, istart+uintptr(lhSize1), litSize1)
		}
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize1
		return lhSize1 + litSize1
	}
	/* direct reference into compressed stream */
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = istart + uintptr(lhSize1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize1
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr + uintptr(litSize1)
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
	return lhSize1 + litSize1
_4:
	;
	lhlCode2 = libc.Uint32FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart))) >> int32(2) & int32(3))
	if blockSizeMax < dstCapacity {
		v10 = blockSizeMax
	} else {
		v10 = dstCapacity
	}
	expectedWriteSize2 = v10
	switch lhlCode2 {
	case uint32(0):
		fallthrough
	case uint32(2):
		fallthrough
	default: /* note : default is impossible, since lhlCode into [0..3] */
		lhSize2 = uint64(1)
		litSize2 = libc.Uint64FromInt32(libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart))) >> int32(3))
	case uint32(1):
		lhSize2 = uint64(2)
		if srcSize < uint64(3) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8122, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		litSize2 = libc.Uint64FromInt32(libc.Int32FromUint16(MEM_readLE16(tls, istart)) >> int32(4))
	case uint32(3):
		lhSize2 = uint64(3)
		if srcSize < uint64(4) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8181, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		litSize2 = uint64(MEM_readLE24(tls, istart) >> int32(4))
		break
	}
	if litSize2 > uint64(0) && dst == libc.UintptrFromInt32(0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if litSize2 > blockSizeMax {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if expectedWriteSize2 < litSize2 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_allocateLiteralsBuffer(tls, dctx, dst, dstCapacity, litSize2, streaming, expectedWriteSize2, uint32(1))
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		libc.Xmemset(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart + uintptr(lhSize2)))), litSize2-libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
		libc.Xmemset(tls, dctx+30372, libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart + uintptr(lhSize2)))), libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16)))
	} else {
		libc.Xmemset(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer, libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(istart + uintptr(lhSize2)))), litSize2)
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize = litSize2
	return lhSize2 + uint64(1)
_5:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+8240, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
_6:
	;
	return r
}

func ZSTD_decodeLiteralsBlock_wrapper(tls *libc.TLS, dctx uintptr, src uintptr, srcSize size_t, dst uintptr, dstCapacity size_t) (r size_t) {
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = 0
	return ZSTD_decodeLiteralsBlock(tls, dctx, src, srcSize, dst, dstCapacity, int32(not_streaming))
}

/* Default FSE distribution tables.
 * These are pre-calculated FSE decoding tables using default distributions as defined in specification :
 * https://github.com/facebook/zstd/blob/release/doc/zstd_compression_format.md#default-distributions
 * They were generated programmatically with following method :
 * - start from default distributions, present in /lib/common/zstd_internal.h
 * - generate tables normally, using ZSTD_buildFSETable()
 * - printout the content of tables
 * - prettify output, report below, test with fuzzer to ensure it's correct */

// C documentation
//
//	/* Default FSE distribution table for Literal Lengths */
var LL_defaultDTable = [65]ZSTD_seqSymbol{
	0: {
		FnextState:        uint16(1),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(1),
		FbaseValue:        uint32(LL_DEFAULTNORMLOG),
	},
	1: {
		FnbBits: uint8(4),
	},
	2: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
	},
	3: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(1),
	},
	4: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(3),
	},
	5: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(4),
	},
	6: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	7: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	8: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	9: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	10: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(12),
	},
	11: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(14),
	},
	12: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16),
	},
	13: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(20),
	},
	14: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(22),
	},
	15: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(28),
	},
	16: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(32),
	},
	17: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(48),
	},
	18: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(64),
	},
	19: {
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(128),
	},
	20: {
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(256),
	},
	21: {
		FnbAdditionalBits: uint8(10),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(1024),
	},
	22: {
		FnbAdditionalBits: uint8(12),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(4096),
	},
	23: {
		FnextState: uint16(32),
		FnbBits:    uint8(4),
	},
	24: {
		FnbBits:    uint8(4),
		FbaseValue: uint32(1),
	},
	25: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(2),
	},
	26: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(4),
	},
	27: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(5),
	},
	28: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	29: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	30: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	31: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	32: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(13),
	},
	33: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16),
	},
	34: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(18),
	},
	35: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(22),
	},
	36: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(24),
	},
	37: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(32),
	},
	38: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(40),
	},
	39: {
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(64),
	},
	40: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(64),
	},
	41: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(128),
	},
	42: {
		FnbAdditionalBits: uint8(9),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(512),
	},
	43: {
		FnbAdditionalBits: uint8(11),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(2048),
	},
	44: {
		FnextState: uint16(48),
		FnbBits:    uint8(4),
	},
	45: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
		FbaseValue: uint32(1),
	},
	46: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(2),
	},
	47: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(3),
	},
	48: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(5),
	},
	49: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	50: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	51: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	52: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	53: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(12),
	},
	54: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(15),
	},
	55: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(18),
	},
	56: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(20),
	},
	57: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(24),
	},
	58: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(28),
	},
	59: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(40),
	},
	60: {
		FnextState:        uint16(32),
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(48),
	},
	61: {
		FnbAdditionalBits: uint8(16),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(65536),
	},
	62: {
		FnbAdditionalBits: uint8(15),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(32768),
	},
	63: {
		FnbAdditionalBits: uint8(14),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(16384),
	},
	64: {
		FnbAdditionalBits: uint8(13),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(8192),
	},
} /* LL_defaultDTable */

// C documentation
//
//	/* Default FSE distribution table for Offset Codes */
var OF_defaultDTable = [33]ZSTD_seqSymbol{
	0: {
		FnextState:        uint16(1),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(1),
		FbaseValue:        uint32(OF_DEFAULTNORMLOG),
	},
	1: {
		FnbBits: uint8(5),
	},
	2: {
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(61),
	},
	3: {
		FnbAdditionalBits: uint8(9),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(509),
	},
	4: {
		FnbAdditionalBits: uint8(15),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(32765),
	},
	5: {
		FnbAdditionalBits: uint8(21),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(2097149),
	},
	6: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(5),
	},
	7: {
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(125),
	},
	8: {
		FnbAdditionalBits: uint8(12),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(4093),
	},
	9: {
		FnbAdditionalBits: uint8(18),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(262141),
	},
	10: {
		FnbAdditionalBits: uint8(23),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(8388605),
	},
	11: {
		FnbAdditionalBits: uint8(5),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(29),
	},
	12: {
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(253),
	},
	13: {
		FnbAdditionalBits: uint8(14),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16381),
	},
	14: {
		FnbAdditionalBits: uint8(20),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1048573),
	},
	15: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1),
	},
	16: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(125),
	},
	17: {
		FnbAdditionalBits: uint8(11),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(2045),
	},
	18: {
		FnbAdditionalBits: uint8(17),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(131069),
	},
	19: {
		FnbAdditionalBits: uint8(22),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(4194301),
	},
	20: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(13),
	},
	21: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(253),
	},
	22: {
		FnbAdditionalBits: uint8(13),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(8189),
	},
	23: {
		FnbAdditionalBits: uint8(19),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(524285),
	},
	24: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1),
	},
	25: {
		FnextState:        uint16(16),
		FnbAdditionalBits: uint8(6),
		FnbBits:           uint8(4),
		FbaseValue:        uint32(61),
	},
	26: {
		FnbAdditionalBits: uint8(10),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(1021),
	},
	27: {
		FnbAdditionalBits: uint8(16),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(65533),
	},
	28: {
		FnbAdditionalBits: uint8(28),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(268435453),
	},
	29: {
		FnbAdditionalBits: uint8(27),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(134217725),
	},
	30: {
		FnbAdditionalBits: uint8(26),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(67108861),
	},
	31: {
		FnbAdditionalBits: uint8(25),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(33554429),
	},
	32: {
		FnbAdditionalBits: uint8(24),
		FnbBits:           uint8(5),
		FbaseValue:        uint32(16777213),
	},
} /* OF_defaultDTable */

// C documentation
//
//	/* Default FSE distribution table for Match Lengths */
var ML_defaultDTable = [65]ZSTD_seqSymbol{
	0: {
		FnextState:        uint16(1),
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(1),
		FbaseValue:        uint32(ML_DEFAULTNORMLOG),
	},
	1: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(3),
	},
	2: {
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	3: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(5),
	},
	4: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	5: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	6: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	7: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	8: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(13),
	},
	9: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(16),
	},
	10: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(19),
	},
	11: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(22),
	},
	12: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(25),
	},
	13: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(28),
	},
	14: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(31),
	},
	15: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(34),
	},
	16: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(37),
	},
	17: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(41),
	},
	18: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(47),
	},
	19: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(59),
	},
	20: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(83),
	},
	21: {
		FnbAdditionalBits: uint8(7),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(131),
	},
	22: {
		FnbAdditionalBits: uint8(9),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(515),
	},
	23: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	24: {
		FnbBits:    uint8(4),
		FbaseValue: uint32(5),
	},
	25: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(6),
	},
	26: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	27: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(9),
	},
	28: {
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	29: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(12),
	},
	30: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(15),
	},
	31: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(18),
	},
	32: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(21),
	},
	33: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(24),
	},
	34: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(27),
	},
	35: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(30),
	},
	36: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(33),
	},
	37: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(35),
	},
	38: {
		FnbAdditionalBits: uint8(1),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(39),
	},
	39: {
		FnbAdditionalBits: uint8(2),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(43),
	},
	40: {
		FnbAdditionalBits: uint8(3),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(51),
	},
	41: {
		FnbAdditionalBits: uint8(4),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(67),
	},
	42: {
		FnbAdditionalBits: uint8(5),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(99),
	},
	43: {
		FnbAdditionalBits: uint8(8),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(259),
	},
	44: {
		FnextState: uint16(32),
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	45: {
		FnextState: uint16(48),
		FnbBits:    uint8(4),
		FbaseValue: uint32(4),
	},
	46: {
		FnextState: uint16(16),
		FnbBits:    uint8(4),
		FbaseValue: uint32(5),
	},
	47: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(7),
	},
	48: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(8),
	},
	49: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(10),
	},
	50: {
		FnextState: uint16(32),
		FnbBits:    uint8(5),
		FbaseValue: uint32(11),
	},
	51: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(14),
	},
	52: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(17),
	},
	53: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(20),
	},
	54: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(23),
	},
	55: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(26),
	},
	56: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(29),
	},
	57: {
		FnbBits:    uint8(6),
		FbaseValue: uint32(32),
	},
	58: {
		FnbAdditionalBits: uint8(16),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(65539),
	},
	59: {
		FnbAdditionalBits: uint8(15),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(32771),
	},
	60: {
		FnbAdditionalBits: uint8(14),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(16387),
	},
	61: {
		FnbAdditionalBits: uint8(13),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(8195),
	},
	62: {
		FnbAdditionalBits: uint8(12),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(4099),
	},
	63: {
		FnbAdditionalBits: uint8(11),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(2051),
	},
	64: {
		FnbAdditionalBits: uint8(10),
		FnbBits:           uint8(6),
		FbaseValue:        uint32(1027),
	},
} /* ML_defaultDTable */

func ZSTD_buildSeqTable_rle(tls *libc.TLS, dt uintptr, baseValue U32, nbAddBits U8) {
	var DTableH, cell, ptr uintptr
	_, _, _ = DTableH, cell, ptr
	ptr = dt
	DTableH = ptr
	cell = dt + uintptr(1)*8
	(*ZSTD_seqSymbol_header)(unsafe.Pointer(DTableH)).FtableLog = uint32(0)
	(*ZSTD_seqSymbol_header)(unsafe.Pointer(DTableH)).FfastMode = uint32(0)
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FnbBits = uint8(0)
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FnextState = uint16(0)
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FnbAdditionalBits = nbAddBits
	(*ZSTD_seqSymbol)(unsafe.Pointer(cell)).FbaseValue = baseValue
}

// C documentation
//
//	/* ZSTD_buildFSETable() :
//	 * generate FSE decoding table for one symbol (ll, ml or off)
//	 * cannot fail if input is valid =>
//	 * all inputs are presumed validated at this stage */
func ZSTD_buildFSETable_body(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var add, sv U64
	var highThreshold, maxSV1, nextState, position1, s, s1, s3, step1, symbol, tableMask1, tableSize, u1, v2 U32
	var i, i1, n, n1 int32
	var largeLimit S16
	var pos, position, s2, step, tableMask, u, uPosition, unroll size_t
	var spread, symbolNext, tableDecode, v11 uintptr
	var v10 U16
	var _ /* DTableH at bp+0 */ ZSTD_seqSymbol_header
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = add, highThreshold, i, i1, largeLimit, maxSV1, n, n1, nextState, pos, position, position1, s, s1, s2, s3, spread, step, step1, sv, symbol, symbolNext, tableDecode, tableMask, tableMask1, tableSize, u, u1, uPosition, unroll, v10, v11, v2
	tableDecode = dt + uintptr(1)*8
	maxSV1 = maxSymbolValue + uint32(1)
	tableSize = libc.Uint32FromInt32(int32(1) << tableLog)
	symbolNext = wksp
	spread = symbolNext + uintptr(libc.Int32FromInt32(MaxML))*2 + libc.UintptrFromInt32(1)*2
	highThreshold = tableSize - uint32(1)
	/* Sanity Checks */
	_ = wkspSize
	/* Init, lay down lowprob symbols */
	(*(*ZSTD_seqSymbol_header)(unsafe.Pointer(bp))).FtableLog = tableLog
	(*(*ZSTD_seqSymbol_header)(unsafe.Pointer(bp))).FfastMode = uint32(1)
	largeLimit = int16(libc.Int32FromInt32(1) << (tableLog - libc.Uint32FromInt32(1)))
	s = uint32(0)
	for {
		if !(s < maxSV1) {
			break
		}
		if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) == -int32(1) {
			v2 = highThreshold
			highThreshold = highThreshold - 1
			(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(v2)*8))).FbaseValue = s
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = uint16(1)
		} else {
			if int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2))) >= int32(largeLimit) {
				(*(*ZSTD_seqSymbol_header)(unsafe.Pointer(bp))).FfastMode = uint32(0)
			}
			*(*U16)(unsafe.Pointer(symbolNext + uintptr(s)*2)) = libc.Uint16FromInt16(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s)*2)))
		}
		goto _1
	_1:
		;
		s = s + 1
	}
	libc.Xmemcpy(tls, dt, bp, libc.Uint64FromInt64(8))
	/* Spread symbols */
	/* Specialized symbol spreading for the case when there are
	 * no low probability (-1 count) symbols. When compressing
	 * small blocks we avoid low probability symbols to hit this
	 * case, since header decoding speed matters more.
	 */
	if highThreshold == tableSize-uint32(1) {
		tableMask = uint64(tableSize - uint32(1))
		step = uint64(tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3))
		/* First lay down the symbols in order.
		 * We use a uint64_t to lay down 8 bytes at a time. This reduces branch
		 * misses since small blocks generally have small table logs, so nearly
		 * all symbols have counts <= 8. We ensure we have 8 bytes at the end of
		 * our buffer to handle the over-write.
		 */
		add = uint64(0x0101010101010101)
		pos = uint64(0)
		sv = uint64(0)
		s1 = uint32(0)
		for {
			if !(s1 < maxSV1) {
				break
			}
			n = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s1)*2)))
			MEM_write64(tls, spread+uintptr(pos), sv)
			i = int32(8)
			for {
				if !(i < n) {
					break
				}
				MEM_write64(tls, spread+uintptr(pos)+uintptr(i), sv)
				goto _4
			_4:
				;
				i = i + int32(8)
			}
			pos = pos + libc.Uint64FromInt32(n)
			goto _3
		_3:
			;
			s1 = s1 + 1
			sv = sv + add
		}
		/* Now we spread those positions across the table.
		 * The benefit of doing it in two stages is that we avoid the
		 * variable size inner loop, which caused lots of branch misses.
		 * Now we can run through all the positions without any branch misses.
		 * We unroll the loop twice, since that is what empirically worked best.
		 */
		position = uint64(0)
		unroll = uint64(2)
		/* FSE_MIN_TABLELOG is 5 */
		s2 = uint64(0)
		for {
			if !(s2 < uint64(tableSize)) {
				break
			}
			u = uint64(0)
			for {
				if !(u < unroll) {
					break
				}
				uPosition = (position + u*step) & tableMask
				(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(uPosition)*8))).FbaseValue = uint32(*(*BYTE)(unsafe.Pointer(spread + uintptr(s2+u))))
				goto _6
			_6:
				;
				u = u + 1
			}
			position = (position + unroll*step) & tableMask
			goto _5
		_5:
			;
			s2 = s2 + unroll
		}
	} else {
		tableMask1 = tableSize - uint32(1)
		step1 = tableSize>>libc.Int32FromInt32(1) + tableSize>>libc.Int32FromInt32(3) + libc.Uint32FromInt32(3)
		position1 = uint32(0)
		s3 = uint32(0)
		for {
			if !(s3 < maxSV1) {
				break
			}
			n1 = int32(*(*int16)(unsafe.Pointer(normalizedCounter + uintptr(s3)*2)))
			i1 = 0
			for {
				if !(i1 < n1) {
					break
				}
				(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(position1)*8))).FbaseValue = s3
				position1 = (position1 + step1) & tableMask1
				for libc.BoolInt64(position1 > highThreshold) != 0 {
					position1 = (position1 + step1) & tableMask1
				} /* lowprob area */
				goto _8
			_8:
				;
				i1 = i1 + 1
			}
			goto _7
		_7:
			;
			s3 = s3 + 1
		}
		/* position must reach all cells once, otherwise normalizedCounter is incorrect */
	}
	/* Build Decoding table */
	u1 = uint32(0)
	for {
		if !(u1 < tableSize) {
			break
		}
		symbol = (*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FbaseValue
		v11 = symbolNext + uintptr(symbol)*2
		v10 = *(*U16)(unsafe.Pointer(v11))
		*(*U16)(unsafe.Pointer(v11)) = *(*U16)(unsafe.Pointer(v11)) + 1
		nextState = uint32(v10)
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnbBits = uint8(tableLog - ZSTD_highbit32(tls, nextState))
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnextState = uint16(nextState<<(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnbBits - tableSize)
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FnbAdditionalBits = *(*U8)(unsafe.Pointer(nbAdditionalBits + uintptr(symbol)))
		(*(*ZSTD_seqSymbol)(unsafe.Pointer(tableDecode + uintptr(u1)*8))).FbaseValue = *(*U32)(unsafe.Pointer(baseValue + uintptr(symbol)*4))
		goto _9
	_9:
		;
		u1 = u1 + 1
	}
}

// C documentation
//
//	/* Avoids the FORCE_INLINE of the _body() function. */
func ZSTD_buildFSETable_body_default(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t) {
	ZSTD_buildFSETable_body(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
}

func ZSTD_buildFSETable_body_bmi2(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t) {
	ZSTD_buildFSETable_body(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
}

func ZSTD_buildFSETable(tls *libc.TLS, dt uintptr, normalizedCounter uintptr, maxSymbolValue uint32, baseValue uintptr, nbAdditionalBits uintptr, tableLog uint32, wksp uintptr, wkspSize size_t, bmi2 int32) {
	if bmi2 != 0 {
		ZSTD_buildFSETable_body_bmi2(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
		return
	}
	_ = bmi2
	ZSTD_buildFSETable_body_default(tls, dt, normalizedCounter, maxSymbolValue, baseValue, nbAdditionalBits, tableLog, wksp, wkspSize)
}

// C documentation
//
//	/*! ZSTD_buildSeqTable() :
//	 * @return : nb bytes read from src,
//	 *           or an error code if it fails */
func ZSTD_buildSeqTable(tls *libc.TLS, DTableSpace uintptr, DTablePtr uintptr, type1 SymbolEncodingType_e, _max uint32, maxLog U32, src uintptr, srcSize size_t, baseValue uintptr, nbAdditionalBits uintptr, defaultTable uintptr, flagRepeatTable U32, ddictIsCold int32, nbSeq int32, wksp uintptr, wkspSize size_t, bmi2 int32) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	*(*uint32)(unsafe.Pointer(bp)) = _max
	var _pos, _size, headerSize, pSize size_t
	var _ptr, pStart uintptr
	var baseline, symbol U32
	var nbBits U8
	var _ /* norm at bp+8 */ [53]S16
	var _ /* tableLog at bp+4 */ uint32
	_, _, _, _, _, _, _, _, _ = _pos, _ptr, _size, baseline, headerSize, nbBits, pSize, pStart, symbol
	switch type1 {
	case int32(set_rle):
		goto _1
	case int32(set_basic):
		goto _2
	case int32(set_repeat):
		goto _3
	case int32(set_compressed):
		goto _4
	default:
		goto _5
	}
	goto _6
_1:
	;
_9:
	;
	if !(srcSize != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	goto _8
_8:
	;
	if 0 != 0 {
		goto _9
	}
	goto _7
_7:
	;
	if uint32(*(*BYTE)(unsafe.Pointer(src))) > *(*uint32)(unsafe.Pointer(bp)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	symbol = uint32(*(*BYTE)(unsafe.Pointer(src)))
	baseline = *(*U32)(unsafe.Pointer(baseValue + uintptr(symbol)*4))
	nbBits = *(*U8)(unsafe.Pointer(nbAdditionalBits + uintptr(symbol)))
	ZSTD_buildSeqTable_rle(tls, DTableSpace, baseline, nbBits)
	*(*uintptr)(unsafe.Pointer(DTablePtr)) = DTableSpace
	return uint64(1)
_2:
	;
	*(*uintptr)(unsafe.Pointer(DTablePtr)) = defaultTable
	return uint64(0)
_3:
	;
	if !(flagRepeatTable != 0) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* prefetch FSE table if used */
	if ddictIsCold != 0 && nbSeq > int32(24) {
		pStart = *(*uintptr)(unsafe.Pointer(DTablePtr))
		pSize = uint64(8) * libc.Uint64FromInt32(libc.Int32FromInt32(1)+libc.Int32FromInt32(1)<<maxLog)
		_ptr = pStart
		_size = pSize
		_pos = uint64(0)
		for {
			if !(_pos < _size) {
				break
			}
			libc.X__builtin_prefetch(tls, _ptr+uintptr(_pos), libc.VaList(bp+128, 0, int32(2)))
			goto _10
		_10:
			;
			_pos = _pos + uint64(CACHELINE_SIZE)
		}
	}
	return uint64(0)
_4:
	;
	headerSize = FSE_readNCount(tls, bp+8, bp, bp+4, src, srcSize)
	if ERR_isError(tls, headerSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	if *(*uint32)(unsafe.Pointer(bp + 4)) > maxLog {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	ZSTD_buildFSETable(tls, DTableSpace, bp+8, *(*uint32)(unsafe.Pointer(bp)), baseValue, nbAdditionalBits, *(*uint32)(unsafe.Pointer(bp + 4)), wksp, wkspSize, bmi2)
	*(*uintptr)(unsafe.Pointer(DTablePtr)) = DTableSpace
	return headerSize
_5:
	;
	if 0 != 0 {
		_force_has_format_string(tls, __ccgo_ts+8240, 0)
	}
	return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
_6:
	;
	return r
}

func ZSTD_decodeSeqHeaders(tls *libc.TLS, dctx uintptr, nbSeqPtr uintptr, src uintptr, srcSize size_t) (r size_t) {
	var LLtype, MLtype, OFtype SymbolEncodingType_e
	var iend, ip, istart, v1 uintptr
	var llhSize, mlhSize, ofhSize size_t
	var nbSeq int32
	_, _, _, _, _, _, _, _, _, _, _ = LLtype, MLtype, OFtype, iend, ip, istart, llhSize, mlhSize, nbSeq, ofhSize, v1
	istart = src
	iend = istart + uintptr(srcSize)
	ip = istart
	/* check */
	if srcSize < uint64(MIN_SEQUENCES_SIZE) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* SeqHead */
	v1 = ip
	ip = ip + 1
	nbSeq = libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(v1)))
	if nbSeq > int32(0x7F) {
		if nbSeq == int32(0xFF) {
			if ip+uintptr(2) > iend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
			}
			nbSeq = libc.Int32FromUint16(MEM_readLE16(tls, ip)) + int32(LONGNBSEQ)
			ip = ip + uintptr(2)
		} else {
			if ip >= iend {
				if 0 != 0 {
					_force_has_format_string(tls, __ccgo_ts+1319, 0)
				}
				return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
			}
			v1 = ip
			ip = ip + 1
			nbSeq = (nbSeq-int32(0x80))<<int32(8) + libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(v1)))
		}
	}
	*(*int32)(unsafe.Pointer(nbSeqPtr)) = nbSeq
	if nbSeq == 0 {
		/* No sequence : section ends immediately */
		if ip != iend {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+8251, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		return libc.Uint64FromInt64(int64(ip) - int64(istart))
	}
	/* FSE table descriptors */
	if ip+uintptr(1) > iend {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	} /* minimum possible size: 1 byte for symbol encoding types */
	if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip)))&int32(3) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	} /* The last field, Reserved, must be all-zeroes. */
	LLtype = libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip))) >> libc.Int32FromInt32(6)
	OFtype = libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip))) >> libc.Int32FromInt32(4) & libc.Int32FromInt32(3)
	MLtype = libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(ip))) >> libc.Int32FromInt32(2) & libc.Int32FromInt32(3)
	ip = ip + 1
	/* Build DTables */
	llhSize = ZSTD_buildSeqTable(tls, dctx+32, dctx, LLtype, uint32(MaxLL), uint32(LLFSELog), ip, libc.Uint64FromInt64(int64(iend)-int64(ip)), uintptr(unsafe.Pointer(&LL_base)), uintptr(unsafe.Pointer(&LL_bits)), uintptr(unsafe.Pointer(&LL_defaultDTable)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold, nbSeq, dctx+27324, uint64(2560), ZSTD_DCtx_get_bmi2(tls, dctx))
	if ZSTD_isError(tls, llhSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8300, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	ip = ip + uintptr(llhSize)
	ofhSize = ZSTD_buildSeqTable(tls, dctx+32+4104, dctx+16, OFtype, uint32(MaxOff), uint32(OffFSELog), ip, libc.Uint64FromInt64(int64(iend)-int64(ip)), uintptr(unsafe.Pointer(&OF_base)), uintptr(unsafe.Pointer(&OF_bits)), uintptr(unsafe.Pointer(&OF_defaultDTable)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold, nbSeq, dctx+27324, uint64(2560), ZSTD_DCtx_get_bmi2(tls, dctx))
	if ZSTD_isError(tls, ofhSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8300, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	ip = ip + uintptr(ofhSize)
	mlhSize = ZSTD_buildSeqTable(tls, dctx+32+6160, dctx+8, MLtype, uint32(MaxML), uint32(MLFSELog), ip, libc.Uint64FromInt64(int64(iend)-int64(ip)), uintptr(unsafe.Pointer(&ML_base)), uintptr(unsafe.Pointer(&ML_bits)), uintptr(unsafe.Pointer(&ML_defaultDTable)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold, nbSeq, dctx+27324, uint64(2560), ZSTD_DCtx_get_bmi2(tls, dctx))
	if ZSTD_isError(tls, mlhSize) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8300, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	ip = ip + uintptr(mlhSize)
	return libc.Uint64FromInt64(int64(ip) - int64(istart))
}

type seq_t = struct {
	FlitLength   size_t
	FmatchLength size_t
	Foffset      size_t
}

type ZSTD_fseState = struct {
	Fstate size_t
	Ftable uintptr
}

type seqState_t = struct {
	FDStream    BIT_DStream_t
	FstateLL    ZSTD_fseState
	FstateOffb  ZSTD_fseState
	FstateML    ZSTD_fseState
	FprevOffset [3]size_t
}

// C documentation
//
//	/*! ZSTD_overlapCopy8() :
//	 *  Copies 8 bytes from ip to op and updates op and ip where ip <= op.
//	 *  If the offset is < 8 then the offset is spread to at least 8 bytes.
//	 *
//	 *  Precondition: *ip <= *op
//	 *  Postcondition: *op - *op >= 8
//	 */
func ZSTD_overlapCopy8(tls *libc.TLS, op uintptr, ip uintptr, offset size_t) {
	var sub2 int32
	_ = sub2
	if offset < uint64(8) {
		sub2 = dec64table[offset]
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)))) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip))))
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)) + 1)) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip)) + 1))
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)) + 2)) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip)) + 2))
		*(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(op)) + 3)) = *(*BYTE)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(ip)) + 3))
		*(*uintptr)(unsafe.Pointer(ip)) += uintptr(dec32table[offset])
		ZSTD_copy4(tls, *(*uintptr)(unsafe.Pointer(op))+uintptr(4), *(*uintptr)(unsafe.Pointer(ip)))
		*(*uintptr)(unsafe.Pointer(ip)) -= uintptr(sub2)
	} else {
		ZSTD_copy8(tls, *(*uintptr)(unsafe.Pointer(op)), *(*uintptr)(unsafe.Pointer(ip)))
	}
	*(*uintptr)(unsafe.Pointer(ip)) += uintptr(8)
	*(*uintptr)(unsafe.Pointer(op)) += uintptr(8)
}

/* close range match, overlap */
var dec32table = [8]U32{
	1: uint32(1),
	2: uint32(2),
	3: uint32(1),
	4: uint32(4),
	5: uint32(4),
	6: uint32(4),
	7: uint32(4),
} /* added */

var dec64table = [8]int32{
	0: int32(8),
	1: int32(8),
	2: int32(8),
	3: int32(7),
	4: int32(8),
	5: int32(9),
	6: int32(10),
	7: int32(11),
} /* subtracted */

// C documentation
//
//	/*! ZSTD_safecopy() :
//	 *  Specialized version of memcpy() that is allowed to READ up to WILDCOPY_OVERLENGTH past the input buffer
//	 *  and write up to 16 bytes past oend_w (op >= oend_w is allowed).
//	 *  This function is only called in the uncommon case where the sequence is near the end of the block. It
//	 *  should be fast for a single long sequence, but can be slow for several short sequences.
//	 *
//	 *  @param ovtype controls the overlap detection
//	 *         - ZSTD_no_overlap: The source and destination are guaranteed to be at least WILDCOPY_VECLEN bytes apart.
//	 *         - ZSTD_overlap_src_before_dst: The src and dst may overlap and may be any distance apart.
//	 *           The src buffer must be before the dst buffer.
//	 */
func ZSTD_safecopy(tls *libc.TLS, _op uintptr, oend_w uintptr, _ip uintptr, length ptrdiff_t, ovtype ZSTD_overlap_e) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _op
	*(*uintptr)(unsafe.Pointer(bp + 8)) = _ip
	var diff ptrdiff_t
	var oend, v1, v2 uintptr
	_, _, _, _ = diff, oend, v1, v2
	diff = int64(*(*uintptr)(unsafe.Pointer(bp))) - int64(*(*uintptr)(unsafe.Pointer(bp + 8)))
	oend = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(length)
	if length < int64(8) {
		/* Handle short lengths. */
		for *(*uintptr)(unsafe.Pointer(bp)) < oend {
			v1 = *(*uintptr)(unsafe.Pointer(bp))
			*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + 1
			v2 = *(*uintptr)(unsafe.Pointer(bp + 8))
			*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp + 8)) + 1
			*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
		}
		return
	}
	if ovtype == int32(ZSTD_overlap_src_before_dst) {
		/* Copy 8 bytes and ensure the offset >= 8 when there can be overlap. */
		ZSTD_overlapCopy8(tls, bp, bp+8, libc.Uint64FromInt64(diff))
		length = length - int64(8)
	}
	if oend <= oend_w {
		/* No risk of overwrite. */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), length, ovtype)
		return
	}
	if *(*uintptr)(unsafe.Pointer(bp)) <= oend_w {
		/* Wildcopy until we get close to the end. */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), int64(oend_w)-int64(*(*uintptr)(unsafe.Pointer(bp))), ovtype)
		*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp + 8)) + uintptr(int64(oend_w)-int64(*(*uintptr)(unsafe.Pointer(bp))))
		*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(int64(oend_w)-int64(*(*uintptr)(unsafe.Pointer(bp))))
	}
	/* Handle the leftovers. */
	for *(*uintptr)(unsafe.Pointer(bp)) < oend {
		v1 = *(*uintptr)(unsafe.Pointer(bp))
		*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp)) + 1
		v2 = *(*uintptr)(unsafe.Pointer(bp + 8))
		*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp + 8)) + 1
		*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
	}
}

// C documentation
//
//	/* ZSTD_safecopyDstBeforeSrc():
//	 * This version allows overlap with dst before src, or handles the non-overlap case with dst after src
//	 * Kept separate from more common ZSTD_safecopy case to avoid performance impact to the safecopy common case */
func ZSTD_safecopyDstBeforeSrc(tls *libc.TLS, op uintptr, ip uintptr, length ptrdiff_t) {
	var diff ptrdiff_t
	var oend, v1, v2 uintptr
	_, _, _, _ = diff, oend, v1, v2
	diff = int64(op) - int64(ip)
	oend = op + uintptr(length)
	if length < int64(8) || diff > int64(-int32(8)) {
		/* Handle short lengths, close overlaps, and dst not before src. */
		for op < oend {
			v1 = op
			op = op + 1
			v2 = ip
			ip = ip + 1
			*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
		}
		return
	}
	if op <= oend-uintptr(WILDCOPY_OVERLENGTH) && diff < int64(-int32(WILDCOPY_VECLEN)) {
		ZSTD_wildcopy(tls, op, ip, int64(oend-uintptr(WILDCOPY_OVERLENGTH))-int64(op), int32(ZSTD_no_overlap))
		ip = ip + uintptr(int64(oend-uintptr(WILDCOPY_OVERLENGTH))-int64(op))
		op = op + uintptr(int64(oend-uintptr(WILDCOPY_OVERLENGTH))-int64(op))
	}
	/* Handle the leftovers. */
	for op < oend {
		v1 = op
		op = op + 1
		v2 = ip
		ip = ip + 1
		*(*BYTE)(unsafe.Pointer(v1)) = *(*BYTE)(unsafe.Pointer(v2))
	}
}

// C documentation
//
//	/* ZSTD_execSequenceEnd():
//	 * This version handles cases that are near the end of the output buffer. It requires
//	 * more careful checks to make sure there is no overflow. By separating out these hard
//	 * and unlikely cases, we can speed up the common cases.
//	 *
//	 * NOTE: This function needs to be fast for a single long sequence, but doesn't need
//	 * to be optimized for many small sequences, since those fall into ZSTD_execSequence().
//	 */
func ZSTD_execSequenceEnd(tls *libc.TLS, op uintptr, oend uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	var iLitEnd, match, oLitEnd, oend_w uintptr
	var length1, sequenceLength size_t
	_, _, _, _, _, _ = iLitEnd, length1, match, oLitEnd, oend_w, sequenceLength
	oLitEnd = op + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	match = oLitEnd - uintptr(sequence.Foffset)
	oend_w = oend - uintptr(WILDCOPY_OVERLENGTH)
	/* bounds checks : careful of address space overflow in 32-bit mode */
	if sequenceLength > libc.Uint64FromInt64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8326, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if sequence.FlitLength > libc.Uint64FromInt64(int64(litLimit)-int64(*(*uintptr)(unsafe.Pointer(litPtr)))) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8363, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* copy literals */
	ZSTD_safecopy(tls, op, oend_w, *(*uintptr)(unsafe.Pointer(litPtr)), libc.Int64FromUint64(sequence.FlitLength), int32(ZSTD_no_overlap))
	op = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd
	/* copy Match */
	if sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix */
		if sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(virtualStart)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		match = dictEnd - uintptr(int64(prefixStart)-int64(match))
		if match+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, match, sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = libc.Uint64FromInt64(int64(dictEnd) - int64(match))
		libc.Xmemmove(tls, oLitEnd, match, length1)
		op = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		match = prefixStart
	}
	ZSTD_safecopy(tls, op, oend_w, match, libc.Int64FromUint64(sequence.FmatchLength), int32(ZSTD_overlap_src_before_dst))
	return sequenceLength
}

// C documentation
//
//	/* ZSTD_execSequenceEndSplitLitBuffer():
//	 * This version is intended to be used during instances where the litBuffer is still split.  It is kept separate to avoid performance impact for the good case.
//	 */
func ZSTD_execSequenceEndSplitLitBuffer(tls *libc.TLS, op uintptr, oend uintptr, oend_w uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	var iLitEnd, match, oLitEnd uintptr
	var length1, sequenceLength size_t
	_, _, _, _, _ = iLitEnd, length1, match, oLitEnd, sequenceLength
	oLitEnd = op + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	match = oLitEnd - uintptr(sequence.Foffset)
	/* bounds checks : careful of address space overflow in 32-bit mode */
	if sequenceLength > libc.Uint64FromInt64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8326, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if sequence.FlitLength > libc.Uint64FromInt64(int64(litLimit)-int64(*(*uintptr)(unsafe.Pointer(litPtr)))) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8363, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
	}
	/* copy literals */
	if op > *(*uintptr)(unsafe.Pointer(litPtr)) && op < *(*uintptr)(unsafe.Pointer(litPtr))+uintptr(sequence.FlitLength) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8397, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(litPtr)), libc.Int64FromUint64(sequence.FlitLength))
	op = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd
	/* copy Match */
	if sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix */
		if sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(virtualStart)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		match = dictEnd - uintptr(int64(prefixStart)-int64(match))
		if match+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, match, sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = libc.Uint64FromInt64(int64(dictEnd) - int64(match))
		libc.Xmemmove(tls, oLitEnd, match, length1)
		op = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		match = prefixStart
	}
	ZSTD_safecopy(tls, op, oend_w, match, libc.Int64FromUint64(sequence.FmatchLength), int32(ZSTD_overlap_src_before_dst))
	return sequenceLength
}

func ZSTD_execSequence(tls *libc.TLS, _op uintptr, oend uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _op
	var iLitEnd, oLitEnd, oMatchEnd, oend_w uintptr
	var length1, sequenceLength size_t
	var _ /* match at bp+8 */ uintptr
	_, _, _, _, _, _ = iLitEnd, length1, oLitEnd, oMatchEnd, oend_w, sequenceLength
	oLitEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	oMatchEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequenceLength) /* risk : address space overflow (32-bits) */
	oend_w = oend - uintptr(WILDCOPY_OVERLENGTH)                          /* risk : address space underflow on oend=NULL */
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	*(*uintptr)(unsafe.Pointer(bp + 8)) = oLitEnd - uintptr(sequence.Foffset)
	/* Handle edge cases in a slow path:
	 *   - Read beyond end of literals
	 *   - Match end is within WILDCOPY_OVERLIMIT of oend
	 *   - 32-bit mode and the match length overflows
	 */
	if libc.BoolInt64(iLitEnd > litLimit || oMatchEnd > oend_w || MEM_32bits(tls) != 0 && libc.Uint64FromInt64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))) < sequenceLength+uint64(WILDCOPY_OVERLENGTH)) != 0 {
		return ZSTD_execSequenceEnd(tls, *(*uintptr)(unsafe.Pointer(bp)), oend, sequence, litPtr, litLimit, prefixStart, virtualStart, dictEnd)
	}
	/* Assumptions (everything else goes into ZSTD_execSequenceEnd()) */
	/* Copy Literals:
	 * Split out litLength <= 16 since it is nearly always true. +1.6% on gcc-9.
	 * We likely don't need the full 32-byte wildcopy.
	 */
	ZSTD_copy16(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(litPtr)))
	if libc.BoolInt64(sequence.FlitLength > libc.Uint64FromInt32(16)) != 0 {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp))+uintptr(16), *(*uintptr)(unsafe.Pointer(litPtr))+uintptr(16), libc.Int64FromUint64(sequence.FlitLength-uint64(16)), int32(ZSTD_no_overlap))
	}
	*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd /* update for next sequence */
	/* Copy Match */
	if sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix -> go into extDict */
		if libc.BoolInt64(sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(virtualStart))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		*(*uintptr)(unsafe.Pointer(bp + 8)) = dictEnd + uintptr(int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(prefixStart))
		if *(*uintptr)(unsafe.Pointer(bp + 8))+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = libc.Uint64FromInt64(int64(dictEnd) - int64(*(*uintptr)(unsafe.Pointer(bp + 8))))
		libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), length1)
		*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		*(*uintptr)(unsafe.Pointer(bp + 8)) = prefixStart
	}
	/* Match within prefix of 1 or more bytes */
	/* Nearly all offsets are >= WILDCOPY_VECLEN bytes, which means we can use wildcopy
	 * without overlap checking.
	 */
	if libc.BoolInt64(sequence.Foffset >= libc.Uint64FromInt32(WILDCOPY_VECLEN)) != 0 {
		/* We bet on a full wildcopy for matches, since we expect matches to be
		 * longer than literals (in general). In silesia, ~10% of matches are longer
		 * than 16 bytes.
		 */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), libc.Int64FromUint64(sequence.FmatchLength), int32(ZSTD_no_overlap))
		return sequenceLength
	}
	/* Copy 8 bytes and spread the offset to be >= 8. */
	ZSTD_overlapCopy8(tls, bp, bp+8, sequence.Foffset)
	/* If the match length is > 8 bytes, then continue with the wildcopy. */
	if sequence.FmatchLength > uint64(8) {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), libc.Int64FromUint64(sequence.FmatchLength)-int64(8), int32(ZSTD_overlap_src_before_dst))
	}
	return sequenceLength
}

func ZSTD_execSequenceSplitLitBuffer(tls *libc.TLS, _op uintptr, oend uintptr, oend_w uintptr, sequence seq_t, litPtr uintptr, litLimit uintptr, prefixStart uintptr, virtualStart uintptr, dictEnd uintptr) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	*(*uintptr)(unsafe.Pointer(bp)) = _op
	var iLitEnd, oLitEnd, oMatchEnd uintptr
	var length1, sequenceLength size_t
	var _ /* match at bp+8 */ uintptr
	_, _, _, _, _ = iLitEnd, length1, oLitEnd, oMatchEnd, sequenceLength
	oLitEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequence.FlitLength)
	sequenceLength = sequence.FlitLength + sequence.FmatchLength
	oMatchEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr(sequenceLength) /* risk : address space overflow (32-bits) */
	iLitEnd = *(*uintptr)(unsafe.Pointer(litPtr)) + uintptr(sequence.FlitLength)
	*(*uintptr)(unsafe.Pointer(bp + 8)) = oLitEnd - uintptr(sequence.Foffset)
	/* Handle edge cases in a slow path:
	 *   - Read beyond end of literals
	 *   - Match end is within WILDCOPY_OVERLIMIT of oend
	 *   - 32-bit mode and the match length overflows
	 */
	if libc.BoolInt64(iLitEnd > litLimit || oMatchEnd > oend_w || MEM_32bits(tls) != 0 && libc.Uint64FromInt64(int64(oend)-int64(*(*uintptr)(unsafe.Pointer(bp)))) < sequenceLength+uint64(WILDCOPY_OVERLENGTH)) != 0 {
		return ZSTD_execSequenceEndSplitLitBuffer(tls, *(*uintptr)(unsafe.Pointer(bp)), oend, oend_w, sequence, litPtr, litLimit, prefixStart, virtualStart, dictEnd)
	}
	/* Assumptions (everything else goes into ZSTD_execSequenceEnd()) */
	/* Copy Literals:
	 * Split out litLength <= 16 since it is nearly always true. +1.6% on gcc-9.
	 * We likely don't need the full 32-byte wildcopy.
	 */
	ZSTD_copy16(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(litPtr)))
	if libc.BoolInt64(sequence.FlitLength > libc.Uint64FromInt32(16)) != 0 {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp))+uintptr(16), *(*uintptr)(unsafe.Pointer(litPtr))+uintptr(16), libc.Int64FromUint64(sequence.FlitLength-uint64(16)), int32(ZSTD_no_overlap))
	}
	*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd
	*(*uintptr)(unsafe.Pointer(litPtr)) = iLitEnd /* update for next sequence */
	/* Copy Match */
	if sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(prefixStart)) {
		/* offset beyond prefix -> go into extDict */
		if libc.BoolInt64(sequence.Foffset > libc.Uint64FromInt64(int64(oLitEnd)-int64(virtualStart))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		*(*uintptr)(unsafe.Pointer(bp + 8)) = dictEnd + uintptr(int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(prefixStart))
		if *(*uintptr)(unsafe.Pointer(bp + 8))+uintptr(sequence.FmatchLength) <= dictEnd {
			libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), sequence.FmatchLength)
			return sequenceLength
		}
		/* span extDict & currentPrefixSegment */
		length1 = libc.Uint64FromInt64(int64(dictEnd) - int64(*(*uintptr)(unsafe.Pointer(bp + 8))))
		libc.Xmemmove(tls, oLitEnd, *(*uintptr)(unsafe.Pointer(bp + 8)), length1)
		*(*uintptr)(unsafe.Pointer(bp)) = oLitEnd + uintptr(length1)
		sequence.FmatchLength -= length1
		*(*uintptr)(unsafe.Pointer(bp + 8)) = prefixStart
	}
	/* Match within prefix of 1 or more bytes */
	/* Nearly all offsets are >= WILDCOPY_VECLEN bytes, which means we can use wildcopy
	 * without overlap checking.
	 */
	if libc.BoolInt64(sequence.Foffset >= libc.Uint64FromInt32(WILDCOPY_VECLEN)) != 0 {
		/* We bet on a full wildcopy for matches, since we expect matches to be
		 * longer than literals (in general). In silesia, ~10% of matches are longer
		 * than 16 bytes.
		 */
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), libc.Int64FromUint64(sequence.FmatchLength), int32(ZSTD_no_overlap))
		return sequenceLength
	}
	/* Copy 8 bytes and spread the offset to be >= 8. */
	ZSTD_overlapCopy8(tls, bp, bp+8, sequence.Foffset)
	/* If the match length is > 8 bytes, then continue with the wildcopy. */
	if sequence.FmatchLength > uint64(8) {
		ZSTD_wildcopy(tls, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), libc.Int64FromUint64(sequence.FmatchLength)-int64(8), int32(ZSTD_overlap_src_before_dst))
	}
	return sequenceLength
}

func ZSTD_initFseState(tls *libc.TLS, DStatePtr uintptr, bitD uintptr, dt uintptr) {
	var DTableH, ptr uintptr
	_, _ = DTableH, ptr
	ptr = dt
	DTableH = ptr
	(*ZSTD_fseState)(unsafe.Pointer(DStatePtr)).Fstate = BIT_readBits(tls, bitD, (*ZSTD_seqSymbol_header)(unsafe.Pointer(DTableH)).FtableLog)
	BIT_reloadDStream(tls, bitD)
	(*ZSTD_fseState)(unsafe.Pointer(DStatePtr)).Ftable = dt + uintptr(1)*8
}

func ZSTD_updateFseStateWithDInfo(tls *libc.TLS, DStatePtr uintptr, bitD uintptr, nextState U16, nbBits U32) {
	var lowBits size_t
	_ = lowBits
	lowBits = BIT_readBits(tls, bitD, nbBits)
	(*ZSTD_fseState)(unsafe.Pointer(DStatePtr)).Fstate = uint64(nextState) + lowBits
}

/* We need to add at most (ZSTD_WINDOWLOG_MAX_32 - 1) bits to read the maximum
 * offset bits. But we can only read at most STREAM_ACCUMULATOR_MIN_32
 * bits before reloading. This value is the maximum number of bytes we read
 * after reloading when we are decoding long offsets.
 */

type ZSTD_longOffset_e = int32

const ZSTD_lo_isRegularOffset = 0
const ZSTD_lo_isLongOffset = 1

// C documentation
//
//	/**
//	 * ZSTD_decodeSequence():
//	 * @p longOffsets : tells the decoder to reload more bit while decoding large offsets
//	 *                  only used in 32-bit mode
//	 * @return : Sequence (litL + matchL + offset)
//	 */
func ZSTD_decodeSequence(tls *libc.TLS, seqState uintptr, longOffsets ZSTD_longOffset_e, isLastSeq int32) (r seq_t) {
	var extraBits, ll0, llnbBits, mlnbBits, ofBase, ofnbBits U32
	var llBits, mlBits, ofBits, totalBits BYTE
	var llDInfo, mlDInfo, ofDInfo uintptr
	var llNext, mlNext, ofNext U16
	var offset, temp, v2 size_t
	var seq seq_t
	var v1 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = extraBits, ll0, llBits, llDInfo, llNext, llnbBits, mlBits, mlDInfo, mlNext, mlnbBits, ofBase, ofBits, ofDInfo, ofNext, offset, ofnbBits, seq, temp, totalBits, v1, v2
	/*
	 * ZSTD_seqSymbol is a 64 bits wide structure.
	 * It can be loaded in one operation
	 * and its fields extracted by simply shifting or bit-extracting on aarch64.
	 * GCC doesn't recognize this and generates more unnecessary ldr/ldrb/ldrh
	 * operations that cause performance drop. This can be avoided by using this
	 * ZSTD_memcpy hack.
	 */
	llDInfo = (*seqState_t)(unsafe.Pointer(seqState)).FstateLL.Ftable + uintptr((*seqState_t)(unsafe.Pointer(seqState)).FstateLL.Fstate)*8
	mlDInfo = (*seqState_t)(unsafe.Pointer(seqState)).FstateML.Ftable + uintptr((*seqState_t)(unsafe.Pointer(seqState)).FstateML.Fstate)*8
	ofDInfo = (*seqState_t)(unsafe.Pointer(seqState)).FstateOffb.Ftable + uintptr((*seqState_t)(unsafe.Pointer(seqState)).FstateOffb.Fstate)*8
	seq.FmatchLength = uint64((*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FbaseValue)
	seq.FlitLength = uint64((*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FbaseValue)
	ofBase = (*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FbaseValue
	llBits = (*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FnbAdditionalBits
	mlBits = (*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FnbAdditionalBits
	ofBits = (*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FnbAdditionalBits
	totalBits = libc.Uint8FromInt32(libc.Int32FromUint8(llBits) + libc.Int32FromUint8(mlBits) + libc.Int32FromUint8(ofBits))
	llNext = (*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FnextState
	mlNext = (*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FnextState
	ofNext = (*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FnextState
	llnbBits = uint32((*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FnbBits)
	mlnbBits = uint32((*ZSTD_seqSymbol)(unsafe.Pointer(mlDInfo)).FnbBits)
	ofnbBits = uint32((*ZSTD_seqSymbol)(unsafe.Pointer(ofDInfo)).FnbBits)
	/*
	 * As gcc has better branch and block analyzers, sometimes it is only
	 * valuable to mark likeliness for clang, it gives around 3-4% of
	 * performance.
	 */
	/* sequence */
	if libc.Int32FromUint8(ofBits) > int32(1) {
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		_ = libc.Uint64FromInt64(1)
		if MEM_32bits(tls) != 0 && longOffsets != 0 && libc.Int32FromUint8(ofBits) >= int32(STREAM_ACCUMULATOR_MIN_32) {
			/* Always read extra bits, this keeps the logic simple,
			 * avoids branches, and avoids accidentally reading 0 bits.
			 */
			extraBits = libc.Uint32FromInt32(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_32) - libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_32))
			offset = uint64(ofBase) + BIT_readBitsFast(tls, seqState, uint32(ofBits)-extraBits)<<extraBits
			BIT_reloadDStream(tls, seqState)
			offset = offset + BIT_readBitsFast(tls, seqState, extraBits)
		} else {
			offset = uint64(ofBase) + BIT_readBitsFast(tls, seqState, uint32(ofBits)) /* <=  (ZSTD_WINDOWLOG_MAX-1) bits */
			if MEM_32bits(tls) != 0 {
				BIT_reloadDStream(tls, seqState)
			}
		}
		*(*size_t)(unsafe.Pointer(seqState + 88 + 2*8)) = *(*size_t)(unsafe.Pointer(seqState + 88 + 1*8))
		*(*size_t)(unsafe.Pointer(seqState + 88 + 1*8)) = *(*size_t)(unsafe.Pointer(seqState + 88))
		*(*size_t)(unsafe.Pointer(seqState + 88)) = offset
	} else {
		ll0 = libc.BoolUint32((*ZSTD_seqSymbol)(unsafe.Pointer(llDInfo)).FbaseValue == libc.Uint32FromInt32(0))
		if libc.BoolInt64(libc.Int32FromUint8(ofBits) == libc.Int32FromInt32(0)) != 0 {
			offset = *(*size_t)(unsafe.Pointer(seqState + 88 + uintptr(ll0)*8))
			*(*size_t)(unsafe.Pointer(seqState + 88 + 1*8)) = *(*size_t)(unsafe.Pointer(seqState + 88 + libc.BoolUintptr(!(ll0 != 0))*8))
			*(*size_t)(unsafe.Pointer(seqState + 88)) = offset
		} else {
			offset = uint64(ofBase+ll0) + BIT_readBitsFast(tls, seqState, uint32(1))
			if offset == uint64(3) {
				v1 = *(*size_t)(unsafe.Pointer(seqState + 88)) - uint64(1)
			} else {
				v1 = *(*size_t)(unsafe.Pointer(seqState + 88 + uintptr(offset)*8))
			}
			temp = v1
			temp = temp - libc.BoolUint64(!(temp != 0)) /* 0 is not valid: input corrupted => force offset to -1 => corruption detected at execSequence */
			if offset != uint64(1) {
				*(*size_t)(unsafe.Pointer(seqState + 88 + 2*8)) = *(*size_t)(unsafe.Pointer(seqState + 88 + 1*8))
			}
			*(*size_t)(unsafe.Pointer(seqState + 88 + 1*8)) = *(*size_t)(unsafe.Pointer(seqState + 88))
			v2 = temp
			offset = v2
			*(*size_t)(unsafe.Pointer(seqState + 88)) = v2
		}
	}
	seq.Foffset = offset
	if libc.Int32FromUint8(mlBits) > 0 {
		seq.FmatchLength += BIT_readBitsFast(tls, seqState, uint32(mlBits))
	}
	if MEM_32bits(tls) != 0 && libc.Int32FromUint8(mlBits)+libc.Int32FromUint8(llBits) >= libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_32)-(libc.Int32FromInt32(ZSTD_WINDOWLOG_MAX_32)-libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_32)) {
		BIT_reloadDStream(tls, seqState)
	}
	if MEM_64bits(tls) != 0 && libc.BoolInt64(libc.Int32FromUint8(totalBits) >= libc.Int32FromInt32(STREAM_ACCUMULATOR_MIN_64)-(libc.Int32FromInt32(LLFSELog)+libc.Int32FromInt32(MLFSELog)+libc.Int32FromInt32(OffFSELog))) != 0 {
		BIT_reloadDStream(tls, seqState)
	}
	/* Ensure there are enough bits to read the rest of data in 64-bit mode. */
	_ = libc.Uint64FromInt64(1)
	if libc.Int32FromUint8(llBits) > 0 {
		seq.FlitLength += BIT_readBitsFast(tls, seqState, uint32(llBits))
	}
	if MEM_32bits(tls) != 0 {
		BIT_reloadDStream(tls, seqState)
	}
	if !(isLastSeq != 0) {
		/* don't update FSE state for last Sequence */
		ZSTD_updateFseStateWithDInfo(tls, seqState+40, seqState, llNext, llnbBits) /* <=  9 bits */
		ZSTD_updateFseStateWithDInfo(tls, seqState+72, seqState, mlNext, mlnbBits) /* <=  9 bits */
		if MEM_32bits(tls) != 0 {
			BIT_reloadDStream(tls, seqState)
		} /* <= 18 bits */
		ZSTD_updateFseStateWithDInfo(tls, seqState+56, seqState, ofNext, ofnbBits) /* <=  8 bits */
		BIT_reloadDStream(tls, seqState)
	}
	return seq
}

func ZSTD_decompressSequences_bodySplitLitBuffer(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	bp := tls.Alloc(128)
	defer tls.Free(128)
	var dictEnd, iend, ip, litBufferEnd, oend, op, ostart, prefixStart, vBase uintptr
	var i, i1 U32
	var lastLLSize, lastLLSize1, leftoverLit, oneSeqSize, oneSeqSize1, oneSeqSize2 size_t
	var sequence, sequence1 seq_t
	var _ /* litPtr at bp+0 */ uintptr
	var _ /* seqState at bp+8 */ seqState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dictEnd, i, i1, iend, ip, lastLLSize, lastLLSize1, leftoverLit, litBufferEnd, oend, oneSeqSize, oneSeqSize1, oneSeqSize2, op, ostart, prefixStart, sequence, sequence1, vBase
	ip = seqStart
	iend = ip + uintptr(seqSize)
	ostart = dst
	oend = ZSTD_maybeNullPtrAdd(tls, ostart, libc.Int64FromUint64(maxDstSize))
	op = ostart
	*(*uintptr)(unsafe.Pointer(bp)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr
	litBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd
	prefixStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart
	vBase = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart
	dictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	/* Literals are split between internal buffer & output buffer */
	if nbSeq != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		i = uint32(0)
		for {
			if !(i < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i)*8)) = uint64(*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i)*4)))
			goto _1
		_1:
			;
			i = i + 1
		}
		if ERR_isError(tls, BIT_initDStream(tls, bp+8, ip, libc.Uint64FromInt64(int64(iend)-int64(ip)))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		ZSTD_initFseState(tls, bp+8+40, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr)
		ZSTD_initFseState(tls, bp+8+56, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr)
		ZSTD_initFseState(tls, bp+8+72, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr)
		_ = libc.Uint64FromInt64(1)
		/* decompress without overrunning litPtr begins */
		sequence = seq_t{} /* some static analyzer believe that @sequence is not initialized (it necessarily is, since for(;;) loop as at least one iteration) */
		/* Align the decompression loop to 32 + 16 bytes.
		 *
		 * zstd compiled with gcc-9 on an Intel i9-9900k shows 10% decompression
		 * speed swings based on the alignment of the decompression loop. This
		 * performance swing is caused by parts of the decompression loop falling
		 * out of the DSB. The entire decompression loop should fit in the DSB,
		 * when it can't we get much worse performance. You can measure if you've
		 * hit the good case or the bad case with this perf command for some
		 * compressed file test.zst:
		 *
		 *   perf stat -e cycles -e instructions -e idq.all_dsb_cycles_any_uops                 *             -e idq.all_mite_cycles_any_uops -- ./zstd -tq test.zst
		 *
		 * If you see most cycles served out of the MITE you've hit the bad case.
		 * If you see most cycles served out of the DSB you've hit the good case.
		 * If it is pretty even then you may be in an okay case.
		 *
		 * This issue has been reproduced on the following CPUs:
		 *   - Kabylake: Macbook Pro (15-inch, 2019) 2.4 GHz Intel Core i9
		 *               Use Instruments->Counters to get DSB/MITE cycles.
		 *               I never got performance swings, but I was able to
		 *               go from the good case of mostly DSB to half of the
		 *               cycles served from MITE.
		 *   - Coffeelake: Intel i9-9900k
		 *   - Coffeelake: Intel i7-9700k
		 *
		 * I haven't been able to reproduce the instability or DSB misses on any
		 * of the following CPUS:
		 *   - Haswell
		 *   - Broadwell: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GH
		 *   - Skylake
		 *
		 * Alignment is done for each of the three major decompression loops:
		 *   - ZSTD_decompressSequences_bodySplitLitBuffer - presplit section of the literal buffer
		 *   - ZSTD_decompressSequences_bodySplitLitBuffer - postsplit section of the literal buffer
		 *   - ZSTD_decompressSequences_body
		 * Alignment choices are made to minimize large swings on bad cases and influence on performance
		 * from changes external to this code, rather than to overoptimize on the current commit.
		 *
		 * If you are seeing performance stability this script can help test.
		 * It tests on 4 commits in zstd where I saw performance change.
		 *
		 *   https://gist.github.com/terrelln/9889fc06a423fd5ca6e99351564473f4
		 */
		/* Handle the initial state where litBuffer is currently split between dst and litExtraBuffer */
		for {
			if !(nbSeq != 0) {
				break
			}
			sequence = ZSTD_decodeSequence(tls, bp+8, isLongOffset, libc.BoolInt32(nbSeq == int32(1)))
			if *(*uintptr)(unsafe.Pointer(bp))+uintptr(sequence.FlitLength) > (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd {
				break
			}
			oneSeqSize = ZSTD_execSequenceSplitLitBuffer(tls, op, oend, *(*uintptr)(unsafe.Pointer(bp))+uintptr(sequence.FlitLength)-uintptr(WILDCOPY_OVERLENGTH), sequence, bp, litBufferEnd, prefixStart, vBase, dictEnd)
			if libc.Int64FromUint32(ZSTD_isError(tls, oneSeqSize)) != 0 {
				return oneSeqSize
			}
			op = op + uintptr(oneSeqSize)
			goto _2
		_2:
			;
			nbSeq = nbSeq - 1
		}
		/* If there are more sequences, they will need to read literals from litExtraBuffer; copy over the remainder from dst and update litPtr and litEnd */
		if nbSeq > 0 {
			leftoverLit = libc.Uint64FromInt64(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
			if leftoverLit != 0 {
				if leftoverLit > libc.Uint64FromInt64(int64(oend)-int64(op)) {
					if 0 != 0 {
						_force_has_format_string(tls, __ccgo_ts+8456, 0)
					}
					return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
				}
				ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(bp)), libc.Int64FromUint64(leftoverLit))
				sequence.FlitLength -= leftoverLit
				op = op + uintptr(leftoverLit)
			}
			*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
			litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
			(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
			oneSeqSize1 = ZSTD_execSequence(tls, op, oend, sequence, bp, litBufferEnd, prefixStart, vBase, dictEnd)
			if libc.Int64FromUint32(ZSTD_isError(tls, oneSeqSize1)) != 0 {
				return oneSeqSize1
			}
			op = op + uintptr(oneSeqSize1)
			nbSeq = nbSeq - 1
		}
		if nbSeq > 0 {
			/* there is remaining lit from extra buffer */
			for {
				if !(nbSeq != 0) {
					break
				}
				sequence1 = ZSTD_decodeSequence(tls, bp+8, isLongOffset, libc.BoolInt32(nbSeq == int32(1)))
				oneSeqSize2 = ZSTD_execSequence(tls, op, oend, sequence1, bp, litBufferEnd, prefixStart, vBase, dictEnd)
				if libc.Int64FromUint32(ZSTD_isError(tls, oneSeqSize2)) != 0 {
					return oneSeqSize2
				}
				op = op + uintptr(oneSeqSize2)
				goto _3
			_3:
				;
				nbSeq = nbSeq - 1
			}
		}
		/* check if reached exact end */
		if nbSeq != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		if !(BIT_endOfDStream(tls, bp+8) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		/* save reps for next block */
		i1 = uint32(0)
		for {
			if !(i1 < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i1)*4)) = uint32(*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i1)*8)))
			goto _4
		_4:
			;
			i1 = i1 + 1
		}
	}
	/* last literal segment */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		/* split hasn't been reached yet, first get dst then copy litExtraBuffer */
		lastLLSize = libc.Uint64FromInt64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
		if lastLLSize > libc.Uint64FromInt64(int64(oend)-int64(op)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		if op != libc.UintptrFromInt32(0) {
			libc.Xmemmove(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize)
			op = op + uintptr(lastLLSize)
		}
		*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
		litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
	}
	/* copy last literals from internal buffer */
	lastLLSize1 = libc.Uint64FromInt64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
	if lastLLSize1 > libc.Uint64FromInt64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if op != libc.UintptrFromInt32(0) {
		libc.Xmemcpy(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize1)
		op = op + uintptr(lastLLSize1)
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_decompressSequences_body(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	bp := tls.Alloc(128)
	defer tls.Free(128)
	var dictEnd, iend, ip, litEnd, oend, op, ostart, prefixStart, vBase, v1 uintptr
	var i, i1 U32
	var lastLLSize, oneSeqSize size_t
	var sequence seq_t
	var _ /* litPtr at bp+0 */ uintptr
	var _ /* seqState at bp+8 */ seqState_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dictEnd, i, i1, iend, ip, lastLLSize, litEnd, oend, oneSeqSize, op, ostart, prefixStart, sequence, vBase, v1
	ip = seqStart
	iend = ip + uintptr(seqSize)
	ostart = dst
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_not_in_dst) {
		v1 = ZSTD_maybeNullPtrAdd(tls, ostart, libc.Int64FromUint64(maxDstSize))
	} else {
		v1 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	}
	oend = v1
	op = ostart
	*(*uintptr)(unsafe.Pointer(bp)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr
	litEnd = *(*uintptr)(unsafe.Pointer(bp)) + uintptr((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitSize)
	prefixStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart
	vBase = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart
	dictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	/* Regen sequences */
	if nbSeq != 0 {
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		i = uint32(0)
		for {
			if !(i < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i)*8)) = uint64(*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i)*4)))
			goto _2
		_2:
			;
			i = i + 1
		}
		if ERR_isError(tls, BIT_initDStream(tls, bp+8, ip, libc.Uint64FromInt64(int64(iend)-int64(ip)))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		ZSTD_initFseState(tls, bp+8+40, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr)
		ZSTD_initFseState(tls, bp+8+56, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr)
		ZSTD_initFseState(tls, bp+8+72, bp+8, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr)
		for {
			if !(nbSeq != 0) {
				break
			}
			sequence = ZSTD_decodeSequence(tls, bp+8, isLongOffset, libc.BoolInt32(nbSeq == int32(1)))
			oneSeqSize = ZSTD_execSequence(tls, op, oend, sequence, bp, litEnd, prefixStart, vBase, dictEnd)
			if libc.Int64FromUint32(ZSTD_isError(tls, oneSeqSize)) != 0 {
				return oneSeqSize
			}
			op = op + uintptr(oneSeqSize)
			goto _3
		_3:
			;
			nbSeq = nbSeq - 1
		}
		/* check if reached exact end */
		if !(BIT_endOfDStream(tls, bp+8) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		/* save reps for next block */
		i1 = uint32(0)
		for {
			if !(i1 < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i1)*4)) = uint32(*(*size_t)(unsafe.Pointer(bp + 8 + 88 + uintptr(i1)*8)))
			goto _4
		_4:
			;
			i1 = i1 + 1
		}
	}
	/* last literal segment */
	lastLLSize = libc.Uint64FromInt64(int64(litEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
	if lastLLSize > libc.Uint64FromInt64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if op != libc.UintptrFromInt32(0) {
		libc.Xmemcpy(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize)
		op = op + uintptr(lastLLSize)
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_decompressSequences_default(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesSplitLitBuffer_default(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_bodySplitLitBuffer(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_prefetchMatch(tls *libc.TLS, prefetchPos size_t, sequence seq_t, prefixStart uintptr, dictEnd uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var match, matchBase, v1 uintptr
	_, _, _ = match, matchBase, v1
	prefetchPos = prefetchPos + sequence.FlitLength
	if sequence.Foffset > prefetchPos {
		v1 = dictEnd
	} else {
		v1 = prefixStart
	}
	matchBase = v1
	/* note : this operation can overflow when seq.offset is really too large, which can only happen when input is corrupted.
	 * No consequence though : memory address is only used for prefetching, not for dereferencing */
	match = ZSTD_wrappedPtrSub(tls, ZSTD_wrappedPtrAdd(tls, matchBase, libc.Int64FromUint64(prefetchPos)), libc.Int64FromUint64(sequence.Foffset))
	libc.X__builtin_prefetch(tls, match, libc.VaList(bp+8, 0, int32(3)))
	libc.X__builtin_prefetch(tls, match+libc.UintptrFromInt32(CACHELINE_SIZE), libc.VaList(bp+8, 0, int32(3))) /* note : it's safe to invoke PREFETCH() on any memory address, including invalid ones */
	return prefetchPos + sequence.FmatchLength
}

// C documentation
//
//	/* This decoding function employs prefetching
//	 * to reduce latency impact of cache misses.
//	 * It's generally employed when block contains a significant portion of long-distance matches
//	 * or when coupled with a "cold" dictionary */
func ZSTD_decompressSequencesLong_body(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	bp := tls.Alloc(320)
	defer tls.Free(320)
	var dictEnd, dictStart, iend, ip, litBufferEnd, oend, op, ostart, prefixStart, sequence2, v1 uintptr
	var i, seqAdvance, seqNb, v2 int32
	var i1 U32
	var lastLLSize, lastLLSize1, leftoverLit, leftoverLit1, oneSeqSize, oneSeqSize1, oneSeqSize2, oneSeqSize3, prefetchPos size_t
	var sequence, sequence1 seq_t
	var v6 uint64
	var _ /* litPtr at bp+0 */ uintptr
	var _ /* seqState at bp+200 */ seqState_t
	var _ /* sequences at bp+8 */ [8]seq_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dictEnd, dictStart, i, i1, iend, ip, lastLLSize, lastLLSize1, leftoverLit, leftoverLit1, litBufferEnd, oend, oneSeqSize, oneSeqSize1, oneSeqSize2, oneSeqSize3, op, ostart, prefetchPos, prefixStart, seqAdvance, seqNb, sequence, sequence1, sequence2, v1, v2, v6
	ip = seqStart
	iend = ip + uintptr(seqSize)
	ostart = dst
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_in_dst) {
		v1 = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBuffer
	} else {
		v1 = ZSTD_maybeNullPtrAdd(tls, ostart, libc.Int64FromUint64(maxDstSize))
	}
	oend = v1
	op = ostart
	*(*uintptr)(unsafe.Pointer(bp)) = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitPtr
	litBufferEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd
	prefixStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart
	dictStart = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart
	dictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd
	/* Regen sequences */
	if nbSeq != 0 {
		if nbSeq < int32(STORED_SEQS) {
			v2 = nbSeq
		} else {
			v2 = int32(STORED_SEQS)
		}
		seqAdvance = v2
		prefetchPos = libc.Uint64FromInt64(int64(op) - int64(prefixStart)) /* track position relative to prefixStart */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FfseEntropy = uint32(1)
		i = 0
		for {
			if !(i < int32(ZSTD_REP_NUM)) {
				break
			}
			*(*size_t)(unsafe.Pointer(bp + 200 + 88 + uintptr(i)*8)) = uint64(*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i)*4)))
			goto _3
		_3:
			;
			i = i + 1
		}
		if ERR_isError(tls, BIT_initDStream(tls, bp+200, ip, libc.Uint64FromInt64(int64(iend)-int64(ip)))) != 0 {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		ZSTD_initFseState(tls, bp+200+40, bp+200, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FLLTptr)
		ZSTD_initFseState(tls, bp+200+56, bp+200, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr)
		ZSTD_initFseState(tls, bp+200+72, bp+200, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FMLTptr)
		/* prepare in advance */
		seqNb = 0
		for {
			if !(seqNb < seqAdvance) {
				break
			}
			sequence = ZSTD_decodeSequence(tls, bp+200, isLongOffset, libc.BoolInt32(seqNb == nbSeq-int32(1)))
			prefetchPos = ZSTD_prefetchMatch(tls, prefetchPos, sequence, prefixStart, dictEnd)
			(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[seqNb] = sequence
			goto _4
		_4:
			;
			seqNb = seqNb + 1
		}
		/* decompress without stomping litBuffer */
		for {
			if !(seqNb < nbSeq) {
				break
			}
			sequence1 = ZSTD_decodeSequence(tls, bp+200, isLongOffset, libc.BoolInt32(seqNb == nbSeq-int32(1)))
			if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) && *(*uintptr)(unsafe.Pointer(bp))+uintptr((*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))].FlitLength) > (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd {
				/* lit buffer is reaching split point, empty out the first buffer and transition to litExtraBuffer */
				leftoverLit = libc.Uint64FromInt64(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
				if leftoverLit != 0 {
					if leftoverLit > libc.Uint64FromInt64(int64(oend)-int64(op)) {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+8456, 0)
						}
						return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
					}
					ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(bp)), libc.Int64FromUint64(leftoverLit))
					(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))].FlitLength -= leftoverLit
					op = op + uintptr(leftoverLit)
				}
				*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
				litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
				oneSeqSize = ZSTD_execSequence(tls, op, oend, (*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))], bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				if ZSTD_isError(tls, oneSeqSize) != 0 {
					return oneSeqSize
				}
				prefetchPos = ZSTD_prefetchMatch(tls, prefetchPos, sequence1, prefixStart, dictEnd)
				(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[seqNb&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))] = sequence1
				op = op + uintptr(oneSeqSize)
			} else {
				if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
					v6 = ZSTD_execSequenceSplitLitBuffer(tls, op, oend, *(*uintptr)(unsafe.Pointer(bp))+uintptr((*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))].FlitLength)-uintptr(WILDCOPY_OVERLENGTH), (*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))], bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				} else {
					v6 = ZSTD_execSequence(tls, op, oend, (*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[(seqNb-int32(STORED_SEQS))&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))], bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				}
				/* lit buffer is either wholly contained in first or second split, or not split at all*/
				oneSeqSize1 = v6
				if ZSTD_isError(tls, oneSeqSize1) != 0 {
					return oneSeqSize1
				}
				prefetchPos = ZSTD_prefetchMatch(tls, prefetchPos, sequence1, prefixStart, dictEnd)
				(*(*[8]seq_t)(unsafe.Pointer(bp + 8)))[seqNb&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1))] = sequence1
				op = op + uintptr(oneSeqSize1)
			}
			goto _5
		_5:
			;
			seqNb = seqNb + 1
		}
		if !(BIT_endOfDStream(tls, bp+200) != 0) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_corruption_detected))
		}
		/* finish queue */
		seqNb = seqNb - seqAdvance
		for {
			if !(seqNb < nbSeq) {
				break
			}
			sequence2 = bp + 8 + uintptr(seqNb&(libc.Int32FromInt32(STORED_SEQS)-libc.Int32FromInt32(1)))*24
			if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) && *(*uintptr)(unsafe.Pointer(bp))+uintptr((*seq_t)(unsafe.Pointer(sequence2)).FlitLength) > (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd {
				leftoverLit1 = libc.Uint64FromInt64(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
				if leftoverLit1 != 0 {
					if leftoverLit1 > libc.Uint64FromInt64(int64(oend)-int64(op)) {
						if 0 != 0 {
							_force_has_format_string(tls, __ccgo_ts+8456, 0)
						}
						return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
					}
					ZSTD_safecopyDstBeforeSrc(tls, op, *(*uintptr)(unsafe.Pointer(bp)), libc.Int64FromUint64(leftoverLit1))
					*(*size_t)(unsafe.Pointer(sequence2)) -= leftoverLit1
					op = op + uintptr(leftoverLit1)
				}
				*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
				litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
				(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation = int32(ZSTD_not_in_dst)
				oneSeqSize2 = ZSTD_execSequence(tls, op, oend, *(*seq_t)(unsafe.Pointer(sequence2)), bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				if ZSTD_isError(tls, oneSeqSize2) != 0 {
					return oneSeqSize2
				}
				op = op + uintptr(oneSeqSize2)
			} else {
				if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
					v6 = ZSTD_execSequenceSplitLitBuffer(tls, op, oend, *(*uintptr)(unsafe.Pointer(bp))+uintptr((*seq_t)(unsafe.Pointer(sequence2)).FlitLength)-uintptr(WILDCOPY_OVERLENGTH), *(*seq_t)(unsafe.Pointer(sequence2)), bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				} else {
					v6 = ZSTD_execSequence(tls, op, oend, *(*seq_t)(unsafe.Pointer(sequence2)), bp, litBufferEnd, prefixStart, dictStart, dictEnd)
				}
				oneSeqSize3 = v6
				if ZSTD_isError(tls, oneSeqSize3) != 0 {
					return oneSeqSize3
				}
				op = op + uintptr(oneSeqSize3)
			}
			goto _7
		_7:
			;
			seqNb = seqNb + 1
		}
		/* save reps for next block */
		i1 = uint32(0)
		for {
			if !(i1 < uint32(ZSTD_REP_NUM)) {
				break
			}
			*(*U32)(unsafe.Pointer(dctx + 32 + 26652 + uintptr(i1)*4)) = uint32(*(*size_t)(unsafe.Pointer(bp + 200 + 88 + uintptr(i1)*8)))
			goto _9
		_9:
			;
			i1 = i1 + 1
		}
	}
	/* last literal segment */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) { /* first deplete literal buffer in dst, then copy litExtraBuffer */
		lastLLSize = libc.Uint64FromInt64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
		if lastLLSize > libc.Uint64FromInt64(int64(oend)-int64(op)) {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+1319, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		if op != libc.UintptrFromInt32(0) {
			libc.Xmemmove(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize)
			op = op + uintptr(lastLLSize)
		}
		*(*uintptr)(unsafe.Pointer(bp)) = dctx + 30372
		litBufferEnd = dctx + 30372 + uintptr(libc.Int32FromInt32(1)<<libc.Int32FromInt32(16))
	}
	lastLLSize1 = libc.Uint64FromInt64(int64(litBufferEnd) - int64(*(*uintptr)(unsafe.Pointer(bp))))
	if lastLLSize1 > libc.Uint64FromInt64(int64(oend)-int64(op)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if op != libc.UintptrFromInt32(0) {
		libc.Xmemmove(tls, op, *(*uintptr)(unsafe.Pointer(bp)), lastLLSize1)
		op = op + uintptr(lastLLSize1)
	}
	return libc.Uint64FromInt64(int64(op) - int64(ostart))
}

func ZSTD_decompressSequencesLong_default(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequencesLong_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequences_bmi2(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesSplitLitBuffer_bmi2(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequences_bodySplitLitBuffer(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesLong_bmi2(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	return ZSTD_decompressSequencesLong_body(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequences(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		return ZSTD_decompressSequences_bmi2(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
	}
	return ZSTD_decompressSequences_default(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

func ZSTD_decompressSequencesSplitLitBuffer(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		return ZSTD_decompressSequencesSplitLitBuffer_bmi2(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
	}
	return ZSTD_decompressSequencesSplitLitBuffer_default(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

// C documentation
//
//	/* ZSTD_decompressSequencesLong() :
//	 * decompression function triggered when a minimum share of offsets is considered "long",
//	 * aka out of cache.
//	 * note : "long" definition seems overloaded here, sometimes meaning "wider than bitstream register", and sometimes meaning "farther than memory cache distance".
//	 * This function will try to mitigate main memory latency through the use of prefetching */
func ZSTD_decompressSequencesLong(tls *libc.TLS, dctx uintptr, dst uintptr, maxDstSize size_t, seqStart uintptr, seqSize size_t, nbSeq int32, isLongOffset ZSTD_longOffset_e) (r size_t) {
	if ZSTD_DCtx_get_bmi2(tls, dctx) != 0 {
		return ZSTD_decompressSequencesLong_bmi2(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
	}
	return ZSTD_decompressSequencesLong_default(tls, dctx, dst, maxDstSize, seqStart, seqSize, nbSeq, isLongOffset)
}

// C documentation
//
//	/**
//	 * @returns The total size of the history referenceable by zstd, including
//	 * both the prefix and the extDict. At @p op any offset larger than this
//	 * is invalid.
//	 */
func ZSTD_totalHistorySize(tls *libc.TLS, op uintptr, virtualStart uintptr) (r size_t) {
	return libc.Uint64FromInt64(int64(op) - int64(virtualStart))
}

type ZSTD_OffsetInfo = struct {
	FlongOffsetShare     uint32
	FmaxNbAdditionalBits uint32
}

// C documentation
//
//	/* ZSTD_getOffsetInfo() :
//	 * condition : offTable must be valid
//	 * @return : "share" of long offsets (arbitrarily defined as > (1<<23))
//	 *           compared to maximum possible of (1<<OffFSELog),
//	 *           as well as the maximum number additional bits required.
//	 */
func ZSTD_getOffsetInfo(tls *libc.TLS, offTable uintptr, nbSeq int32) (r ZSTD_OffsetInfo) {
	var info ZSTD_OffsetInfo
	var max, tableLog, u U32
	var ptr, table uintptr
	var v2 uint32
	_, _, _, _, _, _, _ = info, max, ptr, table, tableLog, u, v2
	info = ZSTD_OffsetInfo{}
	/* If nbSeq == 0, then the offTable is uninitialized, but we have
	 * no sequences, so both values should be 0.
	 */
	if nbSeq != 0 {
		ptr = offTable
		tableLog = (*(*ZSTD_seqSymbol_header)(unsafe.Pointer(ptr))).FtableLog
		table = offTable + uintptr(1)*8
		max = libc.Uint32FromInt32(int32(1) << tableLog)
		/* max not too large */
		u = uint32(0)
		for {
			if !(u < max) {
				break
			}
			if info.FmaxNbAdditionalBits > uint32((*(*ZSTD_seqSymbol)(unsafe.Pointer(table + uintptr(u)*8))).FnbAdditionalBits) {
				v2 = info.FmaxNbAdditionalBits
			} else {
				v2 = uint32((*(*ZSTD_seqSymbol)(unsafe.Pointer(table + uintptr(u)*8))).FnbAdditionalBits)
			}
			info.FmaxNbAdditionalBits = v2
			if libc.Int32FromUint8((*(*ZSTD_seqSymbol)(unsafe.Pointer(table + uintptr(u)*8))).FnbAdditionalBits) > int32(22) {
				info.FlongOffsetShare += uint32(1)
			}
			goto _1
		_1:
			;
			u = u + 1
		}
		info.FlongOffsetShare <<= uint32(OffFSELog) - tableLog /* scale to OffFSELog */
	}
	return info
}

// C documentation
//
//	/**
//	 * @returns The maximum offset we can decode in one read of our bitstream, without
//	 * reloading more bits in the middle of the offset bits read. Any offsets larger
//	 * than this must use the long offset decoder.
//	 */
func ZSTD_maxShortOffset(tls *libc.TLS) (r size_t) {
	var maxOffbase, maxOffset size_t
	var v1 int32
	_, _, _ = maxOffbase, maxOffset, v1
	if MEM_64bits(tls) != 0 {
		/* We can decode any offset without reloading bits.
		 * This might change if the max window size grows.
		 */
		_ = libc.Uint64FromInt64(1)
		return libc.Uint64FromInt32(-libc.Int32FromInt32(1))
	} else {
		if MEM_32bits(tls) != 0 {
			v1 = int32(STREAM_ACCUMULATOR_MIN_32)
		} else {
			v1 = int32(STREAM_ACCUMULATOR_MIN_64)
		}
		/* The maximum offBase is (1 << (STREAM_ACCUMULATOR_MIN + 1)) - 1.
		 * This offBase would require STREAM_ACCUMULATOR_MIN extra bits.
		 * Then we have to subtract ZSTD_REP_NUM to get the maximum possible offset.
		 */
		maxOffbase = libc.Uint64FromInt32(1)<<(libc.Uint32FromInt32(v1)+libc.Uint32FromInt32(1)) - uint64(1)
		maxOffset = maxOffbase - uint64(ZSTD_REP_NUM)
		return maxOffset
	}
	return r
}

func ZSTD_decompressBlock_internal(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t, streaming streaming_operation) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var blockSizeMax, litCSize, seqHSize, totalHistorySize size_t
	var info ZSTD_OffsetInfo
	var ip uintptr
	var isLongOffset ZSTD_longOffset_e
	var minShare U32
	var usePrefetchDecoder, v2 int32
	var v1 uint64
	var v3 bool
	var _ /* nbSeq at bp+0 */ int32
	_, _, _, _, _, _, _, _, _, _, _, _ = blockSizeMax, info, ip, isLongOffset, litCSize, minShare, seqHSize, totalHistorySize, usePrefetchDecoder, v1, v2, v3 /* blockType == blockCompressed */
	ip = src
	/* Note : the wording of the specification
	 * allows compressed block to be sized exactly ZSTD_blockSizeMax(dctx).
	 * This generally does not happen, as it makes little sense,
	 * since an uncompressed block would feature same size and have no decompression cost.
	 * Also, note that decoder from reference libzstd before < v1.5.4
	 * would consider this edge case as an error.
	 * As a consequence, avoid generating compressed blocks of size ZSTD_blockSizeMax(dctx)
	 * for broader compatibility with the deployed ecosystem of zstd decoders */
	if srcSize > ZSTD_blockSizeMax(tls, dctx) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Decode literals section */
	litCSize = ZSTD_decodeLiteralsBlock(tls, dctx, src, srcSize, dst, dstCapacity, streaming)
	if ZSTD_isError(tls, litCSize) != 0 {
		return litCSize
	}
	ip = ip + uintptr(litCSize)
	srcSize = srcSize - litCSize
	/* Build Decoding Tables */
	if dstCapacity < ZSTD_blockSizeMax(tls, dctx) {
		v1 = dstCapacity
	} else {
		v1 = ZSTD_blockSizeMax(tls, dctx)
	}
	/* Compute the maximum block size, which must also work when !frame and fParams are unset.
	 * Additionally, take the min with dstCapacity to ensure that the totalHistorySize fits in a size_t.
	 */
	blockSizeMax = v1
	totalHistorySize = ZSTD_totalHistorySize(tls, ZSTD_maybeNullPtrAdd(tls, dst, libc.Int64FromUint64(blockSizeMax)), (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart)
	/* isLongOffset must be true if there are long offsets.
	 * Offsets are long if they are larger than ZSTD_maxShortOffset().
	 * We don't expect that to be the case in 64-bit mode.
	 *
	 * We check here to see if our history is large enough to allow long offsets.
	 * If it isn't, then we can't possible have (valid) long offsets. If the offset
	 * is invalid, then it is okay to read it incorrectly.
	 *
	 * If isLongOffsets is true, then we will later check our decoding table to see
	 * if it is even possible to generate long offsets.
	 */
	isLongOffset = libc.BoolInt32(MEM_32bits(tls) != 0 && totalHistorySize > ZSTD_maxShortOffset(tls))
	/* These macros control at build-time which decompressor implementation
	 * we use. If neither is defined, we do some inspection and dispatch at
	 * runtime.
	 */
	usePrefetchDecoder = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold
	seqHSize = ZSTD_decodeSeqHeaders(tls, dctx, bp, ip, srcSize)
	if ZSTD_isError(tls, seqHSize) != 0 {
		return seqHSize
	}
	ip = ip + uintptr(seqHSize)
	srcSize = srcSize - seqHSize
	if (dst == libc.UintptrFromInt32(0) || dstCapacity == uint64(0)) && *(*int32)(unsafe.Pointer(bp)) > 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+7990, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if MEM_64bits(tls) != 0 && libc.Bool(uint64(8) == uint64(8)) && libc.Uint64FromInt32(-libc.Int32FromInt32(1))-uint64(dst) < libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(20)) {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+8496, 0)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* If we could potentially have long offsets, or we might want to use the prefetch decoder,
	 * compute information about the share of long offsets, and the maximum nbAdditionalBits.
	 * NOTE: could probably use a larger nbSeq limit
	 */
	if isLongOffset != 0 || !(usePrefetchDecoder != 0) && totalHistorySize > uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(24)) && *(*int32)(unsafe.Pointer(bp)) > int32(8) {
		info = ZSTD_getOffsetInfo(tls, (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FOFTptr, *(*int32)(unsafe.Pointer(bp)))
		if v3 = isLongOffset != 0; v3 {
			if MEM_32bits(tls) != 0 {
				v2 = int32(STREAM_ACCUMULATOR_MIN_32)
			} else {
				v2 = int32(STREAM_ACCUMULATOR_MIN_64)
			}
		}
		if v3 && info.FmaxNbAdditionalBits <= libc.Uint32FromInt32(v2) {
			/* If isLongOffset, but the maximum number of additional bits that we see in our table is small
			 * enough, then we know it is impossible to have too long an offset in this block, so we can
			 * use the regular offset decoder.
			 */
			isLongOffset = int32(ZSTD_lo_isRegularOffset)
		}
		if !(usePrefetchDecoder != 0) {
			if MEM_64bits(tls) != 0 {
				v2 = int32(7)
			} else {
				v2 = int32(20)
			}
			minShare = libc.Uint32FromInt32(v2) /* heuristic values, correspond to 2.73% and 7.81% */
			usePrefetchDecoder = libc.BoolInt32(info.FlongOffsetShare >= minShare)
		}
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FddictIsCold = 0
	if usePrefetchDecoder != 0 {
		return ZSTD_decompressSequencesLong(tls, dctx, dst, dstCapacity, ip, srcSize, *(*int32)(unsafe.Pointer(bp)), isLongOffset)
	}
	/* else */
	if (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FlitBufferLocation == int32(ZSTD_split) {
		return ZSTD_decompressSequencesSplitLitBuffer(tls, dctx, dst, dstCapacity, ip, srcSize, *(*int32)(unsafe.Pointer(bp)), isLongOffset)
	} else {
		return ZSTD_decompressSequences(tls, dctx, dst, dstCapacity, ip, srcSize, *(*int32)(unsafe.Pointer(bp)), isLongOffset)
	}
	return r
}

func ZSTD_checkContinuity(tls *libc.TLS, dctx uintptr, dst uintptr, dstSize size_t) {
	if dst != (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd && dstSize > uint64(0) { /* not contiguous */
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FdictEnd = (*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FvirtualStart = dst - uintptr(int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd)-int64((*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart))
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FprefixStart = dst
		(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dst
	}
}

func ZSTD_decompressBlock_deprecated(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	var dSize, err_code size_t
	_, _ = dSize, err_code
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FisFrameDecompression = 0
	ZSTD_checkContinuity(tls, dctx, dst, dstCapacity)
	dSize = ZSTD_decompressBlock_internal(tls, dctx, dst, dstCapacity, src, srcSize, int32(not_streaming))
	err_code = dSize
	if ERR_isError(tls, err_code) != 0 {
		if 0 != 0 {
			_force_has_format_string(tls, __ccgo_ts+1319, 0)
		}
		return err_code
	}
	(*ZSTD_DCtx)(unsafe.Pointer(dctx)).FpreviousDstEnd = dst + uintptr(dSize)
	return dSize
}

// C documentation
//
//	/* NOTE: Must just wrap ZSTD_decompressBlock_deprecated() */
func ZSTD_decompressBlock(tls *libc.TLS, dctx uintptr, dst uintptr, dstCapacity size_t, src uintptr, srcSize size_t) (r size_t) {
	return ZSTD_decompressBlock_deprecated(tls, dctx, dst, dstCapacity, src, srcSize)
}

type ssize_t = int64

type off_t = int64

type va_list = uintptr

type __isoc_va_list = uintptr

type fpos_t = struct {
	F__lldata [0]int64
	F__align  [0]float64
	F__opaque [16]uint8
}

type _G_fpos64_t = fpos_t

type cookie_io_functions_t = struct {
	Fread   uintptr
	Fwrite  uintptr
	Fseek   uintptr
	Fclose1 uintptr
}

type _IO_cookie_io_functions_t = cookie_io_functions_t

type ZDICT_params_t = struct {
	FcompressionLevel  int32
	FnotificationLevel uint32
	FdictID            uint32
}

/* This can be overridden externally to hide static symbols. */

/* ====================================================================================
 * The definitions in this section are considered experimental.
 * They should never be used with a dynamic library, as they may change in the future.
 * They are provided for advanced usages.
 * Use them only in association with static linking.
 * ==================================================================================== */

/* Deprecated: Remove in v1.6.0 */

// C documentation
//
//	/*! ZDICT_cover_params_t:
//	 *  k and d are the only required parameters.
//	 *  For others, value 0 means default.
//	 */
type ZDICT_cover_params_t = struct {
	Fk                       uint32
	Fd                       uint32
	Fsteps                   uint32
	FnbThreads               uint32
	FsplitPoint              float64
	FshrinkDict              uint32
	FshrinkDictMaxRegression uint32
	FzParams                 ZDICT_params_t
}

type ZDICT_fastCover_params_t = struct {
	Fk                       uint32
	Fd                       uint32
	Ff                       uint32
	Fsteps                   uint32
	FnbThreads               uint32
	FsplitPoint              float64
	Faccel                   uint32
	FshrinkDict              uint32
	FshrinkDictMaxRegression uint32
	FzParams                 ZDICT_params_t
}

type ZDICT_legacy_params_t = struct {
	FselectivityLevel uint32
	FzParams          ZDICT_params_t
}

/**** ended inlining ../zdict.h ****/
/**** start inlining cover.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/threading.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../zdict.h ****/

// C documentation
//
//	/**
//	 * COVER_best_t is used for two purposes:
//	 * 1. Synchronizing threads.
//	 * 2. Saving the best parameters and dictionary.
//	 *
//	 * All of the methods except COVER_best_init() are thread safe if zstd is
//	 * compiled with multithreaded support.
//	 */
type COVER_best_t = struct {
	Fmutex          pthread_mutex_t
	Fcond           pthread_cond_t
	FliveJobs       size_t
	Fdict           uintptr
	FdictSize       size_t
	Fparameters     ZDICT_cover_params_t
	FcompressedSize size_t
}

/**** ended inlining ../zdict.h ****/
/**** start inlining cover.h ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/**** skipping file: ../common/threading.h ****/
/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../zdict.h ****/

// C documentation
//
//	/**
//	 * COVER_best_t is used for two purposes:
//	 * 1. Synchronizing threads.
//	 * 2. Saving the best parameters and dictionary.
//	 *
//	 * All of the methods except COVER_best_init() are thread safe if zstd is
//	 * compiled with multithreaded support.
//	 */
type COVER_best_s = COVER_best_t

// C documentation
//
//	/**
//	 * A segment is a range in the source as well as the score of the segment.
//	 */
type COVER_segment_t = struct {
	Fbegin U32
	Fend   U32
	Fscore U32
}

// C documentation
//
//	/**
//	 *Number of epochs and size of each epoch.
//	 */
type COVER_epoch_info_t = struct {
	Fnum  U32
	Fsize U32
}

// C documentation
//
//	/**
//	 * Struct used for the dictionary selection function.
//	 */
type COVER_dictSelection_t = struct {
	FdictContent         uintptr
	FdictSize            size_t
	FtotalCompressedSize size_t
}

// C documentation
//
//	/**
//	 * Struct used for the dictionary selection function.
//	 */
type COVER_dictSelection = COVER_dictSelection_t

/**** ended inlining cover.h ****/

/*-*************************************
*  Constants
***************************************/
/**
* There are 32bit indexes used to ref samples, so limit samples size to 4GB
* on 64bit builds.
* For 32bit builds we choose 1 GB.
* Most 32bit platforms have 2GB user-mode addressable space and we allocate a large
* contiguous buffer, so 1GB is already a high limit.
 */

// C documentation
//
//	/*-*************************************
//	*  Console display
//	***************************************/
var g_displayLevel = int32(0)

var g_refreshRate = libc.Int64FromInt64(1000000) * libc.Int64FromInt32(15) / libc.Int64FromInt32(100)
var g_time = int64(0)

/*-*************************************
* Hash table
***************************************
* A small specialized hash map for storing activeDmers.
* The map does not resize, so if it becomes full it will loop forever.
* Thus, the map must be large enough to store every value.
* The map implements linear probing and keeps its load less than 0.5.
 */

type COVER_map_pair_t = struct {
	Fkey   U32
	Fvalue U32
}

/*-*************************************
* Hash table
***************************************
* A small specialized hash map for storing activeDmers.
* The map does not resize, so if it becomes full it will loop forever.
* Thus, the map must be large enough to store every value.
* The map implements linear probing and keeps its load less than 0.5.
 */

type COVER_map_pair_t_s = COVER_map_pair_t

type COVER_map_t = struct {
	Fdata     uintptr
	FsizeLog  U32
	Fsize     U32
	FsizeMask U32
}

type COVER_map_s = COVER_map_t

// C documentation
//
//	/**
//	 * Clear the map.
//	 */
func COVER_map_clear(tls *libc.TLS, map1 uintptr) {
	libc.Xmemset(tls, (*COVER_map_t)(unsafe.Pointer(map1)).Fdata, libc.Int32FromUint32(libc.Uint32FromInt32(-libc.Int32FromInt32(1))), uint64((*COVER_map_t)(unsafe.Pointer(map1)).Fsize)*uint64(8))
}

// C documentation
//
//	/**
//	 * Initializes a map of the given size.
//	 * Returns 1 on success and 0 on failure.
//	 * The map must be destroyed with COVER_map_destroy().
//	 * The map is only guaranteed to be large enough to hold size elements.
//	 */
func COVER_map_init(tls *libc.TLS, map1 uintptr, size U32) (r int32) {
	(*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog = ZSTD_highbit32(tls, size) + uint32(2)
	(*COVER_map_t)(unsafe.Pointer(map1)).Fsize = libc.Uint32FromInt32(1) << (*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog
	(*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask = (*COVER_map_t)(unsafe.Pointer(map1)).Fsize - uint32(1)
	(*COVER_map_t)(unsafe.Pointer(map1)).Fdata = libc.Xmalloc(tls, uint64((*COVER_map_t)(unsafe.Pointer(map1)).Fsize)*uint64(8))
	if !((*COVER_map_t)(unsafe.Pointer(map1)).Fdata != 0) {
		(*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog = uint32(0)
		(*COVER_map_t)(unsafe.Pointer(map1)).Fsize = uint32(0)
		return 0
	}
	COVER_map_clear(tls, map1)
	return int32(1)
}

// C documentation
//
//	/**
//	 * Internal hash function
//	 */
var COVER_prime4bytes = uint32(2654435761)

func COVER_map_hash(tls *libc.TLS, map1 uintptr, key U32) (r U32) {
	return key * COVER_prime4bytes >> (uint32(32) - (*COVER_map_t)(unsafe.Pointer(map1)).FsizeLog)
}

// C documentation
//
//	/**
//	 * Helper function that returns the index that a key should be placed into.
//	 */
func COVER_map_index(tls *libc.TLS, map1 uintptr, key U32) (r U32) {
	var hash, i U32
	var pos uintptr
	_, _, _ = hash, i, pos
	hash = COVER_map_hash(tls, map1, key)
	i = hash
	for {
		pos = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(i)*8
		if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue == libc.Uint32FromInt32(-libc.Int32FromInt32(1)) {
			return i
		}
		if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey == key {
			return i
		}
		goto _1
	_1:
		;
		i = (i + uint32(1)) & (*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask
	}
	return r
}

// C documentation
//
//	/**
//	 * Returns the pointer to the value for key.
//	 * If key is not in the map, it is inserted and the value is set to 0.
//	 * The map must not be full.
//	 */
func COVER_map_at(tls *libc.TLS, map1 uintptr, key U32) (r uintptr) {
	var pos uintptr
	_ = pos
	pos = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(COVER_map_index(tls, map1, key))*8
	if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue == libc.Uint32FromInt32(-libc.Int32FromInt32(1)) {
		(*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey = key
		(*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue = uint32(0)
	}
	return pos + 4
}

// C documentation
//
//	/**
//	 * Deletes key from the map if present.
//	 */
func COVER_map_remove(tls *libc.TLS, map1 uintptr, key U32) {
	var del, pos uintptr
	var i, shift U32
	_, _, _, _ = del, i, pos, shift
	i = COVER_map_index(tls, map1, key)
	del = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(i)*8
	shift = uint32(1)
	if (*COVER_map_pair_t)(unsafe.Pointer(del)).Fvalue == libc.Uint32FromInt32(-libc.Int32FromInt32(1)) {
		return
	}
	i = (i + uint32(1)) & (*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask
	for {
		pos = (*COVER_map_t)(unsafe.Pointer(map1)).Fdata + uintptr(i)*8
		/* If the position is empty we are done */
		if (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue == libc.Uint32FromInt32(-libc.Int32FromInt32(1)) {
			(*COVER_map_pair_t)(unsafe.Pointer(del)).Fvalue = libc.Uint32FromInt32(-libc.Int32FromInt32(1))
			return
		}
		/* If pos can be moved to del do so */
		if (i-COVER_map_hash(tls, map1, (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey))&(*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask >= shift {
			(*COVER_map_pair_t)(unsafe.Pointer(del)).Fkey = (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fkey
			(*COVER_map_pair_t)(unsafe.Pointer(del)).Fvalue = (*COVER_map_pair_t)(unsafe.Pointer(pos)).Fvalue
			del = pos
			shift = uint32(1)
		} else {
			shift = shift + 1
		}
		goto _1
	_1:
		;
		i = (i + uint32(1)) & (*COVER_map_t)(unsafe.Pointer(map1)).FsizeMask
	}
}

// C documentation
//
//	/**
//	 * Destroys a map that is inited with COVER_map_init().
//	 */
func COVER_map_destroy(tls *libc.TLS, map1 uintptr) {
	if (*COVER_map_t)(unsafe.Pointer(map1)).Fdata != 0 {
		libc.Xfree(tls, (*COVER_map_t)(unsafe.Pointer(map1)).Fdata)
	}
	(*COVER_map_t)(unsafe.Pointer(map1)).Fdata = libc.UintptrFromInt32(0)
	(*COVER_map_t)(unsafe.Pointer(map1)).Fsize = uint32(0)
}

/*-*************************************
* Context
***************************************/

type COVER_ctx_t = struct {
	Fsamples        uintptr
	Foffsets        uintptr
	FsamplesSizes   uintptr
	FnbSamples      size_t
	FnbTrainSamples size_t
	FnbTestSamples  size_t
	Fsuffix         uintptr
	FsuffixSize     size_t
	Ffreqs          uintptr
	FdmerAt         uintptr
	Fd              uint32
}

/*-*************************************
*  Helper functions
***************************************/

// C documentation
//
//	/**
//	 * Returns the sum of the sample sizes.
//	 */
func COVER_sum(tls *libc.TLS, samplesSizes uintptr, nbSamples uint32) (r size_t) {
	var i uint32
	var sum size_t
	_, _ = i, sum
	sum = uint64(0)
	i = uint32(0)
	for {
		if !(i < nbSamples) {
			break
		}
		sum = sum + *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8))
		goto _1
	_1:
		;
		i = i + 1
	}
	return sum
}

// C documentation
//
//	/**
//	 * Returns -1 if the dmer at lp is less than the dmer at rp.
//	 * Return 0 if the dmers at lp and rp are equal.
//	 * Returns 1 if the dmer at lp is greater than the dmer at rp.
//	 */
func COVER_cmp(tls *libc.TLS, ctx uintptr, lp uintptr, rp uintptr) (r int32) {
	var lhs, rhs U32
	_, _ = lhs, rhs
	lhs = *(*U32)(unsafe.Pointer(lp))
	rhs = *(*U32)(unsafe.Pointer(rp))
	return libc.Xmemcmp(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(lhs), (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(rhs), uint64((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd))
}

// C documentation
//
//	/**
//	 * Faster version for d <= 8.
//	 */
func COVER_cmp8(tls *libc.TLS, ctx uintptr, lp uintptr, rp uintptr) (r int32) {
	var lhs, mask, rhs U64
	var v1 uint64
	_, _, _, _ = lhs, mask, rhs, v1
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd == uint32(8) {
		v1 = libc.Uint64FromInt32(-libc.Int32FromInt32(1))
	} else {
		v1 = libc.Uint64FromInt32(1)<<(libc.Uint32FromInt32(8)*(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd) - uint64(1)
	}
	mask = v1
	lhs = MEM_readLE64(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(*(*U32)(unsafe.Pointer(lp)))) & mask
	rhs = MEM_readLE64(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(*(*U32)(unsafe.Pointer(rp)))) & mask
	if lhs < rhs {
		return -int32(1)
	}
	return libc.BoolInt32(lhs > rhs)
}

// C documentation
//
//	/**
//	 * Same as COVER_cmp() except ties are broken by pointer value
//	 */
func COVER_strict_cmp(tls *libc.TLS, lp uintptr, rp uintptr, g_coverCtx uintptr) (r int32) {
	var result, v1 int32
	_, _ = result, v1
	result = COVER_cmp(tls, g_coverCtx, lp, rp)
	if result == 0 {
		if lp < rp {
			v1 = -int32(1)
		} else {
			v1 = int32(1)
		}
		result = v1
	}
	return result
}

// C documentation
//
//	/**
//	 * Faster version for d <= 8.
//	 */
func COVER_strict_cmp8(tls *libc.TLS, lp uintptr, rp uintptr, g_coverCtx uintptr) (r int32) {
	var result, v1 int32
	_, _ = result, v1
	result = COVER_cmp8(tls, g_coverCtx, lp, rp)
	if result == 0 {
		if lp < rp {
			v1 = -int32(1)
		} else {
			v1 = int32(1)
		}
		result = v1
	}
	return result
}

// C documentation
//
//	/**
//	 * Abstract away divergence of qsort_r() parameters.
//	 * Hopefully when C11 become the norm, we will be able
//	 * to clean it up.
//	 */
func stableSort(tls *libc.TLS, ctx uintptr) {
	var v1 uintptr
	_ = v1
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd <= uint32(8) {
		v1 = __ccgo_fp(COVER_strict_cmp8)
	} else {
		v1 = __ccgo_fp(COVER_strict_cmp)
	}
	libc.Xqsort_r(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize, uint64(4), v1, ctx)
}

// C documentation
//
//	/**
//	 * Returns the first pointer in [first, last) whose element does not compare
//	 * less than value.  If no such element exists it returns last.
//	 */
func COVER_lower_bound(tls *libc.TLS, first uintptr, last uintptr, value size_t) (r uintptr) {
	var count, step size_t
	var ptr, v1 uintptr
	_, _, _, _ = count, ptr, step, v1
	count = libc.Uint64FromInt64((int64(last) - int64(first)) / 8)
	for count != uint64(0) {
		step = count / uint64(2)
		ptr = first
		ptr = ptr + uintptr(step)*8
		if *(*size_t)(unsafe.Pointer(ptr)) < value {
			ptr += 8
			v1 = ptr
			first = v1
			count = count - (step + uint64(1))
		} else {
			count = step
		}
	}
	return first
}

// C documentation
//
//	/**
//	 * Generic groupBy function.
//	 * Groups an array sorted by cmp into groups with equivalent values.
//	 * Calls grp for each group.
//	 */
func COVER_groupBy(tls *libc.TLS, data uintptr, count size_t, size size_t, ctx uintptr, __ccgo_fp_cmp uintptr, __ccgo_fp_grp uintptr) {
	var grpEnd, ptr uintptr
	var num size_t
	_, _, _ = grpEnd, num, ptr
	ptr = data
	num = uint64(0)
	for num < count {
		grpEnd = ptr + uintptr(size)
		num = num + 1
		for num < count && (*(*func(*libc.TLS, uintptr, uintptr, uintptr) int32)(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_cmp})))(tls, ctx, ptr, grpEnd) == 0 {
			grpEnd = grpEnd + uintptr(size)
			num = num + 1
		}
		(*(*func(*libc.TLS, uintptr, uintptr, uintptr))(unsafe.Pointer(&struct{ uintptr }{__ccgo_fp_grp})))(tls, ctx, ptr, grpEnd)
		ptr = grpEnd
	}
}

/*-*************************************
*  Cover functions
***************************************/

// C documentation
//
//	/**
//	 * Called on each group of positions with the same dmer.
//	 * Counts the frequency of each dmer and saves it in the suffix array.
//	 * Fills `ctx->dmerAt`.
//	 */
func COVER_group(tls *libc.TLS, ctx uintptr, group uintptr, groupEnd uintptr) {
	var curOffsetPtr, grpEnd, grpPtr, offsetsEnd, sampleEndPtr uintptr
	var curSampleEnd size_t
	var dmerId, freq U32
	_, _, _, _, _, _, _, _ = curOffsetPtr, curSampleEnd, dmerId, freq, grpEnd, grpPtr, offsetsEnd, sampleEndPtr
	/* The group consists of all the positions with the same first d bytes. */
	grpPtr = group
	grpEnd = groupEnd
	/* The dmerId is how we will reference this dmer.
	 * This allows us to map the whole dmer space to a much smaller space, the
	 * size of the suffix array.
	 */
	dmerId = libc.Uint32FromInt64((int64(grpPtr) - int64((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix)) / 4)
	/* Count the number of samples this dmer shows up in */
	freq = uint32(0)
	/* Details */
	curOffsetPtr = (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets
	offsetsEnd = (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr((*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples)*8
	/* Once *grpPtr >= curSampleEnd this occurrence of the dmer is in a
	 * different sample than the last.
	 */
	curSampleEnd = *(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets))
	for {
		if !(grpPtr != grpEnd) {
			break
		}
		/* Save the dmerId for this position so we can get back to it. */
		*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(*(*U32)(unsafe.Pointer(grpPtr)))*4)) = dmerId
		/* Dictionaries only help for the first reference to the dmer.
		 * After that zstd can reference the match from the previous reference.
		 * So only count each dmer once for each sample it is in.
		 */
		if uint64(*(*U32)(unsafe.Pointer(grpPtr))) < curSampleEnd {
			goto _1
		}
		freq = freq + uint32(1)
		/* Binary search to find the end of the sample *grpPtr is in.
		 * In the common case that grpPtr + 1 == grpEnd we can skip the binary
		 * search because the loop is over.
		 */
		if grpPtr+uintptr(1)*4 != grpEnd {
			sampleEndPtr = COVER_lower_bound(tls, curOffsetPtr, offsetsEnd, uint64(*(*U32)(unsafe.Pointer(grpPtr))))
			curSampleEnd = *(*size_t)(unsafe.Pointer(sampleEndPtr))
			curOffsetPtr = sampleEndPtr + uintptr(1)*8
		}
		goto _1
	_1:
		;
		grpPtr += 4
	}
	/* At this point we are never going to look at this segment of the suffix
	 * array again.  We take advantage of this fact to save memory.
	 * We store the frequency of the dmer in the first position of the group,
	 * which is dmerId.
	 */
	*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix + uintptr(dmerId)*4)) = freq
}

// C documentation
//
//	/**
//	 * Selects the best segment in an epoch.
//	 * Segments of are scored according to the function:
//	 *
//	 * Let F(d) be the frequency of dmer d.
//	 * Let S_i be the dmer at position i of segment S which has length k.
//	 *
//	 *     Score(S) = F(S_1) + F(S_2) + ... + F(S_{k-d+1})
//	 *
//	 * Once the dmer d is in the dictionary we set F(d) = 0.
//	 */
func COVER_selectSegment(tls *libc.TLS, ctx uintptr, freqs uintptr, activeDmers uintptr, begin U32, end U32, parameters ZDICT_cover_params_t) (r COVER_segment_t) {
	var activeSegment, bestSegment COVER_segment_t
	var d, delDmer, dmersInK, freq, k, newBegin, newDmer, newEnd, pos, pos1 U32
	var delDmerOcc, newDmerOcc uintptr
	var v2 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = activeSegment, bestSegment, d, delDmer, delDmerOcc, dmersInK, freq, k, newBegin, newDmer, newDmerOcc, newEnd, pos, pos1, v2
	/* Constants */
	k = parameters.Fk
	d = parameters.Fd
	dmersInK = k - d + uint32(1)
	/* Try each segment (activeSegment) and save the best (bestSegment) */
	bestSegment = COVER_segment_t{}
	/* Reset the activeDmers in the segment */
	COVER_map_clear(tls, activeDmers)
	/* The activeSegment starts at the beginning of the epoch. */
	activeSegment.Fbegin = begin
	activeSegment.Fend = begin
	activeSegment.Fscore = uint32(0)
	/* Slide the activeSegment through the whole epoch.
	 * Save the best segment in bestSegment.
	 */
	for activeSegment.Fend < end {
		/* The dmerId for the dmer at the next position */
		newDmer = *(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(activeSegment.Fend)*4))
		/* The entry in activeDmers for this dmerId */
		newDmerOcc = COVER_map_at(tls, activeDmers, newDmer)
		/* If the dmer isn't already present in the segment add its score. */
		if *(*U32)(unsafe.Pointer(newDmerOcc)) == uint32(0) {
			/* The paper suggest using the L-0.5 norm, but experiments show that it
			 * doesn't help.
			 */
			activeSegment.Fscore += *(*U32)(unsafe.Pointer(freqs + uintptr(newDmer)*4))
		}
		/* Add the dmer to the segment */
		activeSegment.Fend += uint32(1)
		*(*U32)(unsafe.Pointer(newDmerOcc)) += uint32(1)
		/* If the window is now too large, drop the first position */
		if activeSegment.Fend-activeSegment.Fbegin == dmersInK+uint32(1) {
			delDmer = *(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(activeSegment.Fbegin)*4))
			delDmerOcc = COVER_map_at(tls, activeDmers, delDmer)
			activeSegment.Fbegin += uint32(1)
			*(*U32)(unsafe.Pointer(delDmerOcc)) -= uint32(1)
			/* If this is the last occurrence of the dmer, subtract its score */
			if *(*U32)(unsafe.Pointer(delDmerOcc)) == uint32(0) {
				COVER_map_remove(tls, activeDmers, delDmer)
				activeSegment.Fscore -= *(*U32)(unsafe.Pointer(freqs + uintptr(delDmer)*4))
			}
		}
		/* If this segment is the best so far save it */
		if activeSegment.Fscore > bestSegment.Fscore {
			bestSegment = activeSegment
		}
	}
	/* Trim off the zero frequency head and tail from the segment. */
	newBegin = bestSegment.Fend
	newEnd = bestSegment.Fbegin
	pos = bestSegment.Fbegin
	for {
		if !(pos != bestSegment.Fend) {
			break
		}
		freq = *(*U32)(unsafe.Pointer(freqs + uintptr(*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(pos)*4)))*4))
		if freq != uint32(0) {
			if newBegin < pos {
				v2 = newBegin
			} else {
				v2 = pos
			}
			newBegin = v2
			newEnd = pos + uint32(1)
		}
		goto _1
	_1:
		;
		pos = pos + 1
	}
	bestSegment.Fbegin = newBegin
	bestSegment.Fend = newEnd
	pos1 = bestSegment.Fbegin
	for {
		if !(pos1 != bestSegment.Fend) {
			break
		}
		*(*U32)(unsafe.Pointer(freqs + uintptr(*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt + uintptr(pos1)*4)))*4)) = uint32(0)
		goto _3
	_3:
		;
		pos1 = pos1 + 1
	}
	return bestSegment
}

// C documentation
//
//	/**
//	 * Check the validity of the parameters.
//	 * Returns non-zero if the parameters are valid and 0 otherwise.
//	 */
func COVER_checkParameters(tls *libc.TLS, parameters ZDICT_cover_params_t, maxDictSize size_t) (r int32) {
	/* k and d are required parameters */
	if parameters.Fd == uint32(0) || parameters.Fk == uint32(0) {
		return 0
	}
	/* k <= maxDictSize */
	if uint64(parameters.Fk) > maxDictSize {
		return 0
	}
	/* d <= k */
	if parameters.Fd > parameters.Fk {
		return 0
	}
	/* 0 < splitPoint <= 1 */
	if parameters.FsplitPoint <= libc.Float64FromInt32(0) || parameters.FsplitPoint > libc.Float64FromInt32(1) {
		return 0
	}
	return int32(1)
}

// C documentation
//
//	/**
//	 * Clean up a context initialized with `COVER_ctx_init()`.
//	 */
func COVER_ctx_destroy(tls *libc.TLS, ctx uintptr) {
	if !(ctx != 0) {
		return
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix = libc.UintptrFromInt32(0)
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.UintptrFromInt32(0)
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt = libc.UintptrFromInt32(0)
	}
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets != 0 {
		libc.Xfree(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)
		(*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.UintptrFromInt32(0)
	}
}

// C documentation
//
//	/**
//	 * Prepare a context for dictionary building.
//	 * The context is only dependent on the parameter `d` and can be used multiple
//	 * times.
//	 * Returns 0 on success or error code on error.
//	 * The context must be destroyed with `COVER_ctx_destroy()`.
//	 */
func COVER_ctx_init(tls *libc.TLS, ctx uintptr, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, d uint32, splitPoint float64) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var i, i1 U32
	var nbTestSamples, nbTrainSamples, v1, v2 uint32
	var samples, v9 uintptr
	var testSamplesSize, totalSamplesSize, trainingSamplesSize size_t
	var v3, v4, v5 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = i, i1, nbTestSamples, nbTrainSamples, samples, testSamplesSize, totalSamplesSize, trainingSamplesSize, v1, v2, v3, v4, v5, v9
	samples = samplesBuffer
	totalSamplesSize = COVER_sum(tls, samplesSizes, nbSamples)
	if splitPoint < float64(1) {
		v1 = uint32(float64(float64(nbSamples) * splitPoint))
	} else {
		v1 = nbSamples
	}
	/* Split samples into testing and training sets */
	nbTrainSamples = v1
	if splitPoint < float64(1) {
		v2 = nbSamples - nbTrainSamples
	} else {
		v2 = nbSamples
	}
	nbTestSamples = v2
	if splitPoint < float64(1) {
		v3 = COVER_sum(tls, samplesSizes, nbTrainSamples)
	} else {
		v3 = totalSamplesSize
	}
	trainingSamplesSize = v3
	if splitPoint < float64(1) {
		v4 = COVER_sum(tls, samplesSizes+uintptr(nbTrainSamples)*8, nbTestSamples)
	} else {
		v4 = totalSamplesSize
	}
	testSamplesSize = v4
	/* Checks */
	if uint64(d) > libc.Uint64FromInt64(8) {
		v5 = uint64(d)
	} else {
		v5 = libc.Uint64FromInt64(8)
	}
	if totalSamplesSize < v5 || totalSamplesSize >= uint64(libc.Uint32FromInt32(-libc.Int32FromInt32(1))) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8508, libc.VaList(bp+8, uint32(totalSamplesSize>>libc.Int32FromInt32(20)), libc.Uint32FromInt32(-libc.Int32FromInt32(1))>>libc.Int32FromInt32(20)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there are at least 5 training samples */
	if nbTrainSamples < uint32(5) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8572, libc.VaList(bp+8, nbTrainSamples))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there's testing sample */
	if nbTestSamples < uint32(1) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8627, libc.VaList(bp+8, nbTestSamples))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Zero the context */
	libc.Xmemset(tls, ctx, 0, uint64(88))
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8681, libc.VaList(bp+8, nbTrainSamples, uint32(trainingSamplesSize)))
		libc.Xfflush(tls, libc.Xstderr)
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8722, libc.VaList(bp+8, nbTestSamples, uint32(testSamplesSize)))
		libc.Xfflush(tls, libc.Xstderr)
	}
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples = samples
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes = samplesSizes
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples = uint64(nbSamples)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples = uint64(nbTrainSamples)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTestSamples = uint64(nbTestSamples)
	/* Partial suffix array */
	if uint64(d) > libc.Uint64FromInt64(8) {
		v3 = uint64(d)
	} else {
		v3 = libc.Uint64FromInt64(8)
	}
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize = trainingSamplesSize - v3 + uint64(1)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix = libc.Xmalloc(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	/* Maps index to the dmerID */
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt = libc.Xmalloc(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	/* The offsets of each file */
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.Xmalloc(tls, uint64(nbSamples+libc.Uint32FromInt32(1))*uint64(8))
	if !((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix != 0) || !((*COVER_ctx_t)(unsafe.Pointer(ctx)).FdmerAt != 0) || !((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8762, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		COVER_ctx_destroy(tls, ctx)
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.UintptrFromInt32(0)
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd = d
	/* Fill offsets from the samplesSizes */
	*(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)) = uint64(0)
	i = uint32(1)
	for {
		if !(i <= nbSamples) {
			break
		}
		*(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i)*8)) = *(*size_t)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i-uint32(1))*8)) + *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i-uint32(1))*8))
		goto _7
	_7:
		;
		i = i + 1
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8798, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	i1 = uint32(0)
	for {
		if !(uint64(i1) < (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize) {
			break
		}
		*(*U32)(unsafe.Pointer((*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix + uintptr(i1)*4)) = i1
		goto _8
	_8:
		;
		i1 = i1 + 1
	}
	stableSort(tls, ctx)
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8833, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	/* For each dmer group (group of positions with the same first d bytes):
	 * 1. For each position we set dmerAt[position] = dmerID.  The dmerID is
	 *    (groupBeginPtr - suffix).  This allows us to go from position to
	 *    dmerID so we can look up values in freq.
	 * 2. We calculate how many samples the dmer occurs in and save it in
	 *    freqs[dmerId].
	 */
	if (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fd <= uint32(8) {
		v9 = __ccgo_fp(COVER_cmp8)
	} else {
		v9 = __ccgo_fp(COVER_cmp)
	}
	COVER_groupBy(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize, uint64(4), ctx, v9, __ccgo_fp(COVER_group))
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix
	(*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsuffix = libc.UintptrFromInt32(0)
	return uint64(0)
}

func COVER_warnOnSmallCorpus(tls *libc.TLS, maxDictSize size_t, nbDmers size_t, displayLevel int32) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var ratio float64
	_ = ratio
	ratio = float64(nbDmers) / float64(maxDictSize)
	if ratio >= libc.Float64FromInt32(10) {
		return
	}
	if displayLevel >= int32(1) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8856, libc.VaList(bp+8, uint32(maxDictSize), uint32(nbDmers), ratio))
		libc.Xfflush(tls, libc.Xstderr)
	}
}

func COVER_computeEpochs(tls *libc.TLS, maxDictSize U32, nbDmers U32, k U32, passes U32) (r COVER_epoch_info_t) {
	var epochs COVER_epoch_info_t
	var minEpochSize U32
	var v1 uint32
	_, _, _ = epochs, minEpochSize, v1
	minEpochSize = k * uint32(10)
	if libc.Uint32FromInt32(libc.Int32FromInt32(1)) > maxDictSize/k/passes {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(1))
	} else {
		v1 = maxDictSize / k / passes
	}
	epochs.Fnum = v1
	epochs.Fsize = nbDmers / epochs.Fnum
	if epochs.Fsize >= minEpochSize {
		return epochs
	}
	if minEpochSize < nbDmers {
		v1 = minEpochSize
	} else {
		v1 = nbDmers
	}
	epochs.Fsize = v1
	epochs.Fnum = nbDmers / epochs.Fsize
	return epochs
}

// C documentation
//
//	/**
//	 * Given the prepared context build the dictionary.
//	 */
func COVER_buildDictionary(tls *libc.TLS, ctx uintptr, freqs uintptr, activeDmers uintptr, dictBuffer uintptr, dictBufferCapacity size_t, parameters ZDICT_cover_params_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var dict uintptr
	var epoch, maxZeroScoreRun, segmentSize, tail, zeroScoreRun, v5 size_t
	var epochBegin, epochEnd U32
	var epochs COVER_epoch_info_t
	var segment COVER_segment_t
	var v1, v2, v3 uint32
	var v6 uint64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = dict, epoch, epochBegin, epochEnd, epochs, maxZeroScoreRun, segment, segmentSize, tail, zeroScoreRun, v1, v2, v3, v5, v6
	dict = dictBuffer
	tail = dictBufferCapacity
	/* Divide the data into epochs. We will select one segment from each epoch. */
	epochs = COVER_computeEpochs(tls, uint32(dictBufferCapacity), uint32((*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize), parameters.Fk, uint32(4))
	if libc.Uint32FromInt32(libc.Int32FromInt32(100)) < epochs.Fnum>>libc.Int32FromInt32(3) {
		v2 = libc.Uint32FromInt32(libc.Int32FromInt32(100))
	} else {
		v2 = epochs.Fnum >> libc.Int32FromInt32(3)
	}
	if libc.Uint32FromInt32(libc.Int32FromInt32(10)) > v2 {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(10))
	} else {
		if libc.Uint32FromInt32(libc.Int32FromInt32(100)) < epochs.Fnum>>libc.Int32FromInt32(3) {
			v3 = libc.Uint32FromInt32(libc.Int32FromInt32(100))
		} else {
			v3 = epochs.Fnum >> libc.Int32FromInt32(3)
		}
		v1 = v3
	}
	maxZeroScoreRun = uint64(v1)
	zeroScoreRun = uint64(0)
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9136, libc.VaList(bp+8, epochs.Fnum, epochs.Fsize))
		libc.Xfflush(tls, libc.Xstderr)
	}
	/* Loop through the epochs until there are no more segments or the dictionary
	 * is full.
	 */
	epoch = uint64(0)
	for {
		if !(tail > uint64(0)) {
			break
		}
		epochBegin = uint32(epoch * uint64(epochs.Fsize))
		epochEnd = epochBegin + epochs.Fsize
		/* Select a segment */
		segment = COVER_selectSegment(tls, ctx, freqs, activeDmers, epochBegin, epochEnd, parameters)
		/* If the segment covers no dmers, then we are out of content.
		 * There may be new content in other epochs, for continue for some time.
		 */
		if segment.Fscore == uint32(0) {
			zeroScoreRun = zeroScoreRun + 1
			v5 = zeroScoreRun
			if v5 >= maxZeroScoreRun {
				break
			}
			goto _4
		}
		zeroScoreRun = uint64(0)
		/* Trim the segment if necessary and if it is too small then we are done */
		if uint64(segment.Fend-segment.Fbegin+parameters.Fd-libc.Uint32FromInt32(1)) < tail {
			v6 = uint64(segment.Fend - segment.Fbegin + parameters.Fd - libc.Uint32FromInt32(1))
		} else {
			v6 = tail
		}
		segmentSize = v6
		if segmentSize < uint64(parameters.Fd) {
			break
		}
		/* We fill the dictionary from the back to allow the best segments to be
		 * referenced with the smallest offsets.
		 */
		tail = tail - segmentSize
		libc.Xmemcpy(tls, dict+uintptr(tail), (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(segment.Fbegin), segmentSize)
		if g_displayLevel >= int32(2) {
			if libc.Xclock(tls)-g_time > g_refreshRate || g_displayLevel >= int32(4) {
				g_time = libc.Xclock(tls)
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9180, libc.VaList(bp+8, uint32((dictBufferCapacity-tail)*libc.Uint64FromInt32(100)/dictBufferCapacity)))
				libc.Xfflush(tls, libc.Xstderr)
			}
		}
		goto _4
	_4:
		;
		epoch = (epoch + uint64(1)) % uint64(epochs.Fnum)
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9193, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.Xstderr)
	}
	return tail
}

func ZDICT_trainFromBuffer_cover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters ZDICT_cover_params_t) (r size_t) {
	bp := tls.Alloc(128)
	defer tls.Free(128)
	var dict uintptr
	var dictionarySize, initVal, tail size_t
	var _ /* activeDmers at bp+88 */ COVER_map_t
	var _ /* ctx at bp+0 */ COVER_ctx_t
	_, _, _, _ = dict, dictionarySize, initVal, tail
	dict = dictBuffer
	parameters.FsplitPoint = float64(1)
	/* Initialize global data */
	g_displayLevel = libc.Int32FromUint32(parameters.FzParams.FnotificationLevel)
	/* Checks */
	if !(COVER_checkParameters(tls, parameters, dictBufferCapacity) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9200, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9228, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9269, libc.VaList(bp+120, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* Initialize context and activeDmers */
	initVal = COVER_ctx_init(tls, bp, samplesBuffer, samplesSizes, nbSamples, parameters.Fd, parameters.FsplitPoint)
	if ZSTD_isError(tls, initVal) != 0 {
		return initVal
	}
	COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*COVER_ctx_t)(unsafe.Pointer(bp))).FsuffixSize, g_displayLevel)
	if !(COVER_map_init(tls, bp+88, parameters.Fk-parameters.Fd+uint32(1)) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9309, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		COVER_ctx_destroy(tls, bp)
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9353, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	tail = COVER_buildDictionary(tls, bp, (*(*COVER_ctx_t)(unsafe.Pointer(bp))).Ffreqs, bp+88, dictBuffer, dictBufferCapacity, parameters)
	dictionarySize = ZDICT_finalizeDictionary(tls, dict, dictBufferCapacity, dict+uintptr(tail), dictBufferCapacity-tail, samplesBuffer, samplesSizes, nbSamples, parameters.FzParams)
	if !(ZSTD_isError(tls, dictionarySize) != 0) {
		if g_displayLevel >= int32(2) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9374, libc.VaList(bp+120, uint32(dictionarySize)))
			libc.Xfflush(tls, libc.Xstderr)
		}
	}
	COVER_ctx_destroy(tls, bp)
	COVER_map_destroy(tls, bp+88)
	return dictionarySize
	return r
}

func COVER_checkTotalCompressedSize(tls *libc.TLS, parameters ZDICT_cover_params_t, samplesSizes uintptr, samples uintptr, offsets uintptr, nbTrainSamples size_t, nbSamples size_t, dict uintptr, dictBufferCapacity size_t) (r size_t) {
	var cctx, cdict, dst uintptr
	var dstCapacity, i, maxSampleSize, size, totalCompressedSize size_t
	var v1 uint64
	_, _, _, _, _, _, _, _, _ = cctx, cdict, dst, dstCapacity, i, maxSampleSize, size, totalCompressedSize, v1
	totalCompressedSize = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	/* Allocate dst with enough space to compress the maximum sized sample */
	maxSampleSize = uint64(0)
	if parameters.FsplitPoint < float64(1) {
		v1 = nbTrainSamples
	} else {
		v1 = uint64(0)
	}
	i = v1
	for {
		if !(i < nbSamples) {
			break
		}
		if *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8)) > maxSampleSize {
			v1 = *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8))
		} else {
			v1 = maxSampleSize
		}
		maxSampleSize = v1
		goto _2
	_2:
		;
		i = i + 1
	}
	dstCapacity = ZSTD_compressBound(tls, maxSampleSize)
	dst = libc.Xmalloc(tls, dstCapacity)
	/* Create the cctx and cdict */
	cctx = ZSTD_createCCtx(tls)
	cdict = ZSTD_createCDict(tls, dict, dictBufferCapacity, parameters.FzParams.FcompressionLevel)
	if !(dst != 0) || !(cctx != 0) || !(cdict != 0) {
		goto _compressCleanup
	}
	/* Compress each sample and sum their sizes (or error) */
	totalCompressedSize = dictBufferCapacity
	if parameters.FsplitPoint < float64(1) {
		v1 = nbTrainSamples
	} else {
		v1 = uint64(0)
	}
	i = v1
	for {
		if !(i < nbSamples) {
			break
		}
		size = ZSTD_compress_usingCDict(tls, cctx, dst, dstCapacity, samples+uintptr(*(*size_t)(unsafe.Pointer(offsets + uintptr(i)*8))), *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i)*8)), cdict)
		if ZSTD_isError(tls, size) != 0 {
			totalCompressedSize = size
			goto _compressCleanup
		}
		totalCompressedSize = totalCompressedSize + size
		goto _5
	_5:
		;
		i = i + 1
	}
	goto _compressCleanup
_compressCleanup:
	;
	ZSTD_freeCCtx(tls, cctx)
	ZSTD_freeCDict(tls, cdict)
	if dst != 0 {
		libc.Xfree(tls, dst)
	}
	return totalCompressedSize
}

// C documentation
//
//	/**
//	 * Initialize the `COVER_best_t`.
//	 */
func COVER_best_init(tls *libc.TLS, best uintptr) {
	if best == libc.UintptrFromInt32(0) {
		return
	} /* compatible with init on NULL */
	libc.Xpthread_mutex_init(tls, best, libc.UintptrFromInt32(0))
	libc.Xpthread_cond_init(tls, best+40, libc.UintptrFromInt32(0))
	(*COVER_best_t)(unsafe.Pointer(best)).FliveJobs = uint64(0)
	(*COVER_best_t)(unsafe.Pointer(best)).Fdict = libc.UintptrFromInt32(0)
	(*COVER_best_t)(unsafe.Pointer(best)).FdictSize = uint64(0)
	(*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize = libc.Uint64FromInt32(-libc.Int32FromInt32(1))
	libc.Xmemset(tls, best+112, 0, uint64(48))
}

// C documentation
//
//	/**
//	 * Wait until liveJobs == 0.
//	 */
func COVER_best_wait(tls *libc.TLS, best uintptr) {
	if !(best != 0) {
		return
	}
	libc.Xpthread_mutex_lock(tls, best)
	for (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs != uint64(0) {
		libc.Xpthread_cond_wait(tls, best+40, best)
	}
	libc.Xpthread_mutex_unlock(tls, best)
}

// C documentation
//
//	/**
//	 * Call COVER_best_wait() and then destroy the COVER_best_t.
//	 */
func COVER_best_destroy(tls *libc.TLS, best uintptr) {
	if !(best != 0) {
		return
	}
	COVER_best_wait(tls, best)
	if (*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0 {
		libc.Xfree(tls, (*COVER_best_t)(unsafe.Pointer(best)).Fdict)
	}
	libc.Xpthread_mutex_destroy(tls, best)
	libc.Xpthread_cond_destroy(tls, best+40)
}

// C documentation
//
//	/**
//	 * Called when a thread is about to be launched.
//	 * Increments liveJobs.
//	 */
func COVER_best_start(tls *libc.TLS, best uintptr) {
	if !(best != 0) {
		return
	}
	libc.Xpthread_mutex_lock(tls, best)
	(*COVER_best_t)(unsafe.Pointer(best)).FliveJobs = (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs + 1
	libc.Xpthread_mutex_unlock(tls, best)
}

// C documentation
//
//	/**
//	 * Called when a thread finishes executing, both on error or success.
//	 * Decrements liveJobs and signals any waiting threads if liveJobs == 0.
//	 * If this dictionary is the best so far save it and its parameters.
//	 */
func COVER_best_finish(tls *libc.TLS, best uintptr, parameters ZDICT_cover_params_t, selection COVER_dictSelection_t) {
	var compressedSize, dictSize, liveJobs size_t
	var dict uintptr
	_, _, _, _ = compressedSize, dict, dictSize, liveJobs
	dict = selection.FdictContent
	compressedSize = selection.FtotalCompressedSize
	dictSize = selection.FdictSize
	if !(best != 0) {
		return
	}
	libc.Xpthread_mutex_lock(tls, best)
	(*COVER_best_t)(unsafe.Pointer(best)).FliveJobs = (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs - 1
	liveJobs = (*COVER_best_t)(unsafe.Pointer(best)).FliveJobs
	/* If the new dictionary is better */
	if compressedSize < (*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize {
		/* Allocate space if necessary */
		if !((*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0) || (*COVER_best_t)(unsafe.Pointer(best)).FdictSize < dictSize {
			if (*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0 {
				libc.Xfree(tls, (*COVER_best_t)(unsafe.Pointer(best)).Fdict)
			}
			(*COVER_best_t)(unsafe.Pointer(best)).Fdict = libc.Xmalloc(tls, dictSize)
			if !((*COVER_best_t)(unsafe.Pointer(best)).Fdict != 0) {
				(*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
				(*COVER_best_t)(unsafe.Pointer(best)).FdictSize = uint64(0)
				libc.Xpthread_cond_signal(tls, best+40)
				libc.Xpthread_mutex_unlock(tls, best)
				return
			}
		}
		/* Save the dictionary, parameters, and size */
		if dict != 0 {
			libc.Xmemcpy(tls, (*COVER_best_t)(unsafe.Pointer(best)).Fdict, dict, dictSize)
			(*COVER_best_t)(unsafe.Pointer(best)).FdictSize = dictSize
			(*COVER_best_t)(unsafe.Pointer(best)).Fparameters = parameters
			(*COVER_best_t)(unsafe.Pointer(best)).FcompressedSize = compressedSize
		}
	}
	if liveJobs == uint64(0) {
		libc.Xpthread_cond_broadcast(tls, best+40)
	}
	libc.Xpthread_mutex_unlock(tls, best)
}

func setDictSelection(tls *libc.TLS, buf uintptr, s size_t, csz size_t) (r COVER_dictSelection_t) {
	var ds COVER_dictSelection_t
	_ = ds
	ds.FdictContent = buf
	ds.FdictSize = s
	ds.FtotalCompressedSize = csz
	return ds
}

func COVER_dictSelectionError(tls *libc.TLS, error1 size_t) (r COVER_dictSelection_t) {
	return setDictSelection(tls, libc.UintptrFromInt32(0), uint64(0), error1)
}

func COVER_dictSelectionIsError(tls *libc.TLS, selection COVER_dictSelection_t) (r uint32) {
	return libc.BoolUint32(ZSTD_isError(tls, selection.FtotalCompressedSize) != 0 || !(selection.FdictContent != 0))
}

func COVER_dictSelectionFree(tls *libc.TLS, selection COVER_dictSelection_t) {
	libc.Xfree(tls, selection.FdictContent)
}

func COVER_selectDict(tls *libc.TLS, customDictContent uintptr, dictBufferCapacity size_t, dictContentSize size_t, samplesBuffer uintptr, samplesSizes uintptr, nbFinalizeSamples uint32, nbCheckSamples size_t, nbSamples size_t, params ZDICT_cover_params_t, offsets uintptr, totalCompressedSize size_t) (r COVER_dictSelection_t) {
	var candidateDictBuffer, customDictContentEnd, largestDictbuffer uintptr
	var largestCompressed, largestDict size_t
	var regressionTolerance float64
	_, _, _, _, _, _ = candidateDictBuffer, customDictContentEnd, largestCompressed, largestDict, largestDictbuffer, regressionTolerance
	largestDict = uint64(0)
	largestCompressed = uint64(0)
	customDictContentEnd = customDictContent + uintptr(dictContentSize)
	largestDictbuffer = libc.Xmalloc(tls, dictBufferCapacity)
	candidateDictBuffer = libc.Xmalloc(tls, dictBufferCapacity)
	regressionTolerance = float64(params.FshrinkDictMaxRegression)/float64(100) + float64(1)
	if !(largestDictbuffer != 0) || !(candidateDictBuffer != 0) {
		libc.Xfree(tls, largestDictbuffer)
		libc.Xfree(tls, candidateDictBuffer)
		return COVER_dictSelectionError(tls, dictContentSize)
	}
	/* Initial dictionary size and compressed size */
	libc.Xmemcpy(tls, largestDictbuffer, customDictContent, dictContentSize)
	dictContentSize = ZDICT_finalizeDictionary(tls, largestDictbuffer, dictBufferCapacity, customDictContent, dictContentSize, samplesBuffer, samplesSizes, nbFinalizeSamples, params.FzParams)
	if ZDICT_isError(tls, dictContentSize) != 0 {
		libc.Xfree(tls, largestDictbuffer)
		libc.Xfree(tls, candidateDictBuffer)
		return COVER_dictSelectionError(tls, dictContentSize)
	}
	totalCompressedSize = COVER_checkTotalCompressedSize(tls, params, samplesSizes, samplesBuffer, offsets, nbCheckSamples, nbSamples, largestDictbuffer, dictContentSize)
	if ZSTD_isError(tls, totalCompressedSize) != 0 {
		libc.Xfree(tls, largestDictbuffer)
		libc.Xfree(tls, candidateDictBuffer)
		return COVER_dictSelectionError(tls, totalCompressedSize)
	}
	if params.FshrinkDict == uint32(0) {
		libc.Xfree(tls, candidateDictBuffer)
		return setDictSelection(tls, largestDictbuffer, dictContentSize, totalCompressedSize)
	}
	largestDict = dictContentSize
	largestCompressed = totalCompressedSize
	dictContentSize = uint64(ZDICT_DICTSIZE_MIN)
	/* Largest dict is initially at least ZDICT_DICTSIZE_MIN */
	for dictContentSize < largestDict {
		libc.Xmemcpy(tls, candidateDictBuffer, largestDictbuffer, largestDict)
		dictContentSize = ZDICT_finalizeDictionary(tls, candidateDictBuffer, dictBufferCapacity, customDictContentEnd-uintptr(dictContentSize), dictContentSize, samplesBuffer, samplesSizes, nbFinalizeSamples, params.FzParams)
		if ZDICT_isError(tls, dictContentSize) != 0 {
			libc.Xfree(tls, largestDictbuffer)
			libc.Xfree(tls, candidateDictBuffer)
			return COVER_dictSelectionError(tls, dictContentSize)
		}
		totalCompressedSize = COVER_checkTotalCompressedSize(tls, params, samplesSizes, samplesBuffer, offsets, nbCheckSamples, nbSamples, candidateDictBuffer, dictContentSize)
		if ZSTD_isError(tls, totalCompressedSize) != 0 {
			libc.Xfree(tls, largestDictbuffer)
			libc.Xfree(tls, candidateDictBuffer)
			return COVER_dictSelectionError(tls, totalCompressedSize)
		}
		if float64(totalCompressedSize) <= float64(float64(largestCompressed)*regressionTolerance) {
			libc.Xfree(tls, largestDictbuffer)
			return setDictSelection(tls, candidateDictBuffer, dictContentSize, totalCompressedSize)
		}
		dictContentSize = dictContentSize * uint64(2)
	}
	dictContentSize = largestDict
	totalCompressedSize = largestCompressed
	libc.Xfree(tls, candidateDictBuffer)
	return setDictSelection(tls, largestDictbuffer, dictContentSize, totalCompressedSize)
}

// C documentation
//
//	/**
//	 * Parameters for COVER_tryParameters().
//	 */
type COVER_tryParameters_data_t = struct {
	Fctx                uintptr
	Fbest               uintptr
	FdictBufferCapacity size_t
	Fparameters         ZDICT_cover_params_t
}

// C documentation
//
//	/**
//	 * Parameters for COVER_tryParameters().
//	 */
type COVER_tryParameters_data_s = COVER_tryParameters_data_t

// C documentation
//
//	/**
//	 * Tries a set of parameters and updates the COVER_best_t with the results.
//	 * This function is thread safe if zstd is compiled with multithreaded support.
//	 * It takes its parameters as an *OWNING* opaque pointer to support threading.
//	 */
func COVER_tryParameters(tls *libc.TLS, opaque uintptr) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var ctx, data, dict, freqs uintptr
	var dictBufferCapacity, tail, totalCompressedSize size_t
	var parameters ZDICT_cover_params_t
	var selection COVER_dictSelection_t
	var _ /* activeDmers at bp+0 */ COVER_map_t
	_, _, _, _, _, _, _, _, _ = ctx, data, dict, dictBufferCapacity, freqs, parameters, selection, tail, totalCompressedSize
	/* Save parameters as local variables */
	data = opaque
	ctx = (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx
	parameters = (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters
	dictBufferCapacity = (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity
	totalCompressedSize = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	dict = libc.Xmalloc(tls, dictBufferCapacity)
	selection = COVER_dictSelectionError(tls, libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC)))
	freqs = libc.Xmalloc(tls, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	if !(COVER_map_init(tls, bp, parameters.Fk-parameters.Fd+uint32(1)) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9309, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	if !(dict != 0) || !(freqs != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9409, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	/* Copy the frequencies because we need to modify them */
	libc.Xmemcpy(tls, freqs, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsuffixSize*uint64(4))
	/* Build the dictionary */
	tail = COVER_buildDictionary(tls, ctx, freqs, bp, dict, dictBufferCapacity, parameters)
	selection = COVER_selectDict(tls, dict+uintptr(tail), dictBufferCapacity, dictBufferCapacity-tail, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes, uint32((*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples), (*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples, (*COVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples, parameters, (*COVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets, totalCompressedSize)
	if COVER_dictSelectionIsError(tls, selection) != 0 {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9452, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	goto _cleanup
_cleanup:
	;
	libc.Xfree(tls, dict)
	COVER_best_finish(tls, (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest, parameters, selection)
	libc.Xfree(tls, data)
	COVER_map_destroy(tls, bp)
	COVER_dictSelectionFree(tls, selection)
	libc.Xfree(tls, freqs)
}

func ZDICT_optimizeTrainFromBuffer_cover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters uintptr) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var compressedSize, dictSize, initVal size_t
	var d, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, shrinkDict, v2, v3, v4, v5, v6, v7 uint32
	var data, pool uintptr
	var displayLevel, warned, v8 int32
	var splitPoint, v1 float64
	var _ /* best at bp+0 */ COVER_best_t
	var _ /* ctx at bp+168 */ COVER_ctx_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = compressedSize, d, data, dictSize, displayLevel, initVal, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, pool, shrinkDict, splitPoint, warned, v1, v2, v3, v4, v5, v6, v7, v8
	/* constants */
	nbThreads = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FnbThreads
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FsplitPoint <= float64(0) {
		v1 = float64(1)
	} else {
		v1 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FsplitPoint
	}
	splitPoint = v1
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v2 = uint32(6)
	} else {
		v2 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMinD = v2
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v3 = uint32(8)
	} else {
		v3 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMaxD = v3
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v4 = uint32(50)
	} else {
		v4 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMinK = v4
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v5 = uint32(2000)
	} else {
		v5 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMaxK = v5
	if (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fsteps == uint32(0) {
		v6 = uint32(40)
	} else {
		v6 = (*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).Fsteps
	}
	kSteps = v6
	if (kMaxK-kMinK)/kSteps > libc.Uint32FromInt32(libc.Int32FromInt32(1)) {
		v7 = (kMaxK - kMinK) / kSteps
	} else {
		v7 = libc.Uint32FromInt32(libc.Int32FromInt32(1))
	}
	kStepSize = v7
	kIterations = (uint32(1) + (kMaxD-kMinD)/uint32(2)) * (uint32(1) + (kMaxK-kMinK)/kStepSize)
	shrinkDict = uint32(0)
	/* Local variables */
	displayLevel = libc.Int32FromUint32((*ZDICT_cover_params_t)(unsafe.Pointer(parameters)).FzParams.FnotificationLevel)
	iteration = uint32(1)
	pool = libc.UintptrFromInt32(0)
	warned = 0
	/* Checks */
	if splitPoint <= libc.Float64FromInt32(0) || splitPoint > libc.Float64FromInt32(1) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9481, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if kMinK < kMaxD || kMaxK < kMinK {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9481, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9228, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9269, libc.VaList(bp+264, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbThreads > uint32(1) {
		pool = POOL_create(tls, uint64(nbThreads), uint64(1))
		if !(pool != 0) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
	}
	/* Initialization */
	COVER_best_init(tls, bp)
	/* Turn down global display level to clean up display at level 2 and below */
	if displayLevel == 0 {
		v8 = 0
	} else {
		v8 = displayLevel - int32(1)
	}
	g_displayLevel = v8
	/* Loop through d first because each new value needs a new context */
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9503, libc.VaList(bp+264, kIterations))
		libc.Xfflush(tls, libc.Xstderr)
	}
	d = kMinD
	for {
		if !(d <= kMaxD) {
			break
		}
		if displayLevel >= int32(3) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9543, libc.VaList(bp+264, d))
			libc.Xfflush(tls, libc.Xstderr)
		}
		initVal = COVER_ctx_init(tls, bp+168, samplesBuffer, samplesSizes, nbSamples, d, splitPoint)
		if ZSTD_isError(tls, initVal) != 0 {
			if displayLevel >= int32(1) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9549, 0)
				libc.Xfflush(tls, libc.Xstderr)
			}
			COVER_best_destroy(tls, bp)
			POOL_free(tls, pool)
			return initVal
		}
		if !(warned != 0) {
			COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*COVER_ctx_t)(unsafe.Pointer(bp + 168))).FsuffixSize, displayLevel)
			warned = int32(1)
		}
		/* Loop through k reusing the same context */
		k = kMinK
		for {
			if !(k <= kMaxK) {
				break
			}
			/* Prepare the arguments */
			data = libc.Xmalloc(tls, uint64(72))
			if displayLevel >= int32(3) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9579, libc.VaList(bp+264, k))
				libc.Xfflush(tls, libc.Xstderr)
			}
			if !(data != 0) {
				if displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9585, 0)
					libc.Xfflush(tls, libc.Xstderr)
				}
				COVER_best_destroy(tls, bp)
				COVER_ctx_destroy(tls, bp+168)
				POOL_free(tls, pool)
				return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
			}
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx = bp + 168
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest = bp
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity = dictBufferCapacity
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters = *(*ZDICT_cover_params_t)(unsafe.Pointer(parameters))
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fk = k
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fd = d
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FsplitPoint = splitPoint
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fsteps = kSteps
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FshrinkDict = shrinkDict
			(*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FzParams.FnotificationLevel = libc.Uint32FromInt32(g_displayLevel)
			/* Check the parameters */
			if !(COVER_checkParameters(tls, (*COVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters, dictBufferCapacity) != 0) {
				if g_displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9200, 0)
					libc.Xfflush(tls, libc.Xstderr)
				}
				libc.Xfree(tls, data)
				goto _10
			}
			/* Call the function and pass ownership of data to it */
			COVER_best_start(tls, bp)
			if pool != 0 {
				POOL_add(tls, pool, __ccgo_fp(COVER_tryParameters), data)
			} else {
				COVER_tryParameters(tls, data)
			}
			/* Print status */
			if displayLevel >= int32(2) {
				if libc.Xclock(tls)-g_time > g_refreshRate || displayLevel >= int32(4) {
					g_time = libc.Xclock(tls)
					libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9180, libc.VaList(bp+264, iteration*libc.Uint32FromInt32(100)/kIterations))
					libc.Xfflush(tls, libc.Xstderr)
				}
			}
			iteration = iteration + 1
			goto _10
		_10:
			;
			k = k + kStepSize
		}
		COVER_best_wait(tls, bp)
		COVER_ctx_destroy(tls, bp+168)
		goto _9
	_9:
		;
		d = d + uint32(2)
	}
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9193, libc.VaList(bp+264, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.Xstderr)
	}
	/* Fill the output buffer and parameters with output of the best parameters */
	dictSize = (*(*COVER_best_t)(unsafe.Pointer(bp))).FdictSize
	if ZSTD_isError(tls, (*(*COVER_best_t)(unsafe.Pointer(bp))).FcompressedSize) != 0 {
		compressedSize = (*(*COVER_best_t)(unsafe.Pointer(bp))).FcompressedSize
		COVER_best_destroy(tls, bp)
		POOL_free(tls, pool)
		return compressedSize
	}
	*(*ZDICT_cover_params_t)(unsafe.Pointer(parameters)) = (*(*COVER_best_t)(unsafe.Pointer(bp))).Fparameters
	libc.Xmemcpy(tls, dictBuffer, (*(*COVER_best_t)(unsafe.Pointer(bp))).Fdict, dictSize)
	COVER_best_destroy(tls, bp)
	POOL_free(tls, pool)
	return dictSize
	return r
}

/**** ended inlining divsufsort.h ****/

/*- Constants -*/
/* minstacksize = log(SS_BLOCKSIZE) / log(3) * 2 */

/*- Macros -*/

/*- Private Functions -*/

var lg_table = [256]int32{
	0:   -int32(1),
	2:   int32(1),
	3:   int32(1),
	4:   int32(2),
	5:   int32(2),
	6:   int32(2),
	7:   int32(2),
	8:   int32(3),
	9:   int32(3),
	10:  int32(3),
	11:  int32(3),
	12:  int32(3),
	13:  int32(3),
	14:  int32(3),
	15:  int32(3),
	16:  int32(4),
	17:  int32(4),
	18:  int32(4),
	19:  int32(4),
	20:  int32(4),
	21:  int32(4),
	22:  int32(4),
	23:  int32(4),
	24:  int32(4),
	25:  int32(4),
	26:  int32(4),
	27:  int32(4),
	28:  int32(4),
	29:  int32(4),
	30:  int32(4),
	31:  int32(4),
	32:  int32(5),
	33:  int32(5),
	34:  int32(5),
	35:  int32(5),
	36:  int32(5),
	37:  int32(5),
	38:  int32(5),
	39:  int32(5),
	40:  int32(5),
	41:  int32(5),
	42:  int32(5),
	43:  int32(5),
	44:  int32(5),
	45:  int32(5),
	46:  int32(5),
	47:  int32(5),
	48:  int32(5),
	49:  int32(5),
	50:  int32(5),
	51:  int32(5),
	52:  int32(5),
	53:  int32(5),
	54:  int32(5),
	55:  int32(5),
	56:  int32(5),
	57:  int32(5),
	58:  int32(5),
	59:  int32(5),
	60:  int32(5),
	61:  int32(5),
	62:  int32(5),
	63:  int32(5),
	64:  int32(6),
	65:  int32(6),
	66:  int32(6),
	67:  int32(6),
	68:  int32(6),
	69:  int32(6),
	70:  int32(6),
	71:  int32(6),
	72:  int32(6),
	73:  int32(6),
	74:  int32(6),
	75:  int32(6),
	76:  int32(6),
	77:  int32(6),
	78:  int32(6),
	79:  int32(6),
	80:  int32(6),
	81:  int32(6),
	82:  int32(6),
	83:  int32(6),
	84:  int32(6),
	85:  int32(6),
	86:  int32(6),
	87:  int32(6),
	88:  int32(6),
	89:  int32(6),
	90:  int32(6),
	91:  int32(6),
	92:  int32(6),
	93:  int32(6),
	94:  int32(6),
	95:  int32(6),
	96:  int32(6),
	97:  int32(6),
	98:  int32(6),
	99:  int32(6),
	100: int32(6),
	101: int32(6),
	102: int32(6),
	103: int32(6),
	104: int32(6),
	105: int32(6),
	106: int32(6),
	107: int32(6),
	108: int32(6),
	109: int32(6),
	110: int32(6),
	111: int32(6),
	112: int32(6),
	113: int32(6),
	114: int32(6),
	115: int32(6),
	116: int32(6),
	117: int32(6),
	118: int32(6),
	119: int32(6),
	120: int32(6),
	121: int32(6),
	122: int32(6),
	123: int32(6),
	124: int32(6),
	125: int32(6),
	126: int32(6),
	127: int32(6),
	128: int32(7),
	129: int32(7),
	130: int32(7),
	131: int32(7),
	132: int32(7),
	133: int32(7),
	134: int32(7),
	135: int32(7),
	136: int32(7),
	137: int32(7),
	138: int32(7),
	139: int32(7),
	140: int32(7),
	141: int32(7),
	142: int32(7),
	143: int32(7),
	144: int32(7),
	145: int32(7),
	146: int32(7),
	147: int32(7),
	148: int32(7),
	149: int32(7),
	150: int32(7),
	151: int32(7),
	152: int32(7),
	153: int32(7),
	154: int32(7),
	155: int32(7),
	156: int32(7),
	157: int32(7),
	158: int32(7),
	159: int32(7),
	160: int32(7),
	161: int32(7),
	162: int32(7),
	163: int32(7),
	164: int32(7),
	165: int32(7),
	166: int32(7),
	167: int32(7),
	168: int32(7),
	169: int32(7),
	170: int32(7),
	171: int32(7),
	172: int32(7),
	173: int32(7),
	174: int32(7),
	175: int32(7),
	176: int32(7),
	177: int32(7),
	178: int32(7),
	179: int32(7),
	180: int32(7),
	181: int32(7),
	182: int32(7),
	183: int32(7),
	184: int32(7),
	185: int32(7),
	186: int32(7),
	187: int32(7),
	188: int32(7),
	189: int32(7),
	190: int32(7),
	191: int32(7),
	192: int32(7),
	193: int32(7),
	194: int32(7),
	195: int32(7),
	196: int32(7),
	197: int32(7),
	198: int32(7),
	199: int32(7),
	200: int32(7),
	201: int32(7),
	202: int32(7),
	203: int32(7),
	204: int32(7),
	205: int32(7),
	206: int32(7),
	207: int32(7),
	208: int32(7),
	209: int32(7),
	210: int32(7),
	211: int32(7),
	212: int32(7),
	213: int32(7),
	214: int32(7),
	215: int32(7),
	216: int32(7),
	217: int32(7),
	218: int32(7),
	219: int32(7),
	220: int32(7),
	221: int32(7),
	222: int32(7),
	223: int32(7),
	224: int32(7),
	225: int32(7),
	226: int32(7),
	227: int32(7),
	228: int32(7),
	229: int32(7),
	230: int32(7),
	231: int32(7),
	232: int32(7),
	233: int32(7),
	234: int32(7),
	235: int32(7),
	236: int32(7),
	237: int32(7),
	238: int32(7),
	239: int32(7),
	240: int32(7),
	241: int32(7),
	242: int32(7),
	243: int32(7),
	244: int32(7),
	245: int32(7),
	246: int32(7),
	247: int32(7),
	248: int32(7),
	249: int32(7),
	250: int32(7),
	251: int32(7),
	252: int32(7),
	253: int32(7),
	254: int32(7),
	255: int32(7),
}

func ss_ilg(tls *libc.TLS, n int32) (r int32) {
	var v1 int32
	_ = v1
	if n&int32(0xff00) != 0 {
		v1 = int32(8) + lg_table[n>>int32(8)&int32(0xff)]
	} else {
		v1 = 0 + lg_table[n>>0&int32(0xff)]
	}
	return v1
}

var sqq_table = [256]int32{
	1:   int32(16),
	2:   int32(22),
	3:   int32(27),
	4:   int32(32),
	5:   int32(35),
	6:   int32(39),
	7:   int32(42),
	8:   int32(45),
	9:   int32(48),
	10:  int32(50),
	11:  int32(53),
	12:  int32(55),
	13:  int32(57),
	14:  int32(59),
	15:  int32(61),
	16:  int32(64),
	17:  int32(65),
	18:  int32(67),
	19:  int32(69),
	20:  int32(71),
	21:  int32(73),
	22:  int32(75),
	23:  int32(76),
	24:  int32(78),
	25:  int32(80),
	26:  int32(81),
	27:  int32(83),
	28:  int32(84),
	29:  int32(86),
	30:  int32(87),
	31:  int32(89),
	32:  int32(90),
	33:  int32(91),
	34:  int32(93),
	35:  int32(94),
	36:  int32(96),
	37:  int32(97),
	38:  int32(98),
	39:  int32(99),
	40:  int32(101),
	41:  int32(102),
	42:  int32(103),
	43:  int32(104),
	44:  int32(106),
	45:  int32(107),
	46:  int32(108),
	47:  int32(109),
	48:  int32(110),
	49:  int32(112),
	50:  int32(113),
	51:  int32(114),
	52:  int32(115),
	53:  int32(116),
	54:  int32(117),
	55:  int32(118),
	56:  int32(119),
	57:  int32(120),
	58:  int32(121),
	59:  int32(122),
	60:  int32(123),
	61:  int32(124),
	62:  int32(125),
	63:  int32(126),
	64:  int32(128),
	65:  int32(128),
	66:  int32(129),
	67:  int32(130),
	68:  int32(131),
	69:  int32(132),
	70:  int32(133),
	71:  int32(134),
	72:  int32(135),
	73:  int32(136),
	74:  int32(137),
	75:  int32(138),
	76:  int32(139),
	77:  int32(140),
	78:  int32(141),
	79:  int32(142),
	80:  int32(143),
	81:  int32(144),
	82:  int32(144),
	83:  int32(145),
	84:  int32(146),
	85:  int32(147),
	86:  int32(148),
	87:  int32(149),
	88:  int32(150),
	89:  int32(150),
	90:  int32(151),
	91:  int32(152),
	92:  int32(153),
	93:  int32(154),
	94:  int32(155),
	95:  int32(155),
	96:  int32(156),
	97:  int32(157),
	98:  int32(158),
	99:  int32(159),
	100: int32(160),
	101: int32(160),
	102: int32(161),
	103: int32(162),
	104: int32(163),
	105: int32(163),
	106: int32(164),
	107: int32(165),
	108: int32(166),
	109: int32(167),
	110: int32(167),
	111: int32(168),
	112: int32(169),
	113: int32(170),
	114: int32(170),
	115: int32(171),
	116: int32(172),
	117: int32(173),
	118: int32(173),
	119: int32(174),
	120: int32(175),
	121: int32(176),
	122: int32(176),
	123: int32(177),
	124: int32(178),
	125: int32(178),
	126: int32(179),
	127: int32(180),
	128: int32(181),
	129: int32(181),
	130: int32(182),
	131: int32(183),
	132: int32(183),
	133: int32(184),
	134: int32(185),
	135: int32(185),
	136: int32(186),
	137: int32(187),
	138: int32(187),
	139: int32(188),
	140: int32(189),
	141: int32(189),
	142: int32(190),
	143: int32(191),
	144: int32(192),
	145: int32(192),
	146: int32(193),
	147: int32(193),
	148: int32(194),
	149: int32(195),
	150: int32(195),
	151: int32(196),
	152: int32(197),
	153: int32(197),
	154: int32(198),
	155: int32(199),
	156: int32(199),
	157: int32(200),
	158: int32(201),
	159: int32(201),
	160: int32(202),
	161: int32(203),
	162: int32(203),
	163: int32(204),
	164: int32(204),
	165: int32(205),
	166: int32(206),
	167: int32(206),
	168: int32(207),
	169: int32(208),
	170: int32(208),
	171: int32(209),
	172: int32(209),
	173: int32(210),
	174: int32(211),
	175: int32(211),
	176: int32(212),
	177: int32(212),
	178: int32(213),
	179: int32(214),
	180: int32(214),
	181: int32(215),
	182: int32(215),
	183: int32(216),
	184: int32(217),
	185: int32(217),
	186: int32(218),
	187: int32(218),
	188: int32(219),
	189: int32(219),
	190: int32(220),
	191: int32(221),
	192: int32(221),
	193: int32(222),
	194: int32(222),
	195: int32(223),
	196: int32(224),
	197: int32(224),
	198: int32(225),
	199: int32(225),
	200: int32(226),
	201: int32(226),
	202: int32(227),
	203: int32(227),
	204: int32(228),
	205: int32(229),
	206: int32(229),
	207: int32(230),
	208: int32(230),
	209: int32(231),
	210: int32(231),
	211: int32(232),
	212: int32(232),
	213: int32(233),
	214: int32(234),
	215: int32(234),
	216: int32(235),
	217: int32(235),
	218: int32(236),
	219: int32(236),
	220: int32(237),
	221: int32(237),
	222: int32(238),
	223: int32(238),
	224: int32(239),
	225: int32(240),
	226: int32(240),
	227: int32(241),
	228: int32(241),
	229: int32(242),
	230: int32(242),
	231: int32(243),
	232: int32(243),
	233: int32(244),
	234: int32(244),
	235: int32(245),
	236: int32(245),
	237: int32(246),
	238: int32(246),
	239: int32(247),
	240: int32(247),
	241: int32(248),
	242: int32(248),
	243: int32(249),
	244: int32(249),
	245: int32(250),
	246: int32(250),
	247: int32(251),
	248: int32(251),
	249: int32(252),
	250: int32(252),
	251: int32(253),
	252: int32(253),
	253: int32(254),
	254: int32(254),
	255: int32(255),
}

func ss_isqrt(tls *libc.TLS, x int32) (r int32) {
	var e, y, v1, v2, v3 int32
	_, _, _, _, _ = e, y, v1, v2, v3
	if x >= libc.Int32FromInt32(SS_BLOCKSIZE)*libc.Int32FromInt32(SS_BLOCKSIZE) {
		return int32(SS_BLOCKSIZE)
	}
	if libc.Uint32FromInt32(x)&uint32(0xffff0000) != 0 {
		if libc.Uint32FromInt32(x)&uint32(0xff000000) != 0 {
			v2 = int32(24) + lg_table[x>>int32(24)&int32(0xff)]
		} else {
			v2 = int32(16) + lg_table[x>>int32(16)&int32(0xff)]
		}
		v1 = v2
	} else {
		if x&int32(0x0000ff00) != 0 {
			v3 = int32(8) + lg_table[x>>int32(8)&int32(0xff)]
		} else {
			v3 = 0 + lg_table[x>>0&int32(0xff)]
		}
		v1 = v3
	}
	e = v1
	if e >= int32(16) {
		y = sqq_table[x>>(e-int32(6)-e&int32(1))] << (e>>int32(1) - int32(7))
		if e >= int32(24) {
			y = (y + int32(1) + x/y) >> int32(1)
		}
		y = (y + int32(1) + x/y) >> int32(1)
	} else {
		if e >= int32(8) {
			y = sqq_table[x>>(e-int32(6)-e&int32(1))]>>(libc.Int32FromInt32(7)-e>>libc.Int32FromInt32(1)) + int32(1)
		} else {
			return sqq_table[x] >> int32(4)
		}
	}
	if x < y*y {
		v1 = y - int32(1)
	} else {
		v1 = y
	}
	return v1
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Compares two suffixes. */
func ss_compare(tls *libc.TLS, T uintptr, p1 uintptr, p2 uintptr, depth int32) (r int32) {
	var U1, U1n, U2, U2n uintptr
	var v2, v3, v4 int32
	_, _, _, _, _, _, _ = U1, U1n, U2, U2n, v2, v3, v4
	U1 = T + uintptr(depth) + uintptr(*(*int32)(unsafe.Pointer(p1)))
	U2 = T + uintptr(depth) + uintptr(*(*int32)(unsafe.Pointer(p2)))
	U1n = T + uintptr(*(*int32)(unsafe.Pointer(p1 + libc.UintptrFromInt32(1)*4))) + uintptr(2)
	U2n = T + uintptr(*(*int32)(unsafe.Pointer(p2 + libc.UintptrFromInt32(1)*4))) + libc.UintptrFromInt32(2)
	for {
		if !(U1 < U1n && U2 < U2n && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(U1))) == libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(U2)))) {
			break
		}
		goto _1
	_1:
		;
		U1 = U1 + 1
		U2 = U2 + 1
	}
	if U1 < U1n {
		if U2 < U2n {
			v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(U1))) - libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(U2)))
		} else {
			v3 = int32(1)
		}
		v2 = v3
	} else {
		if U2 < U2n {
			v4 = -int32(1)
		} else {
			v4 = 0
		}
		v2 = v4
	}
	return v2
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Insertionsort for small size groups */
func ss_insertionsort(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, last uintptr, depth int32) {
	var i, j, v4 uintptr
	var r, t, v3 int32
	_, _, _, _, _, _ = i, j, r, t, v3, v4
	i = last - uintptr(2)*4
	for {
		if !(first <= i) {
			break
		}
		t = *(*int32)(unsafe.Pointer(i))
		j = i + libc.UintptrFromInt32(1)*4
		for {
			v3 = ss_compare(tls, T, PA+uintptr(t)*4, PA+uintptr(*(*int32)(unsafe.Pointer(j)))*4, depth)
			r = v3
			if !(0 < v3) {
				break
			}
			for {
				*(*int32)(unsafe.Pointer(j - libc.UintptrFromInt32(1)*4)) = *(*int32)(unsafe.Pointer(j))
				goto _5
			_5:
				;
				j += 4
				v4 = j
				if !(v4 < last && *(*int32)(unsafe.Pointer(j)) < 0) {
					break
				}
			}
			if last <= j {
				break
			}
			goto _2
		_2:
		}
		if r == 0 {
			*(*int32)(unsafe.Pointer(j)) = ^*(*int32)(unsafe.Pointer(j))
		}
		*(*int32)(unsafe.Pointer(j - libc.UintptrFromInt32(1)*4)) = t
		goto _1
	_1:
		;
		i -= 4
	}
}

/*---------------------------------------------------------------------------*/

func ss_fixdown(tls *libc.TLS, Td uintptr, PA uintptr, SA uintptr, i int32, size int32) {
	var c, d, e, j, k, v, v2, v3, v4 int32
	_, _, _, _, _, _, _, _, _ = c, d, e, j, k, v, v2, v3, v4
	v = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
	c = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(v)*4))))))
	for {
		v2 = libc.Int32FromInt32(2)*i + libc.Int32FromInt32(1)
		j = v2
		if !(v2 < size) {
			break
		}
		v4 = j
		j = j + 1
		v3 = v4
		k = v3
		d = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(v3)*4)))*4))))))
		v2 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(j)*4)))*4))))))
		e = v2
		if d < v2 {
			k = j
			d = e
		}
		if d <= c {
			break
		}
		goto _1
	_1:
		;
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(k)*4))
		i = k
	}
	*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = v
}

// C documentation
//
//	/* Simple top-down heapsort. */
func ss_heapsort(tls *libc.TLS, Td uintptr, PA uintptr, SA uintptr, size int32) {
	var i, m, t int32
	_, _, _ = i, m, t
	m = size
	if size%int32(2) == 0 {
		m = m - 1
		if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)))*4)))))) < libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)))*4)))))) {
			t = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)) = t
		}
	}
	i = m/int32(2) - int32(1)
	for {
		if !(0 <= i) {
			break
		}
		ss_fixdown(tls, Td, PA, SA, i, m)
		goto _1
	_1:
		;
		i = i - 1
	}
	if size%int32(2) == 0 {
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
		*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = t
		ss_fixdown(tls, Td, PA, SA, 0, m)
	}
	i = m - int32(1)
	for {
		if !(0 < i) {
			break
		}
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
		ss_fixdown(tls, Td, PA, SA, 0, i)
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = t
		goto _2
	_2:
		;
		i = i - 1
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Returns the median of three elements. */
func ss_median3(tls *libc.TLS, Td uintptr, PA uintptr, v1 uintptr, v2 uintptr, v3 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) {
		t = v1
		v1 = v2
		v2 = t
	}
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
		if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
			return v1
		} else {
			return v3
		}
	}
	return v2
}

// C documentation
//
//	/* Returns the median of five elements. */
func ss_median5(tls *libc.TLS, Td uintptr, PA uintptr, v1 uintptr, v2 uintptr, v3 uintptr, v4 uintptr, v5 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
		t = v2
		v2 = v3
		v3 = t
	}
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v5)))*4)))))) {
		t = v4
		v4 = v5
		v5 = t
	}
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) {
		t = v2
		v2 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) {
		t = v1
		v1 = v3
		v3 = t
	}
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) {
		t = v1
		v1 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)))))) > libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)))))) {
		return v4
	}
	return v3
}

// C documentation
//
//	/* Returns the pivot element. */
func ss_pivot(tls *libc.TLS, Td uintptr, PA uintptr, first uintptr, last uintptr) (r uintptr) {
	var middle uintptr
	var t int32
	_, _ = middle, t
	t = int32((int64(last) - int64(first)) / 4)
	middle = first + uintptr(t/int32(2))*4
	if t <= int32(512) {
		if t <= int32(32) {
			return ss_median3(tls, Td, PA, first, middle, last-uintptr(1)*4)
		} else {
			t = t >> int32(2)
			return ss_median5(tls, Td, PA, first, first+uintptr(t)*4, middle, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
		}
	}
	t = t >> int32(3)
	first = ss_median3(tls, Td, PA, first, first+uintptr(t)*4, first+uintptr(t<<libc.Int32FromInt32(1))*4)
	middle = ss_median3(tls, Td, PA, middle-uintptr(t)*4, middle, middle+uintptr(t)*4)
	last = ss_median3(tls, Td, PA, last-uintptr(1)*4-uintptr(t<<libc.Int32FromInt32(1))*4, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
	return ss_median3(tls, Td, PA, first, middle, last)
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Binary partition for substrings. */
func ss_partition(tls *libc.TLS, PA uintptr, first uintptr, last uintptr, depth int32) (r uintptr) {
	var a, b, v3 uintptr
	var t int32
	_, _, _, _ = a, b, t, v3
	a = first - uintptr(1)*4
	b = last
	for {
		for {
			a += 4
			v3 = a
			if !(v3 < b && *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))+depth >= *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a))+int32(1))*4))+int32(1)) {
				break
			}
			*(*int32)(unsafe.Pointer(a)) = ^*(*int32)(unsafe.Pointer(a))
			goto _2
		_2:
		}
		for {
			b -= 4
			v3 = b
			if !(a < v3 && *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))+depth < *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b))+int32(1))*4))+int32(1)) {
				break
			}
			goto _4
		_4:
		}
		if b <= a {
			break
		}
		t = ^*(*int32)(unsafe.Pointer(b))
		*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
		*(*int32)(unsafe.Pointer(a)) = t
		goto _1
	_1:
	}
	if first < a {
		*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
	}
	return a
}

// C documentation
//
//	/* Multikey introsort for medium size groups. */
func ss_mintrosort(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, last uintptr, depth int32) {
	var Td, a, b, c, d, e, f, v12, v15 uintptr
	var limit, s, ssize, t, v, x, v3, v4 int32
	var stack [16]struct {
		Fa uintptr
		Fb uintptr
		Fc int32
		Fd int32
	}
	var v2 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = Td, a, b, c, d, e, f, limit, s, ssize, stack, t, v, x, v12, v15, v2, v3, v4
	x = 0
	ssize = 0
	limit = ss_ilg(tls, int32((int64(last)-int64(first))/4))
	for {
		if (int64(last)-int64(first))/4 <= int64(libc.Int32FromInt32(SS_INSERTIONSORT_THRESHOLD)) {
			if int64(1) < (int64(last)-int64(first))/4 {
				ss_insertionsort(tls, T, PA, first, last, depth)
			}
			if v2 = 0 <= ssize; !v2 {
				libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(48858), uintptr(unsafe.Pointer(&__func__)))
			}
			_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v3 = ssize
			first = stack[v3].Fa
			last = stack[ssize].Fb
			depth = stack[ssize].Fc
			limit = stack[ssize].Fd
			goto _1
		}
		Td = T + uintptr(depth)
		v3 = limit
		limit = limit - 1
		if v3 == 0 {
			ss_heapsort(tls, Td, PA, first, int32((int64(last)-int64(first))/4))
		}
		if limit < 0 {
			a = first + uintptr(1)*4
			v = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first)))*4))))))
			for {
				if !(a < last) {
					break
				}
				v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))))))
				x = v3
				if v3 != v {
					if int64(1) < (int64(a)-int64(first))/4 {
						break
					}
					v = x
					first = a
				}
				goto _5
			_5:
				;
				a += 4
			}
			if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first)))*4))-int32(1))))) < v {
				first = ss_partition(tls, PA, first, a, depth)
			}
			if (int64(a)-int64(first))/4 <= (int64(last)-int64(a))/4 {
				if int64(1) < (int64(a)-int64(first))/4 {
					if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48877), uintptr(unsafe.Pointer(&__func__)))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = a
					stack[ssize].Fb = last
					stack[ssize].Fc = depth
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = -libc.Int32FromInt32(1)
					last = a
					depth = depth + int32(1)
					limit = ss_ilg(tls, int32((int64(a)-int64(first))/4))
				} else {
					first = a
					limit = -libc.Int32FromInt32(1)
				}
			} else {
				if int64(1) < (int64(last)-int64(a))/4 {
					if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48884), uintptr(unsafe.Pointer(&__func__)))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = first
					stack[ssize].Fb = a
					stack[ssize].Fc = depth + int32(1)
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = ss_ilg(tls, int32((int64(a)-int64(first))/4))
					first = a
					limit = -libc.Int32FromInt32(1)
				} else {
					last = a
					depth = depth + int32(1)
					limit = ss_ilg(tls, int32((int64(a)-int64(first))/4))
				}
			}
			goto _1
		}
		/* choose pivot */
		a = ss_pivot(tls, Td, PA, first, last)
		v = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))))))
		t = *(*int32)(unsafe.Pointer(first))
		*(*int32)(unsafe.Pointer(first)) = *(*int32)(unsafe.Pointer(a))
		*(*int32)(unsafe.Pointer(a)) = t
		/* partition */
		b = first
		for {
			b += 4
			v12 = b
			if v2 = v12 < last; v2 {
				v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))))))
				x = v3
			}
			if !(v2 && v3 == v) {
				break
			}
			goto _11
		_11:
		}
		v12 = b
		a = v12
		if v12 < last && x < v {
			for {
				b += 4
				v15 = b
				if v2 = v15 < last; v2 {
					v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))))))
					x = v3
				}
				if !(v2 && v3 <= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
					*(*int32)(unsafe.Pointer(a)) = t
					a += 4
				}
				goto _16
			_16:
			}
		}
		c = last
		for {
			c -= 4
			v12 = c
			if v2 = b < v12; v2 {
				v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4))))))
				x = v3
			}
			if !(v2 && v3 == v) {
				break
			}
			goto _20
		_20:
		}
		v12 = c
		d = v12
		if b < v12 && x > v {
			for {
				c -= 4
				v15 = c
				if v2 = b < v15; v2 {
					v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4))))))
					x = v3
				}
				if !(v2 && v3 >= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(c))
					*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
					*(*int32)(unsafe.Pointer(d)) = t
					d -= 4
				}
				goto _25
			_25:
			}
		}
		for {
			if !(b < c) {
				break
			}
			t = *(*int32)(unsafe.Pointer(b))
			*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(c))
			*(*int32)(unsafe.Pointer(c)) = t
			for {
				b += 4
				v12 = b
				if v2 = v12 < c; v2 {
					v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4))))))
					x = v3
				}
				if !(v2 && v3 <= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
					*(*int32)(unsafe.Pointer(a)) = t
					a += 4
				}
				goto _30
			_30:
			}
			for {
				c -= 4
				v12 = c
				if v2 = b < v12; v2 {
					v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4))))))
					x = v3
				}
				if !(v2 && v3 >= v) {
					break
				}
				if x == v {
					t = *(*int32)(unsafe.Pointer(c))
					*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
					*(*int32)(unsafe.Pointer(d)) = t
					d -= 4
				}
				goto _34
			_34:
			}
			goto _29
		_29:
		}
		if a <= d {
			c = b - uintptr(1)*4
			v3 = int32((int64(a) - int64(first)) / 4)
			s = v3
			v4 = int32((int64(b) - int64(a)) / 4)
			t = v4
			if v3 > v4 {
				s = t
			}
			e = first
			f = b - uintptr(s)*4
			for {
				if !(0 < s) {
					break
				}
				t = *(*int32)(unsafe.Pointer(e))
				*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
				*(*int32)(unsafe.Pointer(f)) = t
				goto _40
			_40:
				;
				s = s - 1
				e += 4
				f += 4
			}
			v3 = int32((int64(d) - int64(c)) / 4)
			s = v3
			v4 = int32((int64(last)-int64(d))/4 - libc.Int64FromInt32(1))
			t = v4
			if v3 > v4 {
				s = t
			}
			e = b
			f = last - uintptr(s)*4
			for {
				if !(0 < s) {
					break
				}
				t = *(*int32)(unsafe.Pointer(e))
				*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
				*(*int32)(unsafe.Pointer(f)) = t
				goto _43
			_43:
				;
				s = s - 1
				e += 4
				f += 4
			}
			a = first + uintptr((int64(b)-int64(a))/4)*4
			c = last - uintptr((int64(d)-int64(c))/4)*4
			if v <= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(a)))*4))-int32(1))))) {
				v12 = a
			} else {
				v12 = ss_partition(tls, PA, a, c, depth)
			}
			b = v12
			if (int64(a)-int64(first))/4 <= (int64(last)-int64(c))/4 {
				if (int64(last)-int64(c))/4 <= (int64(c)-int64(b))/4 {
					if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48934), uintptr(unsafe.Pointer(&__func__)))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = b
					stack[ssize].Fb = c
					stack[ssize].Fc = depth + int32(1)
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48935), uintptr(unsafe.Pointer(&__func__)))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = c
					stack[ssize].Fb = last
					stack[ssize].Fc = depth
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = limit
					last = a
				} else {
					if (int64(a)-int64(first))/4 <= (int64(c)-int64(b))/4 {
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48938), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = c
						stack[ssize].Fb = last
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48939), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = b
						stack[ssize].Fb = c
						stack[ssize].Fc = depth + int32(1)
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
						last = a
					} else {
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48942), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = c
						stack[ssize].Fb = last
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48943), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = first
						stack[ssize].Fb = a
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						first = b
						last = c
						depth = depth + int32(1)
						limit = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					}
				}
			} else {
				if (int64(a)-int64(first))/4 <= (int64(c)-int64(b))/4 {
					if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48948), uintptr(unsafe.Pointer(&__func__)))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = b
					stack[ssize].Fb = c
					stack[ssize].Fc = depth + int32(1)
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48949), uintptr(unsafe.Pointer(&__func__)))
					}
					_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = first
					stack[ssize].Fb = a
					stack[ssize].Fc = depth
					v3 = ssize
					ssize = ssize + 1
					stack[v3].Fd = limit
					first = c
				} else {
					if (int64(last)-int64(c))/4 <= (int64(c)-int64(b))/4 {
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48952), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = first
						stack[ssize].Fb = a
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48953), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = b
						stack[ssize].Fb = c
						stack[ssize].Fc = depth + int32(1)
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = ss_ilg(tls, int32((int64(c)-int64(b))/4))
						first = c
					} else {
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48956), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = first
						stack[ssize].Fb = a
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						if v2 = ssize < int32(SS_MISORT_STACKSIZE); !v2 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(48957), uintptr(unsafe.Pointer(&__func__)))
						}
						_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = c
						stack[ssize].Fb = last
						stack[ssize].Fc = depth
						v3 = ssize
						ssize = ssize + 1
						stack[v3].Fd = limit
						first = b
						last = c
						depth = depth + int32(1)
						limit = ss_ilg(tls, int32((int64(c)-int64(b))/4))
					}
				}
			}
		} else {
			limit = limit + int32(1)
			if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(Td + uintptr(*(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first)))*4))-int32(1))))) < v {
				first = ss_partition(tls, PA, first, last, depth)
				limit = ss_ilg(tls, int32((int64(last)-int64(first))/4))
			}
			depth = depth + int32(1)
		}
		goto _1
	_1:
	}
}

var __func__ = [14]uint8{'s', 's', '_', 'm', 'i', 'n', 't', 'r', 'o', 's', 'o', 'r', 't'}

/*---------------------------------------------------------------------------*/

func ss_blockswap(tls *libc.TLS, a uintptr, b uintptr, n int32) {
	var t int32
	_ = t
	for {
		if !(0 < n) {
			break
		}
		t = *(*int32)(unsafe.Pointer(a))
		*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
		*(*int32)(unsafe.Pointer(b)) = t
		goto _1
	_1:
		;
		n = n - 1
		a += 4
		b += 4
	}
}

func ss_rotate(tls *libc.TLS, first uintptr, middle uintptr, last uintptr) {
	var a, b, v2, v3 uintptr
	var l, r, t int32
	_, _, _, _, _, _, _ = a, b, l, r, t, v2, v3
	l = int32((int64(middle) - int64(first)) / 4)
	r = int32((int64(last) - int64(middle)) / 4)
	for {
		if !(0 < l && 0 < r) {
			break
		}
		if l == r {
			ss_blockswap(tls, first, middle, l)
			break
		}
		if l < r {
			a = last - uintptr(1)*4
			b = middle - libc.UintptrFromInt32(1)*4
			t = *(*int32)(unsafe.Pointer(a))
			for cond := true; cond; cond = int32(1) != 0 {
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
				v3 = b
				b -= 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if b < first {
					*(*int32)(unsafe.Pointer(a)) = t
					last = a
					r = r - (l + int32(1))
					if r <= l {
						break
					}
					a = a - uintptr(1)*4
					b = middle - libc.UintptrFromInt32(1)*4
					t = *(*int32)(unsafe.Pointer(a))
				}
			}
		} else {
			a = first
			b = middle
			t = *(*int32)(unsafe.Pointer(a))
			for cond := true; cond; cond = int32(1) != 0 {
				v2 = a
				a += 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
				v3 = b
				b += 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if last <= b {
					*(*int32)(unsafe.Pointer(a)) = t
					first = a + uintptr(1)*4
					l = l - (r + int32(1))
					if l <= r {
						break
					}
					a = a + uintptr(1)*4
					b = middle
					t = *(*int32)(unsafe.Pointer(a))
				}
			}
		}
		goto _1
	_1:
	}
}

/*---------------------------------------------------------------------------*/

func ss_inplacemerge(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, depth int32) {
	var a, b, p, v4 uintptr
	var half, len1, q, r, x, v3 int32
	_, _, _, _, _, _, _, _, _, _ = a, b, half, len1, p, q, r, x, v3, v4
	for {
		if *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) < 0 {
			x = int32(1)
			p = PA + uintptr(^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)))*4
		} else {
			x = 0
			p = PA + uintptr(*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)))*4
		}
		a = first
		len1 = int32((int64(middle) - int64(first)) / 4)
		half = len1 >> int32(1)
		r = -libc.Int32FromInt32(1)
		for {
			if !(0 < len1) {
				break
			}
			b = a + uintptr(half)*4
			if 0 <= *(*int32)(unsafe.Pointer(b)) {
				v3 = *(*int32)(unsafe.Pointer(b))
			} else {
				v3 = ^*(*int32)(unsafe.Pointer(b))
			}
			q = ss_compare(tls, T, PA+uintptr(v3)*4, p, depth)
			if q < 0 {
				a = b + uintptr(1)*4
				half = half - (len1&int32(1) ^ int32(1))
			} else {
				r = q
			}
			goto _2
		_2:
			;
			len1 = half
			half = half >> int32(1)
		}
		if a < middle {
			if r == 0 {
				*(*int32)(unsafe.Pointer(a)) = ^*(*int32)(unsafe.Pointer(a))
			}
			ss_rotate(tls, a, middle, last)
			last = last - uintptr((int64(middle)-int64(a))/4)*4
			middle = a
			if first == middle {
				break
			}
		}
		last -= 4
		if x != 0 {
			for {
				last -= 4
				v4 = last
				if !(*(*int32)(unsafe.Pointer(v4)) < 0) {
					break
				}
			}
		}
		if middle == last {
			break
		}
		goto _1
	_1:
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Merge-forward with internal buffer. */
func ss_mergeforward(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, buf uintptr, depth int32) {
	var a, b, bufend, c, v2, v3 uintptr
	var r, t int32
	_, _, _, _, _, _, _, _ = a, b, bufend, c, r, t, v2, v3
	bufend = buf + uintptr((int64(middle)-int64(first))/4)*4 - uintptr(1)*4
	ss_blockswap(tls, buf, first, int32((int64(middle)-int64(first))/4))
	v2 = first
	a = v2
	t = *(*int32)(unsafe.Pointer(v2))
	b = buf
	c = middle
	for {
		r = ss_compare(tls, T, PA+uintptr(*(*int32)(unsafe.Pointer(b)))*4, PA+uintptr(*(*int32)(unsafe.Pointer(c)))*4, depth)
		if r < 0 {
			for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
				v2 = a
				a += 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
				if bufend <= b {
					*(*int32)(unsafe.Pointer(bufend)) = t
					return
				}
				v2 = b
				b += 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
			}
		} else {
			if r > 0 {
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
					v2 = a
					a += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
					v3 = c
					c += 4
					*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					if last <= c {
						for b < bufend {
							v2 = a
							a += 4
							*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
							v3 = b
							b += 4
							*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
						}
						*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
						*(*int32)(unsafe.Pointer(b)) = t
						return
					}
				}
			} else {
				*(*int32)(unsafe.Pointer(c)) = ^*(*int32)(unsafe.Pointer(c))
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
					v2 = a
					a += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
					if bufend <= b {
						*(*int32)(unsafe.Pointer(bufend)) = t
						return
					}
					v2 = b
					b += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
				}
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
					v2 = a
					a += 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
					v3 = c
					c += 4
					*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					if last <= c {
						for b < bufend {
							v2 = a
							a += 4
							*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
							v3 = b
							b += 4
							*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
						}
						*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
						*(*int32)(unsafe.Pointer(b)) = t
						return
					}
				}
			}
		}
		goto _1
	_1:
	}
}

// C documentation
//
//	/* Merge-backward with internal buffer. */
func ss_mergebackward(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, buf uintptr, depth int32) {
	var a, b, bufend, c, p1, p2, v2, v3 uintptr
	var r, t, x int32
	_, _, _, _, _, _, _, _, _, _, _ = a, b, bufend, c, p1, p2, r, t, x, v2, v3
	bufend = buf + uintptr((int64(last)-int64(middle))/4)*4 - uintptr(1)*4
	ss_blockswap(tls, buf, middle, int32((int64(last)-int64(middle))/4))
	x = 0
	if *(*int32)(unsafe.Pointer(bufend)) < 0 {
		p1 = PA + uintptr(^*(*int32)(unsafe.Pointer(bufend)))*4
		x = x | int32(1)
	} else {
		p1 = PA + uintptr(*(*int32)(unsafe.Pointer(bufend)))*4
	}
	if *(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)) < 0 {
		p2 = PA + uintptr(^*(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)))*4
		x = x | int32(2)
	} else {
		p2 = PA + uintptr(*(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)))*4
	}
	v2 = last - libc.UintptrFromInt32(1)*4
	a = v2
	t = *(*int32)(unsafe.Pointer(v2))
	b = bufend
	c = middle - libc.UintptrFromInt32(1)*4
	for {
		r = ss_compare(tls, T, p1, p2, depth)
		if 0 < r {
			if x&int32(1) != 0 {
				for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
					v2 = a
					a -= 4
					*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
					v3 = b
					b -= 4
					*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				}
				x = x ^ int32(1)
			}
			v2 = a
			a -= 4
			*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
			if b <= buf {
				*(*int32)(unsafe.Pointer(buf)) = t
				break
			}
			v2 = b
			b -= 4
			*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
			if *(*int32)(unsafe.Pointer(b)) < 0 {
				p1 = PA + uintptr(^*(*int32)(unsafe.Pointer(b)))*4
				x = x | int32(1)
			} else {
				p1 = PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4
			}
		} else {
			if r < 0 {
				if x&int32(2) != 0 {
					for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
						v3 = c
						c -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					x = x ^ int32(2)
				}
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
				v3 = c
				c -= 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if c < first {
					for buf < b {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
						v3 = b
						b -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = t
					break
				}
				if *(*int32)(unsafe.Pointer(c)) < 0 {
					p2 = PA + uintptr(^*(*int32)(unsafe.Pointer(c)))*4
					x = x | int32(2)
				} else {
					p2 = PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4
				}
			} else {
				if x&int32(1) != 0 {
					for cond := true; cond; cond = *(*int32)(unsafe.Pointer(b)) < 0 {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
						v3 = b
						b -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					x = x ^ int32(1)
				}
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = ^*(*int32)(unsafe.Pointer(b))
				if b <= buf {
					*(*int32)(unsafe.Pointer(buf)) = t
					break
				}
				v2 = b
				b -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(a))
				if x&int32(2) != 0 {
					for cond := true; cond; cond = *(*int32)(unsafe.Pointer(c)) < 0 {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
						v3 = c
						c -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					x = x ^ int32(2)
				}
				v2 = a
				a -= 4
				*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(c))
				v3 = c
				c -= 4
				*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
				if c < first {
					for buf < b {
						v2 = a
						a -= 4
						*(*int32)(unsafe.Pointer(v2)) = *(*int32)(unsafe.Pointer(b))
						v3 = b
						b -= 4
						*(*int32)(unsafe.Pointer(v3)) = *(*int32)(unsafe.Pointer(a))
					}
					*(*int32)(unsafe.Pointer(a)) = *(*int32)(unsafe.Pointer(b))
					*(*int32)(unsafe.Pointer(b)) = t
					break
				}
				if *(*int32)(unsafe.Pointer(b)) < 0 {
					p1 = PA + uintptr(^*(*int32)(unsafe.Pointer(b)))*4
					x = x | int32(1)
				} else {
					p1 = PA + uintptr(*(*int32)(unsafe.Pointer(b)))*4
				}
				if *(*int32)(unsafe.Pointer(c)) < 0 {
					p2 = PA + uintptr(^*(*int32)(unsafe.Pointer(c)))*4
					x = x | int32(2)
				} else {
					p2 = PA + uintptr(*(*int32)(unsafe.Pointer(c)))*4
				}
			}
		}
		goto _1
	_1:
	}
}

// C documentation
//
//	/* D&C based merge. */
func ss_swapmerge(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, middle uintptr, last uintptr, buf uintptr, bufsize int32, depth int32) {
	var check, half, len1, m, next, ssize, v2, v5 int32
	var l, lm, r, rm, v20 uintptr
	var stack [32]struct {
		Fa uintptr
		Fb uintptr
		Fc uintptr
		Fd int32
	}
	var v3, v4 bool
	var v17 int64
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = check, half, l, len1, lm, m, next, r, rm, ssize, stack, v17, v2, v20, v3, v4, v5
	check = 0
	ssize = libc.Int32FromInt32(0)
	for {
		if (int64(last)-int64(middle))/4 <= int64(bufsize) {
			if first < middle && middle < last {
				ss_mergebackward(tls, T, PA, first, middle, last, buf, depth)
			}
			if v4 = check&int32(1) != 0; !v4 {
				if v3 = check&int32(2) != 0; v3 {
					if 0 <= *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)) {
						v2 = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					} else {
						v2 = ^*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					}
				}
			}
			if v4 || v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(first)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
			}
			if v3 = check&int32(4) != 0; v3 {
				if 0 <= *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) {
					v2 = *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				} else {
					v2 = ^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				}
			}
			if v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(last)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(last)) = ^*(*int32)(unsafe.Pointer(last))
			}
			if v3 = 0 <= ssize; !v3 {
				libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49211), uintptr(unsafe.Pointer(&__func__1)))
			}
			_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v2 = ssize
			first = stack[v2].Fa
			middle = stack[ssize].Fb
			last = stack[ssize].Fc
			check = stack[ssize].Fd
			goto _1
		}
		if (int64(middle)-int64(first))/4 <= int64(bufsize) {
			if first < middle {
				ss_mergeforward(tls, T, PA, first, middle, last, buf, depth)
			}
			if v4 = check&int32(1) != 0; !v4 {
				if v3 = check&int32(2) != 0; v3 {
					if 0 <= *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)) {
						v2 = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					} else {
						v2 = ^*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					}
				}
			}
			if v4 || v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(first)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
			}
			if v3 = check&int32(4) != 0; v3 {
				if 0 <= *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) {
					v2 = *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				} else {
					v2 = ^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				}
			}
			if v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(last)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(last)) = ^*(*int32)(unsafe.Pointer(last))
			}
			if v3 = 0 <= ssize; !v3 {
				libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49220), uintptr(unsafe.Pointer(&__func__1)))
			}
			_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v2 = ssize
			first = stack[v2].Fa
			middle = stack[ssize].Fb
			last = stack[ssize].Fc
			check = stack[ssize].Fd
			goto _1
		}
		m = 0
		if (int64(middle)-int64(first))/4 < (int64(last)-int64(middle))/4 {
			v17 = (int64(middle) - int64(first)) / 4
		} else {
			v17 = (int64(last) - int64(middle)) / 4
		}
		len1 = int32(v17)
		half = len1 >> libc.Int32FromInt32(1)
		for {
			if !(0 < len1) {
				break
			}
			if 0 <= *(*int32)(unsafe.Pointer(middle + uintptr(m)*4 + uintptr(half)*4)) {
				v2 = *(*int32)(unsafe.Pointer(middle + uintptr(m)*4 + uintptr(half)*4))
			} else {
				v2 = ^*(*int32)(unsafe.Pointer(middle + uintptr(m)*4 + uintptr(half)*4))
			}
			if 0 <= *(*int32)(unsafe.Pointer(middle - uintptr(m)*4 - uintptr(half)*4 - libc.UintptrFromInt32(1)*4)) {
				v5 = *(*int32)(unsafe.Pointer(middle - uintptr(m)*4 - uintptr(half)*4 - libc.UintptrFromInt32(1)*4))
			} else {
				v5 = ^*(*int32)(unsafe.Pointer(middle - uintptr(m)*4 - uintptr(half)*4 - libc.UintptrFromInt32(1)*4))
			}
			if ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(v5)*4, depth) < 0 {
				m = m + (half + int32(1))
				half = half - (len1&int32(1) ^ int32(1))
			}
			goto _16
		_16:
			;
			len1 = half
			half = half >> int32(1)
		}
		if 0 < m {
			lm = middle - uintptr(m)*4
			rm = middle + uintptr(m)*4
			ss_blockswap(tls, lm, middle, m)
			v20 = middle
			r = v20
			l = v20
			next = libc.Int32FromInt32(0)
			if rm < last {
				if *(*int32)(unsafe.Pointer(rm)) < 0 {
					*(*int32)(unsafe.Pointer(rm)) = ^*(*int32)(unsafe.Pointer(rm))
					if first < lm {
						for {
							l -= 4
							v20 = l
							if !(*(*int32)(unsafe.Pointer(v20)) < 0) {
								break
							}
							goto _21
						_21:
						}
						next = next | int32(4)
					}
					next = next | int32(1)
				} else {
					if first < lm {
						for {
							if !(*(*int32)(unsafe.Pointer(r)) < 0) {
								break
							}
							goto _23
						_23:
							;
							r += 4
						}
						next = next | int32(2)
					}
				}
			}
			if (int64(l)-int64(first))/4 <= (int64(last)-int64(r))/4 {
				if v3 = ssize < int32(SS_SMERGE_STACKSIZE); !v3 {
					libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49250), uintptr(unsafe.Pointer(&__func__1)))
				}
				_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
				stack[ssize].Fa = r
				stack[ssize].Fb = rm
				stack[ssize].Fc = last
				v2 = ssize
				ssize = ssize + 1
				stack[v2].Fd = next&libc.Int32FromInt32(3) | check&libc.Int32FromInt32(4)
				middle = lm
				last = l
				check = check&libc.Int32FromInt32(3) | next&libc.Int32FromInt32(4)
			} else {
				if next&int32(2) != 0 && r == middle {
					next = next ^ int32(6)
				}
				if v3 = ssize < int32(SS_SMERGE_STACKSIZE); !v3 {
					libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49254), uintptr(unsafe.Pointer(&__func__1)))
				}
				_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
				stack[ssize].Fa = first
				stack[ssize].Fb = lm
				stack[ssize].Fc = l
				v2 = ssize
				ssize = ssize + 1
				stack[v2].Fd = check&libc.Int32FromInt32(3) | next&libc.Int32FromInt32(4)
				first = r
				middle = rm
				check = next&libc.Int32FromInt32(3) | check&libc.Int32FromInt32(4)
			}
		} else {
			if 0 <= *(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4)) {
				v2 = *(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4))
			} else {
				v2 = ^*(*int32)(unsafe.Pointer(middle - libc.UintptrFromInt32(1)*4))
			}
			if ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(middle)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(middle)) = ^*(*int32)(unsafe.Pointer(middle))
			}
			if v4 = check&int32(1) != 0; !v4 {
				if v3 = check&int32(2) != 0; v3 {
					if 0 <= *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)) {
						v2 = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					} else {
						v2 = ^*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
					}
				}
			}
			if v4 || v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(first)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(first)) = ^*(*int32)(unsafe.Pointer(first))
			}
			if v3 = check&int32(4) != 0; v3 {
				if 0 <= *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4)) {
					v2 = *(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				} else {
					v2 = ^*(*int32)(unsafe.Pointer(last - libc.UintptrFromInt32(1)*4))
				}
			}
			if v3 && ss_compare(tls, T, PA+uintptr(v2)*4, PA+uintptr(*(*int32)(unsafe.Pointer(last)))*4, depth) == 0 {
				*(*int32)(unsafe.Pointer(last)) = ^*(*int32)(unsafe.Pointer(last))
			}
			if v3 = 0 <= ssize; !v3 {
				libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49262), uintptr(unsafe.Pointer(&__func__1)))
			}
			_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if ssize == 0 {
				return
			}
			ssize = ssize - 1
			v2 = ssize
			first = stack[v2].Fa
			middle = stack[ssize].Fb
			last = stack[ssize].Fc
			check = stack[ssize].Fd
		}
		goto _1
	_1:
	}
}

var __func__1 = [13]uint8{'s', 's', '_', 's', 'w', 'a', 'p', 'm', 'e', 'r', 'g', 'e'}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Substring sort */
func sssort(tls *libc.TLS, T uintptr, PA uintptr, first uintptr, last uintptr, buf uintptr, bufsize int32, depth int32, n int32, lastsuffix int32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var a, b, curbuf, middle, v3 uintptr
	var curbufsize, i, j, k, limit, v1 int32
	var v2 bool
	var _ /* PAi at bp+0 */ [2]int32
	_, _, _, _, _, _, _, _, _, _, _, _ = a, b, curbuf, curbufsize, i, j, k, limit, middle, v1, v2, v3
	if lastsuffix != 0 {
		first += 4
	}
	if v2 = bufsize < int32(SS_BLOCKSIZE) && int64(bufsize) < (int64(last)-int64(first))/4; v2 {
		v1 = ss_isqrt(tls, int32((int64(last)-int64(first))/4))
		limit = v1
	}
	if v2 && bufsize < v1 {
		if int32(SS_BLOCKSIZE) < limit {
			limit = int32(SS_BLOCKSIZE)
		}
		v3 = last - uintptr(limit)*4
		middle = v3
		buf = v3
		bufsize = limit
	} else {
		middle = last
		limit = libc.Int32FromInt32(0)
	}
	a = first
	i = libc.Int32FromInt32(0)
	for {
		if !(int64(libc.Int32FromInt32(SS_BLOCKSIZE)) < (int64(middle)-int64(a))/4) {
			break
		}
		ss_mintrosort(tls, T, PA, a, a+uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4, depth)
		curbufsize = int32((int64(last) - int64(a+uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4)) / 4)
		curbuf = a + uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4
		if curbufsize <= bufsize {
			curbufsize = bufsize
			curbuf = buf
		}
		b = a
		k = int32(SS_BLOCKSIZE)
		j = i
		for {
			if !(j&int32(1) != 0) {
				break
			}
			ss_swapmerge(tls, T, PA, b-uintptr(k)*4, b, b+uintptr(k)*4, curbuf, curbufsize, depth)
			goto _5
		_5:
			;
			b = b - uintptr(k)*4
			k = k << int32(1)
			j = j >> int32(1)
		}
		goto _4
	_4:
		;
		a = a + uintptr(libc.Int32FromInt32(SS_BLOCKSIZE))*4
		i = i + 1
	}
	ss_mintrosort(tls, T, PA, a, middle, depth)
	k = int32(SS_BLOCKSIZE)
	for {
		if !(i != 0) {
			break
		}
		if i&int32(1) != 0 {
			ss_swapmerge(tls, T, PA, a-uintptr(k)*4, a, middle, buf, bufsize, depth)
			a = a - uintptr(k)*4
		}
		goto _6
	_6:
		;
		k = k << int32(1)
		i = i >> int32(1)
	}
	if limit != 0 {
		ss_mintrosort(tls, T, PA, middle, last, depth)
		ss_inplacemerge(tls, T, PA, first, middle, last, depth)
	}
	if lastsuffix != 0 {
		(*(*[2]int32)(unsafe.Pointer(bp)))[0] = *(*int32)(unsafe.Pointer(PA + uintptr(*(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4)))*4))
		(*(*[2]int32)(unsafe.Pointer(bp)))[int32(1)] = n - libc.Int32FromInt32(2)
		a = first
		i = *(*int32)(unsafe.Pointer(first - libc.UintptrFromInt32(1)*4))
		for {
			if !(a < last && (*(*int32)(unsafe.Pointer(a)) < 0 || 0 < ss_compare(tls, T, bp, PA+uintptr(*(*int32)(unsafe.Pointer(a)))*4, depth))) {
				break
			}
			*(*int32)(unsafe.Pointer(a - libc.UintptrFromInt32(1)*4)) = *(*int32)(unsafe.Pointer(a))
			goto _7
		_7:
			;
			a += 4
		}
		*(*int32)(unsafe.Pointer(a - libc.UintptrFromInt32(1)*4)) = i
	}
}

/*---------------------------------------------------------------------------*/

func tr_ilg(tls *libc.TLS, n int32) (r int32) {
	var v1, v2, v3 int32
	_, _, _ = v1, v2, v3
	if libc.Uint32FromInt32(n)&uint32(0xffff0000) != 0 {
		if libc.Uint32FromInt32(n)&uint32(0xff000000) != 0 {
			v2 = int32(24) + lg_table[n>>int32(24)&int32(0xff)]
		} else {
			v2 = int32(16) + lg_table[n>>int32(16)&int32(0xff)]
		}
		v1 = v2
	} else {
		if n&int32(0x0000ff00) != 0 {
			v3 = int32(8) + lg_table[n>>int32(8)&int32(0xff)]
		} else {
			v3 = 0 + lg_table[n>>0&int32(0xff)]
		}
		v1 = v3
	}
	return v1
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Simple insertionsort for small size groups. */
func tr_insertionsort(tls *libc.TLS, ISAd uintptr, first uintptr, last uintptr) {
	var a, b, v4 uintptr
	var r, t, v3 int32
	_, _, _, _, _, _ = a, b, r, t, v3, v4
	a = first + uintptr(1)*4
	for {
		if !(a < last) {
			break
		}
		t = *(*int32)(unsafe.Pointer(a))
		b = a - libc.UintptrFromInt32(1)*4
		for {
			v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(t)*4)) - *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
			r = v3
			if !(0 > v3) {
				break
			}
			for {
				*(*int32)(unsafe.Pointer(b + libc.UintptrFromInt32(1)*4)) = *(*int32)(unsafe.Pointer(b))
				goto _5
			_5:
				;
				b -= 4
				v4 = b
				if !(first <= v4 && *(*int32)(unsafe.Pointer(b)) < 0) {
					break
				}
			}
			if b < first {
				break
			}
			goto _2
		_2:
		}
		if r == 0 {
			*(*int32)(unsafe.Pointer(b)) = ^*(*int32)(unsafe.Pointer(b))
		}
		*(*int32)(unsafe.Pointer(b + libc.UintptrFromInt32(1)*4)) = t
		goto _1
	_1:
		;
		a += 4
	}
}

/*---------------------------------------------------------------------------*/

func tr_fixdown(tls *libc.TLS, ISAd uintptr, SA uintptr, i int32, size int32) {
	var c, d, e, j, k, v, v2, v3, v4 int32
	_, _, _, _, _, _, _, _, _ = c, d, e, j, k, v, v2, v3, v4
	v = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
	c = *(*int32)(unsafe.Pointer(ISAd + uintptr(v)*4))
	for {
		v2 = libc.Int32FromInt32(2)*i + libc.Int32FromInt32(1)
		j = v2
		if !(v2 < size) {
			break
		}
		v4 = j
		j = j + 1
		v3 = v4
		k = v3
		d = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(v3)*4)))*4))
		v2 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(j)*4)))*4))
		e = v2
		if d < v2 {
			k = j
			d = e
		}
		if d <= c {
			break
		}
		goto _1
	_1:
		;
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(k)*4))
		i = k
	}
	*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = v
}

// C documentation
//
//	/* Simple top-down heapsort. */
func tr_heapsort(tls *libc.TLS, ISAd uintptr, SA uintptr, size int32) {
	var i, m, t int32
	_, _, _ = i, m, t
	m = size
	if size%int32(2) == 0 {
		m = m - 1
		if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)))*4)) < *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)))*4)) {
			t = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4))
			*(*int32)(unsafe.Pointer(SA + uintptr(m/int32(2))*4)) = t
		}
	}
	i = m/int32(2) - int32(1)
	for {
		if !(0 <= i) {
			break
		}
		tr_fixdown(tls, ISAd, SA, i, m)
		goto _1
	_1:
		;
		i = i - 1
	}
	if size%int32(2) == 0 {
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(m)*4))
		*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = t
		tr_fixdown(tls, ISAd, SA, 0, m)
	}
	i = m - int32(1)
	for {
		if !(0 < i) {
			break
		}
		t = *(*int32)(unsafe.Pointer(SA))
		*(*int32)(unsafe.Pointer(SA)) = *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
		tr_fixdown(tls, ISAd, SA, 0, i)
		*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = t
		goto _2
	_2:
		;
		i = i - 1
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Returns the median of three elements. */
func tr_median3(tls *libc.TLS, ISAd uintptr, v1 uintptr, v2 uintptr, v3 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) {
		t = v1
		v1 = v2
		v2 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
		if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
			return v1
		} else {
			return v3
		}
	}
	return v2
}

// C documentation
//
//	/* Returns the median of five elements. */
func tr_median5(tls *libc.TLS, ISAd uintptr, v1 uintptr, v2 uintptr, v3 uintptr, v4 uintptr, v5 uintptr) (r uintptr) {
	var t uintptr
	_ = t
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
		t = v2
		v2 = v3
		v3 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v5)))*4)) {
		t = v4
		v4 = v5
		v5 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v2)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) {
		t = v2
		v2 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) {
		t = v1
		v1 = v3
		v3 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v1)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) {
		t = v1
		v1 = v4
		v4 = t
		t = v3
		v3 = v5
		v5 = t
	}
	if *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v3)))*4)) > *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(v4)))*4)) {
		return v4
	}
	return v3
}

// C documentation
//
//	/* Returns the pivot element. */
func tr_pivot(tls *libc.TLS, ISAd uintptr, first uintptr, last uintptr) (r uintptr) {
	var middle uintptr
	var t int32
	_, _ = middle, t
	t = int32((int64(last) - int64(first)) / 4)
	middle = first + uintptr(t/int32(2))*4
	if t <= int32(512) {
		if t <= int32(32) {
			return tr_median3(tls, ISAd, first, middle, last-uintptr(1)*4)
		} else {
			t = t >> int32(2)
			return tr_median5(tls, ISAd, first, first+uintptr(t)*4, middle, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
		}
	}
	t = t >> int32(3)
	first = tr_median3(tls, ISAd, first, first+uintptr(t)*4, first+uintptr(t<<libc.Int32FromInt32(1))*4)
	middle = tr_median3(tls, ISAd, middle-uintptr(t)*4, middle, middle+uintptr(t)*4)
	last = tr_median3(tls, ISAd, last-uintptr(1)*4-uintptr(t<<libc.Int32FromInt32(1))*4, last-uintptr(1)*4-uintptr(t)*4, last-uintptr(1)*4)
	return tr_median3(tls, ISAd, first, middle, last)
}

/*---------------------------------------------------------------------------*/

type trbudget_t = struct {
	Fchance int32
	Fremain int32
	Fincval int32
	Fcount  int32
}

/*---------------------------------------------------------------------------*/

type _trbudget_t = trbudget_t

func trbudget_init(tls *libc.TLS, budget uintptr, chance int32, incval int32) {
	var v1 int32
	_ = v1
	(*trbudget_t)(unsafe.Pointer(budget)).Fchance = chance
	v1 = incval
	(*trbudget_t)(unsafe.Pointer(budget)).Fincval = v1
	(*trbudget_t)(unsafe.Pointer(budget)).Fremain = v1
}

func trbudget_check(tls *libc.TLS, budget uintptr, size int32) (r int32) {
	if size <= (*trbudget_t)(unsafe.Pointer(budget)).Fremain {
		*(*int32)(unsafe.Pointer(budget + 4)) -= size
		return int32(1)
	}
	if (*trbudget_t)(unsafe.Pointer(budget)).Fchance == 0 {
		*(*int32)(unsafe.Pointer(budget + 12)) += size
		return 0
	}
	*(*int32)(unsafe.Pointer(budget + 4)) += (*trbudget_t)(unsafe.Pointer(budget)).Fincval - size
	*(*int32)(unsafe.Pointer(budget)) -= int32(1)
	return int32(1)
}

/*---------------------------------------------------------------------------*/

func tr_partition(tls *libc.TLS, ISAd uintptr, first uintptr, middle uintptr, last uintptr, pa uintptr, pb uintptr, v int32) {
	var a, b, c, d, e, f, v2, v5 uintptr
	var s, t, x, v3, v8 int32
	var v4 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _ = a, b, c, d, e, f, s, t, x, v2, v3, v4, v5, v8
	x = 0
	b = middle - uintptr(1)*4
	for {
		b += 4
		v2 = b
		if v4 = v2 < last; v4 {
			v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
			x = v3
		}
		if !(v4 && v3 == v) {
			break
		}
		goto _1
	_1:
	}
	v2 = b
	a = v2
	if v2 < last && x < v {
		for {
			b += 4
			v5 = b
			if v4 = v5 < last; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
				x = v3
			}
			if !(v4 && v3 <= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(b))
				*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
				*(*int32)(unsafe.Pointer(a)) = t
				a += 4
			}
			goto _6
		_6:
		}
	}
	c = last
	for {
		c -= 4
		v2 = c
		if v4 = b < v2; v4 {
			v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(c)))*4))
			x = v3
		}
		if !(v4 && v3 == v) {
			break
		}
		goto _10
	_10:
	}
	v2 = c
	d = v2
	if b < v2 && x > v {
		for {
			c -= 4
			v5 = c
			if v4 = b < v5; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(c)))*4))
				x = v3
			}
			if !(v4 && v3 >= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(c))
				*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
				*(*int32)(unsafe.Pointer(d)) = t
				d -= 4
			}
			goto _15
		_15:
		}
	}
	for {
		if !(b < c) {
			break
		}
		t = *(*int32)(unsafe.Pointer(b))
		*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(c))
		*(*int32)(unsafe.Pointer(c)) = t
		for {
			b += 4
			v2 = b
			if v4 = v2 < c; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(b)))*4))
				x = v3
			}
			if !(v4 && v3 <= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(b))
				*(*int32)(unsafe.Pointer(b)) = *(*int32)(unsafe.Pointer(a))
				*(*int32)(unsafe.Pointer(a)) = t
				a += 4
			}
			goto _20
		_20:
		}
		for {
			c -= 4
			v2 = c
			if v4 = b < v2; v4 {
				v3 = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(c)))*4))
				x = v3
			}
			if !(v4 && v3 >= v) {
				break
			}
			if x == v {
				t = *(*int32)(unsafe.Pointer(c))
				*(*int32)(unsafe.Pointer(c)) = *(*int32)(unsafe.Pointer(d))
				*(*int32)(unsafe.Pointer(d)) = t
				d -= 4
			}
			goto _24
		_24:
		}
		goto _19
	_19:
	}
	if a <= d {
		c = b - uintptr(1)*4
		v3 = int32((int64(a) - int64(first)) / 4)
		s = v3
		v8 = int32((int64(b) - int64(a)) / 4)
		t = v8
		if v3 > v8 {
			s = t
		}
		e = first
		f = b - uintptr(s)*4
		for {
			if !(0 < s) {
				break
			}
			t = *(*int32)(unsafe.Pointer(e))
			*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
			*(*int32)(unsafe.Pointer(f)) = t
			goto _30
		_30:
			;
			s = s - 1
			e += 4
			f += 4
		}
		v3 = int32((int64(d) - int64(c)) / 4)
		s = v3
		v8 = int32((int64(last)-int64(d))/4 - libc.Int64FromInt32(1))
		t = v8
		if v3 > v8 {
			s = t
		}
		e = b
		f = last - uintptr(s)*4
		for {
			if !(0 < s) {
				break
			}
			t = *(*int32)(unsafe.Pointer(e))
			*(*int32)(unsafe.Pointer(e)) = *(*int32)(unsafe.Pointer(f))
			*(*int32)(unsafe.Pointer(f)) = t
			goto _33
		_33:
			;
			s = s - 1
			e += 4
			f += 4
		}
		first = first + uintptr((int64(b)-int64(a))/4)*4
		last = last - uintptr((int64(d)-int64(c))/4)*4
	}
	*(*uintptr)(unsafe.Pointer(pa)) = first
	*(*uintptr)(unsafe.Pointer(pb)) = last
}

func tr_copy(tls *libc.TLS, ISA uintptr, SA uintptr, first uintptr, a uintptr, b uintptr, last uintptr, depth int32) {
	var c, d, e, v3 uintptr
	var s, v, v2 int32
	_, _, _, _, _, _, _ = c, d, e, s, v, v2, v3
	v = int32((int64(b)-int64(SA))/4 - int64(1))
	c = first
	d = a - libc.UintptrFromInt32(1)*4
	for {
		if !(c <= d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d += 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = int32((int64(d) - int64(SA)) / 4)
		}
		goto _1
	_1:
		;
		c += 4
	}
	c = last - uintptr(1)*4
	e = d + uintptr(1)*4
	d = b
	for {
		if !(e < d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d -= 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = int32((int64(d) - int64(SA)) / 4)
		}
		goto _4
	_4:
		;
		c -= 4
	}
}

func tr_partialcopy(tls *libc.TLS, ISA uintptr, SA uintptr, first uintptr, a uintptr, b uintptr, last uintptr, depth int32) {
	var c, d, e, v3 uintptr
	var lastrank, newrank, rank, s, v, v2 int32
	_, _, _, _, _, _, _, _, _, _ = c, d, e, lastrank, newrank, rank, s, v, v2, v3
	newrank = -int32(1)
	v = int32((int64(b)-int64(SA))/4 - int64(1))
	lastrank = -int32(1)
	c = first
	d = a - libc.UintptrFromInt32(1)*4
	for {
		if !(c <= d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d += 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			rank = *(*int32)(unsafe.Pointer(ISA + uintptr(s+depth)*4))
			if lastrank != rank {
				lastrank = rank
				newrank = int32((int64(d) - int64(SA)) / 4)
			}
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = newrank
		}
		goto _1
	_1:
		;
		c += 4
	}
	lastrank = -int32(1)
	e = d
	for {
		if !(first <= e) {
			break
		}
		rank = *(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(e)))*4))
		if lastrank != rank {
			lastrank = rank
			newrank = int32((int64(e) - int64(SA)) / 4)
		}
		if newrank != rank {
			*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(e)))*4)) = newrank
		}
		goto _4
	_4:
		;
		e -= 4
	}
	lastrank = -int32(1)
	c = last - uintptr(1)*4
	e = d + uintptr(1)*4
	d = b
	for {
		if !(e < d) {
			break
		}
		v2 = *(*int32)(unsafe.Pointer(c)) - depth
		s = v2
		if 0 <= v2 && *(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) == v {
			d -= 4
			v3 = d
			*(*int32)(unsafe.Pointer(v3)) = s
			rank = *(*int32)(unsafe.Pointer(ISA + uintptr(s+depth)*4))
			if lastrank != rank {
				lastrank = rank
				newrank = int32((int64(d) - int64(SA)) / 4)
			}
			*(*int32)(unsafe.Pointer(ISA + uintptr(s)*4)) = newrank
		}
		goto _5
	_5:
		;
		c -= 4
	}
}

func tr_introsort(tls *libc.TLS, ISA uintptr, ISAd uintptr, SA uintptr, first uintptr, last uintptr, budget uintptr) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var c, v19 uintptr
	var incr, limit, next, ssize, t, trlink, v, x, v5 int32
	var stack [64]struct {
		Fa uintptr
		Fb uintptr
		Fc uintptr
		Fd int32
		Fe int32
	}
	var v4 bool
	var _ /* a at bp+0 */ uintptr
	var _ /* b at bp+8 */ uintptr
	_, _, _, _, _, _, _, _, _, _, _, _, _ = c, incr, limit, next, ssize, stack, t, trlink, v, x, v19, v4, v5
	x = 0
	incr = int32((int64(ISAd) - int64(ISA)) / 4)
	trlink = -int32(1)
	ssize = 0
	limit = tr_ilg(tls, int32((int64(last)-int64(first))/4))
	for {
		if limit < 0 {
			if limit == -int32(1) {
				/* tandem repeat partition */
				tr_partition(tls, ISAd-uintptr(incr)*4, first, first, last, bp, bp+8, int32((int64(last)-int64(SA))/4-int64(1)))
				/* update ranks */
				if *(*uintptr)(unsafe.Pointer(bp)) < last {
					c = first
					v = int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(SA))/4 - libc.Int64FromInt32(1))
					for {
						if !(c < *(*uintptr)(unsafe.Pointer(bp))) {
							break
						}
						*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
						goto _2
					_2:
						;
						c += 4
					}
				}
				if *(*uintptr)(unsafe.Pointer(bp + 8)) < last {
					c = *(*uintptr)(unsafe.Pointer(bp))
					v = int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(SA))/4 - libc.Int64FromInt32(1))
					for {
						if !(c < *(*uintptr)(unsafe.Pointer(bp + 8))) {
							break
						}
						*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
						goto _3
					_3:
						;
						c += 4
					}
				}
				/* push */
				if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
					if v4 = ssize < int32(TR_STACKSIZE); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49644), uintptr(unsafe.Pointer(&__func__2)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = libc.UintptrFromInt32(0)
					stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
					stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
					stack[ssize].Fd = 0
					v5 = ssize
					ssize = ssize + 1
					stack[v5].Fe = libc.Int32FromInt32(0)
					if v4 = ssize < int32(TR_STACKSIZE); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49645), uintptr(unsafe.Pointer(&__func__2)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					stack[ssize].Fa = ISAd - uintptr(incr)*4
					stack[ssize].Fb = first
					stack[ssize].Fc = last
					stack[ssize].Fd = -int32(2)
					v5 = ssize
					ssize = ssize + 1
					stack[v5].Fe = trlink
					trlink = ssize - int32(2)
				}
				if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
					if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
						if v4 = ssize < int32(TR_STACKSIZE); !v4 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49650), uintptr(unsafe.Pointer(&__func__2)))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
						stack[ssize].Fc = last
						stack[ssize].Fd = tr_ilg(tls, int32((int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4))
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						last = *(*uintptr)(unsafe.Pointer(bp))
						limit = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4))
					} else {
						if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
							first = *(*uintptr)(unsafe.Pointer(bp + 8))
							limit = tr_ilg(tls, int32((int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4))
						} else {
							if v4 = 0 <= ssize; !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49655), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				} else {
					if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
						if v4 = ssize < int32(TR_STACKSIZE); !v4 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49659), uintptr(unsafe.Pointer(&__func__2)))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = first
						stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
						stack[ssize].Fd = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4))
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						first = *(*uintptr)(unsafe.Pointer(bp + 8))
						limit = tr_ilg(tls, int32((int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4))
					} else {
						if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
							last = *(*uintptr)(unsafe.Pointer(bp))
							limit = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4))
						} else {
							if v4 = 0 <= ssize; !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49664), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				}
			} else {
				if limit == -int32(2) {
					/* tandem repeat copy */
					ssize = ssize - 1
					v5 = ssize
					*(*uintptr)(unsafe.Pointer(bp)) = stack[v5].Fb
					/* tandem repeat copy */
					*(*uintptr)(unsafe.Pointer(bp + 8)) = stack[ssize].Fc
					if stack[ssize].Fd == 0 {
						tr_copy(tls, ISA, SA, first, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), last, int32((int64(ISAd)-int64(ISA))/4))
					} else {
						if 0 <= trlink {
							stack[trlink].Fd = -int32(1)
						}
						tr_partialcopy(tls, ISA, SA, first, *(*uintptr)(unsafe.Pointer(bp)), *(*uintptr)(unsafe.Pointer(bp + 8)), last, int32((int64(ISAd)-int64(ISA))/4))
					}
					if v4 = 0 <= ssize; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49676), uintptr(unsafe.Pointer(&__func__2)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if ssize == 0 {
						return
					}
					ssize = ssize - 1
					v5 = ssize
					ISAd = stack[v5].Fa
					first = stack[ssize].Fb
					last = stack[ssize].Fc
					limit = stack[ssize].Fd
					trlink = stack[ssize].Fe
				} else {
					/* sorted partition */
					if 0 <= *(*int32)(unsafe.Pointer(first)) {
						*(*uintptr)(unsafe.Pointer(bp)) = first
						for {
							*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) = int32((int64(*(*uintptr)(unsafe.Pointer(bp))) - int64(SA)) / 4)
							goto _20
						_20:
							;
							*(*uintptr)(unsafe.Pointer(bp)) += 4
							v19 = *(*uintptr)(unsafe.Pointer(bp))
							if !(v19 < last && 0 <= *(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp))))) {
								break
							}
						}
						first = *(*uintptr)(unsafe.Pointer(bp))
					}
					if first < last {
						*(*uintptr)(unsafe.Pointer(bp)) = first
						for {
							*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))) = ^*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp))))
							goto _22
						_22:
							;
							*(*uintptr)(unsafe.Pointer(bp)) += 4
							v19 = *(*uintptr)(unsafe.Pointer(bp))
							if !(*(*int32)(unsafe.Pointer(v19)) < 0) {
								break
							}
						}
						if *(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) != *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) {
							v5 = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4+int64(1)))
						} else {
							v5 = -int32(1)
						}
						next = v5
						*(*uintptr)(unsafe.Pointer(bp)) += 4
						v19 = *(*uintptr)(unsafe.Pointer(bp))
						if v19 < last {
							*(*uintptr)(unsafe.Pointer(bp + 8)) = first
							v = int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(SA))/4 - libc.Int64FromInt32(1))
							for {
								if !(*(*uintptr)(unsafe.Pointer(bp + 8)) < *(*uintptr)(unsafe.Pointer(bp))) {
									break
								}
								*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8)))))*4)) = v
								goto _25
							_25:
								;
								*(*uintptr)(unsafe.Pointer(bp + 8)) += 4
							}
						}
						/* push */
						if trbudget_check(tls, budget, int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4)) != 0 {
							if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49692), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = last
								stack[ssize].Fd = -int32(3)
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								ISAd = ISAd + uintptr(incr)*4
								last = *(*uintptr)(unsafe.Pointer(bp))
								limit = next
							} else {
								if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
									if v4 = ssize < int32(TR_STACKSIZE); !v4 {
										libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49696), uintptr(unsafe.Pointer(&__func__2)))
									}
									_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
									stack[ssize].Fa = ISAd + uintptr(incr)*4
									stack[ssize].Fb = first
									stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
									stack[ssize].Fd = next
									v5 = ssize
									ssize = ssize + 1
									stack[v5].Fe = trlink
									first = *(*uintptr)(unsafe.Pointer(bp))
									limit = -libc.Int32FromInt32(3)
								} else {
									ISAd = ISAd + uintptr(incr)*4
									last = *(*uintptr)(unsafe.Pointer(bp))
									limit = next
								}
							}
						} else {
							if 0 <= trlink {
								stack[trlink].Fd = -int32(1)
							}
							if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
								first = *(*uintptr)(unsafe.Pointer(bp))
								limit = -libc.Int32FromInt32(3)
							} else {
								if v4 = 0 <= ssize; !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49707), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								if ssize == 0 {
									return
								}
								ssize = ssize - 1
								v5 = ssize
								ISAd = stack[v5].Fa
								first = stack[ssize].Fb
								last = stack[ssize].Fc
								limit = stack[ssize].Fd
								trlink = stack[ssize].Fe
							}
						}
					} else {
						if v4 = 0 <= ssize; !v4 {
							libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49711), uintptr(unsafe.Pointer(&__func__2)))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						if ssize == 0 {
							return
						}
						ssize = ssize - 1
						v5 = ssize
						ISAd = stack[v5].Fa
						first = stack[ssize].Fb
						last = stack[ssize].Fc
						limit = stack[ssize].Fd
						trlink = stack[ssize].Fe
					}
				}
			}
			goto _1
		}
		if (int64(last)-int64(first))/4 <= int64(libc.Int32FromInt32(TR_INSERTIONSORT_THRESHOLD)) {
			tr_insertionsort(tls, ISAd, first, last)
			limit = -int32(3)
			goto _1
		}
		v5 = limit
		limit = limit - 1
		if v5 == 0 {
			tr_heapsort(tls, ISAd, first, int32((int64(last)-int64(first))/4))
			*(*uintptr)(unsafe.Pointer(bp)) = last - uintptr(1)*4
			for {
				if !(first < *(*uintptr)(unsafe.Pointer(bp))) {
					break
				}
				x = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4))
				*(*uintptr)(unsafe.Pointer(bp + 8)) = *(*uintptr)(unsafe.Pointer(bp)) - libc.UintptrFromInt32(1)*4
				for {
					if !(first <= *(*uintptr)(unsafe.Pointer(bp + 8)) && *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8)))))*4)) == x) {
						break
					}
					*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8)))) = ^*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp + 8))))
					goto _36
				_36:
					;
					*(*uintptr)(unsafe.Pointer(bp + 8)) -= 4
				}
				goto _35
			_35:
				;
				*(*uintptr)(unsafe.Pointer(bp)) = *(*uintptr)(unsafe.Pointer(bp + 8))
			}
			limit = -int32(3)
			goto _1
		}
		/* choose pivot */
		*(*uintptr)(unsafe.Pointer(bp)) = tr_pivot(tls, ISAd, first, last)
		t = *(*int32)(unsafe.Pointer(first))
		*(*int32)(unsafe.Pointer(first)) = *(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp))))
		*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))) = t
		v = *(*int32)(unsafe.Pointer(ISAd + uintptr(*(*int32)(unsafe.Pointer(first)))*4))
		/* partition */
		tr_partition(tls, ISAd, first, first+uintptr(1)*4, last, bp, bp+8, v)
		if (int64(last)-int64(first))/4 != (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
			if *(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(*(*uintptr)(unsafe.Pointer(bp)))))*4)) != v {
				v5 = tr_ilg(tls, int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4))
			} else {
				v5 = -int32(1)
			}
			next = v5
			/* update ranks */
			c = first
			v = int32((int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(SA))/4 - libc.Int64FromInt32(1))
			for {
				if !(c < *(*uintptr)(unsafe.Pointer(bp))) {
					break
				}
				*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
				goto _38
			_38:
				;
				c += 4
			}
			if *(*uintptr)(unsafe.Pointer(bp + 8)) < last {
				c = *(*uintptr)(unsafe.Pointer(bp))
				v = int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(SA))/4 - libc.Int64FromInt32(1))
				for {
					if !(c < *(*uintptr)(unsafe.Pointer(bp + 8))) {
						break
					}
					*(*int32)(unsafe.Pointer(ISA + uintptr(*(*int32)(unsafe.Pointer(c)))*4)) = v
					goto _39
				_39:
					;
					c += 4
				}
			}
			/* push */
			if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 && trbudget_check(tls, budget, int32((int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4)) != 0 {
				if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
					if (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
						if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49751), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd + uintptr(incr)*4
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fd = next
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49752), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fc = last
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							last = *(*uintptr)(unsafe.Pointer(bp))
						} else {
							if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49755), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								first = *(*uintptr)(unsafe.Pointer(bp + 8))
							} else {
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						}
					} else {
						if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
							if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49762), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fc = last
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49763), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								last = *(*uintptr)(unsafe.Pointer(bp))
							} else {
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49766), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fc = last
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						} else {
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49770), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fc = last
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49771), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = first
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							ISAd = ISAd + uintptr(incr)*4
							first = *(*uintptr)(unsafe.Pointer(bp))
							last = *(*uintptr)(unsafe.Pointer(bp + 8))
							limit = next
						}
					}
				} else {
					if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
						if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49777), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd + uintptr(incr)*4
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fd = next
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49778), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = first
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							first = *(*uintptr)(unsafe.Pointer(bp + 8))
						} else {
							if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49781), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								last = *(*uintptr)(unsafe.Pointer(bp))
							} else {
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						}
					} else {
						if (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 <= (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 {
							if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49788), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = first
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49789), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd + uintptr(incr)*4
								stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp + 8))
								stack[ssize].Fd = next
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								first = *(*uintptr)(unsafe.Pointer(bp + 8))
							} else {
								if v4 = ssize < int32(TR_STACKSIZE); !v4 {
									libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49792), uintptr(unsafe.Pointer(&__func__2)))
								}
								_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
								stack[ssize].Fa = ISAd
								stack[ssize].Fb = first
								stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
								stack[ssize].Fd = limit
								v5 = ssize
								ssize = ssize + 1
								stack[v5].Fe = trlink
								ISAd = ISAd + uintptr(incr)*4
								first = *(*uintptr)(unsafe.Pointer(bp))
								last = *(*uintptr)(unsafe.Pointer(bp + 8))
								limit = next
							}
						} else {
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49796), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = first
							stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							if v4 = ssize < int32(TR_STACKSIZE); !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49797), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							stack[ssize].Fa = ISAd
							stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
							stack[ssize].Fc = last
							stack[ssize].Fd = limit
							v5 = ssize
							ssize = ssize + 1
							stack[v5].Fe = trlink
							ISAd = ISAd + uintptr(incr)*4
							first = *(*uintptr)(unsafe.Pointer(bp))
							last = *(*uintptr)(unsafe.Pointer(bp + 8))
							limit = next
						}
					}
				}
			} else {
				if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp + 8)))-int64(*(*uintptr)(unsafe.Pointer(bp))))/4 && 0 <= trlink {
					stack[trlink].Fd = -int32(1)
				}
				if (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 <= (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
					if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
						if v4 = ssize < int32(TR_STACKSIZE); !v4 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49805), uintptr(unsafe.Pointer(&__func__2)))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = *(*uintptr)(unsafe.Pointer(bp + 8))
						stack[ssize].Fc = last
						stack[ssize].Fd = limit
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						last = *(*uintptr)(unsafe.Pointer(bp))
					} else {
						if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
							first = *(*uintptr)(unsafe.Pointer(bp + 8))
						} else {
							if v4 = 0 <= ssize; !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49810), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				} else {
					if int64(1) < (int64(last)-int64(*(*uintptr)(unsafe.Pointer(bp + 8))))/4 {
						if v4 = ssize < int32(TR_STACKSIZE); !v4 {
							libc.X__assert_fail(tls, __ccgo_ts+9661, __ccgo_ts+9627, int32(49814), uintptr(unsafe.Pointer(&__func__2)))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
						stack[ssize].Fa = ISAd
						stack[ssize].Fb = first
						stack[ssize].Fc = *(*uintptr)(unsafe.Pointer(bp))
						stack[ssize].Fd = limit
						v5 = ssize
						ssize = ssize + 1
						stack[v5].Fe = trlink
						first = *(*uintptr)(unsafe.Pointer(bp + 8))
					} else {
						if int64(1) < (int64(*(*uintptr)(unsafe.Pointer(bp)))-int64(first))/4 {
							last = *(*uintptr)(unsafe.Pointer(bp))
						} else {
							if v4 = 0 <= ssize; !v4 {
								libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49819), uintptr(unsafe.Pointer(&__func__2)))
							}
							_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
							if ssize == 0 {
								return
							}
							ssize = ssize - 1
							v5 = ssize
							ISAd = stack[v5].Fa
							first = stack[ssize].Fb
							last = stack[ssize].Fc
							limit = stack[ssize].Fd
							trlink = stack[ssize].Fe
						}
					}
				}
			}
		} else {
			if trbudget_check(tls, budget, int32((int64(last)-int64(first))/4)) != 0 {
				limit = tr_ilg(tls, int32((int64(last)-int64(first))/4))
				ISAd = ISAd + uintptr(incr)*4
			} else {
				if 0 <= trlink {
					stack[trlink].Fd = -int32(1)
				}
				if v4 = 0 <= ssize; !v4 {
					libc.X__assert_fail(tls, __ccgo_ts+9616, __ccgo_ts+9627, int32(49828), uintptr(unsafe.Pointer(&__func__2)))
				}
				_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
				if ssize == 0 {
					return
				}
				ssize = ssize - 1
				v5 = ssize
				ISAd = stack[v5].Fa
				first = stack[ssize].Fb
				last = stack[ssize].Fc
				limit = stack[ssize].Fd
				trlink = stack[ssize].Fe
			}
		}
		goto _1
	_1:
	}
}

var __func__2 = [13]uint8{'t', 'r', '_', 'i', 'n', 't', 'r', 'o', 's', 'o', 'r', 't'}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Tandem repeat sort */
func trsort(tls *libc.TLS, ISA uintptr, SA uintptr, n int32, depth int32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var ISAd, first, last uintptr
	var skip, t, unsorted, v2 int32
	var _ /* budget at bp+0 */ trbudget_t
	_, _, _, _, _, _, _ = ISAd, first, last, skip, t, unsorted, v2
	trbudget_init(tls, bp, tr_ilg(tls, n)*int32(2)/int32(3), n)
	/*  trbudget_init(&budget, tr_ilg(n) * 3 / 4, n); */
	ISAd = ISA + uintptr(depth)*4
	for {
		if !(-n < *(*int32)(unsafe.Pointer(SA))) {
			break
		}
		first = SA
		skip = 0
		unsorted = 0
		for cond := true; cond; cond = first < SA+uintptr(n)*4 {
			v2 = *(*int32)(unsafe.Pointer(first))
			t = v2
			if v2 < 0 {
				first = first - uintptr(t)*4
				skip = skip + t
			} else {
				if skip != 0 {
					*(*int32)(unsafe.Pointer(first + uintptr(skip)*4)) = skip
					skip = 0
				}
				last = SA + uintptr(*(*int32)(unsafe.Pointer(ISA + uintptr(t)*4)))*4 + uintptr(1)*4
				if int64(1) < (int64(last)-int64(first))/4 {
					(*(*trbudget_t)(unsafe.Pointer(bp))).Fcount = 0
					tr_introsort(tls, ISA, ISAd, SA, first, last, bp)
					if (*(*trbudget_t)(unsafe.Pointer(bp))).Fcount != 0 {
						unsorted = unsorted + (*(*trbudget_t)(unsafe.Pointer(bp))).Fcount
					} else {
						skip = int32((int64(first) - int64(last)) / 4)
					}
				} else {
					if (int64(last)-int64(first))/4 == int64(1) {
						skip = -int32(1)
					}
				}
				first = last
			}
		}
		if skip != 0 {
			*(*int32)(unsafe.Pointer(first + uintptr(skip)*4)) = skip
		}
		if unsorted == 0 {
			break
		}
		goto _1
	_1:
		;
		ISAd = ISAd + uintptr((int64(ISAd)-int64(ISA))/4)*4
	}
}

/*---------------------------------------------------------------------------*/

// C documentation
//
//	/* Sorts suffixes of type B*. */
func sort_typeBstar(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, openMP int32) (r int32) {
	var ISAb, PAb, buf, v17 uintptr
	var bufsize, c0, c1, i, j, k, m, t, v4, v5, v8 int32
	var v6 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = ISAb, PAb, buf, bufsize, c0, c1, i, j, k, m, t, v17, v4, v5, v6, v8
	_ = openMP
	/* Initialize bucket arrays. */
	i = 0
	for {
		if !(i < int32(ALPHABET_SIZE)) {
			break
		}
		*(*int32)(unsafe.Pointer(bucket_A + uintptr(i)*4)) = 0
		goto _1
	_1:
		;
		i = i + 1
	}
	i = 0
	for {
		if !(i < libc.Int32FromInt32(ALPHABET_SIZE)*libc.Int32FromInt32(ALPHABET_SIZE)) {
			break
		}
		*(*int32)(unsafe.Pointer(bucket_B + uintptr(i)*4)) = 0
		goto _2
	_2:
		;
		i = i + 1
	}
	/* Count the number of occurrences of the first one or two characters of each
	   type A, B and B* suffix. Moreover, store the beginning position of all
	   type B* suffixes into the array SA. */
	i = n - int32(1)
	m = n
	c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	for {
		if !(0 <= i) {
			break
		}
		/* type A suffix. */
		for {
			v8 = c0
			c1 = v8
			*(*int32)(unsafe.Pointer(bucket_A + uintptr(v8)*4)) = *(*int32)(unsafe.Pointer(bucket_A + uintptr(v8)*4)) + 1
			goto _7
		_7:
			;
			i = i - 1
			v4 = i
			if v6 = 0 <= v4; v6 {
				v5 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
				c0 = v5
			}
			if !(v6 && v5 >= c1) {
				break
			}
		}
		if 0 <= i {
			/* type B* suffix. */
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4)) = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4)) + 1
			m = m - 1
			v4 = m
			*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) = i
			/* type B suffix. */
			i = i - 1
			c1 = c0
			for {
				if v6 = 0 <= i; v6 {
					v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
					c0 = v4
				}
				if !(v6 && v4 <= c1) {
					break
				}
				*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4)) = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4)) + 1
				goto _10
			_10:
				;
				i = i - 1
				c1 = c0
			}
		}
		goto _3
	_3:
	}
	m = n - m
	/*
	   note:
	     A type B* suffix is lexicographically smaller than a type B suffix that
	     begins with the same first two characters.
	*/
	/* Calculate the index of start/end point of each bucket. */
	c0 = 0
	i = 0
	j = libc.Int32FromInt32(0)
	for {
		if !(c0 < int32(ALPHABET_SIZE)) {
			break
		}
		t = i + *(*int32)(unsafe.Pointer(bucket_A + uintptr(c0)*4))
		*(*int32)(unsafe.Pointer(bucket_A + uintptr(c0)*4)) = i + j /* start point */
		i = t + *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c0)*4))
		c1 = c0 + int32(1)
		for {
			if !(c1 < int32(ALPHABET_SIZE)) {
				break
			}
			j = j + *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4))
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4)) = j /* end point */
			i = i + *(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4))
			goto _14
		_14:
			;
			c1 = c1 + 1
		}
		goto _13
	_13:
		;
		c0 = c0 + 1
	}
	if 0 < m {
		/* Sort the type B* suffixes by their first two characters. */
		PAb = SA + uintptr(n)*4 - uintptr(m)*4
		ISAb = SA + uintptr(m)*4
		i = m - int32(2)
		for {
			if !(0 <= i) {
				break
			}
			t = *(*int32)(unsafe.Pointer(PAb + uintptr(i)*4))
			c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(t))))
			c1 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(t+int32(1)))))
			v17 = bucket_B + uintptr(c0<<int32(8)|c1)*4
			*(*int32)(unsafe.Pointer(v17)) = *(*int32)(unsafe.Pointer(v17)) - 1
			v4 = *(*int32)(unsafe.Pointer(v17))
			*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) = i
			goto _15
		_15:
			;
			i = i - 1
		}
		t = *(*int32)(unsafe.Pointer(PAb + uintptr(m-int32(1))*4))
		c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(t))))
		c1 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(t+int32(1)))))
		v17 = bucket_B + uintptr(c0<<int32(8)|c1)*4
		*(*int32)(unsafe.Pointer(v17)) = *(*int32)(unsafe.Pointer(v17)) - 1
		v4 = *(*int32)(unsafe.Pointer(v17))
		*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) = m - int32(1)
		/* Sort the type B* substrings using sssort. */
		buf = SA + uintptr(m)*4
		/* Sort the type B* substrings using sssort. */
		bufsize = n - libc.Int32FromInt32(2)*m
		c0 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		j = m
		for {
			if !(0 < j) {
				break
			}
			c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(1)
			for {
				if !(c0 < c1) {
					break
				}
				i = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4))
				if int32(1) < j-i {
					sssort(tls, T, PAb, SA+uintptr(i)*4, SA+uintptr(j)*4, buf, bufsize, int32(2), n, libc.BoolInt32(*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) == m-int32(1)))
				}
				goto _21
			_21:
				;
				j = i
				c1 = c1 - 1
			}
			goto _20
		_20:
			;
			c0 = c0 - 1
		}
		/* Compute ranks of type B* substrings. */
		i = m - int32(1)
		for {
			if !(0 <= i) {
				break
			}
			if 0 <= *(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) {
				j = i
				for {
					*(*int32)(unsafe.Pointer(ISAb + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)))*4)) = i
					goto _24
				_24:
					;
					i = i - 1
					v4 = i
					if !(0 <= v4 && 0 <= *(*int32)(unsafe.Pointer(SA + uintptr(i)*4))) {
						break
					}
				}
				*(*int32)(unsafe.Pointer(SA + uintptr(i+int32(1))*4)) = i - j
				if i <= 0 {
					break
				}
			}
			j = i
			for {
				v5 = ^*(*int32)(unsafe.Pointer(SA + uintptr(i)*4))
				*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = v5
				*(*int32)(unsafe.Pointer(ISAb + uintptr(v5)*4)) = j
				goto _26
			_26:
				;
				i = i - 1
				v4 = i
				if !(*(*int32)(unsafe.Pointer(SA + uintptr(v4)*4)) < 0) {
					break
				}
			}
			*(*int32)(unsafe.Pointer(ISAb + uintptr(*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)))*4)) = j
			goto _22
		_22:
			;
			i = i - 1
		}
		/* Construct the inverse suffix array of type B* suffixes using trsort. */
		trsort(tls, ISAb, SA, m, int32(1))
		/* Set the sorted order of type B* suffixes. */
		i = n - int32(1)
		j = m
		c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
		for {
			if !(0 <= i) {
				break
			}
			i = i - 1
			c1 = c0
			for {
				if v6 = 0 <= i; v6 {
					v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
					c0 = v4
				}
				if !(v6 && v4 >= c1) {
					break
				}
				goto _29
			_29:
				;
				i = i - 1
				c1 = c0
			}
			if 0 <= i {
				t = i
				i = i - 1
				c1 = c0
				for {
					if v6 = 0 <= i; v6 {
						v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(i))))
						c0 = v4
					}
					if !(v6 && v4 <= c1) {
						break
					}
					goto _32
				_32:
					;
					i = i - 1
					c1 = c0
				}
				j = j - 1
				v4 = j
				if t == 0 || int32(1) < t-i {
					v5 = t
				} else {
					v5 = ^t
				}
				*(*int32)(unsafe.Pointer(SA + uintptr(*(*int32)(unsafe.Pointer(ISAb + uintptr(v4)*4)))*4)) = v5
			}
			goto _28
		_28:
		}
		/* Calculate the index of start/end point of each bucket. */
		*(*int32)(unsafe.Pointer(bucket_B + uintptr((libc.Int32FromInt32(ALPHABET_SIZE)-libc.Int32FromInt32(1))<<libc.Int32FromInt32(8)|(libc.Int32FromInt32(ALPHABET_SIZE)-libc.Int32FromInt32(1)))*4)) = n /* end point */
		c0 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		k = m - libc.Int32FromInt32(1)
		for {
			if !(0 <= c0) {
				break
			}
			i = *(*int32)(unsafe.Pointer(bucket_A + uintptr(c0+libc.Int32FromInt32(1))*4)) - int32(1)
			c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(1)
			for {
				if !(c0 < c1) {
					break
				}
				t = i - *(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4))
				*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c0)*4)) = i /* end point */
				/* Move all type B* suffixes to the correct position. */
				i = t
				j = *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c1)*4))
				for {
					if !(j <= k) {
						break
					}
					*(*int32)(unsafe.Pointer(SA + uintptr(i)*4)) = *(*int32)(unsafe.Pointer(SA + uintptr(k)*4))
					goto _39
				_39:
					;
					i = i - 1
					k = k - 1
				}
				goto _38
			_38:
				;
				c1 = c1 - 1
			}
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|(c0+int32(1)))*4)) = i - *(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c0)*4)) + int32(1) /* start point */
			*(*int32)(unsafe.Pointer(bucket_B + uintptr(c0<<int32(8)|c0)*4)) = i                                                                                          /* end point */
			goto _37
		_37:
			;
			c0 = c0 - 1
		}
	}
	return m
}

// C documentation
//
//	/* Constructs the suffix array by using the sorted order of type B* suffixes. */
func construct_SA(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, m int32) {
	var c0, c1, c2, s, v3 int32
	var i, j, k, v11 uintptr
	var v4 bool
	_, _, _, _, _, _, _, _, _, _ = c0, c1, c2, i, j, k, s, v11, v3, v4
	if 0 < m {
		/* Construct the sorted order of type B suffixes by using
		   the sorted order of type B* suffixes. */
		c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		for {
			if !(0 <= c1) {
				break
			}
			/* Scan the suffix array from right to left. */
			i = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|(c1+int32(1)))*4)))*4
			j = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(c1+libc.Int32FromInt32(1))*4)))*4 - uintptr(1)*4
			k = libc.UintptrFromInt32(0)
			c2 = -libc.Int32FromInt32(1)
			for {
				if !(i <= j) {
					break
				}
				v3 = *(*int32)(unsafe.Pointer(j))
				s = v3
				if 0 < v3 {
					if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9680, __ccgo_ts+9627, int32(50070), uintptr(unsafe.Pointer(&__func__3)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = s+int32(1) < n && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) <= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s+int32(1))))); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9691, __ccgo_ts+9627, int32(50071), uintptr(unsafe.Pointer(&__func__3)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) <= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9727, __ccgo_ts+9627, int32(50072), uintptr(unsafe.Pointer(&__func__3)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					*(*int32)(unsafe.Pointer(j)) = ^s
					s = s - 1
					v3 = s
					c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
					if 0 < s && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) > c0 {
						s = ^s
					}
					if c0 != c2 {
						if 0 <= c2 {
							*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c2)*4)) = int32((int64(k) - int64(SA)) / 4)
						}
						v3 = c0
						c2 = v3
						k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|v3)*4)))*4
					}
					if v4 = k < j; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9744, __ccgo_ts+9627, int32(50080), uintptr(unsafe.Pointer(&__func__3)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = k != libc.UintptrFromInt32(0); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9750, __ccgo_ts+9627, int32(50080), uintptr(unsafe.Pointer(&__func__3)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					v11 = k
					k -= 4
					*(*int32)(unsafe.Pointer(v11)) = s
				} else {
					if v4 = s == 0 && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1 || s < 0; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9760, __ccgo_ts+9627, int32(50083), uintptr(unsafe.Pointer(&__func__3)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					*(*int32)(unsafe.Pointer(j)) = ^s
				}
				goto _2
			_2:
				;
				j -= 4
			}
			goto _1
		_1:
			;
			c1 = c1 - 1
		}
	}
	/* Construct the suffix array by using
	   the sorted order of type B suffixes. */
	v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	c2 = v3
	k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
	v11 = k
	k += 4
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2))))) < c2 {
		v3 = ^(n - libc.Int32FromInt32(1))
	} else {
		v3 = n - int32(1)
	}
	*(*int32)(unsafe.Pointer(v11)) = v3
	/* Scan the suffix array from left to right. */
	i = SA
	j = SA + uintptr(n)*4
	for {
		if !(i < j) {
			break
		}
		v3 = *(*int32)(unsafe.Pointer(i))
		s = v3
		if 0 < v3 {
			if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) >= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))); !v4 {
				libc.X__assert_fail(tls, __ccgo_ts+9798, __ccgo_ts+9627, int32(50097), uintptr(unsafe.Pointer(&__func__3)))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			s = s - 1
			v3 = s
			c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
			if s == 0 || libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) < c0 {
				s = ^s
			}
			if c0 != c2 {
				*(*int32)(unsafe.Pointer(bucket_A + uintptr(c2)*4)) = int32((int64(k) - int64(SA)) / 4)
				v3 = c0
				c2 = v3
				k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
			}
			if v4 = i < k; !v4 {
				libc.X__assert_fail(tls, __ccgo_ts+9815, __ccgo_ts+9627, int32(50104), uintptr(unsafe.Pointer(&__func__3)))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			v11 = k
			k += 4
			*(*int32)(unsafe.Pointer(v11)) = s
		} else {
			if v4 = s < 0; !v4 {
				libc.X__assert_fail(tls, __ccgo_ts+9821, __ccgo_ts+9627, int32(50107), uintptr(unsafe.Pointer(&__func__3)))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			*(*int32)(unsafe.Pointer(i)) = ^s
		}
		goto _16
	_16:
		;
		i += 4
	}
}

var __func__3 = [13]uint8{'c', 'o', 'n', 's', 't', 'r', 'u', 'c', 't', '_', 'S', 'A'}

// C documentation
//
//	/* Constructs the burrows-wheeler transformed string directly
//	   by using the sorted order of type B* suffixes. */
func construct_BWT(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, m int32) (r int32) {
	var c0, c1, c2, s, v3 int32
	var i, j, k, orig, v11 uintptr
	var v4 bool
	_, _, _, _, _, _, _, _, _, _, _ = c0, c1, c2, i, j, k, orig, s, v11, v3, v4
	if 0 < m {
		/* Construct the sorted order of type B suffixes by using
		   the sorted order of type B* suffixes. */
		c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		for {
			if !(0 <= c1) {
				break
			}
			/* Scan the suffix array from right to left. */
			i = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|(c1+int32(1)))*4)))*4
			j = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(c1+libc.Int32FromInt32(1))*4)))*4 - uintptr(1)*4
			k = libc.UintptrFromInt32(0)
			c2 = -libc.Int32FromInt32(1)
			for {
				if !(i <= j) {
					break
				}
				v3 = *(*int32)(unsafe.Pointer(j))
				s = v3
				if 0 < v3 {
					if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9680, __ccgo_ts+9627, int32(50134), uintptr(unsafe.Pointer(&__func__4)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = s+int32(1) < n && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) <= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s+int32(1))))); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9691, __ccgo_ts+9627, int32(50135), uintptr(unsafe.Pointer(&__func__4)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) <= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9727, __ccgo_ts+9627, int32(50136), uintptr(unsafe.Pointer(&__func__4)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					s = s - 1
					v3 = s
					c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
					*(*int32)(unsafe.Pointer(j)) = ^c0
					if 0 < s && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) > c0 {
						s = ^s
					}
					if c0 != c2 {
						if 0 <= c2 {
							*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c2)*4)) = int32((int64(k) - int64(SA)) / 4)
						}
						v3 = c0
						c2 = v3
						k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|v3)*4)))*4
					}
					if v4 = k < j; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9744, __ccgo_ts+9627, int32(50144), uintptr(unsafe.Pointer(&__func__4)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = k != libc.UintptrFromInt32(0); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9750, __ccgo_ts+9627, int32(50144), uintptr(unsafe.Pointer(&__func__4)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					v11 = k
					k -= 4
					*(*int32)(unsafe.Pointer(v11)) = s
				} else {
					if s != 0 {
						*(*int32)(unsafe.Pointer(j)) = ^s
					} else {
						if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1; !v4 {
							libc.X__assert_fail(tls, __ccgo_ts+9680, __ccgo_ts+9627, int32(50150), uintptr(unsafe.Pointer(&__func__4)))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					}
				}
				goto _2
			_2:
				;
				j -= 4
			}
			goto _1
		_1:
			;
			c1 = c1 - 1
		}
	}
	/* Construct the BWTed string by using
	   the sorted order of type B suffixes. */
	v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	c2 = v3
	k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
	v11 = k
	k += 4
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2))))) < c2 {
		v3 = ^libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2)))))
	} else {
		v3 = n - int32(1)
	}
	*(*int32)(unsafe.Pointer(v11)) = v3
	/* Scan the suffix array from left to right. */
	i = SA
	j = SA + uintptr(n)*4
	orig = SA
	for {
		if !(i < j) {
			break
		}
		v3 = *(*int32)(unsafe.Pointer(i))
		s = v3
		if 0 < v3 {
			if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) >= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))); !v4 {
				libc.X__assert_fail(tls, __ccgo_ts+9798, __ccgo_ts+9627, int32(50164), uintptr(unsafe.Pointer(&__func__4)))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			s = s - 1
			v3 = s
			c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
			*(*int32)(unsafe.Pointer(i)) = c0
			if 0 < s && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) < c0 {
				s = ^libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1)))))
			}
			if c0 != c2 {
				*(*int32)(unsafe.Pointer(bucket_A + uintptr(c2)*4)) = int32((int64(k) - int64(SA)) / 4)
				v3 = c0
				c2 = v3
				k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
			}
			if v4 = i < k; !v4 {
				libc.X__assert_fail(tls, __ccgo_ts+9815, __ccgo_ts+9627, int32(50172), uintptr(unsafe.Pointer(&__func__4)))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			v11 = k
			k += 4
			*(*int32)(unsafe.Pointer(v11)) = s
		} else {
			if s != 0 {
				*(*int32)(unsafe.Pointer(i)) = ^s
			} else {
				orig = i
			}
		}
		goto _16
	_16:
		;
		i += 4
	}
	return int32((int64(orig) - int64(SA)) / 4)
}

var __func__4 = [14]uint8{'c', 'o', 'n', 's', 't', 'r', 'u', 'c', 't', '_', 'B', 'W', 'T'}

// C documentation
//
//	/* Constructs the burrows-wheeler transformed string directly
//	   by using the sorted order of type B* suffixes. */
func construct_BWT_indexes(tls *libc.TLS, T uintptr, SA uintptr, bucket_A uintptr, bucket_B uintptr, n int32, m int32, num_indexes uintptr, indexes uintptr) (r int32) {
	var c0, c1, c2, mod, s, v3 int32
	var i, j, k, orig, v11 uintptr
	var v4 bool
	_, _, _, _, _, _, _, _, _, _, _, _ = c0, c1, c2, i, j, k, mod, orig, s, v11, v3, v4
	mod = n / int32(8)
	mod = mod | mod>>int32(1)
	mod = mod | mod>>int32(2)
	mod = mod | mod>>int32(4)
	mod = mod | mod>>int32(8)
	mod = mod | mod>>int32(16)
	mod = mod >> int32(1)
	*(*uint8)(unsafe.Pointer(num_indexes)) = libc.Uint8FromInt32((n - libc.Int32FromInt32(1)) / (mod + libc.Int32FromInt32(1)))
	if 0 < m {
		/* Construct the sorted order of type B suffixes by using
		   the sorted order of type B* suffixes. */
		c1 = libc.Int32FromInt32(ALPHABET_SIZE) - libc.Int32FromInt32(2)
		for {
			if !(0 <= c1) {
				break
			}
			/* Scan the suffix array from right to left. */
			i = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|(c1+int32(1)))*4)))*4
			j = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(c1+libc.Int32FromInt32(1))*4)))*4 - uintptr(1)*4
			k = libc.UintptrFromInt32(0)
			c2 = -libc.Int32FromInt32(1)
			for {
				if !(i <= j) {
					break
				}
				v3 = *(*int32)(unsafe.Pointer(j))
				s = v3
				if 0 < v3 {
					if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9680, __ccgo_ts+9627, int32(50215), uintptr(unsafe.Pointer(&__func__5)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = s+int32(1) < n && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) <= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s+int32(1))))); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9691, __ccgo_ts+9627, int32(50216), uintptr(unsafe.Pointer(&__func__5)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) <= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9727, __ccgo_ts+9627, int32(50217), uintptr(unsafe.Pointer(&__func__5)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if s&mod == 0 {
						*(*int32)(unsafe.Pointer(indexes + uintptr(s/(mod+int32(1))-int32(1))*4)) = int32((int64(j) - int64(SA)) / 4)
					}
					s = s - 1
					v3 = s
					c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
					*(*int32)(unsafe.Pointer(j)) = ^c0
					if 0 < s && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) > c0 {
						s = ^s
					}
					if c0 != c2 {
						if 0 <= c2 {
							*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|c2)*4)) = int32((int64(k) - int64(SA)) / 4)
						}
						v3 = c0
						c2 = v3
						k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_B + uintptr(c1<<int32(8)|v3)*4)))*4
					}
					if v4 = k < j; !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9744, __ccgo_ts+9627, int32(50228), uintptr(unsafe.Pointer(&__func__5)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					if v4 = k != libc.UintptrFromInt32(0); !v4 {
						libc.X__assert_fail(tls, __ccgo_ts+9750, __ccgo_ts+9627, int32(50228), uintptr(unsafe.Pointer(&__func__5)))
					}
					_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					v11 = k
					k -= 4
					*(*int32)(unsafe.Pointer(v11)) = s
				} else {
					if s != 0 {
						*(*int32)(unsafe.Pointer(j)) = ^s
					} else {
						if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))) == c1; !v4 {
							libc.X__assert_fail(tls, __ccgo_ts+9680, __ccgo_ts+9627, int32(50234), uintptr(unsafe.Pointer(&__func__5)))
						}
						_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
					}
				}
				goto _2
			_2:
				;
				j -= 4
			}
			goto _1
		_1:
			;
			c1 = c1 - 1
		}
	}
	/* Construct the BWTed string by using
	   the sorted order of type B suffixes. */
	v3 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1)))))
	c2 = v3
	k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
	if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2))))) < c2 {
		if (n-int32(1))&mod == 0 {
			*(*int32)(unsafe.Pointer(indexes + uintptr((n-int32(1))/(mod+int32(1))-int32(1))*4)) = int32((int64(k) - int64(SA)) / 4)
		}
		v11 = k
		k += 4
		*(*int32)(unsafe.Pointer(v11)) = ^libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(n-int32(2)))))
	} else {
		v11 = k
		k += 4
		*(*int32)(unsafe.Pointer(v11)) = n - int32(1)
	}
	/* Scan the suffix array from left to right. */
	i = SA
	j = SA + uintptr(n)*4
	orig = SA
	for {
		if !(i < j) {
			break
		}
		v3 = *(*int32)(unsafe.Pointer(i))
		s = v3
		if 0 < v3 {
			if v4 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) >= libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s)))); !v4 {
				libc.X__assert_fail(tls, __ccgo_ts+9798, __ccgo_ts+9627, int32(50255), uintptr(unsafe.Pointer(&__func__5)))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if s&mod == 0 {
				*(*int32)(unsafe.Pointer(indexes + uintptr(s/(mod+int32(1))-int32(1))*4)) = int32((int64(i) - int64(SA)) / 4)
			}
			s = s - 1
			v3 = s
			c0 = libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(v3))))
			*(*int32)(unsafe.Pointer(i)) = c0
			if c0 != c2 {
				*(*int32)(unsafe.Pointer(bucket_A + uintptr(c2)*4)) = int32((int64(k) - int64(SA)) / 4)
				v3 = c0
				c2 = v3
				k = SA + uintptr(*(*int32)(unsafe.Pointer(bucket_A + uintptr(v3)*4)))*4
			}
			if v4 = i < k; !v4 {
				libc.X__assert_fail(tls, __ccgo_ts+9815, __ccgo_ts+9627, int32(50265), uintptr(unsafe.Pointer(&__func__5)))
			}
			_ = v4 || libc.Bool(libc.Int32FromInt32(0) != 0)
			if 0 < s && libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1))))) < c0 {
				if s&mod == 0 {
					*(*int32)(unsafe.Pointer(indexes + uintptr(s/(mod+int32(1))-int32(1))*4)) = int32((int64(k) - int64(SA)) / 4)
				}
				v11 = k
				k += 4
				*(*int32)(unsafe.Pointer(v11)) = ^libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + uintptr(s-int32(1)))))
			} else {
				v11 = k
				k += 4
				*(*int32)(unsafe.Pointer(v11)) = s
			}
		} else {
			if s != 0 {
				*(*int32)(unsafe.Pointer(i)) = ^s
			} else {
				orig = i
			}
		}
		goto _16
	_16:
		;
		i += 4
	}
	return int32((int64(orig) - int64(SA)) / 4)
}

var __func__5 = [22]uint8{'c', 'o', 'n', 's', 't', 'r', 'u', 'c', 't', '_', 'B', 'W', 'T', '_', 'i', 'n', 'd', 'e', 'x', 'e', 's'}

/*---------------------------------------------------------------------------*/

/*- Function -*/

func divsufsort(tls *libc.TLS, T uintptr, SA uintptr, n int32, openMP int32) (r int32) {
	var bucket_A, bucket_B uintptr
	var err, m int32
	_, _, _, _ = bucket_A, bucket_B, err, m
	err = 0
	/* Check arguments. */
	if T == libc.UintptrFromInt32(0) || SA == libc.UintptrFromInt32(0) || n < 0 {
		return -int32(1)
	} else {
		if n == 0 {
			return 0
		} else {
			if n == int32(1) {
				*(*int32)(unsafe.Pointer(SA)) = 0
				return 0
			} else {
				if n == int32(2) {
					m = libc.BoolInt32(libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T))) < libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(T + 1))))
					*(*int32)(unsafe.Pointer(SA + uintptr(m^int32(1))*4)) = 0
					*(*int32)(unsafe.Pointer(SA + uintptr(m)*4)) = libc.Int32FromInt32(1)
					return 0
				}
			}
		}
	}
	bucket_A = libc.Xmalloc(tls, libc.Uint64FromInt32(libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	bucket_B = libc.Xmalloc(tls, libc.Uint64FromInt32(libc.Int32FromInt32(ALPHABET_SIZE)*libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	/* Suffixsort. */
	if bucket_A != libc.UintptrFromInt32(0) && bucket_B != libc.UintptrFromInt32(0) {
		m = sort_typeBstar(tls, T, SA, bucket_A, bucket_B, n, openMP)
		construct_SA(tls, T, SA, bucket_A, bucket_B, n, m)
	} else {
		err = -int32(2)
	}
	libc.Xfree(tls, bucket_B)
	libc.Xfree(tls, bucket_A)
	return err
}

func divbwt(tls *libc.TLS, T uintptr, U uintptr, A uintptr, n int32, num_indexes uintptr, indexes uintptr, openMP int32) (r int32) {
	var B, bucket_A, bucket_B, v1 uintptr
	var i, m, pidx int32
	_, _, _, _, _, _, _ = B, bucket_A, bucket_B, i, m, pidx, v1
	/* Check arguments. */
	if T == libc.UintptrFromInt32(0) || U == libc.UintptrFromInt32(0) || n < 0 {
		return -int32(1)
	} else {
		if n <= int32(1) {
			if n == int32(1) {
				*(*uint8)(unsafe.Pointer(U)) = *(*uint8)(unsafe.Pointer(T))
			}
			return n
		}
	}
	v1 = A
	B = v1
	if v1 == libc.UintptrFromInt32(0) {
		B = libc.Xmalloc(tls, libc.Uint64FromInt32(n+libc.Int32FromInt32(1))*uint64(4))
	}
	bucket_A = libc.Xmalloc(tls, libc.Uint64FromInt32(libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	bucket_B = libc.Xmalloc(tls, libc.Uint64FromInt32(libc.Int32FromInt32(ALPHABET_SIZE)*libc.Int32FromInt32(ALPHABET_SIZE))*libc.Uint64FromInt64(4))
	/* Burrows-Wheeler Transform. */
	if B != libc.UintptrFromInt32(0) && bucket_A != libc.UintptrFromInt32(0) && bucket_B != libc.UintptrFromInt32(0) {
		m = sort_typeBstar(tls, T, B, bucket_A, bucket_B, n, openMP)
		if num_indexes == libc.UintptrFromInt32(0) || indexes == libc.UintptrFromInt32(0) {
			pidx = construct_BWT(tls, T, B, bucket_A, bucket_B, n, m)
		} else {
			pidx = construct_BWT_indexes(tls, T, B, bucket_A, bucket_B, n, m, num_indexes, indexes)
		}
		/* Copy to output string. */
		*(*uint8)(unsafe.Pointer(U)) = *(*uint8)(unsafe.Pointer(T + uintptr(n-int32(1))))
		i = 0
		for {
			if !(i < pidx) {
				break
			}
			*(*uint8)(unsafe.Pointer(U + uintptr(i+int32(1)))) = libc.Uint8FromInt32(*(*int32)(unsafe.Pointer(B + uintptr(i)*4)))
			goto _2
		_2:
			;
			i = i + 1
		}
		i = i + int32(1)
		for {
			if !(i < n) {
				break
			}
			*(*uint8)(unsafe.Pointer(U + uintptr(i))) = libc.Uint8FromInt32(*(*int32)(unsafe.Pointer(B + uintptr(i)*4)))
			goto _3
		_3:
			;
			i = i + 1
		}
		pidx = pidx + int32(1)
	} else {
		pidx = -int32(2)
	}
	libc.Xfree(tls, bucket_B)
	libc.Xfree(tls, bucket_A)
	if A == libc.UintptrFromInt32(0) {
		libc.Xfree(tls, B)
	}
	return pidx
}

/**** ended inlining dictBuilder/divsufsort.c ****/
/**** start inlining dictBuilder/fastcover.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-*************************************
*  Dependencies
***************************************/

/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/pool.h ****/
/**** skipping file: ../common/threading.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../compress/zstd_compress_internal.h ****/
/**** skipping file: ../zdict.h ****/
/**** skipping file: cover.h ****/

/*-*************************************
*  Constants
***************************************/
/**
* There are 32bit indexes used to ref samples, so limit samples size to 4GB
* on 64bit builds.
* For 32bit builds we choose 1 GB.
* Most 32bit platforms have 2GB user-mode addressable space and we allocate a large
* contiguous buffer, so 1GB is already a high limit.
 */

/*-*************************************
*  Console display
***************************************/

// C documentation
//
//	/*-*************************************
//	* Hash Functions
//	***************************************/
//	/**
//	 * Hash the d-byte value pointed to by p and mod 2^f into the frequency vector
//	 */
func FASTCOVER_hashPtrToIndex(tls *libc.TLS, p uintptr, f U32, d uint32) (r size_t) {
	if d == uint32(6) {
		return ZSTD_hash6Ptr(tls, p, f)
	}
	return ZSTD_hash8Ptr(tls, p, f)
}

// C documentation
//
//	/*-*************************************
//	* Acceleration
//	***************************************/
type FASTCOVER_accel_t = struct {
	Ffinalize uint32
	Fskip     uint32
}

var FASTCOVER_defaultAccelParameters = [11]FASTCOVER_accel_t{
	0: {
		Ffinalize: uint32(100),
	},
	1: {
		Ffinalize: uint32(100),
	},
	2: {
		Ffinalize: uint32(50),
		Fskip:     uint32(1),
	},
	3: {
		Ffinalize: uint32(34),
		Fskip:     uint32(2),
	},
	4: {
		Ffinalize: uint32(25),
		Fskip:     uint32(3),
	},
	5: {
		Ffinalize: uint32(20),
		Fskip:     uint32(4),
	},
	6: {
		Ffinalize: uint32(17),
		Fskip:     uint32(5),
	},
	7: {
		Ffinalize: uint32(14),
		Fskip:     uint32(6),
	},
	8: {
		Ffinalize: uint32(13),
		Fskip:     uint32(7),
	},
	9: {
		Ffinalize: uint32(11),
		Fskip:     uint32(8),
	},
	10: {
		Ffinalize: uint32(10),
		Fskip:     uint32(9),
	},
}

// C documentation
//
//	/*-*************************************
//	* Context
//	***************************************/
type FASTCOVER_ctx_t = struct {
	Fsamples        uintptr
	Foffsets        uintptr
	FsamplesSizes   uintptr
	FnbSamples      size_t
	FnbTrainSamples size_t
	FnbTestSamples  size_t
	FnbDmers        size_t
	Ffreqs          uintptr
	Fd              uint32
	Ff              uint32
	FaccelParams    FASTCOVER_accel_t
}

// C documentation
//
//	/*-*************************************
//	*  Helper functions
//	***************************************/
//	/**
//	 * Selects the best segment in an epoch.
//	 * Segments of are scored according to the function:
//	 *
//	 * Let F(d) be the frequency of all dmers with hash value d.
//	 * Let S_i be hash value of the dmer at position i of segment S which has length k.
//	 *
//	 *     Score(S) = F(S_1) + F(S_2) + ... + F(S_{k-d+1})
//	 *
//	 * Once the dmer with hash value d is in the dictionary we set F(d) = 0.
//	 */
func FASTCOVER_selectSegment(tls *libc.TLS, ctx uintptr, freqs uintptr, begin U32, end U32, parameters ZDICT_cover_params_t, segmentFreqs uintptr) (r COVER_segment_t) {
	var activeSegment, bestSegment COVER_segment_t
	var d, dmersInK, f, k, pos U32
	var delIndex, delIndex1, i, idx size_t
	var v1 uintptr
	_, _, _, _, _, _, _, _, _, _, _, _ = activeSegment, bestSegment, d, delIndex, delIndex1, dmersInK, f, i, idx, k, pos, v1
	/* Constants */
	k = parameters.Fk
	d = parameters.Fd
	f = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff
	dmersInK = k - d + uint32(1)
	/* Try each segment (activeSegment) and save the best (bestSegment) */
	bestSegment = COVER_segment_t{}
	/* Reset the activeDmers in the segment */
	/* The activeSegment starts at the beginning of the epoch. */
	activeSegment.Fbegin = begin
	activeSegment.Fend = begin
	activeSegment.Fscore = uint32(0)
	/* Slide the activeSegment through the whole epoch.
	 * Save the best segment in bestSegment.
	 */
	for activeSegment.Fend < end {
		/* Get hash value of current dmer */
		idx = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(activeSegment.Fend), f, d)
		/* Add frequency of this index to score if this is the first occurrence of index in active segment */
		if libc.Int32FromUint16(*(*U16)(unsafe.Pointer(segmentFreqs + uintptr(idx)*2))) == 0 {
			activeSegment.Fscore += *(*U32)(unsafe.Pointer(freqs + uintptr(idx)*4))
		}
		/* Increment end of segment and segmentFreqs*/
		activeSegment.Fend += uint32(1)
		v1 = segmentFreqs + uintptr(idx)*2
		*(*U16)(unsafe.Pointer(v1)) = U16(int32(*(*U16)(unsafe.Pointer(v1))) + libc.Int32FromInt32(1))
		/* If the window is now too large, drop the first position */
		if activeSegment.Fend-activeSegment.Fbegin == dmersInK+uint32(1) {
			/* Get hash value of the dmer to be eliminated from active segment */
			delIndex = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(activeSegment.Fbegin), f, d)
			v1 = segmentFreqs + uintptr(delIndex)*2
			*(*U16)(unsafe.Pointer(v1)) = U16(int32(*(*U16)(unsafe.Pointer(v1))) - libc.Int32FromInt32(1))
			/* Subtract frequency of this index from score if this is the last occurrence of this index in active segment */
			if libc.Int32FromUint16(*(*U16)(unsafe.Pointer(segmentFreqs + uintptr(delIndex)*2))) == 0 {
				activeSegment.Fscore -= *(*U32)(unsafe.Pointer(freqs + uintptr(delIndex)*4))
			}
			/* Increment start of segment */
			activeSegment.Fbegin += uint32(1)
		}
		/* If this segment is the best so far save it */
		if activeSegment.Fscore > bestSegment.Fscore {
			bestSegment = activeSegment
		}
	}
	/* Zero out rest of segmentFreqs array */
	for activeSegment.Fbegin < end {
		delIndex1 = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(activeSegment.Fbegin), f, d)
		v1 = segmentFreqs + uintptr(delIndex1)*2
		*(*U16)(unsafe.Pointer(v1)) = U16(int32(*(*U16)(unsafe.Pointer(v1))) - libc.Int32FromInt32(1))
		activeSegment.Fbegin += uint32(1)
	}
	pos = bestSegment.Fbegin
	for {
		if !(pos != bestSegment.Fend) {
			break
		}
		i = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(pos), f, d)
		*(*U32)(unsafe.Pointer(freqs + uintptr(i)*4)) = uint32(0)
		goto _4
	_4:
		;
		pos = pos + 1
	}
	return bestSegment
}

func FASTCOVER_checkParameters(tls *libc.TLS, parameters ZDICT_cover_params_t, maxDictSize size_t, f uint32, accel uint32) (r int32) {
	/* k, d, and f are required parameters */
	if parameters.Fd == uint32(0) || parameters.Fk == uint32(0) {
		return 0
	}
	/* d has to be 6 or 8 */
	if parameters.Fd != uint32(6) && parameters.Fd != uint32(8) {
		return 0
	}
	/* k <= maxDictSize */
	if uint64(parameters.Fk) > maxDictSize {
		return 0
	}
	/* d <= k */
	if parameters.Fd > parameters.Fk {
		return 0
	}
	/* 0 < f <= FASTCOVER_MAX_F*/
	if f > uint32(FASTCOVER_MAX_F) || f == uint32(0) {
		return 0
	}
	/* 0 < splitPoint <= 1 */
	if parameters.FsplitPoint <= libc.Float64FromInt32(0) || parameters.FsplitPoint > libc.Float64FromInt32(1) {
		return 0
	}
	/* 0 < accel <= 10 */
	if accel > uint32(10) || accel == uint32(0) {
		return 0
	}
	return int32(1)
}

// C documentation
//
//	/**
//	 * Clean up a context initialized with `FASTCOVER_ctx_init()`.
//	 */
func FASTCOVER_ctx_destroy(tls *libc.TLS, ctx uintptr) {
	if !(ctx != 0) {
		return
	}
	libc.Xfree(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.UintptrFromInt32(0)
	libc.Xfree(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.UintptrFromInt32(0)
}

// C documentation
//
//	/**
//	 * Calculate for frequency of hash value of each dmer in ctx->samples
//	 */
func FASTCOVER_computeFrequency(tls *libc.TLS, freqs uintptr, ctx uintptr) {
	var currSampleEnd, dmerIndex, i, start size_t
	var d, f, readLength, skip, v1 uint32
	var v2 bool
	_, _, _, _, _, _, _, _, _, _ = currSampleEnd, d, dmerIndex, f, i, readLength, skip, start, v1, v2
	f = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff
	d = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fd
	skip = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FaccelParams.Fskip
	if d > libc.Uint32FromInt32(libc.Int32FromInt32(8)) {
		v1 = d
	} else {
		v1 = libc.Uint32FromInt32(libc.Int32FromInt32(8))
	}
	readLength = v1
	if v2 = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples >= uint64(5); !v2 {
		libc.X__assert_fail(tls, __ccgo_ts+9827, __ccgo_ts+9627, int32(50646), uintptr(unsafe.Pointer(&__func__6)))
	}
	_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
	if v2 = (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples <= (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples; !v2 {
		libc.X__assert_fail(tls, __ccgo_ts+9852, __ccgo_ts+9627, int32(50647), uintptr(unsafe.Pointer(&__func__6)))
	}
	_ = v2 || libc.Bool(libc.Int32FromInt32(0) != 0)
	i = uint64(0)
	for {
		if !(i < (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples) {
			break
		}
		start = *(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i)*8)) /* start of current dmer */
		currSampleEnd = *(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i+uint64(1))*8))
		for start+uint64(readLength) <= currSampleEnd {
			dmerIndex = FASTCOVER_hashPtrToIndex(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(start), f, d)
			*(*U32)(unsafe.Pointer(freqs + uintptr(dmerIndex)*4)) = *(*U32)(unsafe.Pointer(freqs + uintptr(dmerIndex)*4)) + 1
			start = start + uint64(skip) + uint64(1)
		}
		goto _4
	_4:
		;
		i = i + 1
	}
}

var __func__6 = [27]uint8{'F', 'A', 'S', 'T', 'C', 'O', 'V', 'E', 'R', '_', 'c', 'o', 'm', 'p', 'u', 't', 'e', 'F', 'r', 'e', 'q', 'u', 'e', 'n', 'c', 'y'}

// C documentation
//
//	/**
//	 * Prepare a context for dictionary building.
//	 * The context is only dependent on the parameter `d` and can be used multiple
//	 * times.
//	 * Returns 0 on success or error code on error.
//	 * The context must be destroyed with `FASTCOVER_ctx_destroy()`.
//	 */
func FASTCOVER_ctx_init(tls *libc.TLS, ctx uintptr, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, d uint32, splitPoint float64, f uint32, accelParams FASTCOVER_accel_t) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var i U32
	var nbTestSamples, nbTrainSamples, v1, v2 uint32
	var samples uintptr
	var testSamplesSize, totalSamplesSize, trainingSamplesSize size_t
	var v3, v4, v5 uint64
	var v7 bool
	_, _, _, _, _, _, _, _, _, _, _, _, _ = i, nbTestSamples, nbTrainSamples, samples, testSamplesSize, totalSamplesSize, trainingSamplesSize, v1, v2, v3, v4, v5, v7
	samples = samplesBuffer
	totalSamplesSize = COVER_sum(tls, samplesSizes, nbSamples)
	if splitPoint < float64(1) {
		v1 = uint32(float64(float64(nbSamples) * splitPoint))
	} else {
		v1 = nbSamples
	}
	/* Split samples into testing and training sets */
	nbTrainSamples = v1
	if splitPoint < float64(1) {
		v2 = nbSamples - nbTrainSamples
	} else {
		v2 = nbSamples
	}
	nbTestSamples = v2
	if splitPoint < float64(1) {
		v3 = COVER_sum(tls, samplesSizes, nbTrainSamples)
	} else {
		v3 = totalSamplesSize
	}
	trainingSamplesSize = v3
	if splitPoint < float64(1) {
		v4 = COVER_sum(tls, samplesSizes+uintptr(nbTrainSamples)*8, nbTestSamples)
	} else {
		v4 = totalSamplesSize
	}
	testSamplesSize = v4
	/* Checks */
	if uint64(d) > libc.Uint64FromInt64(8) {
		v5 = uint64(d)
	} else {
		v5 = libc.Uint64FromInt64(8)
	}
	if totalSamplesSize < v5 || totalSamplesSize >= uint64(libc.Uint32FromInt32(-libc.Int32FromInt32(1))) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8508, libc.VaList(bp+8, uint32(totalSamplesSize>>libc.Int32FromInt32(20)), libc.Uint32FromInt32(-libc.Int32FromInt32(1))>>libc.Int32FromInt32(20)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there are at least 5 training samples */
	if nbTrainSamples < uint32(5) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9890, libc.VaList(bp+8, nbTrainSamples))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Check if there's testing sample */
	if nbTestSamples < uint32(1) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9945, libc.VaList(bp+8, nbTestSamples))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	/* Zero the context */
	libc.Xmemset(tls, ctx, 0, uint64(80))
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8681, libc.VaList(bp+8, nbTrainSamples, uint32(trainingSamplesSize)))
		libc.Xfflush(tls, libc.Xstderr)
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8722, libc.VaList(bp+8, nbTestSamples, uint32(testSamplesSize)))
		libc.Xfflush(tls, libc.Xstderr)
	}
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples = samples
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes = samplesSizes
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples = uint64(nbSamples)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples = uint64(nbTrainSamples)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTestSamples = uint64(nbTestSamples)
	if uint64(d) > libc.Uint64FromInt64(8) {
		v3 = uint64(d)
	} else {
		v3 = libc.Uint64FromInt64(8)
	}
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbDmers = trainingSamplesSize - v3 + uint64(1)
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fd = d
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff = f
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FaccelParams = accelParams
	/* The offsets of each file */
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets = libc.Xcalloc(tls, uint64(nbSamples+libc.Uint32FromInt32(1)), uint64(8))
	if (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets == libc.UintptrFromInt32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10000, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		FASTCOVER_ctx_destroy(tls, ctx)
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	/* Fill offsets from the samplesSizes */
	*(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets)) = uint64(0)
	if v7 = nbSamples >= uint32(5); !v7 {
		libc.X__assert_fail(tls, __ccgo_ts+10037, __ccgo_ts+9627, int32(50730), uintptr(unsafe.Pointer(&__func__7)))
	}
	_ = v7 || libc.Bool(libc.Int32FromInt32(0) != 0)
	i = uint32(1)
	for {
		if !(i <= nbSamples) {
			break
		}
		*(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i)*8)) = *(*size_t)(unsafe.Pointer((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets + uintptr(i-uint32(1))*8)) + *(*size_t)(unsafe.Pointer(samplesSizes + uintptr(i-uint32(1))*8))
		goto _8
	_8:
		;
		i = i + 1
	}
	/* Initialize frequency array of size 2^f */
	(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs = libc.Xcalloc(tls, libc.Uint64FromInt32(1)<<f, uint64(4))
	if (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs == libc.UintptrFromInt32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10052, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		FASTCOVER_ctx_destroy(tls, ctx)
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+8833, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	FASTCOVER_computeFrequency(tls, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs, ctx)
	return uint64(0)
}

var __func__7 = [19]uint8{'F', 'A', 'S', 'T', 'C', 'O', 'V', 'E', 'R', '_', 'c', 't', 'x', '_', 'i', 'n', 'i', 't'}

// C documentation
//
//	/**
//	 * Given the prepared context build the dictionary.
//	 */
func FASTCOVER_buildDictionary(tls *libc.TLS, ctx uintptr, freqs uintptr, dictBuffer uintptr, dictBufferCapacity size_t, parameters ZDICT_cover_params_t, segmentFreqs uintptr) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var dict uintptr
	var epoch, maxZeroScoreRun, segmentSize, tail, zeroScoreRun, v2 size_t
	var epochBegin, epochEnd U32
	var epochs COVER_epoch_info_t
	var segment COVER_segment_t
	var v3 uint64
	_, _, _, _, _, _, _, _, _, _, _, _ = dict, epoch, epochBegin, epochEnd, epochs, maxZeroScoreRun, segment, segmentSize, tail, zeroScoreRun, v2, v3
	dict = dictBuffer
	tail = dictBufferCapacity
	/* Divide the data into epochs. We will select one segment from each epoch. */
	epochs = COVER_computeEpochs(tls, uint32(dictBufferCapacity), uint32((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbDmers), parameters.Fk, uint32(1))
	maxZeroScoreRun = uint64(10)
	zeroScoreRun = uint64(0)
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9136, libc.VaList(bp+8, epochs.Fnum, epochs.Fsize))
		libc.Xfflush(tls, libc.Xstderr)
	}
	/* Loop through the epochs until there are no more segments or the dictionary
	 * is full.
	 */
	epoch = uint64(0)
	for {
		if !(tail > uint64(0)) {
			break
		}
		epochBegin = uint32(epoch * uint64(epochs.Fsize))
		epochEnd = epochBegin + epochs.Fsize
		/* Select a segment */
		segment = FASTCOVER_selectSegment(tls, ctx, freqs, epochBegin, epochEnd, parameters, segmentFreqs)
		/* If the segment covers no dmers, then we are out of content.
		 * There may be new content in other epochs, for continue for some time.
		 */
		if segment.Fscore == uint32(0) {
			zeroScoreRun = zeroScoreRun + 1
			v2 = zeroScoreRun
			if v2 >= maxZeroScoreRun {
				break
			}
			goto _1
		}
		zeroScoreRun = uint64(0)
		/* Trim the segment if necessary and if it is too small then we are done */
		if uint64(segment.Fend-segment.Fbegin+parameters.Fd-libc.Uint32FromInt32(1)) < tail {
			v3 = uint64(segment.Fend - segment.Fbegin + parameters.Fd - libc.Uint32FromInt32(1))
		} else {
			v3 = tail
		}
		segmentSize = v3
		if segmentSize < uint64(parameters.Fd) {
			break
		}
		/* We fill the dictionary from the back to allow the best segments to be
		 * referenced with the smallest offsets.
		 */
		tail = tail - segmentSize
		libc.Xmemcpy(tls, dict+uintptr(tail), (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples+uintptr(segment.Fbegin), segmentSize)
		if g_displayLevel >= int32(2) {
			if libc.Xclock(tls)-g_time > g_refreshRate || g_displayLevel >= int32(4) {
				g_time = libc.Xclock(tls)
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9180, libc.VaList(bp+8, uint32((dictBufferCapacity-tail)*libc.Uint64FromInt32(100)/dictBufferCapacity)))
				libc.Xfflush(tls, libc.Xstderr)
			}
		}
		goto _1
	_1:
		;
		epoch = (epoch + uint64(1)) % uint64(epochs.Fnum)
	}
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9193, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.Xstderr)
	}
	return tail
}

// C documentation
//
//	/**
//	 * Parameters for FASTCOVER_tryParameters().
//	 */
type FASTCOVER_tryParameters_data_t = struct {
	Fctx                uintptr
	Fbest               uintptr
	FdictBufferCapacity size_t
	Fparameters         ZDICT_cover_params_t
}

// C documentation
//
//	/**
//	 * Parameters for FASTCOVER_tryParameters().
//	 */
type FASTCOVER_tryParameters_data_s = FASTCOVER_tryParameters_data_t

// C documentation
//
//	/**
//	 * Tries a set of parameters and updates the COVER_best_t with the results.
//	 * This function is thread safe if zstd is compiled with multithreaded support.
//	 * It takes its parameters as an *OWNING* opaque pointer to support threading.
//	 */
func FASTCOVER_tryParameters(tls *libc.TLS, opaque uintptr) {
	var ctx, data, dict, freqs, segmentFreqs uintptr
	var dictBufferCapacity, tail, totalCompressedSize size_t
	var nbFinalizeSamples uint32
	var parameters ZDICT_cover_params_t
	var selection COVER_dictSelection_t
	_, _, _, _, _, _, _, _, _, _, _ = ctx, data, dict, dictBufferCapacity, freqs, nbFinalizeSamples, parameters, segmentFreqs, selection, tail, totalCompressedSize
	/* Save parameters as local variables */
	data = opaque
	ctx = (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx
	parameters = (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters
	dictBufferCapacity = (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity
	totalCompressedSize = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
	/* Initialize array to keep track of frequency of dmer within activeSegment */
	segmentFreqs = libc.Xcalloc(tls, libc.Uint64FromInt32(1)<<(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff, uint64(2))
	/* Allocate space for hash table, dict, and freqs */
	dict = libc.Xmalloc(tls, dictBufferCapacity)
	selection = COVER_dictSelectionError(tls, libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC)))
	freqs = libc.Xmalloc(tls, libc.Uint64FromInt32(1)<<(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff*uint64(4))
	if !(segmentFreqs != 0) || !(dict != 0) || !(freqs != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9409, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	/* Copy the frequencies because we need to modify them */
	libc.Xmemcpy(tls, freqs, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ffreqs, libc.Uint64FromInt32(1)<<(*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Ff*uint64(4))
	/* Build the dictionary */
	tail = FASTCOVER_buildDictionary(tls, ctx, freqs, dict, dictBufferCapacity, parameters, segmentFreqs)
	nbFinalizeSamples = uint32((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples * uint64((*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FaccelParams.Ffinalize) / libc.Uint64FromInt32(100))
	selection = COVER_selectDict(tls, dict+uintptr(tail), dictBufferCapacity, dictBufferCapacity-tail, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Fsamples, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FsamplesSizes, nbFinalizeSamples, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbTrainSamples, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).FnbSamples, parameters, (*FASTCOVER_ctx_t)(unsafe.Pointer(ctx)).Foffsets, totalCompressedSize)
	if COVER_dictSelectionIsError(tls, selection) != 0 {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9452, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	goto _cleanup
_cleanup:
	;
	libc.Xfree(tls, dict)
	COVER_best_finish(tls, (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest, parameters, selection)
	libc.Xfree(tls, data)
	libc.Xfree(tls, segmentFreqs)
	COVER_dictSelectionFree(tls, selection)
	libc.Xfree(tls, freqs)
}

func FASTCOVER_convertToCoverParams(tls *libc.TLS, fastCoverParams ZDICT_fastCover_params_t, coverParams uintptr) {
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).Fk = fastCoverParams.Fk
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).Fd = fastCoverParams.Fd
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).Fsteps = fastCoverParams.Fsteps
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FnbThreads = fastCoverParams.FnbThreads
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FsplitPoint = fastCoverParams.FsplitPoint
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FzParams = fastCoverParams.FzParams
	(*ZDICT_cover_params_t)(unsafe.Pointer(coverParams)).FshrinkDict = fastCoverParams.FshrinkDict
}

func FASTCOVER_convertToFastCoverParams(tls *libc.TLS, coverParams ZDICT_cover_params_t, fastCoverParams uintptr, f uint32, accel uint32) {
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Fk = coverParams.Fk
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Fd = coverParams.Fd
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Fsteps = coverParams.Fsteps
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FnbThreads = coverParams.FnbThreads
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FsplitPoint = coverParams.FsplitPoint
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Ff = f
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).Faccel = accel
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FzParams = coverParams.FzParams
	(*ZDICT_fastCover_params_t)(unsafe.Pointer(fastCoverParams)).FshrinkDict = coverParams.FshrinkDict
}

func ZDICT_trainFromBuffer_fastCover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters ZDICT_fastCover_params_t) (r size_t) {
	bp := tls.Alloc(144)
	defer tls.Free(144)
	var accelParams FASTCOVER_accel_t
	var dict, segmentFreqs uintptr
	var dictionarySize, initVal, tail size_t
	var nbFinalizeSamples, v1 uint32
	var _ /* coverParams at bp+80 */ ZDICT_cover_params_t
	var _ /* ctx at bp+0 */ FASTCOVER_ctx_t
	_, _, _, _, _, _, _, _ = accelParams, dict, dictionarySize, initVal, nbFinalizeSamples, segmentFreqs, tail, v1
	dict = dictBuffer
	/* Initialize global data */
	g_displayLevel = libc.Int32FromUint32(parameters.FzParams.FnotificationLevel)
	/* Assign splitPoint and f if not provided */
	parameters.FsplitPoint = float64(1)
	if parameters.Ff == uint32(0) {
		v1 = uint32(DEFAULT_F)
	} else {
		v1 = parameters.Ff
	}
	parameters.Ff = v1
	if parameters.Faccel == uint32(0) {
		v1 = uint32(DEFAULT_ACCEL)
	} else {
		v1 = parameters.Faccel
	}
	parameters.Faccel = v1
	/* Convert to cover parameter */
	libc.Xmemset(tls, bp+80, 0, uint64(48))
	FASTCOVER_convertToCoverParams(tls, parameters, bp+80)
	/* Checks */
	if !(FASTCOVER_checkParameters(tls, *(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80)), dictBufferCapacity, parameters.Ff, parameters.Faccel) != 0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10089, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10121, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9269, libc.VaList(bp+136, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* Assign corresponding FASTCOVER_accel_t to accelParams*/
	accelParams = FASTCOVER_defaultAccelParameters[parameters.Faccel]
	/* Initialize context */
	initVal = FASTCOVER_ctx_init(tls, bp, samplesBuffer, samplesSizes, nbSamples, (*(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80))).Fd, parameters.FsplitPoint, parameters.Ff, accelParams)
	if ZSTD_isError(tls, initVal) != 0 {
		if g_displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9549, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return initVal
	}
	COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).FnbDmers, g_displayLevel)
	/* Build the dictionary */
	if g_displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9353, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	/* Initialize array to keep track of frequency of dmer within activeSegment */
	segmentFreqs = libc.Xcalloc(tls, libc.Uint64FromInt32(1)<<parameters.Ff, uint64(2))
	tail = FASTCOVER_buildDictionary(tls, bp, (*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).Ffreqs, dictBuffer, dictBufferCapacity, *(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80)), segmentFreqs)
	nbFinalizeSamples = uint32((*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).FnbTrainSamples * uint64((*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp))).FaccelParams.Ffinalize) / libc.Uint64FromInt32(100))
	dictionarySize = ZDICT_finalizeDictionary(tls, dict, dictBufferCapacity, dict+uintptr(tail), dictBufferCapacity-tail, samplesBuffer, samplesSizes, nbFinalizeSamples, (*(*ZDICT_cover_params_t)(unsafe.Pointer(bp + 80))).FzParams)
	if !(ZSTD_isError(tls, dictionarySize) != 0) {
		if g_displayLevel >= int32(2) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9374, libc.VaList(bp+136, uint32(dictionarySize)))
			libc.Xfflush(tls, libc.Xstderr)
		}
	}
	FASTCOVER_ctx_destroy(tls, bp)
	libc.Xfree(tls, segmentFreqs)
	return dictionarySize
	return r
}

func ZDICT_optimizeTrainFromBuffer_fastCover(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, parameters uintptr) (r size_t) {
	bp := tls.Alloc(320)
	defer tls.Free(320)
	var accel, d, f, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, shrinkDict, v2, v3, v4, v5, v6, v7, v8, v9 uint32
	var accelParams FASTCOVER_accel_t
	var compressedSize, dictSize, initVal size_t
	var data, pool uintptr
	var displayLevel, warned, v10 int32
	var splitPoint, v1 float64
	var _ /* best at bp+48 */ COVER_best_t
	var _ /* coverParams at bp+0 */ ZDICT_cover_params_t
	var _ /* ctx at bp+216 */ FASTCOVER_ctx_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = accel, accelParams, compressedSize, d, data, dictSize, displayLevel, f, initVal, iteration, k, kIterations, kMaxD, kMaxK, kMinD, kMinK, kStepSize, kSteps, nbThreads, pool, shrinkDict, splitPoint, warned, v1, v10, v2, v3, v4, v5, v6, v7, v8, v9
	/* constants */
	nbThreads = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FnbThreads
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FsplitPoint <= float64(0) {
		v1 = float64(FASTCOVER_DEFAULT_SPLITPOINT)
	} else {
		v1 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FsplitPoint
	}
	splitPoint = v1
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v2 = uint32(6)
	} else {
		v2 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMinD = v2
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd == uint32(0) {
		v3 = uint32(8)
	} else {
		v3 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fd
	}
	kMaxD = v3
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v4 = uint32(50)
	} else {
		v4 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMinK = v4
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk == uint32(0) {
		v5 = uint32(2000)
	} else {
		v5 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fk
	}
	kMaxK = v5
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fsteps == uint32(0) {
		v6 = uint32(40)
	} else {
		v6 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Fsteps
	}
	kSteps = v6
	if (kMaxK-kMinK)/kSteps > libc.Uint32FromInt32(libc.Int32FromInt32(1)) {
		v7 = (kMaxK - kMinK) / kSteps
	} else {
		v7 = libc.Uint32FromInt32(libc.Int32FromInt32(1))
	}
	kStepSize = v7
	kIterations = (uint32(1) + (kMaxD-kMinD)/uint32(2)) * (uint32(1) + (kMaxK-kMinK)/kStepSize)
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Ff == uint32(0) {
		v8 = uint32(DEFAULT_F)
	} else {
		v8 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Ff
	}
	f = v8
	if (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Faccel == uint32(0) {
		v9 = uint32(DEFAULT_ACCEL)
	} else {
		v9 = (*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).Faccel
	}
	accel = v9
	shrinkDict = uint32(0)
	/* Local variables */
	displayLevel = libc.Int32FromUint32((*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)).FzParams.FnotificationLevel)
	iteration = uint32(1)
	pool = libc.UintptrFromInt32(0)
	warned = 0
	/* Checks */
	if splitPoint <= libc.Float64FromInt32(0) || splitPoint > libc.Float64FromInt32(1) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10166, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if accel == uint32(0) || accel > uint32(FASTCOVER_MAX_ACCEL) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10188, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if kMinK < kMaxD || kMaxK < kMinK {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10205, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_parameter_outOfBound))
	}
	if nbSamples == uint32(0) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10121, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_srcSize_wrong))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		if displayLevel >= int32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9269, libc.VaList(bp+304, int32(ZDICT_DICTSIZE_MIN)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if nbThreads > uint32(1) {
		pool = POOL_create(tls, uint64(nbThreads), uint64(1))
		if !(pool != 0) {
			return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		}
	}
	/* Initialization */
	COVER_best_init(tls, bp+48)
	libc.Xmemset(tls, bp, 0, uint64(48))
	FASTCOVER_convertToCoverParams(tls, *(*ZDICT_fastCover_params_t)(unsafe.Pointer(parameters)), bp)
	accelParams = FASTCOVER_defaultAccelParameters[accel]
	/* Turn down global display level to clean up display at level 2 and below */
	if displayLevel == 0 {
		v10 = 0
	} else {
		v10 = displayLevel - int32(1)
	}
	g_displayLevel = v10
	/* Loop through d first because each new value needs a new context */
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9503, libc.VaList(bp+304, kIterations))
		libc.Xfflush(tls, libc.Xstderr)
	}
	d = kMinD
	for {
		if !(d <= kMaxD) {
			break
		}
		if displayLevel >= int32(3) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9543, libc.VaList(bp+304, d))
			libc.Xfflush(tls, libc.Xstderr)
		}
		initVal = FASTCOVER_ctx_init(tls, bp+216, samplesBuffer, samplesSizes, nbSamples, d, splitPoint, f, accelParams)
		if ZSTD_isError(tls, initVal) != 0 {
			if displayLevel >= int32(1) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9549, 0)
				libc.Xfflush(tls, libc.Xstderr)
			}
			COVER_best_destroy(tls, bp+48)
			POOL_free(tls, pool)
			return initVal
		}
		if !(warned != 0) {
			COVER_warnOnSmallCorpus(tls, dictBufferCapacity, (*(*FASTCOVER_ctx_t)(unsafe.Pointer(bp + 216))).FnbDmers, displayLevel)
			warned = int32(1)
		}
		/* Loop through k reusing the same context */
		k = kMinK
		for {
			if !(k <= kMaxK) {
				break
			}
			/* Prepare the arguments */
			data = libc.Xmalloc(tls, uint64(72))
			if displayLevel >= int32(3) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9579, libc.VaList(bp+304, k))
				libc.Xfflush(tls, libc.Xstderr)
			}
			if !(data != 0) {
				if displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9585, 0)
					libc.Xfflush(tls, libc.Xstderr)
				}
				COVER_best_destroy(tls, bp+48)
				FASTCOVER_ctx_destroy(tls, bp+216)
				POOL_free(tls, pool)
				return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
			}
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx = bp + 216
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fbest = bp + 48
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).FdictBufferCapacity = dictBufferCapacity
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters = *(*ZDICT_cover_params_t)(unsafe.Pointer(bp))
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fk = k
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fd = d
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FsplitPoint = splitPoint
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.Fsteps = kSteps
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FshrinkDict = shrinkDict
			(*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters.FzParams.FnotificationLevel = libc.Uint32FromInt32(g_displayLevel)
			/* Check the parameters */
			if !(FASTCOVER_checkParameters(tls, (*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fparameters, dictBufferCapacity, (*FASTCOVER_ctx_t)(unsafe.Pointer((*FASTCOVER_tryParameters_data_t)(unsafe.Pointer(data)).Fctx)).Ff, accel) != 0) {
				if g_displayLevel >= int32(1) {
					libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10089, 0)
					libc.Xfflush(tls, libc.Xstderr)
				}
				libc.Xfree(tls, data)
				goto _12
			}
			/* Call the function and pass ownership of data to it */
			COVER_best_start(tls, bp+48)
			if pool != 0 {
				POOL_add(tls, pool, __ccgo_fp(FASTCOVER_tryParameters), data)
			} else {
				FASTCOVER_tryParameters(tls, data)
			}
			/* Print status */
			if displayLevel >= int32(2) {
				if libc.Xclock(tls)-g_time > g_refreshRate || displayLevel >= int32(4) {
					g_time = libc.Xclock(tls)
					libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9180, libc.VaList(bp+304, iteration*libc.Uint32FromInt32(100)/kIterations))
					libc.Xfflush(tls, libc.Xstderr)
				}
			}
			iteration = iteration + 1
			goto _12
		_12:
			;
			k = k + kStepSize
		}
		COVER_best_wait(tls, bp+48)
		FASTCOVER_ctx_destroy(tls, bp+216)
		goto _11
	_11:
		;
		d = d + uint32(2)
	}
	if displayLevel >= int32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+9193, libc.VaList(bp+304, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.Xstderr)
	}
	/* Fill the output buffer and parameters with output of the best parameters */
	dictSize = (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).FdictSize
	if ZSTD_isError(tls, (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).FcompressedSize) != 0 {
		compressedSize = (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).FcompressedSize
		COVER_best_destroy(tls, bp+48)
		POOL_free(tls, pool)
		return compressedSize
	}
	FASTCOVER_convertToFastCoverParams(tls, (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).Fparameters, parameters, f, accel)
	libc.Xmemcpy(tls, dictBuffer, (*(*COVER_best_t)(unsafe.Pointer(bp + 48))).Fdict, dictSize)
	COVER_best_destroy(tls, bp+48)
	POOL_free(tls, pool)
	return dictSize
	return r
}

/**** ended inlining dictBuilder/fastcover.c ****/
/**** start inlining dictBuilder/zdict.c ****/
/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
 */

/*-**************************************
*  Tuning parameters
****************************************/

/*-**************************************
*  Compiler Options
****************************************/
/* Unix Large Files support (>4GB) */

/*-*************************************
*  Dependencies
***************************************/

/**** skipping file: ../common/mem.h ****/
/**** skipping file: ../common/fse.h ****/
/**** skipping file: ../common/huf.h ****/
/**** skipping file: ../common/zstd_internal.h ****/
/**** skipping file: ../common/xxhash.h ****/
/**** skipping file: ../compress/zstd_compress_internal.h ****/
/**** skipping file: ../zdict.h ****/
/**** skipping file: divsufsort.h ****/
/**** skipping file: ../common/bits.h ****/

/*-*************************************
*  Constants
***************************************/

var g_selectivity_default = uint32(9)

/*-*************************************
*  Console display
***************************************/

func ZDICT_clockSpan(tls *libc.TLS, nPrevious clock_t) (r clock_t) {
	return libc.Xclock(tls) - nPrevious
}

func ZDICT_printHex(tls *libc.TLS, ptr uintptr, length size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var b uintptr
	var c BYTE
	var u size_t
	_, _, _ = b, c, u
	b = ptr
	u = uint64(0)
	for {
		if !(u < length) {
			break
		}
		c = *(*BYTE)(unsafe.Pointer(b + uintptr(u)))
		if libc.Int32FromUint8(c) < int32(32) || libc.Int32FromUint8(c) > int32(126) {
			c = uint8('.')
		} /* non-printable char */
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10218, libc.VaList(bp+8, libc.Int32FromUint8(c)))
		libc.Xfflush(tls, libc.Xstderr)
		goto _1
	_1:
		;
		u = u + 1
	}
}

// C documentation
//
//	/*-********************************************************
//	*  Helper functions
//	**********************************************************/
func ZDICT_isError(tls *libc.TLS, errorCode size_t) (r uint32) {
	return ERR_isError(tls, errorCode)
}

func ZDICT_getErrorName(tls *libc.TLS, errorCode size_t) (r uintptr) {
	return ERR_getErrorName(tls, errorCode)
}

func ZDICT_getDictID(tls *libc.TLS, dictBuffer uintptr, dictSize size_t) (r uint32) {
	if dictSize < uint64(8) {
		return uint32(0)
	}
	if MEM_readLE32(tls, dictBuffer) != uint32(ZSTD_MAGIC_DICTIONARY) {
		return uint32(0)
	}
	return MEM_readLE32(tls, dictBuffer+uintptr(4))
}

func ZDICT_getDictHeaderSize(tls *libc.TLS, dictBuffer uintptr, dictSize size_t) (r size_t) {
	var bs, wksp uintptr
	var headerSize size_t
	_, _, _ = bs, headerSize, wksp
	if dictSize <= uint64(8) || MEM_readLE32(tls, dictBuffer) != uint32(ZSTD_MAGIC_DICTIONARY) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionary_corrupted))
	}
	bs = libc.Xmalloc(tls, uint64(5632))
	wksp = libc.Xmalloc(tls, libc.Uint64FromInt32(libc.Int32FromInt32(8)<<libc.Int32FromInt32(10)+libc.Int32FromInt32(512)))
	if !(bs != 0) || !(wksp != 0) {
		headerSize = libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	} else {
		ZSTD_reset_compressedBlockState(tls, bs)
		headerSize = ZSTD_loadCEntropy(tls, bs, wksp, dictBuffer, dictSize)
	}
	libc.Xfree(tls, bs)
	libc.Xfree(tls, wksp)
	return headerSize
}

// C documentation
//
//	/*-********************************************************
//	*  Dictionary training functions
//	**********************************************************/
//	/*! ZDICT_count() :
//	    Count the nb of common bytes between 2 pointers.
//	    Note : this function presumes end of buffer followed by noisy guard band.
//	*/
func ZDICT_count(tls *libc.TLS, pIn uintptr, pMatch uintptr) (r size_t) {
	var diff size_t
	var pStart uintptr
	_, _ = diff, pStart
	pStart = pIn
	for {
		diff = MEM_readST(tls, pMatch) ^ MEM_readST(tls, pIn)
		if !(diff != 0) {
			pIn = pIn + uintptr(8)
			pMatch = pMatch + uintptr(8)
			goto _1
		}
		pIn = pIn + uintptr(ZSTD_NbCommonBytes(tls, diff))
		return libc.Uint64FromInt64(int64(pIn) - int64(pStart))
		goto _1
	_1:
	}
	return r
}

type dictItem = struct {
	Fpos     U32
	Flength  U32
	Fsavings U32
}

func ZDICT_initDictItem(tls *libc.TLS, d uintptr) {
	(*dictItem)(unsafe.Pointer(d)).Fpos = uint32(1)
	(*dictItem)(unsafe.Pointer(d)).Flength = uint32(0)
	(*dictItem)(unsafe.Pointer(d)).Fsavings = libc.Uint32FromInt32(-libc.Int32FromInt32(1))
}

func ZDICT_analyzePos(tls *libc.TLS, doneMarks uintptr, suffix uintptr, start U32, buffer uintptr, minRatio U32, notificationLevel U32) (r dictItem) {
	bp := tls.Alloc(576)
	defer tls.Free(576)
	var b uintptr
	var c, currentChar BYTE
	var currentCount, currentID, end, id, id1, idx, l, length4, mml, p, pEnd, patternEnd, refinedEnd, refinedStart, selectedCount, selectedID, testedPos, u U32
	var i int32
	var length, length1, length2, length3, maxLength, pos size_t
	var pattern16 U16
	var savings [64]U32
	var _ /* cumulLength at bp+256 */ [64]U32
	var _ /* lengthList at bp+0 */ [64]U32
	var _ /* solution at bp+512 */ dictItem
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = b, c, currentChar, currentCount, currentID, end, i, id, id1, idx, l, length, length1, length2, length3, length4, maxLength, mml, p, pEnd, pattern16, patternEnd, pos, refinedEnd, refinedStart, savings, selectedCount, selectedID, testedPos, u
	*(*[64]U32)(unsafe.Pointer(bp)) = [64]U32{}
	*(*[64]U32)(unsafe.Pointer(bp + 256)) = [64]U32{}
	savings = [64]U32{}
	b = buffer
	maxLength = uint64(LLIMIT)
	pos = libc.Uint64FromInt32(*(*int32)(unsafe.Pointer(suffix + uintptr(start)*4)))
	end = start
	/* init */
	libc.Xmemset(tls, bp+512, 0, uint64(12))
	*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(pos))) = uint8(1)
	/* trivial repetition cases */
	if libc.Int32FromUint16(MEM_read16(tls, b+uintptr(pos)+uintptr(0))) == libc.Int32FromUint16(MEM_read16(tls, b+uintptr(pos)+uintptr(2))) || libc.Int32FromUint16(MEM_read16(tls, b+uintptr(pos)+uintptr(1))) == libc.Int32FromUint16(MEM_read16(tls, b+uintptr(pos)+uintptr(3))) || libc.Int32FromUint16(MEM_read16(tls, b+uintptr(pos)+uintptr(2))) == libc.Int32FromUint16(MEM_read16(tls, b+uintptr(pos)+uintptr(4))) {
		/* skip and mark segment */
		pattern16 = MEM_read16(tls, b+uintptr(pos)+uintptr(4))
		patternEnd = uint32(6)
		for libc.Int32FromUint16(MEM_read16(tls, b+uintptr(pos)+uintptr(patternEnd))) == libc.Int32FromUint16(pattern16) {
			patternEnd = patternEnd + uint32(2)
		}
		if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(b + uintptr(pos+uint64(patternEnd))))) == libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(b + uintptr(pos+uint64(patternEnd)-uint64(1))))) {
			patternEnd = patternEnd + 1
		}
		u = uint32(1)
		for {
			if !(u < patternEnd) {
				break
			}
			*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(pos+uint64(u)))) = uint8(1)
			goto _1
		_1:
			;
			u = u + 1
		}
		return *(*dictItem)(unsafe.Pointer(bp + 512))
	}
	/* look forward */
	for cond := true; cond; cond = length >= uint64(MINMATCHLENGTH) {
		end = end + 1
		length = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(end)*4))))
	}
	/* look backward */
	for cond := true; cond; cond = length1 >= uint64(MINMATCHLENGTH) {
		length1 = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(start)*4 - libc.UintptrFromInt32(1)*4))))
		if length1 >= uint64(MINMATCHLENGTH) {
			start = start - 1
		}
	}
	/* exit if not found a minimum nb of repetitions */
	if end-start < minRatio {
		idx = start
		for {
			if !(idx < end) {
				break
			}
			*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(idx)*4))))) = uint8(1)
			goto _2
		_2:
			;
			idx = idx + 1
		}
		return *(*dictItem)(unsafe.Pointer(bp + 512))
	}
	refinedStart = start
	refinedEnd = end
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10221, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10223, libc.VaList(bp+536, end-start, int32(MINMATCHLENGTH), uint32(pos)))
		libc.Xfflush(tls, libc.Xstderr)
	}
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10221, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	mml = uint32(MINMATCHLENGTH)
	for {
		currentChar = uint8(0)
		currentCount = uint32(0)
		currentID = refinedStart
		selectedCount = uint32(0)
		selectedID = currentID
		id = refinedStart
		for {
			if !(id < refinedEnd) {
				break
			}
			if libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(b + uintptr(libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(suffix + uintptr(id)*4)))+mml)))) != libc.Int32FromUint8(currentChar) {
				if currentCount > selectedCount {
					selectedCount = currentCount
					selectedID = currentID
				}
				currentID = id
				currentChar = *(*BYTE)(unsafe.Pointer(b + uintptr(libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(suffix + uintptr(id)*4)))+mml)))
				currentCount = uint32(0)
			}
			currentCount = currentCount + 1
			goto _4
		_4:
			;
			id = id + 1
		}
		if currentCount > selectedCount { /* for last */
			selectedCount = currentCount
			selectedID = currentID
		}
		if selectedCount < minRatio {
			break
		}
		refinedStart = selectedID
		refinedEnd = refinedStart + selectedCount
		goto _3
	_3:
		;
		mml = mml + 1
	}
	/* evaluate gain based on new dict */
	start = refinedStart
	pos = libc.Uint64FromInt32(*(*int32)(unsafe.Pointer(suffix + uintptr(refinedStart)*4)))
	end = start
	libc.Xmemset(tls, bp, 0, uint64(256))
	/* look forward */
	for cond := true; cond; cond = length2 >= uint64(MINMATCHLENGTH) {
		end = end + 1
		length2 = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(end)*4))))
		if length2 >= uint64(LLIMIT) {
			length2 = libc.Uint64FromInt32(libc.Int32FromInt32(LLIMIT) - libc.Int32FromInt32(1))
		}
		(*(*[64]U32)(unsafe.Pointer(bp)))[length2] = (*(*[64]U32)(unsafe.Pointer(bp)))[length2] + 1
	}
	/* look backward */
	length3 = uint64(MINMATCHLENGTH)
	for libc.BoolInt32(length3 >= uint64(MINMATCHLENGTH))&libc.BoolInt32(start > uint32(0)) != 0 {
		length3 = ZDICT_count(tls, b+uintptr(pos), b+uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(start-uint32(1))*4))))
		if length3 >= uint64(LLIMIT) {
			length3 = libc.Uint64FromInt32(libc.Int32FromInt32(LLIMIT) - libc.Int32FromInt32(1))
		}
		(*(*[64]U32)(unsafe.Pointer(bp)))[length3] = (*(*[64]U32)(unsafe.Pointer(bp)))[length3] + 1
		if length3 >= uint64(MINMATCHLENGTH) {
			start = start - 1
		}
	}
	/* largest useful length */
	libc.Xmemset(tls, bp+256, 0, uint64(256))
	(*(*[64]U32)(unsafe.Pointer(bp + 256)))[maxLength-uint64(1)] = (*(*[64]U32)(unsafe.Pointer(bp)))[maxLength-uint64(1)]
	i = libc.Int32FromUint64(maxLength - libc.Uint64FromInt32(2))
	for {
		if !(i >= 0) {
			break
		}
		(*(*[64]U32)(unsafe.Pointer(bp + 256)))[i] = (*(*[64]U32)(unsafe.Pointer(bp + 256)))[i+int32(1)] + (*(*[64]U32)(unsafe.Pointer(bp)))[i]
		goto _5
	_5:
		;
		i = i - 1
	}
	i = libc.Int32FromInt32(LLIMIT) - libc.Int32FromInt32(1)
	for {
		if !(i >= int32(MINMATCHLENGTH)) {
			break
		}
		if (*(*[64]U32)(unsafe.Pointer(bp + 256)))[i] >= minRatio {
			break
		}
		goto _6
	_6:
		;
		i = i - 1
	}
	maxLength = libc.Uint64FromInt32(i)
	/* reduce maxLength in case of final into repetitive data */
	l = uint32(maxLength)
	c = *(*BYTE)(unsafe.Pointer(b + uintptr(pos+maxLength-uint64(1))))
	for libc.Int32FromUint8(*(*BYTE)(unsafe.Pointer(b + uintptr(pos+uint64(l)-uint64(2))))) == libc.Int32FromUint8(c) {
		l = l - 1
	}
	maxLength = uint64(l)
	if maxLength < uint64(MINMATCHLENGTH) {
		return *(*dictItem)(unsafe.Pointer(bp + 512))
	} /* skip : no long-enough solution */
	/* calculate savings */
	savings[int32(5)] = uint32(0)
	i = int32(MINMATCHLENGTH)
	for {
		if !(i <= libc.Int32FromUint64(maxLength)) {
			break
		}
		savings[i] = savings[i-int32(1)] + (*(*[64]U32)(unsafe.Pointer(bp)))[i]*libc.Uint32FromInt32(i-libc.Int32FromInt32(3))
		goto _7
	_7:
		;
		i = i + 1
	}
	if notificationLevel >= uint32(4) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10270, libc.VaList(bp+536, uint32(pos), uint32(maxLength), savings[maxLength], float64(savings[maxLength])/float64(maxLength)))
		libc.Xfflush(tls, libc.Xstderr)
	}
	(*(*dictItem)(unsafe.Pointer(bp + 512))).Fpos = uint32(pos)
	(*(*dictItem)(unsafe.Pointer(bp + 512))).Flength = uint32(maxLength)
	(*(*dictItem)(unsafe.Pointer(bp + 512))).Fsavings = savings[maxLength]
	/* mark positions done */
	id1 = start
	for {
		if !(id1 < end) {
			break
		}
		testedPos = libc.Uint32FromInt32(*(*int32)(unsafe.Pointer(suffix + uintptr(id1)*4)))
		if uint64(testedPos) == pos {
			length4 = (*(*dictItem)(unsafe.Pointer(bp + 512))).Flength
		} else {
			length4 = uint32(ZDICT_count(tls, b+uintptr(pos), b+uintptr(testedPos)))
			if length4 > (*(*dictItem)(unsafe.Pointer(bp + 512))).Flength {
				length4 = (*(*dictItem)(unsafe.Pointer(bp + 512))).Flength
			}
		}
		pEnd = testedPos + length4
		p = testedPos
		for {
			if !(p < pEnd) {
				break
			}
			*(*BYTE)(unsafe.Pointer(doneMarks + uintptr(p))) = uint8(1)
			goto _9
		_9:
			;
			p = p + 1
		}
		goto _8
	_8:
		;
		id1 = id1 + 1
	}
	return *(*dictItem)(unsafe.Pointer(bp + 512))
}

func isIncluded(tls *libc.TLS, in uintptr, container uintptr, length size_t) (r int32) {
	var into, ip uintptr
	var u size_t
	_, _, _ = into, ip, u
	ip = in
	into = container
	u = uint64(0)
	for {
		if !(u < length) {
			break
		} /* works because end of buffer is a noisy guard band */
		if libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(ip + uintptr(u)))) != libc.Int32FromUint8(*(*uint8)(unsafe.Pointer(into + uintptr(u)))) {
			break
		}
		goto _1
	_1:
		;
		u = u + 1
	}
	return libc.BoolInt32(u == length)
}

// C documentation
//
//	/*! ZDICT_tryMerge() :
//	    check if dictItem can be merged, do it if possible
//	    @return : id of destination elt, 0 if not merged
//	*/
func ZDICT_tryMerge(tls *libc.TLS, table uintptr, elt dictItem, eltNbToSkip U32, buffer uintptr) (r U32) {
	var addedLength, eltEnd, tableSize, u U32
	var addedLength1, v3 int32
	var addedLength2 size_t
	var buf uintptr
	var v4 uint32
	_, _, _, _, _, _, _, _, _ = addedLength, addedLength1, addedLength2, buf, eltEnd, tableSize, u, v3, v4
	tableSize = (*dictItem)(unsafe.Pointer(table)).Fpos
	eltEnd = elt.Fpos + elt.Flength
	buf = buffer
	u = uint32(1)
	for {
		if !(u < tableSize) {
			break
		}
		if u == eltNbToSkip {
			goto _1
		}
		if (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos > elt.Fpos && (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos <= eltEnd { /* overlap, existing > new */
			/* append */
			addedLength = (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos - elt.Fpos
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength += addedLength
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos = elt.Fpos
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Fsavings * addedLength / elt.Flength /* rough approx */
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Flength / uint32(8)                  /* rough approx bonus */
			elt = *(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))
			/* sort : improve rank */
			for u > uint32(1) && (*(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))).Fsavings < elt.Fsavings {
				*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))
				u = u - 1
			}
			*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = elt
			return u
		}
		goto _1
	_1:
		;
		u = u + 1
	}
	/* front overlap */
	u = uint32(1)
	for {
		if !(u < tableSize) {
			break
		}
		if u == eltNbToSkip {
			goto _2
		}
		if (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos+(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength >= elt.Fpos && (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos < elt.Fpos { /* overlap, existing < new */
			/* append */
			addedLength1 = libc.Int32FromUint32(eltEnd) - libc.Int32FromUint32((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos+(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength)
			(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Flength / uint32(8) /* rough approx bonus */
			if addedLength1 > 0 {                                                                     /* otherwise, elt fully included into existing */
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength += libc.Uint32FromInt32(addedLength1)
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += elt.Fsavings * libc.Uint32FromInt32(addedLength1) / elt.Flength /* rough approx */
			}
			/* sort : improve rank */
			elt = *(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))
			for u > uint32(1) && (*(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))).Fsavings < elt.Fsavings {
				*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(u-uint32(1))*12))
				u = u - 1
			}
			*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = elt
			return u
		}
		if MEM_read64(tls, buf+uintptr((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos)) == MEM_read64(tls, buf+uintptr(elt.Fpos)+uintptr(1)) {
			if isIncluded(tls, buf+uintptr((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos), buf+uintptr(elt.Fpos)+uintptr(1), uint64((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength)) != 0 {
				if libc.Int32FromUint32(elt.Flength)-libc.Int32FromUint32((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength) > int32(1) {
					v3 = libc.Int32FromUint32(elt.Flength) - libc.Int32FromUint32((*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength)
				} else {
					v3 = int32(1)
				}
				addedLength2 = libc.Uint64FromInt32(v3)
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fpos = elt.Fpos
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Fsavings += uint32(uint64(elt.Fsavings) * addedLength2 / uint64(elt.Flength))
				if elt.Flength < (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength+uint32(1) {
					v4 = elt.Flength
				} else {
					v4 = (*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength + uint32(1)
				}
				(*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12))).Flength = v4
				return u
			}
		}
		goto _2
	_2:
		;
		u = u + 1
	}
	return uint32(0)
}

func ZDICT_removeDictItem(tls *libc.TLS, table uintptr, id U32) {
	var max, u U32
	_, _ = max, u
	/* convention : table[0].pos stores nb of elts */
	max = (*(*dictItem)(unsafe.Pointer(table))).Fpos
	if !(id != 0) {
		return
	} /* protection, should never happen */
	u = id
	for {
		if !(u < max-uint32(1)) {
			break
		}
		*(*dictItem)(unsafe.Pointer(table + uintptr(u)*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(u+uint32(1))*12))
		goto _1
	_1:
		;
		u = u + 1
	}
	(*dictItem)(unsafe.Pointer(table)).Fpos = (*dictItem)(unsafe.Pointer(table)).Fpos - 1
}

func ZDICT_insertDictItem(tls *libc.TLS, table uintptr, maxSize U32, elt dictItem, buffer uintptr) {
	var current, mergeId, newMerge, nextElt U32
	_, _, _, _ = current, mergeId, newMerge, nextElt
	/* merge if possible */
	mergeId = ZDICT_tryMerge(tls, table, elt, uint32(0), buffer)
	if mergeId != 0 {
		newMerge = uint32(1)
		for newMerge != 0 {
			newMerge = ZDICT_tryMerge(tls, table, *(*dictItem)(unsafe.Pointer(table + uintptr(mergeId)*12)), mergeId, buffer)
			if newMerge != 0 {
				ZDICT_removeDictItem(tls, table, mergeId)
			}
			mergeId = newMerge
		}
		return
	}
	/* insert */
	nextElt = (*dictItem)(unsafe.Pointer(table)).Fpos
	if nextElt >= maxSize {
		nextElt = maxSize - uint32(1)
	}
	current = nextElt - uint32(1)
	for (*(*dictItem)(unsafe.Pointer(table + uintptr(current)*12))).Fsavings < elt.Fsavings {
		*(*dictItem)(unsafe.Pointer(table + uintptr(current+uint32(1))*12)) = *(*dictItem)(unsafe.Pointer(table + uintptr(current)*12))
		current = current - 1
	}
	*(*dictItem)(unsafe.Pointer(table + uintptr(current+uint32(1))*12)) = elt
	(*dictItem)(unsafe.Pointer(table)).Fpos = nextElt + uint32(1)
}

func ZDICT_dictSize(tls *libc.TLS, dictList uintptr) (r U32) {
	var dictSize, u U32
	_, _ = dictSize, u
	dictSize = uint32(0)
	u = uint32(1)
	for {
		if !(u < (*(*dictItem)(unsafe.Pointer(dictList))).Fpos) {
			break
		}
		dictSize = dictSize + (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Flength
		goto _1
	_1:
		;
		u = u + 1
	}
	return dictSize
}

func ZDICT_trainBuffer_legacy(tls *libc.TLS, dictList uintptr, dictListSize U32, buffer uintptr, bufferSize size_t, fileSizes uintptr, nbFiles uint32, minRatio uint32, notificationLevel U32) (r size_t) {
	bp := tls.Alloc(32)
	defer tls.Free(32)
	var cursor U32
	var displayClock, refreshRate clock_t
	var divSuftSortResult int32
	var doneMarks, filePos, reverseSuffix, suffix, suffix0 uintptr
	var pos, result size_t
	var solution dictItem
	var v1 uint32
	_, _, _, _, _, _, _, _, _, _, _, _, _ = cursor, displayClock, divSuftSortResult, doneMarks, filePos, pos, refreshRate, result, reverseSuffix, solution, suffix, suffix0, v1
	suffix0 = libc.Xmalloc(tls, (bufferSize+uint64(2))*uint64(4))
	suffix = suffix0 + uintptr(1)*4
	reverseSuffix = libc.Xmalloc(tls, bufferSize*uint64(4))
	doneMarks = libc.Xmalloc(tls, (bufferSize+uint64(16))*uint64(1)) /* +16 for overflow security */
	filePos = libc.Xmalloc(tls, uint64(nbFiles)*uint64(4))
	result = uint64(0)
	displayClock = 0
	refreshRate = libc.Int64FromInt64(1000000) * libc.Int64FromInt32(3) / libc.Int64FromInt32(10)
	/* init */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10341, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.Xstderr)
	} /* clean display line */
	if !(suffix0 != 0) || !(reverseSuffix != 0) || !(doneMarks != 0) || !(filePos != 0) {
		result = libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		goto _cleanup
	}
	if minRatio < uint32(MINRATIO) {
		minRatio = uint32(MINRATIO)
	}
	libc.Xmemset(tls, doneMarks, 0, bufferSize+uint64(16))
	/* limit sample set size (divsufsort limitation)*/
	if bufferSize > uint64(libc.Uint32FromUint32(2000)<<libc.Int32FromInt32(20)) {
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10348, libc.VaList(bp+8, libc.Uint32FromUint32(2000)<<libc.Int32FromInt32(20)>>libc.Int32FromInt32(20)))
			libc.Xfflush(tls, libc.Xstderr)
		}
	}
	for bufferSize > uint64(libc.Uint32FromUint32(2000)<<libc.Int32FromInt32(20)) {
		nbFiles = nbFiles - 1
		v1 = nbFiles
		bufferSize = bufferSize - *(*size_t)(unsafe.Pointer(fileSizes + uintptr(v1)*8))
	}
	/* sort */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10393, libc.VaList(bp+8, nbFiles, uint32(bufferSize>>libc.Int32FromInt32(20))))
		libc.Xfflush(tls, libc.Xstderr)
	}
	divSuftSortResult = divsufsort(tls, buffer, suffix, libc.Int32FromUint64(bufferSize), 0)
	if divSuftSortResult != 0 {
		result = libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		goto _cleanup
	}
	*(*int32)(unsafe.Pointer(suffix + uintptr(bufferSize)*4)) = libc.Int32FromUint64(bufferSize) /* leads into noise */
	*(*int32)(unsafe.Pointer(suffix0)) = libc.Int32FromUint64(bufferSize)                        /* leads into noise */
	/* build reverse suffix sort */
	pos = uint64(0)
	for {
		if !(pos < bufferSize) {
			break
		}
		*(*U32)(unsafe.Pointer(reverseSuffix + uintptr(*(*int32)(unsafe.Pointer(suffix + uintptr(pos)*4)))*4)) = uint32(pos)
		goto _2
	_2:
		;
		pos = pos + 1
	}
	/* note filePos tracks borders between samples.
	   It's not used at this stage, but planned to become useful in a later update */
	*(*U32)(unsafe.Pointer(filePos)) = uint32(0)
	pos = uint64(1)
	for {
		if !(pos < uint64(nbFiles)) {
			break
		}
		*(*U32)(unsafe.Pointer(filePos + uintptr(pos)*4)) = uint32(uint64(*(*U32)(unsafe.Pointer(filePos + uintptr(pos-uint64(1))*4))) + *(*size_t)(unsafe.Pointer(fileSizes + uintptr(pos-uint64(1))*8)))
		goto _3
	_3:
		;
		pos = pos + 1
	}
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10435, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	if notificationLevel >= uint32(3) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10458, libc.VaList(bp+8, minRatio))
		libc.Xfflush(tls, libc.Xstderr)
	}
	cursor = uint32(0)
	for {
		if !(uint64(cursor) < bufferSize) {
			break
		}
		if *(*BYTE)(unsafe.Pointer(doneMarks + uintptr(cursor))) != 0 {
			cursor = cursor + 1
			goto _4
		}
		solution = ZDICT_analyzePos(tls, doneMarks, suffix, *(*U32)(unsafe.Pointer(reverseSuffix + uintptr(cursor)*4)), buffer, minRatio, notificationLevel)
		if solution.Flength == uint32(0) {
			cursor = cursor + 1
			goto _4
		}
		ZDICT_insertDictItem(tls, dictList, dictListSize, solution, buffer)
		cursor = cursor + solution.Flength
		if notificationLevel >= uint32(2) {
			if ZDICT_clockSpan(tls, displayClock) > refreshRate {
				displayClock = libc.Xclock(tls)
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10479, libc.VaList(bp+8, float64(float64(cursor)/float64(bufferSize)*float64(100))))
				libc.Xfflush(tls, libc.Xstderr)
			}
			if notificationLevel >= uint32(4) {
				libc.Xfflush(tls, libc.Xstderr)
			}
		}
		goto _4
	_4:
	}
	goto _cleanup
_cleanup:
	;
	libc.Xfree(tls, suffix0)
	libc.Xfree(tls, reverseSuffix)
	libc.Xfree(tls, doneMarks)
	libc.Xfree(tls, filePos)
	return result
}

func ZDICT_fillNoise(tls *libc.TLS, buffer uintptr, length size_t) {
	var acc, prime1, prime2 uint32
	var p size_t
	_, _, _, _ = acc, p, prime1, prime2
	prime1 = uint32(2654435761)
	prime2 = uint32(2246822519)
	acc = prime1
	p = uint64(0)
	p = uint64(0)
	for {
		if !(p < length) {
			break
		}
		acc = acc * prime2
		*(*uint8)(unsafe.Pointer(buffer + uintptr(p))) = uint8(acc >> libc.Int32FromInt32(21))
		goto _1
	_1:
		;
		p = p + 1
	}
}

type EStats_ress_t = struct {
	Fdict      uintptr
	Fzc        uintptr
	FworkPlace uintptr
}

func ZDICT_countEStats(tls *libc.TLS, esr EStats_ress_t, params uintptr, countLit uintptr, offsetcodeCount uintptr, matchlengthCount uintptr, litlengthCount uintptr, repOffsets uintptr, src uintptr, srcSize size_t, notificationLevel U32) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var blockSizeMax, cSize, errorCode size_t
	var bytePtr, codePtr, codePtr1, codePtr2, seq, seqStorePtr uintptr
	var nbSeq, offset1, offset2, u, u1, u2 U32
	var v1 int32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = blockSizeMax, bytePtr, cSize, codePtr, codePtr1, codePtr2, errorCode, nbSeq, offset1, offset2, seq, seqStorePtr, u, u1, u2, v1
	if libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX) < int32(1)<<(*ZSTD_parameters)(unsafe.Pointer(params)).FcParams.FwindowLog {
		v1 = libc.Int32FromInt32(1) << libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)
	} else {
		v1 = int32(1) << (*ZSTD_parameters)(unsafe.Pointer(params)).FcParams.FwindowLog
	}
	blockSizeMax = libc.Uint64FromInt32(v1)
	if srcSize > blockSizeMax {
		srcSize = blockSizeMax
	} /* protection vs large samples */
	errorCode = ZSTD_compressBegin_usingCDict_deprecated(tls, esr.Fzc, esr.Fdict)
	if ZSTD_isError(tls, errorCode) != 0 {
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10491, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		return
	}
	cSize = ZSTD_compressBlock_deprecated(tls, esr.Fzc, esr.FworkPlace, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)), src, srcSize)
	if ZSTD_isError(tls, cSize) != 0 {
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10540, libc.VaList(bp+8, uint32(srcSize)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		return
	}
	if cSize != 0 { /* if == 0; block is not compressible */
		seqStorePtr = ZSTD_getSeqStore(tls, esr.Fzc)
		/* literals stats */
		bytePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FlitStart
		for {
			if !(bytePtr < (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Flit) {
				break
			}
			*(*uint32)(unsafe.Pointer(countLit + uintptr(*(*BYTE)(unsafe.Pointer(bytePtr)))*4)) = *(*uint32)(unsafe.Pointer(countLit + uintptr(*(*BYTE)(unsafe.Pointer(bytePtr)))*4)) + 1
			goto _2
		_2:
			;
			bytePtr = bytePtr + 1
		}
		/* seqStats */
		nbSeq = libc.Uint32FromInt64((int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).Fsequences) - int64((*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart)) / 8)
		ZSTD_seqToCodes(tls, seqStorePtr)
		codePtr = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FofCode
		u = uint32(0)
		for {
			if !(u < nbSeq) {
				break
			}
			*(*uint32)(unsafe.Pointer(offsetcodeCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr + uintptr(u))))*4)) = *(*uint32)(unsafe.Pointer(offsetcodeCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr + uintptr(u))))*4)) + 1
			goto _3
		_3:
			;
			u = u + 1
		}
		codePtr1 = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FmlCode
		u1 = uint32(0)
		for {
			if !(u1 < nbSeq) {
				break
			}
			*(*uint32)(unsafe.Pointer(matchlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr1 + uintptr(u1))))*4)) = *(*uint32)(unsafe.Pointer(matchlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr1 + uintptr(u1))))*4)) + 1
			goto _4
		_4:
			;
			u1 = u1 + 1
		}
		codePtr2 = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FllCode
		u2 = uint32(0)
		for {
			if !(u2 < nbSeq) {
				break
			}
			*(*uint32)(unsafe.Pointer(litlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr2 + uintptr(u2))))*4)) = *(*uint32)(unsafe.Pointer(litlengthCount + uintptr(*(*BYTE)(unsafe.Pointer(codePtr2 + uintptr(u2))))*4)) + 1
			goto _5
		_5:
			;
			u2 = u2 + 1
		}
		if nbSeq >= uint32(2) { /* rep offsets */
			seq = (*SeqStore_t)(unsafe.Pointer(seqStorePtr)).FsequencesStart
			offset1 = (*(*SeqDef)(unsafe.Pointer(seq))).FoffBase - uint32(ZSTD_REP_NUM)
			offset2 = (*(*SeqDef)(unsafe.Pointer(seq + 1*8))).FoffBase - uint32(ZSTD_REP_NUM)
			if offset1 >= uint32(MAXREPOFFSET) {
				offset1 = uint32(0)
			}
			if offset2 >= uint32(MAXREPOFFSET) {
				offset2 = uint32(0)
			}
			*(*U32)(unsafe.Pointer(repOffsets + uintptr(offset1)*4)) += uint32(3)
			*(*U32)(unsafe.Pointer(repOffsets + uintptr(offset2)*4)) += uint32(1)
		}
	}
}

func ZDICT_totalSampleSize(tls *libc.TLS, fileSizes uintptr, nbFiles uint32) (r size_t) {
	var total size_t
	var u uint32
	_, _ = total, u
	total = uint64(0)
	u = uint32(0)
	for {
		if !(u < nbFiles) {
			break
		}
		total = total + *(*size_t)(unsafe.Pointer(fileSizes + uintptr(u)*8))
		goto _1
	_1:
		;
		u = u + 1
	}
	return total
}

type offsetCount_t = struct {
	Foffset U32
	Fcount  U32
}

func ZDICT_insertSortCount(tls *libc.TLS, table uintptr, val U32, count U32) {
	var tmp offsetCount_t
	var u U32
	_, _ = tmp, u
	(*(*offsetCount_t)(unsafe.Pointer(table + 3*8))).Foffset = val
	(*(*offsetCount_t)(unsafe.Pointer(table + 3*8))).Fcount = count
	u = uint32(ZSTD_REP_NUM)
	for {
		if !(u > uint32(0)) {
			break
		}
		if (*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u-uint32(1))*8))).Fcount >= (*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u)*8))).Fcount {
			break
		}
		tmp = *(*offsetCount_t)(unsafe.Pointer(table + uintptr(u-uint32(1))*8))
		*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u-uint32(1))*8)) = *(*offsetCount_t)(unsafe.Pointer(table + uintptr(u)*8))
		*(*offsetCount_t)(unsafe.Pointer(table + uintptr(u)*8)) = tmp
		goto _1
	_1:
		;
		u = u - 1
	}
}

// C documentation
//
//	/* ZDICT_flatLit() :
//	 * rewrite `countLit` to contain a mostly flat but still compressible distribution of literals.
//	 * necessary to avoid generating a non-compressible distribution that HUF_writeCTable() cannot encode.
//	 */
func ZDICT_flatLit(tls *libc.TLS, countLit uintptr) {
	var u int32
	_ = u
	u = int32(1)
	for {
		if !(u < int32(256)) {
			break
		}
		*(*uint32)(unsafe.Pointer(countLit + uintptr(u)*4)) = uint32(2)
		goto _1
	_1:
		;
		u = u + 1
	}
	*(*uint32)(unsafe.Pointer(countLit)) = uint32(4)
	*(*uint32)(unsafe.Pointer(countLit + 253*4)) = uint32(1)
	*(*uint32)(unsafe.Pointer(countLit + 254*4)) = uint32(1)
}

func ZDICT_analyzeEntropy(tls *libc.TLS, dstBuffer uintptr, maxDstSize size_t, compressionLevel int32, srcBuffer uintptr, fileSizes uintptr, nbFiles uint32, dictBuffer uintptr, dictBufferSize size_t, notificationLevel uint32) (r size_t) {
	bp := tls.Alloc(12864)
	defer tls.Free(12864)
	var Offlog, huffLog, llLog, mlLog, offcodeMax, offset, total, u, v5, v6 U32
	var averageSampleSize, eSize, errorCode, hhSize, lhSize, maxNbBits, mhSize, ohSize, pos, totalSrcSize size_t
	var dstPtr uintptr
	var esr EStats_ress_t
	var v9 bool
	var _ /* bestRepOffset at bp+7900 */ [4]offsetCount_t
	var _ /* countLit at bp+0 */ [256]uint32
	var _ /* hufTable at bp+1024 */ [257]HUF_CElt
	var _ /* litLengthCount at bp+3588 */ [36]uint32
	var _ /* litLengthNCount at bp+3732 */ [36]int16
	var _ /* matchLengthCount at bp+3268 */ [53]uint32
	var _ /* matchLengthNCount at bp+3480 */ [53]int16
	var _ /* offcodeCount at bp+3080 */ [31]uint32
	var _ /* offcodeNCount at bp+3204 */ [31]int16
	var _ /* params at bp+7932 */ ZSTD_parameters
	var _ /* repOffset at bp+3804 */ [1024]U32
	var _ /* wksp at bp+7972 */ [1216]U32
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = Offlog, averageSampleSize, dstPtr, eSize, errorCode, esr, hhSize, huffLog, lhSize, llLog, maxNbBits, mhSize, mlLog, offcodeMax, offset, ohSize, pos, total, totalSrcSize, u, v5, v6, v9
	offcodeMax = ZSTD_highbit32(tls, uint32(dictBufferSize+libc.Uint64FromInt32(libc.Int32FromInt32(128)*(libc.Int32FromInt32(1)<<libc.Int32FromInt32(10)))))
	esr = EStats_ress_t{}
	huffLog = uint32(11)
	Offlog = uint32(OffFSELog)
	mlLog = uint32(MLFSELog)
	llLog = uint32(LLFSELog)
	pos = uint64(0)
	eSize = uint64(0)
	totalSrcSize = ZDICT_totalSampleSize(tls, fileSizes, nbFiles)
	averageSampleSize = totalSrcSize / uint64(nbFiles+libc.BoolUint32(!(nbFiles != 0)))
	dstPtr = dstBuffer
	/* init */
	if offcodeMax > uint32(OFFCODE_MAX) {
		eSize = libc.Uint64FromInt32(-int32(ZSTD_error_dictionaryCreation_failed))
		goto _cleanup
	} /* too large dictionary */
	u = uint32(0)
	for {
		if !(u < uint32(256)) {
			break
		}
		(*(*[256]uint32)(unsafe.Pointer(bp)))[u] = uint32(1)
		goto _1
	_1:
		;
		u = u + 1
	} /* any character must be described */
	u = uint32(0)
	for {
		if !(u <= offcodeMax) {
			break
		}
		(*(*[31]uint32)(unsafe.Pointer(bp + 3080)))[u] = uint32(1)
		goto _2
	_2:
		;
		u = u + 1
	}
	u = uint32(0)
	for {
		if !(u <= uint32(MaxML)) {
			break
		}
		(*(*[53]uint32)(unsafe.Pointer(bp + 3268)))[u] = uint32(1)
		goto _3
	_3:
		;
		u = u + 1
	}
	u = uint32(0)
	for {
		if !(u <= uint32(MaxLL)) {
			break
		}
		(*(*[36]uint32)(unsafe.Pointer(bp + 3588)))[u] = uint32(1)
		goto _4
	_4:
		;
		u = u + 1
	}
	libc.Xmemset(tls, bp+3804, 0, uint64(4096))
	v6 = libc.Uint32FromInt32(1)
	(*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[int32(8)] = v6
	v5 = v6
	(*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[int32(4)] = v5
	(*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[int32(1)] = v5
	libc.Xmemset(tls, bp+7900, 0, uint64(32))
	if compressionLevel == 0 {
		compressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	}
	*(*ZSTD_parameters)(unsafe.Pointer(bp + 7932)) = ZSTD_getParams(tls, compressionLevel, averageSampleSize, dictBufferSize)
	esr.Fdict = ZSTD_createCDict_advanced(tls, dictBuffer, dictBufferSize, int32(ZSTD_dlm_byRef), int32(ZSTD_dct_rawContent), (*(*ZSTD_parameters)(unsafe.Pointer(bp + 7932))).FcParams, ZSTD_defaultCMem)
	esr.Fzc = ZSTD_createCCtx(tls)
	esr.FworkPlace = libc.Xmalloc(tls, libc.Uint64FromInt32(libc.Int32FromInt32(1)<<libc.Int32FromInt32(ZSTD_BLOCKSIZELOG_MAX)))
	if !(esr.Fdict != 0) || !(esr.Fzc != 0) || !(esr.FworkPlace != 0) {
		eSize = libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10586, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	/* collect stats on all samples */
	u = uint32(0)
	for {
		if !(u < nbFiles) {
			break
		}
		ZDICT_countEStats(tls, esr, bp+7932, bp, bp+3080, bp+3268, bp+3588, bp+3804, srcBuffer+uintptr(pos), *(*size_t)(unsafe.Pointer(fileSizes + uintptr(u)*8)), notificationLevel)
		pos = pos + *(*size_t)(unsafe.Pointer(fileSizes + uintptr(u)*8))
		goto _7
	_7:
		;
		u = u + 1
	}
	if notificationLevel >= uint32(4) {
		/* writeStats */
		if notificationLevel >= uint32(4) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10606, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		u = uint32(0)
		for {
			if !(u <= offcodeMax) {
				break
			}
			if notificationLevel >= uint32(4) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10634, libc.VaList(bp+12848, u, (*(*[31]uint32)(unsafe.Pointer(bp + 3080)))[u]))
				libc.Xfflush(tls, libc.Xstderr)
			}
			goto _8
		_8:
			;
			u = u + 1
		}
	}
	/* analyze, build stats, starting with literals */
	maxNbBits = HUF_buildCTable_wksp(tls, bp+1024, bp, uint32(255), huffLog, bp+7972, uint64(4864))
	if ERR_isError(tls, maxNbBits) != 0 {
		eSize = maxNbBits
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10645, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	if maxNbBits == uint64(8) { /* not compressible : will fail on HUF_writeCTable() */
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10670, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		ZDICT_flatLit(tls, bp) /* replace distribution by a fake "mostly flat but still compressible" distribution, that HUF_writeCTable() can encode */
		maxNbBits = HUF_buildCTable_wksp(tls, bp+1024, bp, uint32(255), huffLog, bp+7972, uint64(4864))
		if v9 = maxNbBits == uint64(9); !v9 {
			libc.X__assert_fail(tls, __ccgo_ts+10770, __ccgo_ts+9627, int32(51858), uintptr(unsafe.Pointer(&__func__8)))
		}
		_ = v9 || libc.Bool(libc.Int32FromInt32(0) != 0)
	}
	huffLog = uint32(maxNbBits)
	/* looking for most common first offsets */
	offset = uint32(1)
	for {
		if !(offset < uint32(MAXREPOFFSET)) {
			break
		}
		ZDICT_insertSortCount(tls, bp+7900, offset, (*(*[1024]U32)(unsafe.Pointer(bp + 3804)))[offset])
		goto _10
	_10:
		;
		offset = offset + 1
	}
	/* note : the result of this phase should be used to better appreciate the impact on statistics */
	total = uint32(0)
	u = uint32(0)
	for {
		if !(u <= offcodeMax) {
			break
		}
		total = total + (*(*[31]uint32)(unsafe.Pointer(bp + 3080)))[u]
		goto _11
	_11:
		;
		u = u + 1
	}
	errorCode = FSE_normalizeCount(tls, bp+3204, Offlog, bp+3080, uint64(total), offcodeMax, uint32(1))
	if ERR_isError(tls, errorCode) != 0 {
		eSize = errorCode
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10783, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	Offlog = uint32(errorCode)
	total = uint32(0)
	u = uint32(0)
	for {
		if !(u <= uint32(MaxML)) {
			break
		}
		total = total + (*(*[53]uint32)(unsafe.Pointer(bp + 3268)))[u]
		goto _12
	_12:
		;
		u = u + 1
	}
	errorCode = FSE_normalizeCount(tls, bp+3480, mlLog, bp+3268, uint64(total), uint32(MaxML), uint32(1))
	if ERR_isError(tls, errorCode) != 0 {
		eSize = errorCode
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10828, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	mlLog = uint32(errorCode)
	total = uint32(0)
	u = uint32(0)
	for {
		if !(u <= uint32(MaxLL)) {
			break
		}
		total = total + (*(*[36]uint32)(unsafe.Pointer(bp + 3588)))[u]
		goto _13
	_13:
		;
		u = u + 1
	}
	errorCode = FSE_normalizeCount(tls, bp+3732, llLog, bp+3588, uint64(total), uint32(MaxLL), uint32(1))
	if ERR_isError(tls, errorCode) != 0 {
		eSize = errorCode
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10877, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	llLog = uint32(errorCode)
	/* write result to buffer */
	hhSize = HUF_writeCTable_wksp(tls, dstPtr, maxDstSize, bp+1024, uint32(255), huffLog, bp+7972, uint64(4864))
	if ERR_isError(tls, hhSize) != 0 {
		eSize = hhSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10924, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(hhSize)
	maxDstSize = maxDstSize - hhSize
	eSize = eSize + hhSize
	ohSize = FSE_writeNCount(tls, dstPtr, maxDstSize, bp+3204, uint32(OFFCODE_MAX), Offlog)
	if ERR_isError(tls, ohSize) != 0 {
		eSize = ohSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10948, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(ohSize)
	maxDstSize = maxDstSize - ohSize
	eSize = eSize + ohSize
	mhSize = FSE_writeNCount(tls, dstPtr, maxDstSize, bp+3480, uint32(MaxML), mlLog)
	if ERR_isError(tls, mhSize) != 0 {
		eSize = mhSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10991, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(mhSize)
	maxDstSize = maxDstSize - mhSize
	eSize = eSize + mhSize
	lhSize = FSE_writeNCount(tls, dstPtr, maxDstSize, bp+3732, uint32(MaxLL), llLog)
	if ERR_isError(tls, lhSize) != 0 {
		eSize = lhSize
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11038, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	dstPtr = dstPtr + uintptr(lhSize)
	maxDstSize = maxDstSize - lhSize
	eSize = eSize + lhSize
	if maxDstSize < uint64(12) {
		eSize = libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		if notificationLevel >= uint32(1) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11083, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
		goto _cleanup
	}
	/* at this stage, we don't use the result of "most common first offset",
	 * as the impact of statistics is not properly evaluated */
	MEM_writeLE32(tls, dstPtr+uintptr(0), repStartValue[0])
	MEM_writeLE32(tls, dstPtr+uintptr(4), repStartValue[int32(1)])
	MEM_writeLE32(tls, dstPtr+uintptr(8), repStartValue[int32(2)])
	eSize = eSize + uint64(12)
	goto _cleanup
_cleanup:
	;
	ZSTD_freeCDict(tls, esr.Fdict)
	ZSTD_freeCCtx(tls, esr.Fzc)
	libc.Xfree(tls, esr.FworkPlace)
	return eSize
}

var __func__8 = [21]uint8{'Z', 'D', 'I', 'C', 'T', '_', 'a', 'n', 'a', 'l', 'y', 'z', 'e', 'E', 'n', 't', 'r', 'o', 'p', 'y'}

// C documentation
//
//	/**
//	 * @returns the maximum repcode value
//	 */
func ZDICT_maxRep(tls *libc.TLS, reps uintptr) (r1 U32) {
	var maxRep U32
	var r int32
	var v2 uint32
	_, _, _ = maxRep, r, v2
	maxRep = *(*U32)(unsafe.Pointer(reps))
	r = int32(1)
	for {
		if !(r < int32(ZSTD_REP_NUM)) {
			break
		}
		if maxRep > *(*U32)(unsafe.Pointer(reps + uintptr(r)*4)) {
			v2 = maxRep
		} else {
			v2 = *(*U32)(unsafe.Pointer(reps + uintptr(r)*4))
		}
		maxRep = v2
		goto _1
	_1:
		;
		r = r + 1
	}
	return maxRep
}

func ZDICT_finalizeDictionary(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, customDictContent uintptr, dictContentSize size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_params_t) (r size_t) {
	bp := tls.Alloc(272)
	defer tls.Free(272)
	var compliantID, dictID, notificationLevel U32
	var compressionLevel, v1 int32
	var dictSize, eSize, hSize, minContentSize, paddingSize size_t
	var outDictContent, outDictHeader, outDictPadding uintptr
	var randomID U64
	var v2 uint32
	var v3 bool
	var _ /* header at bp+0 */ [256]BYTE
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = compliantID, compressionLevel, dictID, dictSize, eSize, hSize, minContentSize, notificationLevel, outDictContent, outDictHeader, outDictPadding, paddingSize, randomID, v1, v2, v3
	if params.FcompressionLevel == 0 {
		v1 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v1 = params.FcompressionLevel
	}
	compressionLevel = v1
	notificationLevel = params.FnotificationLevel
	/* The final dictionary content must be at least as large as the largest repcode */
	minContentSize = uint64(ZDICT_maxRep(tls, uintptr(unsafe.Pointer(&repStartValue))))
	/* check conditions */
	if dictBufferCapacity < dictContentSize {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	if dictBufferCapacity < uint64(ZDICT_DICTSIZE_MIN) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	}
	/* dictionary header */
	MEM_writeLE32(tls, bp, uint32(ZSTD_MAGIC_DICTIONARY))
	randomID = XXH_INLINE_XXH64(tls, customDictContent, dictContentSize, uint64(0))
	compliantID = uint32(randomID%uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)-libc.Uint32FromInt32(32768)) + uint64(32768))
	if params.FdictID != 0 {
		v2 = params.FdictID
	} else {
		v2 = compliantID
	}
	dictID = v2
	MEM_writeLE32(tls, bp+uintptr(4), dictID)
	hSize = uint64(8)
	/* entropy tables */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10341, libc.VaList(bp+264, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.Xstderr)
	} /* clean display line */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11122, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	eSize = ZDICT_analyzeEntropy(tls, bp+uintptr(hSize), uint64(HBUFFSIZE)-hSize, compressionLevel, samplesBuffer, samplesSizes, nbSamples, customDictContent, dictContentSize, notificationLevel)
	if ZDICT_isError(tls, eSize) != 0 {
		return eSize
	}
	hSize = hSize + eSize
	/* Shrink the content size if it doesn't fit in the buffer */
	if hSize+dictContentSize > dictBufferCapacity {
		dictContentSize = dictBufferCapacity - hSize
	}
	/* Pad the dictionary content with zeros if it is too small */
	if dictContentSize < minContentSize {
		if hSize+minContentSize > dictBufferCapacity {
			if 0 != 0 {
				_force_has_format_string(tls, __ccgo_ts+11139, 0)
			}
			return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
		}
		paddingSize = minContentSize - dictContentSize
	} else {
		paddingSize = uint64(0)
	}
	dictSize = hSize + paddingSize + dictContentSize
	/* The dictionary consists of the header, optional padding, and the content.
	 * The padding comes before the content because the "best" position in the
	 * dictionary is the last byte.
	 */
	outDictHeader = dictBuffer
	outDictPadding = outDictHeader + uintptr(hSize)
	outDictContent = outDictPadding + uintptr(paddingSize)
	if v3 = dictSize <= dictBufferCapacity; !v3 {
		libc.X__assert_fail(tls, __ccgo_ts+11187, __ccgo_ts+9627, int32(52046), uintptr(unsafe.Pointer(&__func__9)))
	}
	_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
	if v3 = outDictContent+uintptr(dictContentSize) == dictBuffer+uintptr(dictSize); !v3 {
		libc.X__assert_fail(tls, __ccgo_ts+11218, __ccgo_ts+9627, int32(52047), uintptr(unsafe.Pointer(&__func__9)))
	}
	_ = v3 || libc.Bool(libc.Int32FromInt32(0) != 0)
	/* First copy the customDictContent into its final location.
	 * `customDictContent` and `dictBuffer` may overlap, so we must
	 * do this before any other writes into the output buffer.
	 * Then copy the header & padding into the output buffer.
	 */
	libc.Xmemmove(tls, outDictContent, customDictContent, dictContentSize)
	libc.Xmemcpy(tls, outDictHeader, bp, hSize)
	libc.Xmemset(tls, outDictPadding, 0, paddingSize)
	return dictSize
	return r
}

var __func__9 = [25]uint8{'Z', 'D', 'I', 'C', 'T', '_', 'f', 'i', 'n', 'a', 'l', 'i', 'z', 'e', 'D', 'i', 'c', 't', 'i', 'o', 'n', 'a', 'r', 'y'}

func ZDICT_addEntropyTablesFromBuffer_advanced(tls *libc.TLS, dictBuffer uintptr, dictContentSize size_t, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_params_t) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var compliantID, dictID, notificationLevel U32
	var compressionLevel, v1 int32
	var eSize, hSize size_t
	var randomID U64
	var v2 uint32
	var v3 uint64
	_, _, _, _, _, _, _, _, _, _ = compliantID, compressionLevel, dictID, eSize, hSize, notificationLevel, randomID, v1, v2, v3
	if params.FcompressionLevel == 0 {
		v1 = int32(ZSTD_CLEVEL_DEFAULT)
	} else {
		v1 = params.FcompressionLevel
	}
	compressionLevel = v1
	notificationLevel = params.FnotificationLevel
	hSize = uint64(8)
	/* calculate entropy tables */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+10341, libc.VaList(bp+8, __ccgo_ts+1319))
		libc.Xfflush(tls, libc.Xstderr)
	} /* clean display line */
	if notificationLevel >= uint32(2) {
		libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11122, 0)
		libc.Xfflush(tls, libc.Xstderr)
	}
	eSize = ZDICT_analyzeEntropy(tls, dictBuffer+uintptr(hSize), dictBufferCapacity-hSize, compressionLevel, samplesBuffer, samplesSizes, nbSamples, dictBuffer+uintptr(dictBufferCapacity)-uintptr(dictContentSize), dictContentSize, notificationLevel)
	if ZDICT_isError(tls, eSize) != 0 {
		return eSize
	}
	hSize = hSize + eSize
	/* add dictionary header (after entropy tables) */
	MEM_writeLE32(tls, dictBuffer, uint32(ZSTD_MAGIC_DICTIONARY))
	randomID = XXH_INLINE_XXH64(tls, dictBuffer+uintptr(dictBufferCapacity)-uintptr(dictContentSize), dictContentSize, uint64(0))
	compliantID = uint32(randomID%uint64(libc.Uint32FromUint32(1)<<libc.Int32FromInt32(31)-libc.Uint32FromInt32(32768)) + uint64(32768))
	if params.FdictID != 0 {
		v2 = params.FdictID
	} else {
		v2 = compliantID
	}
	dictID = v2
	MEM_writeLE32(tls, dictBuffer+uintptr(4), dictID)
	if hSize+dictContentSize < dictBufferCapacity {
		libc.Xmemmove(tls, dictBuffer+uintptr(hSize), dictBuffer+uintptr(dictBufferCapacity)-uintptr(dictContentSize), dictContentSize)
	}
	if dictBufferCapacity < hSize+dictContentSize {
		v3 = dictBufferCapacity
	} else {
		v3 = hSize + dictContentSize
	}
	return v3
}

// C documentation
//
//	/*! ZDICT_trainFromBuffer_unsafe_legacy() :
//	*   Warning : `samplesBuffer` must be followed by noisy guard band !!!
//	*   @return : size of dictionary, or an error code which can be tested with ZDICT_isError()
//	*/
func ZDICT_trainFromBuffer_unsafe_legacy(tls *libc.TLS, dictBuffer uintptr, maxDictSize size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_legacy_params_t) (r size_t) {
	bp := tls.Alloc(48)
	defer tls.Free(48)
	var currentSize, dictListSize, l, max, n, notificationLevel, printedLength, u1 U32
	var dictContentSize, dictContentSize1, length, minRep, nb, pos, proposedSelectivity, selectivity, u, v1, v2, v3, v4, v5 uint32
	var dictList, ptr uintptr
	var dictSize, samplesBuffSize, targetDictSize size_t
	_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _ = currentSize, dictContentSize, dictContentSize1, dictList, dictListSize, dictSize, l, length, max, minRep, n, nb, notificationLevel, pos, printedLength, proposedSelectivity, ptr, samplesBuffSize, selectivity, targetDictSize, u, u1, v1, v2, v3, v4, v5
	if libc.Uint32FromInt32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT)) > nbSamples {
		v2 = libc.Uint32FromInt32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT))
	} else {
		v2 = nbSamples
	}
	if v2 > uint32(maxDictSize/libc.Uint64FromInt32(16)) {
		if libc.Uint32FromInt32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT)) > nbSamples {
			v3 = libc.Uint32FromInt32(libc.Int32FromInt32(DICTLISTSIZE_DEFAULT))
		} else {
			v3 = nbSamples
		}
		v1 = v3
	} else {
		v1 = uint32(maxDictSize / libc.Uint64FromInt32(16))
	}
	dictListSize = v1
	dictList = libc.Xmalloc(tls, uint64(dictListSize)*uint64(12))
	if params.FselectivityLevel == uint32(0) {
		v4 = g_selectivity_default
	} else {
		v4 = params.FselectivityLevel
	}
	selectivity = v4
	if selectivity > uint32(30) {
		v5 = uint32(MINRATIO)
	} else {
		v5 = nbSamples >> selectivity
	}
	minRep = v5
	targetDictSize = maxDictSize
	samplesBuffSize = ZDICT_totalSampleSize(tls, samplesSizes, nbSamples)
	dictSize = uint64(0)
	notificationLevel = params.FzParams.FnotificationLevel
	/* checks */
	if !(dictList != 0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	if maxDictSize < uint64(ZDICT_DICTSIZE_MIN) {
		libc.Xfree(tls, dictList)
		return libc.Uint64FromInt32(-int32(ZSTD_error_dstSize_tooSmall))
	} /* requested dictionary size is too small */
	if samplesBuffSize < libc.Uint64FromInt32(libc.Int32FromInt32(ZDICT_CONTENTSIZE_MIN)*libc.Int32FromInt32(MINRATIO)) {
		libc.Xfree(tls, dictList)
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionaryCreation_failed))
	} /* not enough source to create dictionary */
	/* init */
	ZDICT_initDictItem(tls, dictList)
	/* build dictionary */
	ZDICT_trainBuffer_legacy(tls, dictList, dictListSize, samplesBuffer, samplesBuffSize, samplesSizes, nbSamples, minRep, notificationLevel)
	/* display best matches */
	if params.FzParams.FnotificationLevel >= uint32(3) {
		if libc.Uint32FromInt32(libc.Int32FromInt32(25)) < (*(*dictItem)(unsafe.Pointer(dictList))).Fpos {
			v1 = libc.Uint32FromInt32(libc.Int32FromInt32(25))
		} else {
			v1 = (*(*dictItem)(unsafe.Pointer(dictList))).Fpos
		}
		nb = v1
		dictContentSize = ZDICT_dictSize(tls, dictList)
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11283, libc.VaList(bp+8, (*(*dictItem)(unsafe.Pointer(dictList))).Fpos-uint32(1), dictContentSize))
			libc.Xfflush(tls, libc.Xstderr)
		}
		if notificationLevel >= uint32(3) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11323, libc.VaList(bp+8, nb-uint32(1)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		u = uint32(1)
		for {
			if !(u < nb) {
				break
			}
			pos = (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Fpos
			length = (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Flength
			if libc.Uint32FromInt32(libc.Int32FromInt32(40)) < length {
				v1 = libc.Uint32FromInt32(libc.Int32FromInt32(40))
			} else {
				v1 = length
			}
			printedLength = v1
			if uint64(pos) > samplesBuffSize || uint64(pos+length) > samplesBuffSize {
				libc.Xfree(tls, dictList)
				return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC)) /* should never happen */
			}
			if notificationLevel >= uint32(3) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11347, libc.VaList(bp+8, u, length, pos, (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u)*12))).Fsavings))
				libc.Xfflush(tls, libc.Xstderr)
			}
			ZDICT_printHex(tls, samplesBuffer+uintptr(pos), uint64(printedLength))
			if notificationLevel >= uint32(3) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11393, 0)
				libc.Xfflush(tls, libc.Xstderr)
			}
			goto _7
		_7:
			;
			u = u + 1
		}
	}
	/* create dictionary */
	dictContentSize1 = ZDICT_dictSize(tls, dictList)
	if dictContentSize1 < uint32(ZDICT_CONTENTSIZE_MIN) {
		libc.Xfree(tls, dictList)
		return libc.Uint64FromInt32(-int32(ZSTD_error_dictionaryCreation_failed))
	} /* dictionary content too small */
	if uint64(dictContentSize1) < targetDictSize/uint64(4) {
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11397, libc.VaList(bp+8, dictContentSize1, uint32(maxDictSize)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		if samplesBuffSize < uint64(10)*targetDictSize {
			if notificationLevel >= uint32(2) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11476, libc.VaList(bp+8, uint32(samplesBuffSize>>libc.Int32FromInt32(20))))
				libc.Xfflush(tls, libc.Xstderr)
			}
		}
		if minRep > uint32(MINRATIO) {
			if notificationLevel >= uint32(2) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11543, libc.VaList(bp+8, selectivity+uint32(1)))
				libc.Xfflush(tls, libc.Xstderr)
			}
			if notificationLevel >= uint32(2) {
				libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11616, 0)
				libc.Xfflush(tls, libc.Xstderr)
			}
		}
	}
	if uint64(dictContentSize1) > targetDictSize*uint64(3) && nbSamples > libc.Uint32FromInt32(libc.Int32FromInt32(2)*libc.Int32FromInt32(MINRATIO)) && selectivity > uint32(1) {
		proposedSelectivity = selectivity - uint32(1)
		for nbSamples>>proposedSelectivity <= uint32(MINRATIO) {
			proposedSelectivity = proposedSelectivity - 1
		}
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11707, libc.VaList(bp+8, dictContentSize1, uint32(maxDictSize)))
			libc.Xfflush(tls, libc.Xstderr)
		}
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11787, libc.VaList(bp+8, proposedSelectivity))
			libc.Xfflush(tls, libc.Xstderr)
		}
		if notificationLevel >= uint32(2) {
			libc.Xfprintf(tls, libc.Xstderr, __ccgo_ts+11865, 0)
			libc.Xfflush(tls, libc.Xstderr)
		}
	}
	/* limit dictionary size */
	max = (*dictItem)(unsafe.Pointer(dictList)).Fpos /* convention : nb of useful elts within dictList */
	currentSize = uint32(0)
	n = uint32(1)
	for {
		if !(n < max) {
			break
		}
		currentSize = currentSize + (*(*dictItem)(unsafe.Pointer(dictList + uintptr(n)*12))).Flength
		if uint64(currentSize) > targetDictSize {
			currentSize = currentSize - (*(*dictItem)(unsafe.Pointer(dictList + uintptr(n)*12))).Flength
			break
		}
		goto _9
	_9:
		;
		n = n + 1
	}
	(*dictItem)(unsafe.Pointer(dictList)).Fpos = n
	dictContentSize1 = currentSize
	/* build dict content */
	ptr = dictBuffer + uintptr(maxDictSize)
	u1 = uint32(1)
	for {
		if !(u1 < (*dictItem)(unsafe.Pointer(dictList)).Fpos) {
			break
		}
		l = (*(*dictItem)(unsafe.Pointer(dictList + uintptr(u1)*12))).Flength
		ptr = ptr - uintptr(l)
		if ptr < dictBuffer {
			libc.Xfree(tls, dictList)
			return libc.Uint64FromInt32(-int32(ZSTD_error_GENERIC))
		} /* should not happen */
		libc.Xmemcpy(tls, ptr, samplesBuffer+uintptr((*(*dictItem)(unsafe.Pointer(dictList + uintptr(u1)*12))).Fpos), uint64(l))
		goto _10
	_10:
		;
		u1 = u1 + 1
	}
	dictSize = ZDICT_addEntropyTablesFromBuffer_advanced(tls, dictBuffer, uint64(dictContentSize1), maxDictSize, samplesBuffer, samplesSizes, nbSamples, params.FzParams)
	/* clean up */
	libc.Xfree(tls, dictList)
	return dictSize
}

// C documentation
//
//	/* ZDICT_trainFromBuffer_legacy() :
//	 * issue : samplesBuffer need to be followed by a noisy guard band.
//	 * work around : duplicate the buffer, and add the noise */
func ZDICT_trainFromBuffer_legacy(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32, params ZDICT_legacy_params_t) (r size_t) {
	var newBuff uintptr
	var result, sBuffSize size_t
	_, _, _ = newBuff, result, sBuffSize
	sBuffSize = ZDICT_totalSampleSize(tls, samplesSizes, nbSamples)
	if sBuffSize < libc.Uint64FromInt32(libc.Int32FromInt32(ZDICT_CONTENTSIZE_MIN)*libc.Int32FromInt32(MINRATIO)) {
		return uint64(0)
	} /* not enough content => no dictionary */
	newBuff = libc.Xmalloc(tls, sBuffSize+uint64(NOISELENGTH))
	if !(newBuff != 0) {
		return libc.Uint64FromInt32(-int32(ZSTD_error_memory_allocation))
	}
	libc.Xmemcpy(tls, newBuff, samplesBuffer, sBuffSize)
	ZDICT_fillNoise(tls, newBuff+uintptr(sBuffSize), uint64(NOISELENGTH)) /* guard band, for end of buffer condition */
	result = ZDICT_trainFromBuffer_unsafe_legacy(tls, dictBuffer, dictBufferCapacity, newBuff, samplesSizes, nbSamples, params)
	libc.Xfree(tls, newBuff)
	return result
}

func ZDICT_trainFromBuffer(tls *libc.TLS, dictBuffer uintptr, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32) (r size_t) {
	bp := tls.Alloc(64)
	defer tls.Free(64)
	var _ /* params at bp+0 */ ZDICT_fastCover_params_t
	libc.Xmemset(tls, bp, 0, uint64(56))
	(*(*ZDICT_fastCover_params_t)(unsafe.Pointer(bp))).Fd = uint32(8)
	(*(*ZDICT_fastCover_params_t)(unsafe.Pointer(bp))).Fsteps = uint32(4)
	/* Use default level since no compression level information is available */
	(*(*ZDICT_fastCover_params_t)(unsafe.Pointer(bp))).FzParams.FcompressionLevel = int32(ZSTD_CLEVEL_DEFAULT)
	return ZDICT_optimizeTrainFromBuffer_fastCover(tls, dictBuffer, dictBufferCapacity, samplesBuffer, samplesSizes, nbSamples, bp)
}

func ZDICT_addEntropyTablesFromBuffer(tls *libc.TLS, dictBuffer uintptr, dictContentSize size_t, dictBufferCapacity size_t, samplesBuffer uintptr, samplesSizes uintptr, nbSamples uint32) (r size_t) {
	bp := tls.Alloc(16)
	defer tls.Free(16)
	var _ /* params at bp+0 */ ZDICT_params_t
	libc.Xmemset(tls, bp, 0, uint64(12))
	return ZDICT_addEntropyTablesFromBuffer_advanced(tls, dictBuffer, dictContentSize, dictBufferCapacity, samplesBuffer, samplesSizes, nbSamples, *(*ZDICT_params_t)(unsafe.Pointer(bp)))
}

func __ccgo_fp(f interface{}) uintptr {
	type iface [2]uintptr
	return (*iface)(unsafe.Pointer(&f))[1]
}

/**** ended inlining threading.h ****/

// C documentation
//
//	/* create fake symbol to avoid empty translation unit warning */
var g_ZSTD_threading_useless_symbol int32

/*
 * Provides 64-bit math support.
 * Need:
 * U64 ZSTD_div64(U64 dividend, U32 divisor)
 */

/* Need:
 * assert()
 */

/* Need:
 * ZSTD_DEBUG_PRINT()
 */

/* Only requested when <stdint.h> is known to be present.
 * Need:
 * intptr_t
 */
/**** ended inlining common/zstd_deps.h ****/

/**** start inlining common/debug.c ****/
/* ******************************************************************
 * debug
 * Part of FSE library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * You can contact the author at :
 * - Source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/*
 * This module only hosts one global variable
 * which can be used to dynamically influence the verbosity of traces,
 * such as DEBUGLOG and RAWLOG
 */

/**** start inlining debug.h ****/
/* ******************************************************************
 * debug
 * Part of FSE library
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * You can contact the author at :
 * - Source repository : https://github.com/Cyan4973/FiniteStateEntropy
 *
 * This source code is licensed under both the BSD-style license (found in the
 * LICENSE file in the root directory of this source tree) and the GPLv2 (found
 * in the COPYING file in the root directory of this source tree).
 * You may select, at your option, one of the above-listed licenses.
****************************************************************** */

/*
 * The purpose of this header is to enable debug functions.
 * They regroup assert(), DEBUGLOG() and RAWLOG() for run-time,
 * and DEBUG_STATIC_ASSERT() for compile-time.
 *
 * By default, DEBUGLEVEL==0, which means run-time debug is disabled.
 *
 * Level 1 enables assert() only.
 * Starting level 2, traces can be generated and pushed to stderr.
 * The higher the level, the more verbose the traces.
 *
 * It's possible to dynamically adjust level using variable g_debug_level,
 * which is only declared if DEBUGLEVEL>=2,
 * and is a global variable, not multi-thread protected (use with care)
 */

/* static assert is triggered at compile time, leaving no runtime artefact.
 * static assert only works with compile-time constants.
 * Also, this variant can only be used inside a function. */

/* DEBUGLEVEL is expected to be defined externally,
 * typically through compiler command line.
 * Value must be a number. */

/* recommended values for DEBUGLEVEL :
 * 0 : release mode, no debug, all run-time checks disabled
 * 1 : enables assert() only, no display
 * 2 : reserved, for currently active debug path
 * 3 : events once per object lifetime (CCtx, CDict, etc.)
 * 4 : events once per frame
 * 5 : events once per block
 * 6 : events once per sequence (verbose)
 * 7+: events at every position (*very* verbose)
 *
 * It's generally inconvenient to output traces > 5.
 * In which case, it's possible to selectively trigger high verbosity levels
 * by modifying g_debug_level.
 */

/**** ended inlining debug.h ****/

// C documentation
//
//	/* We only use this when DEBUGLEVEL>=2, but we get -Werror=pedantic errors if a
//	 * translation unit is empty. So remove this from Linux kernel builds, but
//	 * otherwise just leave it in.
//	 */
var g_debuglevel int32

var __ccgo_ts = (*reflect.StringHeader)(unsafe.Pointer(&__ccgo_ts1)).Data

var __ccgo_ts1 = "Unspecified error code\x00No error detected\x00Error (generic)\x00Unknown frame descriptor\x00Version not supported\x00Unsupported frame parameter\x00Frame requires too much memory for decoding\x00Data corruption detected\x00Restored data doesn't match checksum\x00Header of Literals' block doesn't respect format specification\x00Unsupported parameter\x00Unsupported combination of parameters\x00Parameter is out of bound\x00Context should be init first\x00Allocation error : not enough memory\x00workSpace buffer is not large enough\x00Operation not authorized at current processing stage\x00tableLog requires too much memory : unsupported\x00Unsupported max Symbol Value : too large\x00Specified maxSymbolValue is too small\x00This mode cannot generate an uncompressed block\x00pledged buffer stability condition is not respected\x00Dictionary is corrupted\x00Dictionary mismatch\x00Cannot create Dictionary from provided samples\x00Destination buffer is too small\x00Src size is incorrect\x00Operation on NULL destination buffer\x00Operation made no progress over multiple calls, due to output buffer being full\x00Operation made no progress over multiple calls, due to input being empty\x00Frame index is too large\x00An I/O error occurred when reading/seeking\x00Destination buffer is wrong\x00Source buffer is wrong\x00Block-level external sequence producer returned an error code\x00External sequences are not valid\x00\x001.5.7\x00table phase - alignment initial allocation failed!\x00NULL pointer!\x00dst buf too small for uncompressed block\x00 \x00not enough space for compression\x00not enough space\x00FSE_normalizeCount failed\x00FSE_writeNCount failed\x00FSE_buildCTable_wksp failed\x00impossible to reach\x00not enough space remaining\x00ZSTD_encodeSequences failed\x00ZSTD_compressSubBlock_literal failed\x00ZSTD_compressSubBlock_sequences failed\x00ZSTD_compressSubBlock failed\x00ZSTD_noCompressBlock failed\x00not compatible with static CCtx\x00can only set params in cctx init stage\x00MT not compatible with static alloc\x00unknown parameter\x00Param out of bounds\x00The context is in the wrong stage!\x00Can't override parameters with cdict attached (some must be inherited from the cdict).\x00Can't set pledgedSrcSize when not in init stage.\x00ZSTD_createCDict_advanced failed\x00Can't load a dictionary when cctx is not in init stage.\x00static CCtx can't allocate for an internal copy of dictionary\x00allocation failed for dictionary content\x00Can't ref a dict when ctx not in init stage.\x00Can't ref a pool when ctx not in init stage.\x00Can't ref a prefix when ctx not in init stage.\x00Reset parameters is only possible during init stage.\x00Estimate CCtx size is supported for single-threaded compression only.\x00failed a workspace allocation in ZSTD_reset_matchState\x00cctx size estimate failed!\x00static cctx : no resize\x00couldn't allocate prevCBlock\x00couldn't allocate nextCBlock\x00couldn't allocate tmpWorkspace\x00Can't copy a ctx that's not in init stage.\x00ZSTD_compressLiterals failed\x00Can't fit seq hdr in output buf!\x00ZSTD_buildSequencesStatistics failed!\x00ZSTD_entropyCompressSeqStore_internal failed\x00External sequence producer returned error code %lu\x00Got zero sequences from external sequence producer for a non-empty src buffer!\x00nbExternalSeqs == outSeqsCapacity but lastSeq is not a block delimiter!\x00Long-distance matching with external sequence producer enabled is not currently supported.\x00External sequences imply too large a block!\x00Failed to copy external sequences to seqStore!\x00Not enough space to copy sequences\x00targetCBlockSize != 0\x00nbWorkers != 0\x00ZSTD_compress2 failed\x00HIST_count_wksp failed\x00HUF_buildCTable_wksp\x00ZSTD_buildBlockEntropyStats_literals failed\x00ZSTD_buildBlockEntropyStats_sequences failed\x00Block header doesn't fit\x00ZSTD_entropyCompressSeqStore failed!\x00copyBlockSequences failed\x00Nocompress block failed\x00RLE compress block failed\x00Compressing single block from splitBlock_internal() failed!\x00Compressing chunk failed!\x00ZSTD_buildSeqStore failed\x00Uncompressible block\x00Splitting blocks failed!\x00ZSTD_compressSuperBlock failed\x00ZSTD_compressBlock_targetCBlockSize_body failed\x00not enough space to store compressed block\x00ZSTD_compressBlock_targetCBlockSize failed\x00ZSTD_compressBlock_splitBlock failed\x00ZSTD_compressBlock_internal failed\x00dst buf is too small to fit worst-case frame header size.\x00Not enough room for skippable frame\x00Src size too large for skippable frame\x00Skippable frame magic number variant not supported\x00dst buf is too small to write frame trailer empty block.\x00missing init (ZSTD_compressBegin)\x00ZSTD_writeFrameHeader failed\x00ZSTD_compress_frameChunk failed\x00%s\x00error : pledgedSrcSize = %u, while realSrcSize >= %u\x00input is larger than a block\x00ZSTD_loadCEntropy failed\x00ZSTD_compress_insertDictionary failed\x00init missing\x00no room for epilogue\x00no room for checksum\x00ZSTD_compressContinue_internal failed\x00ZSTD_writeEpilogue failed\x00error : pledgedSrcSize = %u, while realSrcSize = %u\x00call ZSTD_initCStream() first!\x00ZSTD_compressEnd failed\x00ZSTD_compressContinue failed\x00ZSTD_c_stableInBuffer enabled but input differs!\x00ZSTD_c_stableOutBuffer enabled but output size differs!\x00External sequence producer isn't supported with nbWorkers >= 1\x00invalid output buffer\x00invalid input buffer\x00invalid endDirective\x00stableInBuffer condition not respected: wrong src pointer\x00stableInBuffer condition not respected: externally modified pos\x00compressStream2 initialization failed\x00invalid buffers\x00ZSTDMT_compressStream_generic failed\x00ZSTD_compressStream2_simpleArgs failed\x00Offset too large!\x00Matchlength too small for the minMatch\x00Sequence validation failed\x00Not enough memory allocated. Try adjusting ZSTD_c_minMatch.\x00Block delimiter not found.\x00Blocksize doesn't agree with block delimiter!\x00delimiter format error : both matchlength and offset must be == 0\x00Reached end of sequences without finding a block delimiter\x00Error while determining block size with explicit delimiters\x00sequences incorrectly define a too large block\x00sequences define a frame longer than source\x00No room for empty frame block header\x00Error while trying to determine block size\x00Bad sequence copy\x00not enough dstCapacity to write a new compressed block\x00Compressing sequences of block failed\x00ZSTD_rleCompressBlock failed\x00CCtx initialization failed\x00Compressing blocks failed!\x00Requires at least 1 end-of-block\x00Error while trying to determine nb of sequences for a block\x00discrepancy: Sequences require more literals than present in buffer\x00Bad sequence conversion\x00ZSTD_compressSequencesAndLiterals cannot generate an uncompressed block\x00literals must be entirely and exactly consumed\x00Sequences must represent a total of exactly srcSize=%zu\x00literals buffer is not large enough: must be at least 8 bytes larger than litSize (risk of read out-of-bound)\x00This mode is only compatible with explicit delimiters\x00This mode is not compatible with Sequence validation\x00this mode is not compatible with frame checksum\x00ZSTD_compressStream2(,,ZSTD_e_end) failed\x00Failed to init fast loop args\x00corruption\x00Failed to init asm args\x00Hash set is full!\x00Expanded hashset allocation failed!\x00not compatible with static DCtx\x00invalid parameter : src==NULL, but srcSize>0\x00first bytes don't correspond to any supported magic number\x00reserved bits, must be zero\x00headerSize too small\x00invalid block type\x00Block decompression failure\x00invalid skippable frame\x00At least one frame successfully completed, but following bytes are garbage: it's more likely to be a srcSize error, specifying more input bytes than size of frame(s). Note: one could be unlucky, it might be a corruption error instead, happening right at the place where we expect zstd magic bytes. But this is _much_ less likely than a srcSize field error.\x00input not entirely consumed\x00not allowed\x00Block Size Exceeds Maximum\x00ZSTD_copyRawBlock failed\x00Decompressed Block Size Exceeds Maximum\x00dict is too small\x00Failed to allocate memory for hash set!\x00Static dctx does not support multiple DDicts!\x00ZSTD_d_stableOutBuffer enabled but output differs!\x00forbidden. in: pos: %u   vs size: %u\x00forbidden. out: pos: %u   vs size: %u\x00First few bytes detected incorrect\x00ZSTD_obm_stable passed but ZSTD_outBuffer is too small\x00should never happen\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need up to 5 for case 3\x00NULL not handled\x00Not enough literals (%zu) for the 4-streams mode (min %u)\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need lhSize = 3\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need lhSize+1 = 3\x00srcSize >= MIN_CBLOCK_SIZE == 2; here we need lhSize+1 = 4\x00impossible\x00extraneous data present in the Sequences section\x00ZSTD_buildSeqTable failed\x00last match must fit within dstBuffer\x00try to read beyond literal buffer\x00output should not catch up to and overwrite literal buffer\x00remaining lit must fit within dstBuffer\x00invalid dst\x00Total samples size is too large (%u MB), maximum size is %u MB\n\x00Total number of training samples is %u and is invalid.\x00Total number of testing samples is %u and is invalid.\x00Training on %u samples of total size %u\n\x00Testing on %u samples of total size %u\n\x00Failed to allocate scratch buffers\n\x00Constructing partial suffix array\n\x00Computing frequencies\n\x00WARNING: The maximum dictionary size %u is too large compared to the source size %u! size(source)/size(dictionary) = %f, but it should be >= 10! This may lead to a subpar dictionary! We recommend training on sources at least 10x, and preferably 100x the size of the dictionary! \n\x00Breaking content into %u epochs of size %u\n\x00\r%u%%       \x00\r%79s\r\x00Cover parameters incorrect\n\x00Cover must have at least one input file\n\x00dictBufferCapacity must be at least %u\n\x00Failed to allocate dmer map: out of memory\n\x00Building dictionary\n\x00Constructed dictionary of size %u\n\x00Failed to allocate buffers: out of memory\n\x00Failed to select dictionary\n\x00Incorrect parameters\n\x00Trying %u different sets of parameters\n\x00d=%u\n\x00Failed to initialize context\n\x00k=%u\n\x00Failed to allocate parameters\n\x000 <= ssize\x00/tmp/zstd-build-3522643984/zstd.c\x00ssize < STACK_SIZE\x00T[s] == c1\x00((s + 1) < n) && (T[s] <= T[s + 1])\x00T[s - 1] <= T[s]\x00k < j\x00k != NULL\x00((s == 0) && (T[s] == c1)) || (s < 0)\x00T[s - 1] >= T[s]\x00i < k\x00s < 0\x00ctx->nbTrainSamples >= 5\x00ctx->nbTrainSamples <= ctx->nbSamples\x00Total number of training samples is %u and is invalid\n\x00Total number of testing samples is %u and is invalid.\n\x00Failed to allocate scratch buffers \n\x00nbSamples >= 5\x00Failed to allocate frequency table \n\x00FASTCOVER parameters incorrect\n\x00FASTCOVER must have at least one input file\n\x00Incorrect splitPoint\n\x00Incorrect accel\n\x00Incorrect k\n\x00%c\x00\n\x00found %3u matches of length >= %i at pos %7u  \x00Selected dict at position %u, of length %u : saves %u (ratio: %.2f)  \n\x00\r%70s\r\x00sample set too large : reduced to %u MB ...\n\x00sorting %u files of total size %u MB ...\n\x00finding patterns ... \n\x00minimum ratio : %u \n\x00\r%4.2f %% \r\x00warning : ZSTD_compressBegin_usingCDict failed \n\x00warning : could not compress sample size %u \n\x00Not enough memory \n\x00Offset Code Frequencies : \n\x00%2u :%7u \n\x00 HUF_buildCTable error \n\x00warning : pathological dataset : literals are not compressible : samples are noisy or too regular \n\x00maxNbBits==9\x00FSE_normalizeCount error with offcodeCount \n\x00FSE_normalizeCount error with matchLengthCount \n\x00FSE_normalizeCount error with litLengthCount \n\x00HUF_writeCTable error \n\x00FSE_writeNCount error with offcodeNCount \n\x00FSE_writeNCount error with matchLengthNCount \n\x00FSE_writeNCount error with litlengthNCount \n\x00not enough space to write RepOffsets \n\x00statistics ... \n\x00dictBufferCapacity too small to fit max repcode\x00dictSize <= dictBufferCapacity\x00outDictContent + dictContentSize == (BYTE*)dictBuffer + dictSize\x00\n %u segments found, of total size %u \n\x00list %u best segments \n\x00%3u:%3u bytes at pos %8u, savings %7u bytes |\x00| \n\x00!  warning : selected content significantly smaller than requested (%u < %u) \n\x00!  consider increasing the number of samples (total size : %u MB)\n\x00!  consider increasing selectivity to produce larger dictionary (-s%u) \n\x00!  note : larger dictionaries are not necessarily better, test its efficiency on samples \n\x00!  note : calculated dictionary significantly larger than requested (%u > %u) \n\x00!  consider increasing dictionary size, or produce denser dictionary (-s%u) \n\x00!  always test dictionary efficiency on real samples \n\x00"
